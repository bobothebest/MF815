{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsWC7cdyZIaA",
        "outputId": "3096fcaf-0743-40a2-fec1-df2693af3b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow>=2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (3.20.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.15.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.2.2)\n",
            "Mounted at /content/gdrive\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive'\n",
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install \"tensorflow>=2.15.0\"\n",
        "!pip install --upgrade tensorflow-hub\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/drive/MyDrive\n",
        "# Import the libraries\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import ceil\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import collections\n",
        "import random\n",
        "import time\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, Dropout, LSTM, Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up"
      ],
      "metadata": {
        "id": "dzt2flDgZjNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We now set the directory to access the data\n",
        "def find(name, path):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        if name in files:\n",
        "            return os.path.join(root, name)\n",
        "\n",
        "# The folder with the data and this script should be saved in your drive.\n",
        "script_name = 'CourseWork_v2.ipynb'\n",
        "script_path = find(script_name, '/content/gdrive/My Drive')\n",
        "#DIRECTORY = '.'#os.path.dirname(script_path)\n",
        "# If your Drive is too large and the \"find\" function takes to much time, you can set the directory manually :\n",
        "\n",
        "#SUMMARY_PATH = '/content/drive/MyDrive/MutualFundSummary'\n",
        "#SUMMARY_LABELS_PATH = '/content/drive/MyDrive/MutualFundLabels.csv'\n",
        "\n",
        "DIRECTORY = '/content/gdrive/MyDrive/Colab Notebooks/NLP_app'\n",
        "\n",
        "SUMMARY_PATH = '/content/gdrive/MyDrive/Colab Notebooks/NLP_app/MutualFundSummary'\n",
        "SUMMARY_LABELS_PATH = '/content/gdrive/MyDrive/Colab Notebooks/MF815/NLP/NLP_app/MutualFundLabels.csv'\n",
        "\n",
        "glove_word2vec = 'glove.6B.50d.txt'\n",
        "our_word2vec = 'word2vec_perso.txt'\n",
        "\n",
        "# Progress bar\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "# Save a word2vec dictionary.\n",
        "def save_word2vec(filename):\n",
        "    with open(os.path.join('/content/drive/MyDrive', filename),'a' , encoding='utf-8') as f :\n",
        "        for k, v in word2vec.items():\n",
        "            line = k+' '+str(list(v)).strip('[]').replace(',','')+'\\n'\n",
        "            f.write(line)\n",
        "\n",
        "# Load a word2vec dictionary.\n",
        "def load_word2vec(filename):\n",
        "    word2vec = {}\n",
        "    with open(os.path.join('/content/drive/MyDrive', filename), encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            try :\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vec = np.asarray(values[1:], dtype='float32')\n",
        "                word2vec[word] = vec\n",
        "            except :\n",
        "                None\n",
        "    return word2vec\n",
        "\n",
        "# read the repo in PATH and append the texts in a list\n",
        "def get_data(PATH):\n",
        "    list_dir = os.listdir(PATH)\n",
        "    texts = []\n",
        "    fund_names = []\n",
        "    out = display(progress(0, len(list_dir)-1), display_id=True)\n",
        "    for ii, filename in enumerate(list_dir) :\n",
        "        with open(PATH+'/'+filename, 'r', encoding=\"utf8\") as f :\n",
        "            txt = f.read()\n",
        "            try :\n",
        "                txt_split = txt.split('<head_breaker>')\n",
        "                summary = txt_split[1].strip()\n",
        "                fund_name = txt_split[0].strip()\n",
        "            except :\n",
        "                summary = txt\n",
        "                fund_name = ''\n",
        "        texts.append(summary)\n",
        "        fund_names.append(fund_name)\n",
        "        out.update(progress(ii, len(list_dir)-1))\n",
        "    return fund_names, texts"
      ],
      "metadata": {
        "id": "gM6VrA9iZpgV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "0n98w1vUZ1ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "#labels = pd.read_csv('/content/MutualFundLabels.csv')\n",
        "labels = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/MutualFundLabels.csv')\n",
        "labels = labels.drop('Performance fee?', axis=1)\n",
        "removes = labels.loc[(labels['Ivestment Strategy']=='Long Short Funds (High Risk)')| (labels['Ivestment Strategy']=='Commodities Fund (Low Risk)')]\n",
        "labels_clean = labels.drop(removes.index)\n",
        "\n",
        "fund_names, summaries = get_data(SUMMARY_PATH)\n",
        "cleaned_fund_names = labels_clean['fund_name'].tolist()\n",
        "fund_name_counts = {name: 0 for name in fund_names}\n",
        "\n",
        "for name in fund_names:\n",
        "    if name in cleaned_fund_names:\n",
        "        fund_name_counts[name] += 1\n",
        "\n",
        "single_occurrences = {name: count for name, count in fund_name_counts.items() if count == 1}\n",
        "\n",
        "print(f\"Number of matching fund names that appear exactly once: {len(single_occurrences)}\")\n",
        "\n",
        "multiple_occurrences = {name: count for name, count in fund_name_counts.items() if count > 1}\n",
        "\n",
        "if multiple_occurrences:\n",
        "    print(f\"There are fund names that appear more than once:\")\n",
        "    for name, count in multiple_occurrences.items():\n",
        "        print(f\"{name}: {count} times\")\n",
        "else:\n",
        "    print(\"No fund names appear more than once.\")\n",
        "\n",
        "labels_clean_filtered = labels_clean[labels_clean['fund_name'].isin(fund_names)]\n",
        "\n",
        "df_summaries = pd.DataFrame(data={'fund_name':fund_names, 'summary':summaries})\n",
        "\n",
        "merge = labels_clean_filtered.merge(df_summaries, on=['fund_name'], how='left')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "MBP4j7DwZ2nM",
        "outputId": "16894f9e-aeec-4f45-8d87-38791e6eb1ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='544'\n",
              "            max='544',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            544\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of matching fund names that appear exactly once: 461\n",
            "No fund names appear more than once.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, X_test, y, y_test = train_test_split(merge['summary'], merge['Ivestment Strategy'], test_size=0.2, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state=42)"
      ],
      "metadata": {
        "id": "sjvq7XU-adEu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\")+list(string.punctuation)+['``',\"''\"]+[\"]\",\"[\",\"*\"]+['doe', 'ha', 'wa'])\n",
        "\n",
        "# clean and tokenize without lemmatizing\n",
        "def tokenizer(txt):\n",
        "    txt = txt.replace('\\n', ' ').replace('\\t', ' ').lower()\n",
        "    word_tokens = word_tokenize(txt)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_sentence = [w for w in filtered_sentence if re.sub(\"[^A-Za-z ]+\",'',w) != '']\n",
        "    return filtered_sentence\n",
        "\n",
        "train_text_words = np.concatenate([tokenizer(summary) for summary in X_train])\n",
        "\n",
        "train_text_words[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60UUVD-Kayaz",
        "outputId": "ca058955-0a7a-464d-bfc9-15bb4851fd98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['mainstay', 'vp', 'epoch', 'u.s.', 'small', 'cap', 'portfolio',\n",
              "       'investment', 'objective', 'portfolio', 'seeks', 'long-term',\n",
              "       'capital', 'appreciation', 'investing', 'primarily', 'securities',\n",
              "       'small-cap', 'companies', 'fees'], dtype='<U44')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-gram Model"
      ],
      "metadata": {
        "id": "GWV983HjbASj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "batch_size = 128\n",
        "num_epochs = 2\n",
        "\n",
        "# Word2vec parameters\n",
        "embedding_size = 50\n",
        "max_vocabulary_size = 5000\n",
        "min_occurrence = 10\n",
        "skip_window = 3\n",
        "num_skips = 4\n",
        "\n",
        "count = [('UNK', -1)]\n",
        "count.extend(collections.Counter(train_text_words).most_common(max_vocabulary_size - 1))\n",
        "# Remove samples with less than 'min_occurrence' occurrences\n",
        "for i in range(len(count) - 1, -1, -1):\n",
        "    if count[i][1] < min_occurrence:\n",
        "        count.pop(i)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "word2id = dict()\n",
        "for i, (word, _)in enumerate(count):\n",
        "    word2id[word] = i\n",
        "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
        "vocab_size = len(id2word)\n",
        "\n",
        "data = list()\n",
        "unk_count = 0\n",
        "for word in train_text_words:\n",
        "    # Retrieve a word id, or assign it index 0 ('UNK') if not in dictionary\n",
        "    index = word2id.get(word, 0)\n",
        "    if index == 0:\n",
        "        unk_count += 1\n",
        "    data.append(index)\n",
        "count[0] = ('UNK', unk_count)\n",
        "\n",
        "# build OneHot vector from index\n",
        "def to_one_hot(data_point_index, vocab_size):\n",
        "    temp = np.zeros(vocab_size)\n",
        "    temp[data_point_index] = 1\n",
        "    return temp\n",
        "\n",
        "# Generate training batch for the skip-gram model\n",
        "def batch_generator(batch_size, num_skips, skip_window, vocab_size):\n",
        "    data_index = 0\n",
        "    while True :\n",
        "        assert batch_size % num_skips == 0\n",
        "        assert num_skips <= 2 * skip_window\n",
        "        # batch is filled with 128 inputs\n",
        "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "        # labels is filled with 128 outputs\n",
        "        labels = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "        span = 2 * skip_window + 1\n",
        "        # buffer keep track of the visited indexes visited\n",
        "        buffer = collections.deque(maxlen=span)\n",
        "        if data_index + span > len(data):\n",
        "            data_index = 0\n",
        "            # We stop the loop when we went through all the corpus\n",
        "            break\n",
        "        buffer.extend(data[data_index:data_index + span])\n",
        "        data_index += span\n",
        "        for i in range(batch_size // num_skips):\n",
        "            # Take the context current word\n",
        "            context_words = [w for w in range(span) if w != skip_window]\n",
        "            # Randomly select num_skips words in the context\n",
        "            words_to_use = random.sample(context_words, num_skips)\n",
        "            for j, context_word in enumerate(words_to_use):\n",
        "                # Creates one raw data\n",
        "                batch[i * num_skips + j] = buffer[skip_window]\n",
        "                labels[i * num_skips + j] = buffer[context_word]\n",
        "            if data_index == len(data):\n",
        "                buffer.extend(data[0:span])\n",
        "                data_index = span\n",
        "            else:\n",
        "                buffer.append(data[data_index])\n",
        "                data_index += 1\n",
        "        # Backtrack a little bit to avoid skipping words in the end of a batch\n",
        "        data_index = (data_index + len(data) - span) % len(data)\n",
        "\n",
        "        # translate word index to on-hot representation\n",
        "        batch_one_hot = np.array([to_one_hot(b, vocab_size) for b in batch])\n",
        "        labels_one_hot = np.array([to_one_hot(l, vocab_size) for l in labels])\n",
        "\n",
        "        # output one batch\n",
        "        yield batch_one_hot, labels_one_hot"
      ],
      "metadata": {
        "id": "V2LJo_H2bDIY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create en compile the Autoencoder\n",
        "def creat_word2vec_model():\n",
        "    input_word = Input(shape=(vocab_size,))\n",
        "\n",
        "    encoded = Dense(embedding_size, activation='linear')(input_word)\n",
        "    decoded = Dense(vocab_size, activation='softmax')(encoded)\n",
        "\n",
        "    autoencoder = Model(input_word, decoded)\n",
        "    encoder = Model(input_word, encoded)\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return encoder, autoencoder\n",
        "\n",
        "encoder, autoencoder = creat_word2vec_model()\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuIv1F86bDLm",
        "outputId": "3474de47-64a3-4b05-a430-86ffb29515f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 2821)]            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 50)                141100    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2821)              143871    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 284971 (1.09 MB)\n",
            "Trainable params: 284971 (1.09 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit_generator(batch_generator(batch_size, num_skips, skip_window, vocab_size), steps_per_epoch=ceil(len(data) / batch_size), epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaFAcoJYbDQM",
        "outputId": "1ef54886-45ea-462c-d69b-00f6b1fa5297"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-a9705c45700e>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  autoencoder.fit_generator(batch_generator(batch_size, num_skips, skip_window, vocab_size), steps_per_epoch=ceil(len(data) / batch_size), epochs=num_epochs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3990/3990 [==============================] - 19s 4ms/step - loss: 0.0221\n",
            "Epoch 2/2\n",
            "3990/3990 [==============================] - 15s 4ms/step - loss: 0.0027\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d1b2bc27880>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vecotrize(word):\n",
        "    word_one_hot = to_one_hot(word2id[word], vocab_size)\n",
        "    return encoder.predict(np.array([word_one_hot]))[0]\n",
        "\n",
        "word2vec = {w : vecotrize(w) for w in word2id.keys()}\n",
        "save_word2vec('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/train_word2vec')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veIJ897obDSx",
        "outputId": "a77226f5-8de3-466a-99d9-9dab601a6ade"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemma_tokenizer(text):\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text.replace(\"'\",\" \"))]\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\")+list(string.punctuation)+['``',\"''\",\"’\"]+[\"]\",\"[\",\"*\"]+['doe', 'ha', 'wa'] +['--']+ [''])"
      ],
      "metadata": {
        "id": "B8HTJeyTbDUv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 18\n",
        "tfidf = TfidfVectorizer(input='content', tokenizer=lemma_tokenizer, stop_words=list(stop_words), max_features=max_features)\n",
        "tfidf_train = tfidf.fit_transform(X_train)\n",
        "key_words = tfidf.get_feature_names_out() # 常用关键词"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDWBISmobDWt",
        "outputId": "32e38533-02a1-4c5e-f683-d1b121281019"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_n_closer(word, n, word2vec):\n",
        "    vect = word2vec[word]\n",
        "    dist_dict = {k: cosine(v, vect) for k, v in word2vec.items()}\n",
        "    closer_words = []\n",
        "    for _ in range(n):\n",
        "        min_key = min(dist_dict.keys(), key=lambda k: dist_dict[k])\n",
        "        closer_words.append(min_key)\n",
        "        del dist_dict[min_key]\n",
        "    return closer_words\n",
        "\n",
        "##knowledge base\n",
        "def create_knowledge_base(num_neighbors, word2vec, key_words):\n",
        "    knowledge_base = set()\n",
        "    out = display(progress(0, len(key_words)-1), display_id=True)\n",
        "    for ii, key_word in enumerate(key_words) :\n",
        "        knowledge_base.add(key_word)\n",
        "        neighbors = []\n",
        "        try :\n",
        "            neighbors = get_n_closer(key_word, num_neighbors, word2vec)\n",
        "        except :\n",
        "            print(key_word + ' not in word2vec')\n",
        "\n",
        "        knowledge_base.update(neighbors)\n",
        "\n",
        "        out.update(progress(ii, len(key_words)-1))\n",
        "    return knowledge_base\n",
        "\n",
        "knowledge_base = create_knowledge_base(5, word2vec, key_words)\n",
        "print(knowledge_base)\n",
        "\n",
        "# Takes a summary, the knowledge base and some hyper parameters and returns the \"num_sent\" sentences\n",
        "# of the summary that are closer to the the knowledge base in term of spacial distances.\n",
        "def extract_sentence_distance(summary, knowledge, n_closer, n_reject, num_sent):\n",
        "    # Split the summary into sentences.\n",
        "    sentences = sent_tokenize(summary)\n",
        "    sentence_scores = []\n",
        "    # Loop over the sentences.\n",
        "    for j, sentence in enumerate(sentences):\n",
        "        # we tokenize and clean the sentence\n",
        "        tokens = tokenizer(sentence)\n",
        "\n",
        "        sentence_barycentre = np.zeros(embedding_size)\n",
        "        effective_len = 0\n",
        "        # Compute the barycentre of the sentence\n",
        "        for token in tokens :\n",
        "            try :\n",
        "                sentence_barycentre += np.array(word2vec[token])\n",
        "                effective_len += 1\n",
        "            except KeyError :\n",
        "                pass\n",
        "            except :\n",
        "                raise\n",
        "\n",
        "        # Reject sentences with less than n_reject words in our word2vec map\n",
        "        if effective_len <= n_reject :\n",
        "            sentence_scores.append(1)\n",
        "\n",
        "        else :\n",
        "            sentence_barycentre = sentence_barycentre/effective_len\n",
        "            # Compute the distance sentece_barycentre -> words in our knowledge base\n",
        "            barycentre_distance = [cosine(sentence_barycentre, word2vec[key_word]) for key_word in knowledge]\n",
        "            barycentre_distance.sort()\n",
        "            # Create the score of the sentence by averaging the \"n_closer\" smallest distances\n",
        "            score = np.mean(barycentre_distance[:n_closer])\n",
        "            sentence_scores.append(score)\n",
        "    # Select the \"num_sent\" sentences that have the smallest score (smallest distance score with the knowledge base)\n",
        "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
        "    top_sentences = sentences[:num_sent]\n",
        "    return ' '.join(top_sentences)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "bPHfDiS5bDYp",
        "outputId": "a574cb71-b9d8-46b6-82b0-19615b1a24e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='17'\n",
              "            max='17',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            17\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'unable', 'sales', 'faith', 'equity', 'expense', 'correct', 'sumsion', 'healthcare', 'finance', 'involve', 'neutral', 'index', 'share', 'msci', 'reflects', 'leveraging', 'path', 'april', 'risk/return', 'return', 'n', 'portfolio', 'deaf', 'uk', 'ordinarily', 'fee', 'r', 'c', 'gas', 'reference', 'amt', 'geographic', 'delayed', 'equivalently', 'accounts', 'severe', 'yields', 'pace', 'continuing', 'channels', 'list', 'security', 'respective', 'earned', 'fund', 'value', 'shares', 'proportionately', 'investment', 'untimely', 'after-effects', 'inverse', 'procedures', 'may', 'make', 'pro', 'sixth', 'risk', 'confidential', 'reflecting', 'models', 'calculations', 'specialized', 'allocation', 'attractiveness', 'year', 'call', 'company', 'manager', 'session', 'issuer-specific', 'closing', 'frontier', 'performance', 'selection', 'relatively', 'repay', 'qualifying', 'january', 'tax', 'perspective', 'employed', 'market', 'equivalent', 'reorganization', 'class', 'sources', 'reimbursement', 'social', 'high-conviction'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Measure Distance"
      ],
      "metadata": {
        "id": "Tr1sAobfb5wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the train,validation and test dataframe\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "X_valid_df = pd.DataFrame(X_valid)\n",
        "X_test_df = pd.DataFrame(X_test)\n",
        "\n",
        "X_train_df['sentences_distance'] = X_train_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
        "X_valid_df['sentences_distance'] = X_valid_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
        "X_test_df['sentences_distance'] = X_test_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)"
      ],
      "metadata": {
        "id": "UsItC879fq8S"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentence_match(summary, knowledge, num_sent):\n",
        "    sentences = sent_tokenize(summary)\n",
        "    sentence_scores = []\n",
        "    for j, sentence in enumerate(sentences):\n",
        "        set_tokens = set(tokenizer(sentence))\n",
        "\n",
        "        # Find the number of common words between the knowledge base and the sentence\n",
        "        inter_knwoledge = set_tokens.intersection(knowledge)\n",
        "\n",
        "        sentence_scores.append(len(inter_knwoledge))\n",
        "\n",
        "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
        "    top_sentences = sentences[len(sentences)-num_sent-1:]\n",
        "    return ' '.join(top_sentences)\n",
        "\n",
        "X_train_df['sentences_match'] = X_train_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "X_valid_df['sentences_match'] = X_valid_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "X_test_df['sentences_match'] = X_test_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "\n",
        "# produce train_X and test_X\n",
        "train_X = X_train_df['sentences_match'].values\n",
        "train_X = [' '.join(tokenizer(txt)) for txt in train_X]\n",
        "\n",
        "valid_X = X_valid_df['sentences_match'].values\n",
        "valid_X = [' '.join(tokenizer(txt)) for txt in valid_X]\n",
        "\n",
        "test_X = X_test_df['sentences_match'].values\n",
        "test_X = [' '.join(tokenizer(txt)) for txt in test_X]\n",
        "\n",
        "# produce train_y and valid_y\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "encoded_train_y = encoder.fit_transform(y_train)\n",
        "label_train_y = to_categorical(encoded_train_y, num_classes=3)\n",
        "\n",
        "encoded_valid_y = encoder.transform(y_valid)\n",
        "label_valid_y = to_categorical(encoded_valid_y, num_classes=3)\n",
        "\n",
        "encoded_test_y = encoder.fit_transform(y_test)\n",
        "label_test_y = to_categorical(encoded_test_y, num_classes=3)\n",
        "\n",
        "num_words = 2500 # Size of the vocabulary used. we only consider the 2500 most common words. The other words are removed from the texts.\n",
        "maxlen = 150 # Number of word considered for each document. we cut or lengthen the texts to have texts of 150 words.\n",
        "word_dimension = 50 # dimension of our word vectors.\n",
        "\n",
        "keras_tokenizer = Tokenizer(num_words=num_words)\n",
        "\n",
        "keras_tokenizer.fit_on_texts(train_X)\n",
        "\n",
        "word_index = keras_tokenizer.word_index\n",
        "\n",
        "sequences_train = keras_tokenizer.texts_to_sequences(train_X)\n",
        "sequences_valid = keras_tokenizer.texts_to_sequences(valid_X)\n",
        "sequences_test = keras_tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "# truncate or lenthen each text so they have the same length.\n",
        "feature_train = pad_sequences(sequences_train, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "feature_valid = pad_sequences(sequences_valid, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "feature_test = pad_sequences(sequences_test, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "\n",
        "# create our embedding matrix\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, word_dimension))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "9myjGWaxb8i7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classifier"
      ],
      "metadata": {
        "id": "iQJBBnlOcVrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_CNN_model():\n",
        "    CNN = Sequential()\n",
        "    # The Embedding layer takes the embedding matrix as an argument and transform the inputed the sequences of index to sequences of vectors.\n",
        "    CNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False))\n",
        "\n",
        "    CNN.add(Convolution1D(64, 5, activation = 'relu'))\n",
        "    CNN.add(MaxPooling1D(pool_size = 5))\n",
        "\n",
        "    CNN.add(Convolution1D(32, 5, activation = 'relu'))\n",
        "    CNN.add(MaxPooling1D(pool_size = 5))\n",
        "\n",
        "    CNN.add(Flatten())\n",
        "    CNN.add(Dense(units = 128 , activation = 'relu'))\n",
        "    CNN.add(Dropout(0.5))\n",
        "    CNN.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "    CNN.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    return CNN\n",
        "\n",
        "CNN_model = create_CNN_model()\n",
        "CNN_history = CNN_model.fit(feature_train, label_train_y, epochs=800, batch_size=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgkctFJIcCRh",
        "outputId": "8a445831-72e2-4e95-8dfa-d8948fefb71c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "3/3 [==============================] - 1s 7ms/step - loss: 1.1284 - accuracy: 0.4388\n",
            "Epoch 2/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0411 - accuracy: 0.5612\n",
            "Epoch 3/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9870 - accuracy: 0.5714\n",
            "Epoch 4/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9801 - accuracy: 0.5782\n",
            "Epoch 5/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9759 - accuracy: 0.5748\n",
            "Epoch 6/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9792 - accuracy: 0.5714\n",
            "Epoch 7/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9683 - accuracy: 0.5680\n",
            "Epoch 8/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9650 - accuracy: 0.5714\n",
            "Epoch 9/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9570 - accuracy: 0.5714\n",
            "Epoch 10/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9853 - accuracy: 0.5714\n",
            "Epoch 11/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9565 - accuracy: 0.5816\n",
            "Epoch 12/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9493 - accuracy: 0.5714\n",
            "Epoch 13/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9532 - accuracy: 0.5714\n",
            "Epoch 14/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9308 - accuracy: 0.5680\n",
            "Epoch 15/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9274 - accuracy: 0.5714\n",
            "Epoch 16/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9335 - accuracy: 0.5578\n",
            "Epoch 17/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9094 - accuracy: 0.5714\n",
            "Epoch 18/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8957 - accuracy: 0.5782\n",
            "Epoch 19/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8859 - accuracy: 0.5850\n",
            "Epoch 20/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8708 - accuracy: 0.6224\n",
            "Epoch 21/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8642 - accuracy: 0.6156\n",
            "Epoch 22/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8595 - accuracy: 0.6122\n",
            "Epoch 23/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8502 - accuracy: 0.6088\n",
            "Epoch 24/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8272 - accuracy: 0.6463\n",
            "Epoch 25/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8227 - accuracy: 0.6497\n",
            "Epoch 26/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7958 - accuracy: 0.6633\n",
            "Epoch 27/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8037 - accuracy: 0.6599\n",
            "Epoch 28/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7810 - accuracy: 0.6361\n",
            "Epoch 29/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7641 - accuracy: 0.6905\n",
            "Epoch 30/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7580 - accuracy: 0.6905\n",
            "Epoch 31/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7491 - accuracy: 0.6939\n",
            "Epoch 32/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7411 - accuracy: 0.6939\n",
            "Epoch 33/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7104 - accuracy: 0.7109\n",
            "Epoch 34/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7157 - accuracy: 0.6871\n",
            "Epoch 35/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7107 - accuracy: 0.7075\n",
            "Epoch 36/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6944 - accuracy: 0.7279\n",
            "Epoch 37/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6805 - accuracy: 0.7041\n",
            "Epoch 38/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6645 - accuracy: 0.7381\n",
            "Epoch 39/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6385 - accuracy: 0.7449\n",
            "Epoch 40/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6266 - accuracy: 0.7483\n",
            "Epoch 41/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6088 - accuracy: 0.7653\n",
            "Epoch 42/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6159 - accuracy: 0.7449\n",
            "Epoch 43/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5967 - accuracy: 0.7347\n",
            "Epoch 44/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5962 - accuracy: 0.7381\n",
            "Epoch 45/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5706 - accuracy: 0.7823\n",
            "Epoch 46/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5506 - accuracy: 0.7755\n",
            "Epoch 47/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5423 - accuracy: 0.7789\n",
            "Epoch 48/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5459 - accuracy: 0.7721\n",
            "Epoch 49/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5130 - accuracy: 0.8061\n",
            "Epoch 50/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5087 - accuracy: 0.7993\n",
            "Epoch 51/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5107 - accuracy: 0.7857\n",
            "Epoch 52/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4884 - accuracy: 0.7925\n",
            "Epoch 53/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4687 - accuracy: 0.8129\n",
            "Epoch 54/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4359 - accuracy: 0.8197\n",
            "Epoch 55/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4552 - accuracy: 0.8163\n",
            "Epoch 56/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4371 - accuracy: 0.8367\n",
            "Epoch 57/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4445 - accuracy: 0.8265\n",
            "Epoch 58/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4344 - accuracy: 0.8231\n",
            "Epoch 59/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3971 - accuracy: 0.8367\n",
            "Epoch 60/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4006 - accuracy: 0.8571\n",
            "Epoch 61/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4010 - accuracy: 0.8401\n",
            "Epoch 62/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3778 - accuracy: 0.8435\n",
            "Epoch 63/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3768 - accuracy: 0.8367\n",
            "Epoch 64/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3639 - accuracy: 0.8435\n",
            "Epoch 65/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3779 - accuracy: 0.8469\n",
            "Epoch 66/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3797 - accuracy: 0.8605\n",
            "Epoch 67/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3593 - accuracy: 0.8571\n",
            "Epoch 68/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3306 - accuracy: 0.8605\n",
            "Epoch 69/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8673\n",
            "Epoch 70/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3094 - accuracy: 0.8673\n",
            "Epoch 71/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3276 - accuracy: 0.8605\n",
            "Epoch 72/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3089 - accuracy: 0.8673\n",
            "Epoch 73/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2894 - accuracy: 0.8707\n",
            "Epoch 74/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2932 - accuracy: 0.8707\n",
            "Epoch 75/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3016 - accuracy: 0.8741\n",
            "Epoch 76/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2943 - accuracy: 0.8810\n",
            "Epoch 77/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2921 - accuracy: 0.8844\n",
            "Epoch 78/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2738 - accuracy: 0.8844\n",
            "Epoch 79/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2849 - accuracy: 0.8707\n",
            "Epoch 80/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2761 - accuracy: 0.8741\n",
            "Epoch 81/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2860 - accuracy: 0.8639\n",
            "Epoch 82/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2808 - accuracy: 0.8844\n",
            "Epoch 83/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2838 - accuracy: 0.8980\n",
            "Epoch 84/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2539 - accuracy: 0.9184\n",
            "Epoch 85/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2473 - accuracy: 0.8980\n",
            "Epoch 86/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2510 - accuracy: 0.9048\n",
            "Epoch 87/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2497 - accuracy: 0.9048\n",
            "Epoch 88/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2579 - accuracy: 0.8878\n",
            "Epoch 89/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2379 - accuracy: 0.8980\n",
            "Epoch 90/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2333 - accuracy: 0.9218\n",
            "Epoch 91/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2379 - accuracy: 0.9184\n",
            "Epoch 92/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2541 - accuracy: 0.8912\n",
            "Epoch 93/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2376 - accuracy: 0.9014\n",
            "Epoch 94/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2247 - accuracy: 0.9082\n",
            "Epoch 95/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2211 - accuracy: 0.9388\n",
            "Epoch 96/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2270 - accuracy: 0.9184\n",
            "Epoch 97/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2194 - accuracy: 0.9218\n",
            "Epoch 98/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2215 - accuracy: 0.9354\n",
            "Epoch 99/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2255 - accuracy: 0.9184\n",
            "Epoch 100/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1977 - accuracy: 0.9320\n",
            "Epoch 101/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2154 - accuracy: 0.9184\n",
            "Epoch 102/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1990 - accuracy: 0.9354\n",
            "Epoch 103/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1941 - accuracy: 0.9456\n",
            "Epoch 104/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2028 - accuracy: 0.9252\n",
            "Epoch 105/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2078 - accuracy: 0.9252\n",
            "Epoch 106/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2120 - accuracy: 0.9252\n",
            "Epoch 107/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1942 - accuracy: 0.9388\n",
            "Epoch 108/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1850 - accuracy: 0.9218\n",
            "Epoch 109/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1855 - accuracy: 0.9422\n",
            "Epoch 110/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1767 - accuracy: 0.9422\n",
            "Epoch 111/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1703 - accuracy: 0.9422\n",
            "Epoch 112/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1797 - accuracy: 0.9422\n",
            "Epoch 113/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1702 - accuracy: 0.9422\n",
            "Epoch 114/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1756 - accuracy: 0.9320\n",
            "Epoch 115/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1637 - accuracy: 0.9422\n",
            "Epoch 116/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1616 - accuracy: 0.9422\n",
            "Epoch 117/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1615 - accuracy: 0.9456\n",
            "Epoch 118/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1756 - accuracy: 0.9354\n",
            "Epoch 119/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1508 - accuracy: 0.9490\n",
            "Epoch 120/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1690 - accuracy: 0.9490\n",
            "Epoch 121/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.9388\n",
            "Epoch 122/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1524 - accuracy: 0.9422\n",
            "Epoch 123/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1491 - accuracy: 0.9524\n",
            "Epoch 124/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1455 - accuracy: 0.9490\n",
            "Epoch 125/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.9660\n",
            "Epoch 126/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1376 - accuracy: 0.9524\n",
            "Epoch 127/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1571 - accuracy: 0.9354\n",
            "Epoch 128/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1597 - accuracy: 0.9422\n",
            "Epoch 129/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1409 - accuracy: 0.9456\n",
            "Epoch 130/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1444 - accuracy: 0.9456\n",
            "Epoch 131/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1483 - accuracy: 0.9558\n",
            "Epoch 132/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1393 - accuracy: 0.9592\n",
            "Epoch 133/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1380 - accuracy: 0.9422\n",
            "Epoch 134/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1319 - accuracy: 0.9558\n",
            "Epoch 135/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1340 - accuracy: 0.9626\n",
            "Epoch 136/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1319 - accuracy: 0.9490\n",
            "Epoch 137/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1260 - accuracy: 0.9456\n",
            "Epoch 138/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1540 - accuracy: 0.9422\n",
            "Epoch 139/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1254 - accuracy: 0.9558\n",
            "Epoch 140/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1472 - accuracy: 0.9422\n",
            "Epoch 141/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9524\n",
            "Epoch 142/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1363 - accuracy: 0.9592\n",
            "Epoch 143/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1324 - accuracy: 0.9524\n",
            "Epoch 144/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1177 - accuracy: 0.9558\n",
            "Epoch 145/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1212 - accuracy: 0.9524\n",
            "Epoch 146/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1361 - accuracy: 0.9456\n",
            "Epoch 147/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1331 - accuracy: 0.9490\n",
            "Epoch 148/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1068 - accuracy: 0.9626\n",
            "Epoch 149/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1233 - accuracy: 0.9626\n",
            "Epoch 150/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1267 - accuracy: 0.9626\n",
            "Epoch 151/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1106 - accuracy: 0.9728\n",
            "Epoch 152/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1153 - accuracy: 0.9524\n",
            "Epoch 153/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1059 - accuracy: 0.9626\n",
            "Epoch 154/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1073 - accuracy: 0.9660\n",
            "Epoch 155/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1147 - accuracy: 0.9558\n",
            "Epoch 156/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1087 - accuracy: 0.9762\n",
            "Epoch 157/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1040 - accuracy: 0.9694\n",
            "Epoch 158/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1139 - accuracy: 0.9626\n",
            "Epoch 159/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1109 - accuracy: 0.9592\n",
            "Epoch 160/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9694\n",
            "Epoch 161/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0967 - accuracy: 0.9762\n",
            "Epoch 162/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1036 - accuracy: 0.9626\n",
            "Epoch 163/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0955 - accuracy: 0.9728\n",
            "Epoch 164/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0977 - accuracy: 0.9762\n",
            "Epoch 165/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0866 - accuracy: 0.9762\n",
            "Epoch 166/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1047 - accuracy: 0.9626\n",
            "Epoch 167/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1087 - accuracy: 0.9694\n",
            "Epoch 168/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1077 - accuracy: 0.9660\n",
            "Epoch 169/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0969 - accuracy: 0.9728\n",
            "Epoch 170/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0823 - accuracy: 0.9796\n",
            "Epoch 171/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1022 - accuracy: 0.9660\n",
            "Epoch 172/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0954 - accuracy: 0.9728\n",
            "Epoch 173/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0962 - accuracy: 0.9796\n",
            "Epoch 174/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0836 - accuracy: 0.9728\n",
            "Epoch 175/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0990 - accuracy: 0.9694\n",
            "Epoch 176/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0815 - accuracy: 0.9728\n",
            "Epoch 177/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0798 - accuracy: 0.9762\n",
            "Epoch 178/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0961 - accuracy: 0.9728\n",
            "Epoch 179/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9830\n",
            "Epoch 180/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0738 - accuracy: 0.9796\n",
            "Epoch 181/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0795 - accuracy: 0.9728\n",
            "Epoch 182/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0825 - accuracy: 0.9796\n",
            "Epoch 183/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0948 - accuracy: 0.9762\n",
            "Epoch 184/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0807 - accuracy: 0.9830\n",
            "Epoch 185/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0777 - accuracy: 0.9728\n",
            "Epoch 186/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0906 - accuracy: 0.9626\n",
            "Epoch 187/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0790 - accuracy: 0.9728\n",
            "Epoch 188/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0817 - accuracy: 0.9796\n",
            "Epoch 189/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0782 - accuracy: 0.9728\n",
            "Epoch 190/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0817 - accuracy: 0.9796\n",
            "Epoch 191/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0813 - accuracy: 0.9796\n",
            "Epoch 192/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0861 - accuracy: 0.9728\n",
            "Epoch 193/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0834 - accuracy: 0.9762\n",
            "Epoch 194/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0828 - accuracy: 0.9728\n",
            "Epoch 195/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0841 - accuracy: 0.9762\n",
            "Epoch 196/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0665 - accuracy: 0.9796\n",
            "Epoch 197/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0800 - accuracy: 0.9762\n",
            "Epoch 198/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0815 - accuracy: 0.9762\n",
            "Epoch 199/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0799 - accuracy: 0.9728\n",
            "Epoch 200/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0715 - accuracy: 0.9762\n",
            "Epoch 201/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0837 - accuracy: 0.9796\n",
            "Epoch 202/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0667 - accuracy: 0.9796\n",
            "Epoch 203/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0688 - accuracy: 0.9796\n",
            "Epoch 204/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0760 - accuracy: 0.9728\n",
            "Epoch 205/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0843 - accuracy: 0.9626\n",
            "Epoch 206/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0744 - accuracy: 0.9796\n",
            "Epoch 207/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0699 - accuracy: 0.9728\n",
            "Epoch 208/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0651 - accuracy: 0.9830\n",
            "Epoch 209/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0721 - accuracy: 0.9796\n",
            "Epoch 210/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0746 - accuracy: 0.9762\n",
            "Epoch 211/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0694 - accuracy: 0.9796\n",
            "Epoch 212/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0769 - accuracy: 0.9728\n",
            "Epoch 213/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0614 - accuracy: 0.9796\n",
            "Epoch 214/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0692 - accuracy: 0.9864\n",
            "Epoch 215/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0726 - accuracy: 0.9728\n",
            "Epoch 216/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.9796\n",
            "Epoch 217/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0617 - accuracy: 0.9762\n",
            "Epoch 218/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0701 - accuracy: 0.9796\n",
            "Epoch 219/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9762\n",
            "Epoch 220/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0743 - accuracy: 0.9762\n",
            "Epoch 221/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0653 - accuracy: 0.9762\n",
            "Epoch 222/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0687 - accuracy: 0.9762\n",
            "Epoch 223/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0613 - accuracy: 0.9762\n",
            "Epoch 224/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0719 - accuracy: 0.9796\n",
            "Epoch 225/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0572 - accuracy: 0.9864\n",
            "Epoch 226/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0604 - accuracy: 0.9796\n",
            "Epoch 227/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0598 - accuracy: 0.9864\n",
            "Epoch 228/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0655 - accuracy: 0.9796\n",
            "Epoch 229/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 0.9830\n",
            "Epoch 230/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0603 - accuracy: 0.9762\n",
            "Epoch 231/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0666 - accuracy: 0.9830\n",
            "Epoch 232/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0501 - accuracy: 0.9864\n",
            "Epoch 233/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0478 - accuracy: 0.9898\n",
            "Epoch 234/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0721 - accuracy: 0.9762\n",
            "Epoch 235/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0537 - accuracy: 0.9830\n",
            "Epoch 236/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0589 - accuracy: 0.9864\n",
            "Epoch 237/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 0.9864\n",
            "Epoch 238/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0541 - accuracy: 0.9796\n",
            "Epoch 239/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0542 - accuracy: 0.9864\n",
            "Epoch 240/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0551 - accuracy: 0.9830\n",
            "Epoch 241/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0603 - accuracy: 0.9762\n",
            "Epoch 242/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0551 - accuracy: 0.9830\n",
            "Epoch 243/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0515 - accuracy: 0.9864\n",
            "Epoch 244/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9762\n",
            "Epoch 245/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0524 - accuracy: 0.9864\n",
            "Epoch 246/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0614 - accuracy: 0.9830\n",
            "Epoch 247/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0491 - accuracy: 0.9830\n",
            "Epoch 248/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0501 - accuracy: 0.9864\n",
            "Epoch 249/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0509 - accuracy: 0.9932\n",
            "Epoch 250/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0487 - accuracy: 0.9830\n",
            "Epoch 251/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0473 - accuracy: 0.9864\n",
            "Epoch 252/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0516 - accuracy: 0.9830\n",
            "Epoch 253/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.9830\n",
            "Epoch 254/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0524 - accuracy: 0.9830\n",
            "Epoch 255/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0527 - accuracy: 0.9762\n",
            "Epoch 256/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0465 - accuracy: 0.9898\n",
            "Epoch 257/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0409 - accuracy: 0.9864\n",
            "Epoch 258/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0636 - accuracy: 0.9796\n",
            "Epoch 259/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0472 - accuracy: 0.9864\n",
            "Epoch 260/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0481 - accuracy: 0.9830\n",
            "Epoch 261/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9762\n",
            "Epoch 262/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0464 - accuracy: 0.9898\n",
            "Epoch 263/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0562 - accuracy: 0.9796\n",
            "Epoch 264/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 0.9898\n",
            "Epoch 265/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0510 - accuracy: 0.9864\n",
            "Epoch 266/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0513 - accuracy: 0.9864\n",
            "Epoch 267/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9762\n",
            "Epoch 268/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0584 - accuracy: 0.9762\n",
            "Epoch 269/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0528 - accuracy: 0.9830\n",
            "Epoch 270/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0567 - accuracy: 0.9864\n",
            "Epoch 271/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0443 - accuracy: 0.9898\n",
            "Epoch 272/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0682 - accuracy: 0.9762\n",
            "Epoch 273/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0455 - accuracy: 0.9864\n",
            "Epoch 274/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0524 - accuracy: 0.9864\n",
            "Epoch 275/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0499 - accuracy: 0.9864\n",
            "Epoch 276/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0361 - accuracy: 0.9898\n",
            "Epoch 277/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0451 - accuracy: 0.9762\n",
            "Epoch 278/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0546 - accuracy: 0.9898\n",
            "Epoch 279/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0409 - accuracy: 0.9898\n",
            "Epoch 280/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0491 - accuracy: 0.9796\n",
            "Epoch 281/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0419 - accuracy: 0.9830\n",
            "Epoch 282/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0474 - accuracy: 0.9830\n",
            "Epoch 283/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0368 - accuracy: 0.9898\n",
            "Epoch 284/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0426 - accuracy: 0.9864\n",
            "Epoch 285/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0432 - accuracy: 0.9762\n",
            "Epoch 286/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0462 - accuracy: 0.9864\n",
            "Epoch 287/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0411 - accuracy: 0.9898\n",
            "Epoch 288/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0547 - accuracy: 0.9830\n",
            "Epoch 289/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0462 - accuracy: 0.9830\n",
            "Epoch 290/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0340 - accuracy: 0.9898\n",
            "Epoch 291/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0435 - accuracy: 0.9898\n",
            "Epoch 292/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0378 - accuracy: 0.9932\n",
            "Epoch 293/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0427 - accuracy: 0.9864\n",
            "Epoch 294/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0380 - accuracy: 0.9898\n",
            "Epoch 295/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0492 - accuracy: 0.9830\n",
            "Epoch 296/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0432 - accuracy: 0.9932\n",
            "Epoch 297/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0390 - accuracy: 0.9864\n",
            "Epoch 298/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9864\n",
            "Epoch 299/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0434 - accuracy: 0.9830\n",
            "Epoch 300/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0403 - accuracy: 0.9898\n",
            "Epoch 301/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0382 - accuracy: 0.9830\n",
            "Epoch 302/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 0.9864\n",
            "Epoch 303/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 0.9864\n",
            "Epoch 304/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0430 - accuracy: 0.9864\n",
            "Epoch 305/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0380 - accuracy: 0.9898\n",
            "Epoch 306/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0346 - accuracy: 0.9898\n",
            "Epoch 307/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.9898\n",
            "Epoch 308/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0376 - accuracy: 0.9898\n",
            "Epoch 309/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0394 - accuracy: 0.9898\n",
            "Epoch 310/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0369 - accuracy: 0.9966\n",
            "Epoch 311/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0337 - accuracy: 0.9898\n",
            "Epoch 312/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.9830\n",
            "Epoch 313/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0330 - accuracy: 0.9898\n",
            "Epoch 314/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0384 - accuracy: 0.9898\n",
            "Epoch 315/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0360 - accuracy: 0.9864\n",
            "Epoch 316/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0382 - accuracy: 0.9830\n",
            "Epoch 317/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0376 - accuracy: 0.9898\n",
            "Epoch 318/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0312 - accuracy: 0.9864\n",
            "Epoch 319/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0428 - accuracy: 0.9898\n",
            "Epoch 320/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0281 - accuracy: 0.9830\n",
            "Epoch 321/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0393 - accuracy: 0.9830\n",
            "Epoch 322/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0350 - accuracy: 0.9864\n",
            "Epoch 323/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0282 - accuracy: 0.9898\n",
            "Epoch 324/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0341 - accuracy: 0.9898\n",
            "Epoch 325/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0410 - accuracy: 0.9830\n",
            "Epoch 326/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0321 - accuracy: 0.9864\n",
            "Epoch 327/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0377 - accuracy: 0.9864\n",
            "Epoch 328/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0347 - accuracy: 0.9864\n",
            "Epoch 329/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0395 - accuracy: 0.9830\n",
            "Epoch 330/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0304 - accuracy: 0.9898\n",
            "Epoch 331/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0343 - accuracy: 0.9898\n",
            "Epoch 332/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0410 - accuracy: 0.9830\n",
            "Epoch 333/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0271 - accuracy: 0.9898\n",
            "Epoch 334/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0339 - accuracy: 0.9898\n",
            "Epoch 335/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 0.9898\n",
            "Epoch 336/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0429 - accuracy: 0.9796\n",
            "Epoch 337/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0307 - accuracy: 0.9898\n",
            "Epoch 338/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0260 - accuracy: 0.9932\n",
            "Epoch 339/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0366 - accuracy: 0.9864\n",
            "Epoch 340/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0349 - accuracy: 0.9898\n",
            "Epoch 341/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0376 - accuracy: 0.9864\n",
            "Epoch 342/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0315 - accuracy: 0.9864\n",
            "Epoch 343/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0292 - accuracy: 0.9898\n",
            "Epoch 344/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0267 - accuracy: 0.9898\n",
            "Epoch 345/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.9864\n",
            "Epoch 346/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0297 - accuracy: 0.9898\n",
            "Epoch 347/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0309 - accuracy: 0.9864\n",
            "Epoch 348/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0294 - accuracy: 0.9864\n",
            "Epoch 349/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0339 - accuracy: 0.9864\n",
            "Epoch 350/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0303 - accuracy: 0.9932\n",
            "Epoch 351/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0361 - accuracy: 0.9864\n",
            "Epoch 352/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.9932\n",
            "Epoch 353/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0290 - accuracy: 0.9966\n",
            "Epoch 354/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0287 - accuracy: 0.9932\n",
            "Epoch 355/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9898\n",
            "Epoch 356/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0351 - accuracy: 0.9932\n",
            "Epoch 357/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0289 - accuracy: 0.9932\n",
            "Epoch 358/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 0.9932\n",
            "Epoch 359/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0287 - accuracy: 0.9932\n",
            "Epoch 360/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0265 - accuracy: 0.9932\n",
            "Epoch 361/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0293 - accuracy: 0.9932\n",
            "Epoch 362/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0277 - accuracy: 0.9932\n",
            "Epoch 363/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0315 - accuracy: 0.9932\n",
            "Epoch 364/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0283 - accuracy: 0.9932\n",
            "Epoch 365/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0246 - accuracy: 0.9898\n",
            "Epoch 366/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0257 - accuracy: 0.9898\n",
            "Epoch 367/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0272 - accuracy: 0.9932\n",
            "Epoch 368/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0295 - accuracy: 0.9932\n",
            "Epoch 369/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0292 - accuracy: 0.9898\n",
            "Epoch 370/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0267 - accuracy: 0.9932\n",
            "Epoch 371/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0244 - accuracy: 0.9932\n",
            "Epoch 372/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0256 - accuracy: 0.9898\n",
            "Epoch 373/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0330 - accuracy: 0.9898\n",
            "Epoch 374/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0348 - accuracy: 0.9864\n",
            "Epoch 375/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 0.9932\n",
            "Epoch 376/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0256 - accuracy: 0.9898\n",
            "Epoch 377/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0278 - accuracy: 0.9898\n",
            "Epoch 378/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0299 - accuracy: 0.9932\n",
            "Epoch 379/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0275 - accuracy: 0.9898\n",
            "Epoch 380/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0253 - accuracy: 0.9966\n",
            "Epoch 381/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0297 - accuracy: 0.9932\n",
            "Epoch 382/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0278 - accuracy: 0.9932\n",
            "Epoch 383/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 0.9932\n",
            "Epoch 384/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0263 - accuracy: 0.9932\n",
            "Epoch 385/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 0.9898\n",
            "Epoch 386/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0257 - accuracy: 0.9932\n",
            "Epoch 387/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0360 - accuracy: 0.9864\n",
            "Epoch 388/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9932\n",
            "Epoch 389/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0260 - accuracy: 0.9864\n",
            "Epoch 390/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0232 - accuracy: 0.9966\n",
            "Epoch 391/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0238 - accuracy: 0.9864\n",
            "Epoch 392/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0256 - accuracy: 0.9932\n",
            "Epoch 393/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0282 - accuracy: 0.9932\n",
            "Epoch 394/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0204 - accuracy: 0.9932\n",
            "Epoch 395/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0214 - accuracy: 0.9932\n",
            "Epoch 396/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 0.9932\n",
            "Epoch 397/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0275 - accuracy: 0.9932\n",
            "Epoch 398/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0308 - accuracy: 0.9932\n",
            "Epoch 399/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0238 - accuracy: 0.9932\n",
            "Epoch 400/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0226 - accuracy: 0.9932\n",
            "Epoch 401/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0318 - accuracy: 0.9898\n",
            "Epoch 402/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0167 - accuracy: 0.9932\n",
            "Epoch 403/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0331 - accuracy: 0.9830\n",
            "Epoch 404/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 0.9932\n",
            "Epoch 405/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0324 - accuracy: 0.9864\n",
            "Epoch 406/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0272 - accuracy: 0.9966\n",
            "Epoch 407/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0272 - accuracy: 0.9898\n",
            "Epoch 408/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0199 - accuracy: 0.9898\n",
            "Epoch 409/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0290 - accuracy: 0.9898\n",
            "Epoch 410/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0233 - accuracy: 0.9932\n",
            "Epoch 411/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0225 - accuracy: 0.9898\n",
            "Epoch 412/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0251 - accuracy: 0.9932\n",
            "Epoch 413/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 0.9932\n",
            "Epoch 414/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0286 - accuracy: 0.9932\n",
            "Epoch 415/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0273 - accuracy: 0.9898\n",
            "Epoch 416/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0251 - accuracy: 0.9932\n",
            "Epoch 417/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0208 - accuracy: 0.9932\n",
            "Epoch 418/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0169 - accuracy: 0.9932\n",
            "Epoch 419/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0304 - accuracy: 0.9864\n",
            "Epoch 420/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0204 - accuracy: 0.9932\n",
            "Epoch 421/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0355 - accuracy: 0.9898\n",
            "Epoch 422/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0274 - accuracy: 0.9932\n",
            "Epoch 423/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0312 - accuracy: 0.9864\n",
            "Epoch 424/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0299 - accuracy: 0.9864\n",
            "Epoch 425/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0307 - accuracy: 0.9864\n",
            "Epoch 426/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0229 - accuracy: 0.9932\n",
            "Epoch 427/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0293 - accuracy: 0.9898\n",
            "Epoch 428/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0288 - accuracy: 0.9932\n",
            "Epoch 429/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0377 - accuracy: 0.9898\n",
            "Epoch 430/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0216 - accuracy: 0.9966\n",
            "Epoch 431/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0446 - accuracy: 0.9830\n",
            "Epoch 432/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0293 - accuracy: 0.9966\n",
            "Epoch 433/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0261 - accuracy: 0.9932\n",
            "Epoch 434/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 0.9932\n",
            "Epoch 435/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 0.9864\n",
            "Epoch 436/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0260 - accuracy: 0.9932\n",
            "Epoch 437/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0241 - accuracy: 0.9932\n",
            "Epoch 438/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0241 - accuracy: 0.9898\n",
            "Epoch 439/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0236 - accuracy: 0.9932\n",
            "Epoch 440/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0275 - accuracy: 0.9898\n",
            "Epoch 441/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0213 - accuracy: 0.9932\n",
            "Epoch 442/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0253 - accuracy: 0.9932\n",
            "Epoch 443/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0235 - accuracy: 0.9932\n",
            "Epoch 444/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0331 - accuracy: 0.9898\n",
            "Epoch 445/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0216 - accuracy: 0.9898\n",
            "Epoch 446/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0245 - accuracy: 0.9898\n",
            "Epoch 447/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0246 - accuracy: 0.9898\n",
            "Epoch 448/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 0.9898\n",
            "Epoch 449/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0236 - accuracy: 0.9932\n",
            "Epoch 450/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0204 - accuracy: 0.9932\n",
            "Epoch 451/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 0.9932\n",
            "Epoch 452/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0214 - accuracy: 0.9932\n",
            "Epoch 453/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0231 - accuracy: 0.9932\n",
            "Epoch 454/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0261 - accuracy: 0.9898\n",
            "Epoch 455/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0205 - accuracy: 0.9932\n",
            "Epoch 456/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0202 - accuracy: 0.9932\n",
            "Epoch 457/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0214 - accuracy: 0.9898\n",
            "Epoch 458/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0290 - accuracy: 0.9864\n",
            "Epoch 459/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 0.9966\n",
            "Epoch 460/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0185 - accuracy: 0.9932\n",
            "Epoch 461/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0247 - accuracy: 0.9898\n",
            "Epoch 462/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0218 - accuracy: 0.9898\n",
            "Epoch 463/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0205 - accuracy: 0.9932\n",
            "Epoch 464/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0228 - accuracy: 0.9932\n",
            "Epoch 465/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 0.9932\n",
            "Epoch 466/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 0.9932\n",
            "Epoch 467/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0194 - accuracy: 0.9966\n",
            "Epoch 468/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 0.9966\n",
            "Epoch 469/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0147 - accuracy: 0.9932\n",
            "Epoch 470/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 0.9932\n",
            "Epoch 471/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0328 - accuracy: 0.9898\n",
            "Epoch 472/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0270 - accuracy: 0.9898\n",
            "Epoch 473/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0287 - accuracy: 0.9898\n",
            "Epoch 474/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0239 - accuracy: 0.9898\n",
            "Epoch 475/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0226 - accuracy: 0.9932\n",
            "Epoch 476/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0206 - accuracy: 0.9932\n",
            "Epoch 477/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0184 - accuracy: 0.9932\n",
            "Epoch 478/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0139 - accuracy: 0.9966\n",
            "Epoch 479/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0175 - accuracy: 0.9966\n",
            "Epoch 480/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0150 - accuracy: 0.9932\n",
            "Epoch 481/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0205 - accuracy: 0.9864\n",
            "Epoch 482/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0160 - accuracy: 0.9966\n",
            "Epoch 483/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 0.9966\n",
            "Epoch 484/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0136 - accuracy: 0.9966\n",
            "Epoch 485/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0156 - accuracy: 0.9966\n",
            "Epoch 486/800\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.0197 - accuracy: 0.9932\n",
            "Epoch 487/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0226 - accuracy: 0.9898\n",
            "Epoch 488/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 0.9932\n",
            "Epoch 489/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 0.9966\n",
            "Epoch 490/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0217 - accuracy: 0.9932\n",
            "Epoch 491/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0166 - accuracy: 0.9932\n",
            "Epoch 492/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0198 - accuracy: 0.9932\n",
            "Epoch 493/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 494/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0265 - accuracy: 0.9932\n",
            "Epoch 495/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 0.9932\n",
            "Epoch 496/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0165 - accuracy: 0.9932\n",
            "Epoch 497/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0173 - accuracy: 0.9932\n",
            "Epoch 498/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0144 - accuracy: 0.9966\n",
            "Epoch 499/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.9966\n",
            "Epoch 500/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0184 - accuracy: 0.9932\n",
            "Epoch 501/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0189 - accuracy: 0.9932\n",
            "Epoch 502/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0160 - accuracy: 0.9932\n",
            "Epoch 503/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0149 - accuracy: 0.9966\n",
            "Epoch 504/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 505/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0134 - accuracy: 0.9932\n",
            "Epoch 506/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 0.9932\n",
            "Epoch 507/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 0.9932\n",
            "Epoch 508/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0142 - accuracy: 0.9932\n",
            "Epoch 509/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 0.9966\n",
            "Epoch 510/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 0.9966\n",
            "Epoch 511/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0163 - accuracy: 0.9932\n",
            "Epoch 512/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0137 - accuracy: 0.9932\n",
            "Epoch 513/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0212 - accuracy: 0.9932\n",
            "Epoch 514/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0177 - accuracy: 0.9932\n",
            "Epoch 515/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0177 - accuracy: 0.9932\n",
            "Epoch 516/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0152 - accuracy: 0.9966\n",
            "Epoch 517/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0166 - accuracy: 0.9898\n",
            "Epoch 518/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 0.9932\n",
            "Epoch 519/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0158 - accuracy: 0.9966\n",
            "Epoch 520/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 0.9898\n",
            "Epoch 521/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0148 - accuracy: 0.9932\n",
            "Epoch 522/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 523/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0184 - accuracy: 0.9932\n",
            "Epoch 524/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 0.9966\n",
            "Epoch 525/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 1.0000\n",
            "Epoch 526/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 0.9932\n",
            "Epoch 527/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 0.9932\n",
            "Epoch 528/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0178 - accuracy: 0.9932\n",
            "Epoch 529/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0179 - accuracy: 0.9966\n",
            "Epoch 530/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 0.9932\n",
            "Epoch 531/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 0.9932\n",
            "Epoch 532/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 0.9966\n",
            "Epoch 533/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0158 - accuracy: 0.9932\n",
            "Epoch 534/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 535/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 0.9932\n",
            "Epoch 536/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 537/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 0.9966\n",
            "Epoch 538/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0202 - accuracy: 0.9898\n",
            "Epoch 539/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 540/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0147 - accuracy: 0.9932\n",
            "Epoch 541/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 542/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 0.9966\n",
            "Epoch 543/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.9966\n",
            "Epoch 544/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0145 - accuracy: 0.9966\n",
            "Epoch 545/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9966\n",
            "Epoch 546/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0181 - accuracy: 0.9932\n",
            "Epoch 547/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 0.9898\n",
            "Epoch 548/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0170 - accuracy: 0.9898\n",
            "Epoch 549/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9932\n",
            "Epoch 550/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0189 - accuracy: 0.9898\n",
            "Epoch 551/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 0.9966\n",
            "Epoch 552/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.9966\n",
            "Epoch 553/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0125 - accuracy: 0.9966\n",
            "Epoch 554/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 555/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.9966\n",
            "Epoch 556/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0159 - accuracy: 0.9932\n",
            "Epoch 557/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 0.9966\n",
            "Epoch 558/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 0.9932\n",
            "Epoch 559/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0177 - accuracy: 0.9932\n",
            "Epoch 560/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 0.9932\n",
            "Epoch 561/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0145 - accuracy: 0.9932\n",
            "Epoch 562/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0178 - accuracy: 0.9932\n",
            "Epoch 563/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0184 - accuracy: 0.9898\n",
            "Epoch 564/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0187 - accuracy: 0.9932\n",
            "Epoch 565/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0173 - accuracy: 0.9966\n",
            "Epoch 566/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 0.9932\n",
            "Epoch 567/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 0.9898\n",
            "Epoch 568/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.9966\n",
            "Epoch 569/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 0.9932\n",
            "Epoch 570/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 0.9932\n",
            "Epoch 571/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.9966\n",
            "Epoch 572/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9966\n",
            "Epoch 573/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 0.9966\n",
            "Epoch 574/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.9932\n",
            "Epoch 575/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0149 - accuracy: 0.9932\n",
            "Epoch 576/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 0.9898\n",
            "Epoch 577/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.9966\n",
            "Epoch 578/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 0.9932\n",
            "Epoch 579/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.9966\n",
            "Epoch 580/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0160 - accuracy: 0.9932\n",
            "Epoch 581/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 0.9932\n",
            "Epoch 582/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 0.9966\n",
            "Epoch 583/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.9966\n",
            "Epoch 584/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 0.9966\n",
            "Epoch 585/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 0.9966\n",
            "Epoch 586/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.9966\n",
            "Epoch 587/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0130 - accuracy: 0.9932\n",
            "Epoch 588/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 0.9932\n",
            "Epoch 589/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 0.9932\n",
            "Epoch 590/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 591/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 0.9966\n",
            "Epoch 592/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 593/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0136 - accuracy: 0.9932\n",
            "Epoch 594/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.9966\n",
            "Epoch 595/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.9966\n",
            "Epoch 596/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0180 - accuracy: 0.9932\n",
            "Epoch 597/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0191 - accuracy: 0.9932\n",
            "Epoch 598/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 0.9966\n",
            "Epoch 599/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 0.9932\n",
            "Epoch 600/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 1.0000\n",
            "Epoch 601/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 0.9966\n",
            "Epoch 602/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.9966\n",
            "Epoch 603/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0142 - accuracy: 0.9932\n",
            "Epoch 604/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 605/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 0.9966\n",
            "Epoch 606/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.9966\n",
            "Epoch 607/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9932\n",
            "Epoch 608/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0107 - accuracy: 0.9966\n",
            "Epoch 609/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9966\n",
            "Epoch 610/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 0.9932\n",
            "Epoch 611/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0138 - accuracy: 0.9966\n",
            "Epoch 612/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.9966\n",
            "Epoch 613/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 0.9966\n",
            "Epoch 614/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 0.9966\n",
            "Epoch 615/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.9966\n",
            "Epoch 616/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.9932\n",
            "Epoch 617/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.9966\n",
            "Epoch 618/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0163 - accuracy: 0.9966\n",
            "Epoch 619/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0153 - accuracy: 0.9932\n",
            "Epoch 620/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9932\n",
            "Epoch 621/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0107 - accuracy: 0.9966\n",
            "Epoch 622/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 0.9966\n",
            "Epoch 623/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0120 - accuracy: 0.9966\n",
            "Epoch 624/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 0.9932\n",
            "Epoch 625/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9966\n",
            "Epoch 626/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 0.9966\n",
            "Epoch 627/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 0.9932\n",
            "Epoch 628/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 0.9932\n",
            "Epoch 629/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0159 - accuracy: 0.9932\n",
            "Epoch 630/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.9966\n",
            "Epoch 631/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.9966\n",
            "Epoch 632/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 0.9966\n",
            "Epoch 633/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.9932\n",
            "Epoch 634/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 0.9966\n",
            "Epoch 635/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9966\n",
            "Epoch 636/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.9966\n",
            "Epoch 637/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9966\n",
            "Epoch 638/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 0.9932\n",
            "Epoch 639/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 0.9932\n",
            "Epoch 640/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.9966\n",
            "Epoch 641/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0168 - accuracy: 0.9932\n",
            "Epoch 642/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 0.9932\n",
            "Epoch 643/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9966\n",
            "Epoch 644/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9966\n",
            "Epoch 645/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.9966\n",
            "Epoch 646/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9966\n",
            "Epoch 647/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0155 - accuracy: 0.9932\n",
            "Epoch 648/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 0.9932\n",
            "Epoch 649/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0137 - accuracy: 0.9966\n",
            "Epoch 650/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 0.9966\n",
            "Epoch 651/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.9966\n",
            "Epoch 652/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.9966\n",
            "Epoch 653/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0145 - accuracy: 0.9966\n",
            "Epoch 654/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 0.9966\n",
            "Epoch 655/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.9966\n",
            "Epoch 656/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 0.9966\n",
            "Epoch 657/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.9966\n",
            "Epoch 658/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 0.9932\n",
            "Epoch 659/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 660/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0130 - accuracy: 0.9966\n",
            "Epoch 661/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0142 - accuracy: 0.9966\n",
            "Epoch 662/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.9966\n",
            "Epoch 663/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.9932\n",
            "Epoch 664/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 0.9966\n",
            "Epoch 665/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0099 - accuracy: 0.9966\n",
            "Epoch 666/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0082 - accuracy: 0.9966\n",
            "Epoch 667/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0140 - accuracy: 0.9932\n",
            "Epoch 668/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 0.9966\n",
            "Epoch 669/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9966\n",
            "Epoch 670/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.9966\n",
            "Epoch 671/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 0.9966\n",
            "Epoch 672/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.9966\n",
            "Epoch 673/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.9966\n",
            "Epoch 674/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.9966\n",
            "Epoch 675/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 0.9966\n",
            "Epoch 676/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 0.9966\n",
            "Epoch 677/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 0.9932\n",
            "Epoch 678/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0084 - accuracy: 0.9966\n",
            "Epoch 679/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 0.9966\n",
            "Epoch 680/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 0.9966\n",
            "Epoch 681/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.9932\n",
            "Epoch 682/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 683/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9966\n",
            "Epoch 684/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.9966\n",
            "Epoch 685/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.9966\n",
            "Epoch 686/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.9966\n",
            "Epoch 687/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 0.9966\n",
            "Epoch 688/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.9966\n",
            "Epoch 689/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0077 - accuracy: 0.9966\n",
            "Epoch 690/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0121 - accuracy: 0.9966\n",
            "Epoch 691/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0121 - accuracy: 0.9966\n",
            "Epoch 692/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 0.9932\n",
            "Epoch 693/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9932\n",
            "Epoch 694/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.9966\n",
            "Epoch 695/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 0.9966\n",
            "Epoch 696/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.9966\n",
            "Epoch 697/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0107 - accuracy: 0.9966\n",
            "Epoch 698/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0085 - accuracy: 0.9966\n",
            "Epoch 699/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 0.9966\n",
            "Epoch 700/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 0.9966\n",
            "Epoch 701/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.9966\n",
            "Epoch 702/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.9966\n",
            "Epoch 703/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 0.9966\n",
            "Epoch 704/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.9966\n",
            "Epoch 705/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0075 - accuracy: 0.9966\n",
            "Epoch 706/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.9966\n",
            "Epoch 707/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.9932\n",
            "Epoch 708/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.9966\n",
            "Epoch 709/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.9966\n",
            "Epoch 710/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 0.9966\n",
            "Epoch 711/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.9966\n",
            "Epoch 712/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 0.9966\n",
            "Epoch 713/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0084 - accuracy: 0.9966\n",
            "Epoch 714/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0092 - accuracy: 0.9966\n",
            "Epoch 715/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 0.9966\n",
            "Epoch 716/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 0.9932\n",
            "Epoch 717/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 718/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0091 - accuracy: 0.9966\n",
            "Epoch 719/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9966\n",
            "Epoch 720/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.9966\n",
            "Epoch 721/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9966\n",
            "Epoch 722/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0157 - accuracy: 0.9932\n",
            "Epoch 723/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9966\n",
            "Epoch 724/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.9966\n",
            "Epoch 725/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 0.9932\n",
            "Epoch 726/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0087 - accuracy: 0.9966\n",
            "Epoch 727/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9966\n",
            "Epoch 728/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 729/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.9966\n",
            "Epoch 730/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0157 - accuracy: 0.9932\n",
            "Epoch 731/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 0.9966\n",
            "Epoch 732/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0085 - accuracy: 0.9966\n",
            "Epoch 733/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.9966\n",
            "Epoch 734/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0072 - accuracy: 0.9966\n",
            "Epoch 735/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.9966\n",
            "Epoch 736/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 0.9932\n",
            "Epoch 737/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0149 - accuracy: 0.9966\n",
            "Epoch 738/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 0.9966\n",
            "Epoch 739/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 0.9966\n",
            "Epoch 740/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0080 - accuracy: 0.9966\n",
            "Epoch 741/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.9932\n",
            "Epoch 742/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0084 - accuracy: 0.9966\n",
            "Epoch 743/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 0.9966\n",
            "Epoch 744/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.9966\n",
            "Epoch 745/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 0.9966\n",
            "Epoch 746/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.9966\n",
            "Epoch 747/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 0.9932\n",
            "Epoch 748/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0283 - accuracy: 0.9898\n",
            "Epoch 749/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 0.9966\n",
            "Epoch 750/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0270 - accuracy: 0.9830\n",
            "Epoch 751/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0180 - accuracy: 0.9864\n",
            "Epoch 752/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9864\n",
            "Epoch 753/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0514 - accuracy: 0.9830\n",
            "Epoch 754/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 0.9932\n",
            "Epoch 755/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0305 - accuracy: 0.9932\n",
            "Epoch 756/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0217 - accuracy: 0.9966\n",
            "Epoch 757/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0348 - accuracy: 0.9864\n",
            "Epoch 758/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0264 - accuracy: 0.9932\n",
            "Epoch 759/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0378 - accuracy: 0.9830\n",
            "Epoch 760/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0419 - accuracy: 0.9830\n",
            "Epoch 761/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0298 - accuracy: 0.9830\n",
            "Epoch 762/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0279 - accuracy: 0.9898\n",
            "Epoch 763/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 0.9932\n",
            "Epoch 764/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0324 - accuracy: 0.9830\n",
            "Epoch 765/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9796\n",
            "Epoch 766/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0421 - accuracy: 0.9762\n",
            "Epoch 767/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0390 - accuracy: 0.9966\n",
            "Epoch 768/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0524 - accuracy: 0.9898\n",
            "Epoch 769/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0470 - accuracy: 0.9830\n",
            "Epoch 770/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0403 - accuracy: 0.9830\n",
            "Epoch 771/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 0.9932\n",
            "Epoch 772/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0427 - accuracy: 0.9830\n",
            "Epoch 773/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 0.9898\n",
            "Epoch 774/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0303 - accuracy: 0.9898\n",
            "Epoch 775/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0302 - accuracy: 0.9864\n",
            "Epoch 776/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0559 - accuracy: 0.9830\n",
            "Epoch 777/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0294 - accuracy: 0.9932\n",
            "Epoch 778/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 779/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0247 - accuracy: 0.9898\n",
            "Epoch 780/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0233 - accuracy: 0.9898\n",
            "Epoch 781/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0172 - accuracy: 0.9932\n",
            "Epoch 782/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0246 - accuracy: 0.9864\n",
            "Epoch 783/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0230 - accuracy: 0.9898\n",
            "Epoch 784/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 0.9932\n",
            "Epoch 785/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 0.9932\n",
            "Epoch 786/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0132 - accuracy: 0.9966\n",
            "Epoch 787/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 0.9932\n",
            "Epoch 788/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 0.9966\n",
            "Epoch 789/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 0.9966\n",
            "Epoch 790/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 0.9932\n",
            "Epoch 791/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 792/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0107 - accuracy: 0.9966\n",
            "Epoch 793/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0256 - accuracy: 0.9864\n",
            "Epoch 794/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0188 - accuracy: 0.9932\n",
            "Epoch 795/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.9966\n",
            "Epoch 796/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0167 - accuracy: 0.9932\n",
            "Epoch 797/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0152 - accuracy: 0.9966\n",
            "Epoch 798/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.9966\n",
            "Epoch 799/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0177 - accuracy: 0.9898\n",
            "Epoch 800/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 0.9966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(CNN_history.history['accuracy'])\n",
        "plt.title('CNN Model accuracy with class=3')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "IpeWC-qfcfYJ",
        "outputId": "99711aad-c845-4ea0-d318-8de577054202"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss values\n",
        "plt.plot(CNN_history.history['loss'])\n",
        "plt.title('CNN Model loss with class=3')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "oICNYaJecisq",
        "outputId": "f5eba2f7-d9ad-4a78-9129-bb40a7eb28c4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "v7AcicTnclp2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_CNN = CNN_model.predict(feature_valid)\n",
        "\n",
        "# convert the validation vector\n",
        "valid_y_CNN = y_valid_CNN.copy()\n",
        "for i in range(len(y_valid_CNN)):\n",
        "    j = np.where(y_valid_CNN[i] == np.amax(y_valid_CNN[i]))\n",
        "    valid_y_CNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aD6kQPFcn4X",
        "outputId": "db02a29c-97e5-41eb-914c-7e912ea596e6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 3ms/step\n",
            "0.6486486486486487\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.60      0.69        15\n",
            "           1       0.64      0.75      0.69        36\n",
            "           2       0.57      0.52      0.55        23\n",
            "\n",
            "   micro avg       0.65      0.65      0.65        74\n",
            "   macro avg       0.68      0.62      0.64        74\n",
            "weighted avg       0.66      0.65      0.65        74\n",
            " samples avg       0.65      0.65      0.65        74\n",
            "\n",
            "auc score:  0.7111055658349134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_CNN = CNN_model.predict(feature_test)\n",
        "# convert the test vector\n",
        "test_y_CNN = y_test_CNN.copy()\n",
        "for i in range(len(y_test_CNN)):\n",
        "    j = np.where(y_test_CNN[i] == np.amax(y_test_CNN[i]))\n",
        "    test_y_CNN[i] = [0, 0, 0]\n",
        "    test_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_test_y,test_y_CNN))\n",
        "print(classification_report(label_test_y,test_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_test_y,test_y_CNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ_cj8dTcc6e",
        "outputId": "00470305-f326-486d-d3fd-f349ae182d8c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 3ms/step\n",
            "0.6021505376344086\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.47      0.46        17\n",
            "           1       0.65      0.70      0.67        43\n",
            "           2       0.62      0.55      0.58        33\n",
            "\n",
            "   micro avg       0.60      0.60      0.60        93\n",
            "   macro avg       0.57      0.57      0.57        93\n",
            "weighted avg       0.60      0.60      0.60        93\n",
            " samples avg       0.60      0.60      0.60        93\n",
            "\n",
            "auc score:  0.67980081977526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WkHcgfdb7IX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yzLyaISB7IaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9E4HqsZA7Ica"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def create_CNN_model():\n",
        "    CNN = Sequential()\n",
        "    CNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=True))\n",
        "\n",
        "    CNN.add(Convolution1D(256, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Convolution1D(128, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Convolution1D(64, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Flatten())\n",
        "    CNN.add(Dense(units = 512 , activation = 'relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(Dropout(0.3))\n",
        "    CNN.add(Dense(units = 256 , activation = 'relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(Dropout(0.3))\n",
        "    CNN.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "    opt = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "    CNN.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    return CNN\n",
        "\n",
        "CNN_model = create_CNN_model()\n",
        "\n",
        "# 创建ModelCheckpoint回调函数\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5',\n",
        "                             monitor='val_accuracy',\n",
        "                             save_best_only=True,\n",
        "                             mode='max',\n",
        "                             verbose=1)\n",
        "\n",
        "class_weights = {0: 1.0, 1: 0.5, 2: 3.0}\n",
        "CNN_history = CNN_model.fit(feature_train, label_train_y,\n",
        "                            epochs=700, batch_size=128,\n",
        "                            validation_data=(feature_valid, label_valid_y),\n",
        "                            class_weight=class_weights,\n",
        "                            callbacks=[checkpoint])  # 将回调函数传递给fit函数"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mva0MOGzxmY",
        "outputId": "d7e976c2-e2e2-4e8e-d966-c127c5056a2b"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 9.0830 - accuracy: 0.3707\n",
            "Epoch 1: val_accuracy improved from -inf to 0.31081, saving model to best_model.h5\n",
            "3/3 [==============================] - 3s 252ms/step - loss: 9.0830 - accuracy: 0.3707 - val_loss: 7.9690 - val_accuracy: 0.3108\n",
            "Epoch 2/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 8.1236 - accuracy: 0.2656"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 8.0880 - accuracy: 0.3027 - val_loss: 7.8911 - val_accuracy: 0.3108\n",
            "Epoch 3/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.6254 - accuracy: 0.5034\n",
            "Epoch 3: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 7.6254 - accuracy: 0.5034 - val_loss: 7.8668 - val_accuracy: 0.3108\n",
            "Epoch 4/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.5257 - accuracy: 0.4626\n",
            "Epoch 4: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 7.5257 - accuracy: 0.4626 - val_loss: 7.8083 - val_accuracy: 0.3108\n",
            "Epoch 5/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.4340 - accuracy: 0.5306\n",
            "Epoch 5: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 7.4340 - accuracy: 0.5306 - val_loss: 7.7605 - val_accuracy: 0.3108\n",
            "Epoch 6/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.2647 - accuracy: 0.5238\n",
            "Epoch 6: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 7.2647 - accuracy: 0.5238 - val_loss: 7.6657 - val_accuracy: 0.3108\n",
            "Epoch 7/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.0760 - accuracy: 0.6837\n",
            "Epoch 7: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 7.0760 - accuracy: 0.6837 - val_loss: 7.5702 - val_accuracy: 0.3108\n",
            "Epoch 8/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.9598 - accuracy: 0.7041\n",
            "Epoch 8: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 6.9598 - accuracy: 0.7041 - val_loss: 7.4943 - val_accuracy: 0.3108\n",
            "Epoch 9/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.8002 - accuracy: 0.7381\n",
            "Epoch 9: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 6.8002 - accuracy: 0.7381 - val_loss: 7.4109 - val_accuracy: 0.3108\n",
            "Epoch 10/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.7152 - accuracy: 0.7687\n",
            "Epoch 10: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 6.7152 - accuracy: 0.7687 - val_loss: 7.3394 - val_accuracy: 0.3108\n",
            "Epoch 11/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.5313 - accuracy: 0.8231\n",
            "Epoch 11: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 6.5313 - accuracy: 0.8231 - val_loss: 7.2658 - val_accuracy: 0.3108\n",
            "Epoch 12/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.4286 - accuracy: 0.8810\n",
            "Epoch 12: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 6.4286 - accuracy: 0.8810 - val_loss: 7.1958 - val_accuracy: 0.3108\n",
            "Epoch 13/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.3219 - accuracy: 0.8912\n",
            "Epoch 13: val_accuracy improved from 0.31081 to 0.33784, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 6.3219 - accuracy: 0.8912 - val_loss: 7.1162 - val_accuracy: 0.3378\n",
            "Epoch 14/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.2208 - accuracy: 0.9184\n",
            "Epoch 14: val_accuracy improved from 0.33784 to 0.37838, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 6.2208 - accuracy: 0.9184 - val_loss: 7.0436 - val_accuracy: 0.3784\n",
            "Epoch 15/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.1634 - accuracy: 0.8946\n",
            "Epoch 15: val_accuracy improved from 0.37838 to 0.47297, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 6.1634 - accuracy: 0.8946 - val_loss: 6.9603 - val_accuracy: 0.4730\n",
            "Epoch 16/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.0614 - accuracy: 0.8980\n",
            "Epoch 16: val_accuracy improved from 0.47297 to 0.48649, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 6.0614 - accuracy: 0.8980 - val_loss: 6.8847 - val_accuracy: 0.4865\n",
            "Epoch 17/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.9576 - accuracy: 0.9490\n",
            "Epoch 17: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 5.9576 - accuracy: 0.9490 - val_loss: 6.8205 - val_accuracy: 0.4865\n",
            "Epoch 18/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.8762 - accuracy: 0.9490\n",
            "Epoch 18: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 5.8762 - accuracy: 0.9490 - val_loss: 6.7558 - val_accuracy: 0.4865\n",
            "Epoch 19/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.7810 - accuracy: 0.9660\n",
            "Epoch 19: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 5.7810 - accuracy: 0.9660 - val_loss: 6.6906 - val_accuracy: 0.4865\n",
            "Epoch 20/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.6937 - accuracy: 0.9728\n",
            "Epoch 20: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 5.6937 - accuracy: 0.9728 - val_loss: 6.6258 - val_accuracy: 0.4865\n",
            "Epoch 21/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.6217 - accuracy: 0.9844\n",
            "Epoch 21: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 5.6338 - accuracy: 0.9830 - val_loss: 6.5594 - val_accuracy: 0.4865\n",
            "Epoch 22/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.6030 - accuracy: 0.9762\n",
            "Epoch 22: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 5.6030 - accuracy: 0.9762 - val_loss: 6.4915 - val_accuracy: 0.4865\n",
            "Epoch 23/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.4750 - accuracy: 0.9796\n",
            "Epoch 23: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 5.4750 - accuracy: 0.9796 - val_loss: 6.4278 - val_accuracy: 0.4865\n",
            "Epoch 24/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.4102 - accuracy: 0.9864\n",
            "Epoch 24: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 5.4102 - accuracy: 0.9864 - val_loss: 6.3643 - val_accuracy: 0.4865\n",
            "Epoch 25/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.3560 - accuracy: 0.9796\n",
            "Epoch 25: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 5.3560 - accuracy: 0.9796 - val_loss: 6.2946 - val_accuracy: 0.4865\n",
            "Epoch 26/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.2984 - accuracy: 0.9796\n",
            "Epoch 26: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 5.2984 - accuracy: 0.9796 - val_loss: 6.2286 - val_accuracy: 0.4865\n",
            "Epoch 27/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.2395 - accuracy: 0.9796\n",
            "Epoch 27: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 5.2395 - accuracy: 0.9796 - val_loss: 6.1591 - val_accuracy: 0.4865\n",
            "Epoch 28/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.1700 - accuracy: 0.9626\n",
            "Epoch 28: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 5.1700 - accuracy: 0.9626 - val_loss: 6.0892 - val_accuracy: 0.4865\n",
            "Epoch 29/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.1142 - accuracy: 0.9796\n",
            "Epoch 29: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 5.1142 - accuracy: 0.9796 - val_loss: 6.0225 - val_accuracy: 0.4865\n",
            "Epoch 30/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.0596 - accuracy: 0.9762\n",
            "Epoch 30: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 5.0596 - accuracy: 0.9762 - val_loss: 5.9585 - val_accuracy: 0.4865\n",
            "Epoch 31/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.9854 - accuracy: 0.9728\n",
            "Epoch 31: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 4.9854 - accuracy: 0.9728 - val_loss: 5.8964 - val_accuracy: 0.4865\n",
            "Epoch 32/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.9492 - accuracy: 0.9626\n",
            "Epoch 32: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 4.9492 - accuracy: 0.9626 - val_loss: 5.8383 - val_accuracy: 0.4865\n",
            "Epoch 33/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.8865 - accuracy: 0.9728\n",
            "Epoch 33: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 4.8865 - accuracy: 0.9728 - val_loss: 5.7811 - val_accuracy: 0.4865\n",
            "Epoch 34/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.8294 - accuracy: 0.9609\n",
            "Epoch 34: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 4.7996 - accuracy: 0.9762 - val_loss: 5.7206 - val_accuracy: 0.4865\n",
            "Epoch 35/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.7306 - accuracy: 0.9728\n",
            "Epoch 35: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 4.7306 - accuracy: 0.9728 - val_loss: 5.6651 - val_accuracy: 0.4865\n",
            "Epoch 36/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.6826 - accuracy: 0.9830\n",
            "Epoch 36: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 4.6826 - accuracy: 0.9830 - val_loss: 5.6145 - val_accuracy: 0.4865\n",
            "Epoch 37/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.6215 - accuracy: 0.9728\n",
            "Epoch 37: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 4.6215 - accuracy: 0.9728 - val_loss: 5.5620 - val_accuracy: 0.4865\n",
            "Epoch 38/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.5644 - accuracy: 0.9830\n",
            "Epoch 38: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 4.5644 - accuracy: 0.9830 - val_loss: 5.5117 - val_accuracy: 0.4865\n",
            "Epoch 39/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.5244 - accuracy: 0.9830\n",
            "Epoch 39: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 4.5244 - accuracy: 0.9830 - val_loss: 5.4593 - val_accuracy: 0.4865\n",
            "Epoch 40/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.4660 - accuracy: 0.9796\n",
            "Epoch 40: val_accuracy improved from 0.48649 to 0.52703, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 4.4660 - accuracy: 0.9796 - val_loss: 5.4084 - val_accuracy: 0.5270\n",
            "Epoch 41/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.4140 - accuracy: 0.9694\n",
            "Epoch 41: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 4.4140 - accuracy: 0.9694 - val_loss: 5.3554 - val_accuracy: 0.5270\n",
            "Epoch 42/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.3702 - accuracy: 0.9762\n",
            "Epoch 42: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 4.3702 - accuracy: 0.9762 - val_loss: 5.3017 - val_accuracy: 0.5270\n",
            "Epoch 43/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.3045 - accuracy: 0.9830\n",
            "Epoch 43: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 4.3045 - accuracy: 0.9830 - val_loss: 5.2428 - val_accuracy: 0.5270\n",
            "Epoch 44/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.2607 - accuracy: 0.9796\n",
            "Epoch 44: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 4.2607 - accuracy: 0.9796 - val_loss: 5.1814 - val_accuracy: 0.4865\n",
            "Epoch 45/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.1902 - accuracy: 0.9864\n",
            "Epoch 45: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 4.1902 - accuracy: 0.9864 - val_loss: 5.1230 - val_accuracy: 0.4865\n",
            "Epoch 46/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.1451 - accuracy: 0.9830\n",
            "Epoch 46: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 4.1451 - accuracy: 0.9830 - val_loss: 5.0692 - val_accuracy: 0.4865\n",
            "Epoch 47/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.0844 - accuracy: 0.9898\n",
            "Epoch 47: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 4.0844 - accuracy: 0.9898 - val_loss: 5.0132 - val_accuracy: 0.4865\n",
            "Epoch 48/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.0591 - accuracy: 0.9898\n",
            "Epoch 48: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 4.0591 - accuracy: 0.9898 - val_loss: 4.9577 - val_accuracy: 0.4865\n",
            "Epoch 49/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.9956 - accuracy: 0.9898\n",
            "Epoch 49: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 3.9956 - accuracy: 0.9898 - val_loss: 4.9100 - val_accuracy: 0.4865\n",
            "Epoch 50/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.9591 - accuracy: 0.9805\n",
            "Epoch 50: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.9550 - accuracy: 0.9796 - val_loss: 4.8671 - val_accuracy: 0.4865\n",
            "Epoch 51/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.9009 - accuracy: 0.9830\n",
            "Epoch 51: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 3.9009 - accuracy: 0.9830 - val_loss: 4.8211 - val_accuracy: 0.4865\n",
            "Epoch 52/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.8534 - accuracy: 0.9898\n",
            "Epoch 52: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 3.8534 - accuracy: 0.9898 - val_loss: 4.7728 - val_accuracy: 0.4865\n",
            "Epoch 53/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.7977 - accuracy: 0.9830\n",
            "Epoch 53: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 3.7977 - accuracy: 0.9830 - val_loss: 4.7264 - val_accuracy: 0.4865\n",
            "Epoch 54/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.7600 - accuracy: 0.9830\n",
            "Epoch 54: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 3.7600 - accuracy: 0.9830 - val_loss: 4.6804 - val_accuracy: 0.4865\n",
            "Epoch 55/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.7300 - accuracy: 0.9805\n",
            "Epoch 55: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.7241 - accuracy: 0.9830 - val_loss: 4.6455 - val_accuracy: 0.4865\n",
            "Epoch 56/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.6686 - accuracy: 0.9864\n",
            "Epoch 56: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.6686 - accuracy: 0.9864 - val_loss: 4.6158 - val_accuracy: 0.4865\n",
            "Epoch 57/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.6246 - accuracy: 0.9830\n",
            "Epoch 57: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 3.6246 - accuracy: 0.9830 - val_loss: 4.5877 - val_accuracy: 0.4865\n",
            "Epoch 58/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.5853 - accuracy: 0.9766\n",
            "Epoch 58: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 3.5761 - accuracy: 0.9830 - val_loss: 4.5450 - val_accuracy: 0.4865\n",
            "Epoch 59/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.5358 - accuracy: 0.9864\n",
            "Epoch 59: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.5358 - accuracy: 0.9864 - val_loss: 4.4959 - val_accuracy: 0.4865\n",
            "Epoch 60/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.5060 - accuracy: 0.9864\n",
            "Epoch 60: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 3.5060 - accuracy: 0.9864 - val_loss: 4.4565 - val_accuracy: 0.4865\n",
            "Epoch 61/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4572 - accuracy: 1.0000\n",
            "Epoch 61: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 3.4522 - accuracy: 0.9864 - val_loss: 4.4010 - val_accuracy: 0.5000\n",
            "Epoch 62/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4335 - accuracy: 0.9766\n",
            "Epoch 62: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 3.4449 - accuracy: 0.9864 - val_loss: 4.3879 - val_accuracy: 0.5000\n",
            "Epoch 63/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.3740 - accuracy: 0.9805\n",
            "Epoch 63: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.3689 - accuracy: 0.9830 - val_loss: 4.3905 - val_accuracy: 0.5135\n",
            "Epoch 64/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.3363 - accuracy: 0.9922\n",
            "Epoch 64: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 3.3432 - accuracy: 0.9864 - val_loss: 4.3472 - val_accuracy: 0.5135\n",
            "Epoch 65/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.3022 - accuracy: 0.9762\n",
            "Epoch 65: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 3.3022 - accuracy: 0.9762 - val_loss: 4.3043 - val_accuracy: 0.5000\n",
            "Epoch 66/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.2547 - accuracy: 0.9864\n",
            "Epoch 66: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.2547 - accuracy: 0.9864 - val_loss: 4.2845 - val_accuracy: 0.5000\n",
            "Epoch 67/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.2246 - accuracy: 0.9796\n",
            "Epoch 67: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 3.2246 - accuracy: 0.9796 - val_loss: 4.2580 - val_accuracy: 0.5000\n",
            "Epoch 68/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1874 - accuracy: 0.9844\n",
            "Epoch 68: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 3.1902 - accuracy: 0.9830 - val_loss: 4.2290 - val_accuracy: 0.5000\n",
            "Epoch 69/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.1425 - accuracy: 0.9830\n",
            "Epoch 69: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 3.1425 - accuracy: 0.9830 - val_loss: 4.1644 - val_accuracy: 0.5000\n",
            "Epoch 70/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.0950 - accuracy: 0.9922\n",
            "Epoch 70: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.1073 - accuracy: 0.9864 - val_loss: 4.0976 - val_accuracy: 0.5000\n",
            "Epoch 71/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0655 - accuracy: 0.9796\n",
            "Epoch 71: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 3.0655 - accuracy: 0.9796 - val_loss: 4.0288 - val_accuracy: 0.5000\n",
            "Epoch 72/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.0325 - accuracy: 0.9844\n",
            "Epoch 72: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.0334 - accuracy: 0.9796 - val_loss: 3.9656 - val_accuracy: 0.5000\n",
            "Epoch 73/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0107 - accuracy: 0.9762\n",
            "Epoch 73: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 3.0107 - accuracy: 0.9762 - val_loss: 3.9127 - val_accuracy: 0.5000\n",
            "Epoch 74/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.9564 - accuracy: 0.9883\n",
            "Epoch 74: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 2.9597 - accuracy: 0.9864 - val_loss: 3.8661 - val_accuracy: 0.5135\n",
            "Epoch 75/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9258 - accuracy: 0.9844\n",
            "Epoch 75: val_accuracy improved from 0.52703 to 0.54054, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 2.9233 - accuracy: 0.9830 - val_loss: 3.8156 - val_accuracy: 0.5405\n",
            "Epoch 76/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.8994 - accuracy: 0.9762\n",
            "Epoch 76: val_accuracy did not improve from 0.54054\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 2.8994 - accuracy: 0.9762 - val_loss: 3.7681 - val_accuracy: 0.5405\n",
            "Epoch 77/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.8493 - accuracy: 0.9883\n",
            "Epoch 77: val_accuracy improved from 0.54054 to 0.58108, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 2.8568 - accuracy: 0.9796 - val_loss: 3.7160 - val_accuracy: 0.5811\n",
            "Epoch 78/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.8230 - accuracy: 0.9830\n",
            "Epoch 78: val_accuracy improved from 0.58108 to 0.60811, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 2.8230 - accuracy: 0.9830 - val_loss: 3.6726 - val_accuracy: 0.6081\n",
            "Epoch 79/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.7874 - accuracy: 0.9796\n",
            "Epoch 79: val_accuracy did not improve from 0.60811\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 2.7874 - accuracy: 0.9796 - val_loss: 3.6322 - val_accuracy: 0.6081\n",
            "Epoch 80/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7801 - accuracy: 0.9766\n",
            "Epoch 80: val_accuracy did not improve from 0.60811\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 2.7647 - accuracy: 0.9830 - val_loss: 3.5922 - val_accuracy: 0.6081\n",
            "Epoch 81/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7523 - accuracy: 0.9766\n",
            "Epoch 81: val_accuracy did not improve from 0.60811\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 2.7327 - accuracy: 0.9796 - val_loss: 3.5533 - val_accuracy: 0.6081\n",
            "Epoch 82/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6869 - accuracy: 0.9864\n",
            "Epoch 82: val_accuracy improved from 0.60811 to 0.66216, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 2.6869 - accuracy: 0.9864 - val_loss: 3.5155 - val_accuracy: 0.6622\n",
            "Epoch 83/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6606 - accuracy: 0.9830\n",
            "Epoch 83: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 2.6606 - accuracy: 0.9830 - val_loss: 3.4815 - val_accuracy: 0.6486\n",
            "Epoch 84/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6288 - accuracy: 0.9830\n",
            "Epoch 84: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 2.6288 - accuracy: 0.9830 - val_loss: 3.4538 - val_accuracy: 0.6486\n",
            "Epoch 85/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5912 - accuracy: 0.9898\n",
            "Epoch 85: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 2.5912 - accuracy: 0.9898 - val_loss: 3.4268 - val_accuracy: 0.6486\n",
            "Epoch 86/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5712 - accuracy: 0.9796\n",
            "Epoch 86: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 2.5712 - accuracy: 0.9796 - val_loss: 3.3964 - val_accuracy: 0.6486\n",
            "Epoch 87/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5283 - accuracy: 0.9830\n",
            "Epoch 87: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 2.5283 - accuracy: 0.9830 - val_loss: 3.3620 - val_accuracy: 0.6351\n",
            "Epoch 88/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5061 - accuracy: 0.9762\n",
            "Epoch 88: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 2.5061 - accuracy: 0.9762 - val_loss: 3.3332 - val_accuracy: 0.6351\n",
            "Epoch 89/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4748 - accuracy: 0.9796\n",
            "Epoch 89: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.4748 - accuracy: 0.9796 - val_loss: 3.3074 - val_accuracy: 0.5946\n",
            "Epoch 90/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4396 - accuracy: 0.9830\n",
            "Epoch 90: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.4396 - accuracy: 0.9830 - val_loss: 3.2804 - val_accuracy: 0.5946\n",
            "Epoch 91/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4163 - accuracy: 0.9796\n",
            "Epoch 91: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.4163 - accuracy: 0.9796 - val_loss: 3.2583 - val_accuracy: 0.5946\n",
            "Epoch 92/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.3854 - accuracy: 0.9796\n",
            "Epoch 92: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 2.3854 - accuracy: 0.9796 - val_loss: 3.2529 - val_accuracy: 0.5946\n",
            "Epoch 93/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3500 - accuracy: 0.9844\n",
            "Epoch 93: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.3526 - accuracy: 0.9796 - val_loss: 3.2364 - val_accuracy: 0.5946\n",
            "Epoch 94/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.3246 - accuracy: 0.9796\n",
            "Epoch 94: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 2.3246 - accuracy: 0.9796 - val_loss: 3.2129 - val_accuracy: 0.5946\n",
            "Epoch 95/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2967 - accuracy: 0.9844\n",
            "Epoch 95: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.3092 - accuracy: 0.9762 - val_loss: 3.2264 - val_accuracy: 0.5811\n",
            "Epoch 96/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2699 - accuracy: 0.9922\n",
            "Epoch 96: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.2678 - accuracy: 0.9796 - val_loss: 3.2554 - val_accuracy: 0.5946\n",
            "Epoch 97/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.2434 - accuracy: 0.9830\n",
            "Epoch 97: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 2.2434 - accuracy: 0.9830 - val_loss: 3.2895 - val_accuracy: 0.5676\n",
            "Epoch 98/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.2227 - accuracy: 0.9796\n",
            "Epoch 98: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.2227 - accuracy: 0.9796 - val_loss: 3.3436 - val_accuracy: 0.5541\n",
            "Epoch 99/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2127 - accuracy: 0.9766\n",
            "Epoch 99: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 2.1983 - accuracy: 0.9796 - val_loss: 3.3640 - val_accuracy: 0.5541\n",
            "Epoch 100/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1826 - accuracy: 0.9766\n",
            "Epoch 100: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 2.1670 - accuracy: 0.9796 - val_loss: 3.3792 - val_accuracy: 0.5541\n",
            "Epoch 101/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1685 - accuracy: 0.9688\n",
            "Epoch 101: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 2.1406 - accuracy: 0.9796 - val_loss: 3.3873 - val_accuracy: 0.5541\n",
            "Epoch 102/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.1153 - accuracy: 0.9844\n",
            "Epoch 102: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 2.1112 - accuracy: 0.9864 - val_loss: 3.3530 - val_accuracy: 0.5405\n",
            "Epoch 103/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0843 - accuracy: 0.9844\n",
            "Epoch 103: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 2.0839 - accuracy: 0.9864 - val_loss: 3.2974 - val_accuracy: 0.5405\n",
            "Epoch 104/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.0592 - accuracy: 0.9932\n",
            "Epoch 104: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 2.0592 - accuracy: 0.9932 - val_loss: 3.2130 - val_accuracy: 0.5676\n",
            "Epoch 105/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.0391 - accuracy: 0.9796\n",
            "Epoch 105: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 2.0391 - accuracy: 0.9796 - val_loss: 3.1275 - val_accuracy: 0.5541\n",
            "Epoch 106/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0077 - accuracy: 0.9844\n",
            "Epoch 106: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0115 - accuracy: 0.9830 - val_loss: 3.0562 - val_accuracy: 0.5541\n",
            "Epoch 107/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9850 - accuracy: 0.9830\n",
            "Epoch 107: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 1.9850 - accuracy: 0.9830 - val_loss: 3.0155 - val_accuracy: 0.5946\n",
            "Epoch 108/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9679 - accuracy: 0.9830\n",
            "Epoch 108: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 1.9679 - accuracy: 0.9830 - val_loss: 2.9945 - val_accuracy: 0.5946\n",
            "Epoch 109/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9425 - accuracy: 0.9830\n",
            "Epoch 109: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.9425 - accuracy: 0.9830 - val_loss: 2.9866 - val_accuracy: 0.5946\n",
            "Epoch 110/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9200 - accuracy: 0.9830\n",
            "Epoch 110: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 1.9200 - accuracy: 0.9830 - val_loss: 2.9479 - val_accuracy: 0.6216\n",
            "Epoch 111/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9014 - accuracy: 0.9922\n",
            "Epoch 111: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.8972 - accuracy: 0.9830 - val_loss: 2.8894 - val_accuracy: 0.6486\n",
            "Epoch 112/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8789 - accuracy: 0.9844\n",
            "Epoch 112: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8688 - accuracy: 0.9898 - val_loss: 2.8539 - val_accuracy: 0.6486\n",
            "Epoch 113/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.8616 - accuracy: 0.9898\n",
            "Epoch 113: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 1.8616 - accuracy: 0.9898 - val_loss: 2.8715 - val_accuracy: 0.6351\n",
            "Epoch 114/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.8299 - accuracy: 0.9898\n",
            "Epoch 114: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 1.8299 - accuracy: 0.9898 - val_loss: 2.8898 - val_accuracy: 0.6216\n",
            "Epoch 115/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.8076 - accuracy: 0.9864\n",
            "Epoch 115: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.8076 - accuracy: 0.9864 - val_loss: 2.9285 - val_accuracy: 0.5676\n",
            "Epoch 116/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7919 - accuracy: 0.9922\n",
            "Epoch 116: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.7834 - accuracy: 0.9864 - val_loss: 2.9258 - val_accuracy: 0.5811\n",
            "Epoch 117/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7695 - accuracy: 0.9922\n",
            "Epoch 117: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.7780 - accuracy: 0.9830 - val_loss: 2.9548 - val_accuracy: 0.5811\n",
            "Epoch 118/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7587 - accuracy: 0.9766\n",
            "Epoch 118: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.7456 - accuracy: 0.9830 - val_loss: 2.9765 - val_accuracy: 0.5811\n",
            "Epoch 119/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.7199 - accuracy: 0.9830\n",
            "Epoch 119: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 1.7199 - accuracy: 0.9830 - val_loss: 3.0315 - val_accuracy: 0.5676\n",
            "Epoch 120/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7118 - accuracy: 0.9922\n",
            "Epoch 120: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.7086 - accuracy: 0.9796 - val_loss: 3.0268 - val_accuracy: 0.5676\n",
            "Epoch 121/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.6841 - accuracy: 0.9805\n",
            "Epoch 121: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 1.6841 - accuracy: 0.9796 - val_loss: 2.9750 - val_accuracy: 0.5811\n",
            "Epoch 122/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.6672 - accuracy: 0.9830\n",
            "Epoch 122: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.6672 - accuracy: 0.9830 - val_loss: 3.0275 - val_accuracy: 0.5946\n",
            "Epoch 123/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.6440 - accuracy: 0.9864\n",
            "Epoch 123: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.6440 - accuracy: 0.9864 - val_loss: 3.1135 - val_accuracy: 0.5946\n",
            "Epoch 124/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6202 - accuracy: 1.0000\n",
            "Epoch 124: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.6255 - accuracy: 0.9898 - val_loss: 3.1640 - val_accuracy: 0.5946\n",
            "Epoch 125/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.6079 - accuracy: 0.9898\n",
            "Epoch 125: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.6079 - accuracy: 0.9898 - val_loss: 3.1534 - val_accuracy: 0.5946\n",
            "Epoch 126/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.5870 - accuracy: 0.9864\n",
            "Epoch 126: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.5870 - accuracy: 0.9864 - val_loss: 3.1140 - val_accuracy: 0.5946\n",
            "Epoch 127/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5698 - accuracy: 0.9688\n",
            "Epoch 127: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5688 - accuracy: 0.9864 - val_loss: 3.2108 - val_accuracy: 0.5676\n",
            "Epoch 128/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.5452 - accuracy: 0.9864\n",
            "Epoch 128: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 1.5452 - accuracy: 0.9864 - val_loss: 3.3448 - val_accuracy: 0.5270\n",
            "Epoch 129/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5521 - accuracy: 0.9922\n",
            "Epoch 129: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5451 - accuracy: 0.9762 - val_loss: 3.3333 - val_accuracy: 0.5270\n",
            "Epoch 130/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.5190 - accuracy: 0.9766\n",
            "Epoch 130: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 1.5148 - accuracy: 0.9796 - val_loss: 3.2886 - val_accuracy: 0.5270\n",
            "Epoch 131/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4949 - accuracy: 0.9844\n",
            "Epoch 131: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4960 - accuracy: 0.9830 - val_loss: 3.1780 - val_accuracy: 0.5676\n",
            "Epoch 132/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4710 - accuracy: 0.9864\n",
            "Epoch 132: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 1.4710 - accuracy: 0.9864 - val_loss: 2.9894 - val_accuracy: 0.5676\n",
            "Epoch 133/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4575 - accuracy: 0.9830\n",
            "Epoch 133: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 1.4575 - accuracy: 0.9830 - val_loss: 2.8383 - val_accuracy: 0.6351\n",
            "Epoch 134/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4459 - accuracy: 0.9844\n",
            "Epoch 134: val_accuracy improved from 0.66216 to 0.67568, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 1.4412 - accuracy: 0.9830 - val_loss: 2.7469 - val_accuracy: 0.6757\n",
            "Epoch 135/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4252 - accuracy: 0.9898\n",
            "Epoch 135: val_accuracy did not improve from 0.67568\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 1.4252 - accuracy: 0.9898 - val_loss: 2.6764 - val_accuracy: 0.6757\n",
            "Epoch 136/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4139 - accuracy: 0.9844\n",
            "Epoch 136: val_accuracy improved from 0.67568 to 0.68919, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 1.4116 - accuracy: 0.9830 - val_loss: 2.6010 - val_accuracy: 0.6892\n",
            "Epoch 137/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3902 - accuracy: 0.9844\n",
            "Epoch 137: val_accuracy improved from 0.68919 to 0.70270, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 1.3919 - accuracy: 0.9830 - val_loss: 2.5148 - val_accuracy: 0.7027\n",
            "Epoch 138/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.3869 - accuracy: 0.9762\n",
            "Epoch 138: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 1.3869 - accuracy: 0.9762 - val_loss: 2.4215 - val_accuracy: 0.7027\n",
            "Epoch 139/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3526 - accuracy: 0.9844\n",
            "Epoch 139: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3594 - accuracy: 0.9830 - val_loss: 2.3814 - val_accuracy: 0.6757\n",
            "Epoch 140/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3480 - accuracy: 0.9688\n",
            "Epoch 140: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.3475 - accuracy: 0.9762 - val_loss: 2.3561 - val_accuracy: 0.6757\n",
            "Epoch 141/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3376 - accuracy: 0.9766\n",
            "Epoch 141: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.3367 - accuracy: 0.9864 - val_loss: 2.3442 - val_accuracy: 0.6757\n",
            "Epoch 142/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.3120 - accuracy: 0.9864\n",
            "Epoch 142: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.3120 - accuracy: 0.9864 - val_loss: 2.3441 - val_accuracy: 0.6757\n",
            "Epoch 143/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.2965 - accuracy: 0.9864\n",
            "Epoch 143: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.2965 - accuracy: 0.9864 - val_loss: 2.3399 - val_accuracy: 0.6757\n",
            "Epoch 144/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2886 - accuracy: 0.9844\n",
            "Epoch 144: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.2817 - accuracy: 0.9898 - val_loss: 2.3281 - val_accuracy: 0.6757\n",
            "Epoch 145/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.2613 - accuracy: 0.9966\n",
            "Epoch 145: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 1.2613 - accuracy: 0.9966 - val_loss: 2.3317 - val_accuracy: 0.6892\n",
            "Epoch 146/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.2520 - accuracy: 0.9883\n",
            "Epoch 146: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.2487 - accuracy: 0.9898 - val_loss: 2.3115 - val_accuracy: 0.6892\n",
            "Epoch 147/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.2436 - accuracy: 0.9830\n",
            "Epoch 147: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 1.2436 - accuracy: 0.9830 - val_loss: 2.2735 - val_accuracy: 0.6892\n",
            "Epoch 148/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.2327 - accuracy: 0.9805\n",
            "Epoch 148: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.2279 - accuracy: 0.9830 - val_loss: 2.2706 - val_accuracy: 0.6892\n",
            "Epoch 149/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.2080 - accuracy: 0.9844\n",
            "Epoch 149: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.2099 - accuracy: 0.9796 - val_loss: 2.2736 - val_accuracy: 0.6892\n",
            "Epoch 150/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.1949 - accuracy: 0.9830\n",
            "Epoch 150: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.1949 - accuracy: 0.9830 - val_loss: 2.2666 - val_accuracy: 0.6892\n",
            "Epoch 151/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1939 - accuracy: 0.9766\n",
            "Epoch 151: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1791 - accuracy: 0.9796 - val_loss: 2.2254 - val_accuracy: 0.6892\n",
            "Epoch 152/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.1648 - accuracy: 0.9898\n",
            "Epoch 152: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 1.1648 - accuracy: 0.9898 - val_loss: 2.1916 - val_accuracy: 0.6892\n",
            "Epoch 153/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1557 - accuracy: 0.9688\n",
            "Epoch 153: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1557 - accuracy: 0.9830 - val_loss: 2.1551 - val_accuracy: 0.6757\n",
            "Epoch 154/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1431 - accuracy: 0.9766\n",
            "Epoch 154: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1426 - accuracy: 0.9830 - val_loss: 2.1050 - val_accuracy: 0.6892\n",
            "Epoch 155/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1202 - accuracy: 0.9844\n",
            "Epoch 155: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.1290 - accuracy: 0.9864 - val_loss: 2.0350 - val_accuracy: 0.7027\n",
            "Epoch 156/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1272 - accuracy: 0.9609\n",
            "Epoch 156: val_accuracy improved from 0.70270 to 0.71622, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 1.1219 - accuracy: 0.9796 - val_loss: 1.9751 - val_accuracy: 0.7162\n",
            "Epoch 157/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.9864\n",
            "Epoch 157: val_accuracy improved from 0.71622 to 0.72973, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 1.0988 - accuracy: 0.9864 - val_loss: 1.9478 - val_accuracy: 0.7297\n",
            "Epoch 158/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.0827 - accuracy: 0.9883\n",
            "Epoch 158: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 1.0861 - accuracy: 0.9830 - val_loss: 1.9383 - val_accuracy: 0.7162\n",
            "Epoch 159/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0758 - accuracy: 0.9922\n",
            "Epoch 159: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0733 - accuracy: 0.9864 - val_loss: 1.9386 - val_accuracy: 0.7162\n",
            "Epoch 160/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0689 - accuracy: 1.0000\n",
            "Epoch 160: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0659 - accuracy: 0.9830 - val_loss: 1.9468 - val_accuracy: 0.7027\n",
            "Epoch 161/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0819 - accuracy: 0.9609\n",
            "Epoch 161: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0527 - accuracy: 0.9796 - val_loss: 1.9532 - val_accuracy: 0.7027\n",
            "Epoch 162/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0519 - accuracy: 0.9688\n",
            "Epoch 162: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0352 - accuracy: 0.9830 - val_loss: 1.9536 - val_accuracy: 0.7297\n",
            "Epoch 163/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0241 - accuracy: 1.0000\n",
            "Epoch 163: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.0243 - accuracy: 0.9932 - val_loss: 1.9447 - val_accuracy: 0.7297\n",
            "Epoch 164/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0297 - accuracy: 0.9844\n",
            "Epoch 164: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.0207 - accuracy: 0.9830 - val_loss: 1.9294 - val_accuracy: 0.7027\n",
            "Epoch 165/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9950 - accuracy: 0.9922\n",
            "Epoch 165: val_accuracy improved from 0.72973 to 0.74324, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 1.0012 - accuracy: 0.9830 - val_loss: 1.9349 - val_accuracy: 0.7432\n",
            "Epoch 166/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9750 - accuracy: 1.0000\n",
            "Epoch 166: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.9906 - accuracy: 0.9830 - val_loss: 1.9282 - val_accuracy: 0.7297\n",
            "Epoch 167/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9753 - accuracy: 0.9830\n",
            "Epoch 167: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.9753 - accuracy: 0.9830 - val_loss: 1.9132 - val_accuracy: 0.7297\n",
            "Epoch 168/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9617 - accuracy: 0.9922\n",
            "Epoch 168: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.9660 - accuracy: 0.9796 - val_loss: 1.9045 - val_accuracy: 0.7297\n",
            "Epoch 169/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9500 - accuracy: 0.9844\n",
            "Epoch 169: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.9588 - accuracy: 0.9830 - val_loss: 1.9143 - val_accuracy: 0.7297\n",
            "Epoch 170/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9407 - accuracy: 0.9844\n",
            "Epoch 170: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.9463 - accuracy: 0.9830 - val_loss: 1.9240 - val_accuracy: 0.7297\n",
            "Epoch 171/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9285 - accuracy: 0.9898\n",
            "Epoch 171: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.9285 - accuracy: 0.9898 - val_loss: 1.9194 - val_accuracy: 0.7297\n",
            "Epoch 172/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9242 - accuracy: 0.9922\n",
            "Epoch 172: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.9219 - accuracy: 0.9796 - val_loss: 1.9203 - val_accuracy: 0.7297\n",
            "Epoch 173/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9236 - accuracy: 0.9688\n",
            "Epoch 173: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9128 - accuracy: 0.9796 - val_loss: 1.9248 - val_accuracy: 0.7432\n",
            "Epoch 174/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.9121 - accuracy: 0.9766\n",
            "Epoch 174: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.9078 - accuracy: 0.9796 - val_loss: 1.9256 - val_accuracy: 0.7162\n",
            "Epoch 175/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9017 - accuracy: 0.9922\n",
            "Epoch 175: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.8975 - accuracy: 0.9796 - val_loss: 1.9238 - val_accuracy: 0.7162\n",
            "Epoch 176/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8853 - accuracy: 0.9830\n",
            "Epoch 176: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.8853 - accuracy: 0.9830 - val_loss: 1.9149 - val_accuracy: 0.7297\n",
            "Epoch 177/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8749 - accuracy: 0.9796\n",
            "Epoch 177: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.8749 - accuracy: 0.9796 - val_loss: 1.9027 - val_accuracy: 0.7297\n",
            "Epoch 178/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8781 - accuracy: 0.9844\n",
            "Epoch 178: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.8639 - accuracy: 0.9796 - val_loss: 1.8704 - val_accuracy: 0.7432\n",
            "Epoch 179/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8563 - accuracy: 0.9762\n",
            "Epoch 179: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.8563 - accuracy: 0.9762 - val_loss: 1.8284 - val_accuracy: 0.7297\n",
            "Epoch 180/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8451 - accuracy: 1.0000\n",
            "Epoch 180: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.8399 - accuracy: 0.9864 - val_loss: 1.8092 - val_accuracy: 0.7432\n",
            "Epoch 181/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8527 - accuracy: 0.9688\n",
            "Epoch 181: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.8322 - accuracy: 0.9830 - val_loss: 1.8103 - val_accuracy: 0.7162\n",
            "Epoch 182/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8226 - accuracy: 0.9864\n",
            "Epoch 182: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.8226 - accuracy: 0.9864 - val_loss: 1.8278 - val_accuracy: 0.7297\n",
            "Epoch 183/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8103 - accuracy: 0.9830\n",
            "Epoch 183: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.8103 - accuracy: 0.9830 - val_loss: 1.8372 - val_accuracy: 0.7297\n",
            "Epoch 184/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8007 - accuracy: 0.9844\n",
            "Epoch 184: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.8044 - accuracy: 0.9796 - val_loss: 1.8271 - val_accuracy: 0.7297\n",
            "Epoch 185/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.7990 - accuracy: 0.9805\n",
            "Epoch 185: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.7981 - accuracy: 0.9796 - val_loss: 1.8101 - val_accuracy: 0.7297\n",
            "Epoch 186/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7904 - accuracy: 0.9766\n",
            "Epoch 186: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.7851 - accuracy: 0.9796 - val_loss: 1.8074 - val_accuracy: 0.7432\n",
            "Epoch 187/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7956 - accuracy: 0.9609\n",
            "Epoch 187: val_accuracy improved from 0.74324 to 0.75676, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.7801 - accuracy: 0.9796 - val_loss: 1.7966 - val_accuracy: 0.7568\n",
            "Epoch 188/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7717 - accuracy: 0.9796\n",
            "Epoch 188: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.7717 - accuracy: 0.9796 - val_loss: 1.7700 - val_accuracy: 0.7568\n",
            "Epoch 189/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.7631 - accuracy: 0.9844\n",
            "Epoch 189: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.7598 - accuracy: 0.9864 - val_loss: 1.7485 - val_accuracy: 0.7568\n",
            "Epoch 190/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7660 - accuracy: 0.9766\n",
            "Epoch 190: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.7560 - accuracy: 0.9762 - val_loss: 1.7093 - val_accuracy: 0.7432\n",
            "Epoch 191/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7422 - accuracy: 0.9766\n",
            "Epoch 191: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.7426 - accuracy: 0.9830 - val_loss: 1.6652 - val_accuracy: 0.7297\n",
            "Epoch 192/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7636 - accuracy: 0.9766\n",
            "Epoch 192: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.7378 - accuracy: 0.9864 - val_loss: 1.6196 - val_accuracy: 0.7297\n",
            "Epoch 193/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7245 - accuracy: 0.9830\n",
            "Epoch 193: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.7245 - accuracy: 0.9830 - val_loss: 1.6003 - val_accuracy: 0.7162\n",
            "Epoch 194/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7295 - accuracy: 0.9844\n",
            "Epoch 194: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.7170 - accuracy: 0.9864 - val_loss: 1.5883 - val_accuracy: 0.6892\n",
            "Epoch 195/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7023 - accuracy: 0.9844\n",
            "Epoch 195: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.7101 - accuracy: 0.9898 - val_loss: 1.5947 - val_accuracy: 0.7027\n",
            "Epoch 196/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6872 - accuracy: 0.9922\n",
            "Epoch 196: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.6987 - accuracy: 0.9898 - val_loss: 1.5682 - val_accuracy: 0.7432\n",
            "Epoch 197/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6895 - accuracy: 0.9864\n",
            "Epoch 197: val_accuracy improved from 0.75676 to 0.77027, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.6895 - accuracy: 0.9864 - val_loss: 1.6424 - val_accuracy: 0.7703\n",
            "Epoch 198/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6784 - accuracy: 0.9844\n",
            "Epoch 198: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.6850 - accuracy: 0.9830 - val_loss: 1.7145 - val_accuracy: 0.7162\n",
            "Epoch 199/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6760 - accuracy: 0.9922\n",
            "Epoch 199: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.6777 - accuracy: 0.9796 - val_loss: 1.7648 - val_accuracy: 0.7027\n",
            "Epoch 200/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6627 - accuracy: 0.9844\n",
            "Epoch 200: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.6691 - accuracy: 0.9796 - val_loss: 1.7760 - val_accuracy: 0.7027\n",
            "Epoch 201/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6500 - accuracy: 0.9844\n",
            "Epoch 201: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.6589 - accuracy: 0.9796 - val_loss: 1.7972 - val_accuracy: 0.6892\n",
            "Epoch 202/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6457 - accuracy: 0.9844\n",
            "Epoch 202: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.6583 - accuracy: 0.9762 - val_loss: 1.8059 - val_accuracy: 0.7027\n",
            "Epoch 203/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6453 - accuracy: 0.9762\n",
            "Epoch 203: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.6453 - accuracy: 0.9762 - val_loss: 1.7938 - val_accuracy: 0.7027\n",
            "Epoch 204/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6372 - accuracy: 0.9830\n",
            "Epoch 204: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.6372 - accuracy: 0.9830 - val_loss: 1.7809 - val_accuracy: 0.7027\n",
            "Epoch 205/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6398 - accuracy: 0.9844\n",
            "Epoch 205: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6353 - accuracy: 0.9762 - val_loss: 1.7589 - val_accuracy: 0.7162\n",
            "Epoch 206/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6235 - accuracy: 0.9766\n",
            "Epoch 206: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.6249 - accuracy: 0.9796 - val_loss: 1.7503 - val_accuracy: 0.7297\n",
            "Epoch 207/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6174 - accuracy: 0.9864\n",
            "Epoch 207: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.6174 - accuracy: 0.9864 - val_loss: 1.8069 - val_accuracy: 0.7297\n",
            "Epoch 208/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6119 - accuracy: 0.9922\n",
            "Epoch 208: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.6081 - accuracy: 0.9898 - val_loss: 1.8358 - val_accuracy: 0.7297\n",
            "Epoch 209/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6088 - accuracy: 0.9844\n",
            "Epoch 209: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.6029 - accuracy: 0.9796 - val_loss: 1.9086 - val_accuracy: 0.7027\n",
            "Epoch 210/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5874 - accuracy: 0.9766\n",
            "Epoch 210: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.5961 - accuracy: 0.9796 - val_loss: 1.9518 - val_accuracy: 0.7027\n",
            "Epoch 211/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.9830\n",
            "Epoch 211: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.5877 - accuracy: 0.9830 - val_loss: 1.9230 - val_accuracy: 0.7027\n",
            "Epoch 212/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5830 - accuracy: 0.9864\n",
            "Epoch 212: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.5830 - accuracy: 0.9864 - val_loss: 1.8625 - val_accuracy: 0.7027\n",
            "Epoch 213/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5760 - accuracy: 0.9830\n",
            "Epoch 213: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.5760 - accuracy: 0.9830 - val_loss: 1.7696 - val_accuracy: 0.7297\n",
            "Epoch 214/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5599 - accuracy: 0.9844\n",
            "Epoch 214: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.5663 - accuracy: 0.9830 - val_loss: 1.6813 - val_accuracy: 0.7568\n",
            "Epoch 215/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5538 - accuracy: 0.9922\n",
            "Epoch 215: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5647 - accuracy: 0.9830 - val_loss: 1.6288 - val_accuracy: 0.7703\n",
            "Epoch 216/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5708 - accuracy: 0.9766\n",
            "Epoch 216: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.5634 - accuracy: 0.9796 - val_loss: 1.6189 - val_accuracy: 0.7703\n",
            "Epoch 217/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5446 - accuracy: 0.9844\n",
            "Epoch 217: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5518 - accuracy: 0.9796 - val_loss: 1.5847 - val_accuracy: 0.7703\n",
            "Epoch 218/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5524 - accuracy: 0.9922\n",
            "Epoch 218: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5434 - accuracy: 0.9864 - val_loss: 1.5714 - val_accuracy: 0.7703\n",
            "Epoch 219/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5296 - accuracy: 0.9844\n",
            "Epoch 219: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5407 - accuracy: 0.9796 - val_loss: 1.5626 - val_accuracy: 0.7297\n",
            "Epoch 220/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5373 - accuracy: 0.9766\n",
            "Epoch 220: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5390 - accuracy: 0.9796 - val_loss: 1.5271 - val_accuracy: 0.7297\n",
            "Epoch 221/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5303 - accuracy: 0.9805\n",
            "Epoch 221: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.5269 - accuracy: 0.9830 - val_loss: 1.5127 - val_accuracy: 0.7162\n",
            "Epoch 222/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5135 - accuracy: 0.9844\n",
            "Epoch 222: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.5190 - accuracy: 0.9830 - val_loss: 1.4854 - val_accuracy: 0.7432\n",
            "Epoch 223/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5154 - accuracy: 0.9844\n",
            "Epoch 223: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.5149 - accuracy: 0.9830 - val_loss: 1.4530 - val_accuracy: 0.7432\n",
            "Epoch 224/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.9830\n",
            "Epoch 224: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.5112 - accuracy: 0.9830 - val_loss: 1.4313 - val_accuracy: 0.7297\n",
            "Epoch 225/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5172 - accuracy: 0.9844\n",
            "Epoch 225: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.5034 - accuracy: 0.9830 - val_loss: 1.4172 - val_accuracy: 0.7297\n",
            "Epoch 226/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5012 - accuracy: 0.9766\n",
            "Epoch 226: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4979 - accuracy: 0.9830 - val_loss: 1.4098 - val_accuracy: 0.7432\n",
            "Epoch 227/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4821 - accuracy: 0.9844\n",
            "Epoch 227: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4934 - accuracy: 0.9796 - val_loss: 1.4066 - val_accuracy: 0.7432\n",
            "Epoch 228/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4781 - accuracy: 0.9844\n",
            "Epoch 228: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4845 - accuracy: 0.9796 - val_loss: 1.3968 - val_accuracy: 0.7432\n",
            "Epoch 229/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4692 - accuracy: 0.9844\n",
            "Epoch 229: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4791 - accuracy: 0.9796 - val_loss: 1.3956 - val_accuracy: 0.7432\n",
            "Epoch 230/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4771 - accuracy: 0.9796\n",
            "Epoch 230: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.4771 - accuracy: 0.9796 - val_loss: 1.3988 - val_accuracy: 0.7432\n",
            "Epoch 231/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4670 - accuracy: 0.9864\n",
            "Epoch 231: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.4670 - accuracy: 0.9864 - val_loss: 1.4059 - val_accuracy: 0.7432\n",
            "Epoch 232/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4670 - accuracy: 0.9844\n",
            "Epoch 232: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4664 - accuracy: 0.9830 - val_loss: 1.4074 - val_accuracy: 0.7432\n",
            "Epoch 233/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4595 - accuracy: 0.9766\n",
            "Epoch 233: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4581 - accuracy: 0.9830 - val_loss: 1.4167 - val_accuracy: 0.7297\n",
            "Epoch 234/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4721 - accuracy: 0.9844\n",
            "Epoch 234: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4574 - accuracy: 0.9796 - val_loss: 1.4279 - val_accuracy: 0.7162\n",
            "Epoch 235/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4646 - accuracy: 0.9766\n",
            "Epoch 235: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4533 - accuracy: 0.9796 - val_loss: 1.4647 - val_accuracy: 0.7027\n",
            "Epoch 236/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4458 - accuracy: 0.9844\n",
            "Epoch 236: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4468 - accuracy: 0.9796 - val_loss: 1.4669 - val_accuracy: 0.7432\n",
            "Epoch 237/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4295 - accuracy: 0.9844\n",
            "Epoch 237: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4407 - accuracy: 0.9796 - val_loss: 1.4545 - val_accuracy: 0.7432\n",
            "Epoch 238/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4226 - accuracy: 0.9922\n",
            "Epoch 238: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4417 - accuracy: 0.9796 - val_loss: 1.4742 - val_accuracy: 0.7297\n",
            "Epoch 239/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4327 - accuracy: 0.9830\n",
            "Epoch 239: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.4327 - accuracy: 0.9830 - val_loss: 1.5477 - val_accuracy: 0.7297\n",
            "Epoch 240/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4181 - accuracy: 0.9844\n",
            "Epoch 240: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4271 - accuracy: 0.9830 - val_loss: 1.6373 - val_accuracy: 0.7297\n",
            "Epoch 241/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4415 - accuracy: 0.9688\n",
            "Epoch 241: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4256 - accuracy: 0.9830 - val_loss: 1.7219 - val_accuracy: 0.7162\n",
            "Epoch 242/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4121 - accuracy: 0.9844\n",
            "Epoch 242: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4208 - accuracy: 0.9796 - val_loss: 1.7422 - val_accuracy: 0.7162\n",
            "Epoch 243/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4080 - accuracy: 0.9766\n",
            "Epoch 243: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4138 - accuracy: 0.9830 - val_loss: 1.7089 - val_accuracy: 0.7162\n",
            "Epoch 244/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.9762\n",
            "Epoch 244: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.4117 - accuracy: 0.9762 - val_loss: 1.6561 - val_accuracy: 0.7027\n",
            "Epoch 245/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4145 - accuracy: 0.9766\n",
            "Epoch 245: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4037 - accuracy: 0.9796 - val_loss: 1.5927 - val_accuracy: 0.7027\n",
            "Epoch 246/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4099 - accuracy: 0.9766\n",
            "Epoch 246: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3996 - accuracy: 0.9796 - val_loss: 1.5082 - val_accuracy: 0.7162\n",
            "Epoch 247/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3985 - accuracy: 0.9766\n",
            "Epoch 247: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.3953 - accuracy: 0.9796 - val_loss: 1.4634 - val_accuracy: 0.7432\n",
            "Epoch 248/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4051 - accuracy: 0.9844\n",
            "Epoch 248: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3921 - accuracy: 0.9830 - val_loss: 1.4516 - val_accuracy: 0.7297\n",
            "Epoch 249/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4006 - accuracy: 0.9609\n",
            "Epoch 249: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3861 - accuracy: 0.9796 - val_loss: 1.4542 - val_accuracy: 0.7297\n",
            "Epoch 250/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4174 - accuracy: 0.9531\n",
            "Epoch 250: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3874 - accuracy: 0.9762 - val_loss: 1.4400 - val_accuracy: 0.7162\n",
            "Epoch 251/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3835 - accuracy: 0.9844\n",
            "Epoch 251: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3740 - accuracy: 0.9830 - val_loss: 1.4449 - val_accuracy: 0.7162\n",
            "Epoch 252/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3795 - accuracy: 0.9922\n",
            "Epoch 252: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.3705 - accuracy: 0.9864 - val_loss: 1.4482 - val_accuracy: 0.7297\n",
            "Epoch 253/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3617 - accuracy: 0.9766\n",
            "Epoch 253: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3684 - accuracy: 0.9796 - val_loss: 1.4580 - val_accuracy: 0.7297\n",
            "Epoch 254/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3784 - accuracy: 0.9609\n",
            "Epoch 254: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.3650 - accuracy: 0.9830 - val_loss: 1.4768 - val_accuracy: 0.7297\n",
            "Epoch 255/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3641 - accuracy: 0.9844\n",
            "Epoch 255: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3605 - accuracy: 0.9864 - val_loss: 1.5100 - val_accuracy: 0.7297\n",
            "Epoch 256/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3608 - accuracy: 1.0000\n",
            "Epoch 256: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3559 - accuracy: 0.9932 - val_loss: 1.5192 - val_accuracy: 0.7297\n",
            "Epoch 257/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3436 - accuracy: 1.0000\n",
            "Epoch 257: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3528 - accuracy: 0.9864 - val_loss: 1.5231 - val_accuracy: 0.7297\n",
            "Epoch 258/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3516 - accuracy: 0.9922\n",
            "Epoch 258: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3518 - accuracy: 0.9864 - val_loss: 1.5022 - val_accuracy: 0.7297\n",
            "Epoch 259/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3491 - accuracy: 0.9844\n",
            "Epoch 259: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3440 - accuracy: 0.9864 - val_loss: 1.4861 - val_accuracy: 0.7297\n",
            "Epoch 260/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3507 - accuracy: 0.9844\n",
            "Epoch 260: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3435 - accuracy: 0.9796 - val_loss: 1.4577 - val_accuracy: 0.7432\n",
            "Epoch 261/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3598 - accuracy: 0.9609\n",
            "Epoch 261: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3399 - accuracy: 0.9762 - val_loss: 1.4392 - val_accuracy: 0.7432\n",
            "Epoch 262/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3637 - accuracy: 0.9688\n",
            "Epoch 262: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3408 - accuracy: 0.9762 - val_loss: 1.4020 - val_accuracy: 0.7432\n",
            "Epoch 263/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.9864\n",
            "Epoch 263: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.3301 - accuracy: 0.9864 - val_loss: 1.3803 - val_accuracy: 0.7432\n",
            "Epoch 264/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3467 - accuracy: 0.9766\n",
            "Epoch 264: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3297 - accuracy: 0.9830 - val_loss: 1.3577 - val_accuracy: 0.7432\n",
            "Epoch 265/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3259 - accuracy: 0.9796\n",
            "Epoch 265: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.3259 - accuracy: 0.9796 - val_loss: 1.3328 - val_accuracy: 0.7568\n",
            "Epoch 266/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3267 - accuracy: 0.9922\n",
            "Epoch 266: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3191 - accuracy: 0.9830 - val_loss: 1.3131 - val_accuracy: 0.7568\n",
            "Epoch 267/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3337 - accuracy: 0.9609\n",
            "Epoch 267: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3157 - accuracy: 0.9796 - val_loss: 1.3057 - val_accuracy: 0.7568\n",
            "Epoch 268/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.9796\n",
            "Epoch 268: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.3138 - accuracy: 0.9796 - val_loss: 1.3117 - val_accuracy: 0.7432\n",
            "Epoch 269/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3258 - accuracy: 0.9844\n",
            "Epoch 269: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.3151 - accuracy: 0.9830 - val_loss: 1.3192 - val_accuracy: 0.7162\n",
            "Epoch 270/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3089 - accuracy: 0.9844\n",
            "Epoch 270: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3090 - accuracy: 0.9830 - val_loss: 1.3169 - val_accuracy: 0.7027\n",
            "Epoch 271/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3106 - accuracy: 0.9766\n",
            "Epoch 271: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3028 - accuracy: 0.9796 - val_loss: 1.3085 - val_accuracy: 0.7027\n",
            "Epoch 272/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.9762\n",
            "Epoch 272: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.3078 - accuracy: 0.9762 - val_loss: 1.2874 - val_accuracy: 0.7297\n",
            "Epoch 273/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3140 - accuracy: 0.9766\n",
            "Epoch 273: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3011 - accuracy: 0.9796 - val_loss: 1.2779 - val_accuracy: 0.7432\n",
            "Epoch 274/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9796\n",
            "Epoch 274: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.2942 - accuracy: 0.9796 - val_loss: 1.2793 - val_accuracy: 0.7432\n",
            "Epoch 275/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3004 - accuracy: 0.9844\n",
            "Epoch 275: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2923 - accuracy: 0.9796 - val_loss: 1.2894 - val_accuracy: 0.7432\n",
            "Epoch 276/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.9830\n",
            "Epoch 276: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.2922 - accuracy: 0.9830 - val_loss: 1.2954 - val_accuracy: 0.7432\n",
            "Epoch 277/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2866 - accuracy: 0.9805\n",
            "Epoch 277: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.2873 - accuracy: 0.9796 - val_loss: 1.3011 - val_accuracy: 0.7432\n",
            "Epoch 278/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2817 - accuracy: 0.9766\n",
            "Epoch 278: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2833 - accuracy: 0.9796 - val_loss: 1.2986 - val_accuracy: 0.7568\n",
            "Epoch 279/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2808 - accuracy: 0.9766\n",
            "Epoch 279: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2819 - accuracy: 0.9830 - val_loss: 1.2992 - val_accuracy: 0.7568\n",
            "Epoch 280/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2652 - accuracy: 1.0000\n",
            "Epoch 280: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2757 - accuracy: 0.9830 - val_loss: 1.2863 - val_accuracy: 0.7568\n",
            "Epoch 281/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2863 - accuracy: 0.9688\n",
            "Epoch 281: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2703 - accuracy: 0.9830 - val_loss: 1.2717 - val_accuracy: 0.7297\n",
            "Epoch 282/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2540 - accuracy: 1.0000\n",
            "Epoch 282: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2764 - accuracy: 0.9898 - val_loss: 1.2550 - val_accuracy: 0.7297\n",
            "Epoch 283/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2570 - accuracy: 0.9922\n",
            "Epoch 283: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2700 - accuracy: 0.9898 - val_loss: 1.2280 - val_accuracy: 0.7568\n",
            "Epoch 284/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2622 - accuracy: 0.9922\n",
            "Epoch 284: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2653 - accuracy: 0.9898 - val_loss: 1.2295 - val_accuracy: 0.7703\n",
            "Epoch 285/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.9796\n",
            "Epoch 285: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.2628 - accuracy: 0.9796 - val_loss: 1.2622 - val_accuracy: 0.7568\n",
            "Epoch 286/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2685 - accuracy: 0.9766\n",
            "Epoch 286: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2657 - accuracy: 0.9796 - val_loss: 1.2934 - val_accuracy: 0.7162\n",
            "Epoch 287/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2572 - accuracy: 0.9844\n",
            "Epoch 287: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2564 - accuracy: 0.9796 - val_loss: 1.3114 - val_accuracy: 0.7027\n",
            "Epoch 288/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2690 - accuracy: 0.9609\n",
            "Epoch 288: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2570 - accuracy: 0.9796 - val_loss: 1.2947 - val_accuracy: 0.7162\n",
            "Epoch 289/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2513 - accuracy: 0.9766\n",
            "Epoch 289: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2493 - accuracy: 0.9796 - val_loss: 1.2790 - val_accuracy: 0.7162\n",
            "Epoch 290/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.9796\n",
            "Epoch 290: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.2492 - accuracy: 0.9796 - val_loss: 1.2711 - val_accuracy: 0.7162\n",
            "Epoch 291/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2523 - accuracy: 0.9766\n",
            "Epoch 291: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2487 - accuracy: 0.9830 - val_loss: 1.2683 - val_accuracy: 0.7162\n",
            "Epoch 292/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2606 - accuracy: 0.9766\n",
            "Epoch 292: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2472 - accuracy: 0.9796 - val_loss: 1.2689 - val_accuracy: 0.7162\n",
            "Epoch 293/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2524 - accuracy: 0.9766\n",
            "Epoch 293: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2392 - accuracy: 0.9830 - val_loss: 1.2620 - val_accuracy: 0.7162\n",
            "Epoch 294/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2311 - accuracy: 0.9844\n",
            "Epoch 294: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.2370 - accuracy: 0.9864 - val_loss: 1.2515 - val_accuracy: 0.7297\n",
            "Epoch 295/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.9796\n",
            "Epoch 295: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.2386 - accuracy: 0.9796 - val_loss: 1.2704 - val_accuracy: 0.7297\n",
            "Epoch 296/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2432 - accuracy: 0.9844\n",
            "Epoch 296: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2354 - accuracy: 0.9830 - val_loss: 1.2877 - val_accuracy: 0.7297\n",
            "Epoch 297/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2463 - accuracy: 0.9844\n",
            "Epoch 297: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2367 - accuracy: 0.9762 - val_loss: 1.2931 - val_accuracy: 0.7297\n",
            "Epoch 298/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2233 - accuracy: 0.9805\n",
            "Epoch 298: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.2292 - accuracy: 0.9830 - val_loss: 1.2979 - val_accuracy: 0.7297\n",
            "Epoch 299/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2347 - accuracy: 0.9688\n",
            "Epoch 299: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2307 - accuracy: 0.9762 - val_loss: 1.3031 - val_accuracy: 0.7432\n",
            "Epoch 300/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 0.9796\n",
            "Epoch 300: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.2274 - accuracy: 0.9796 - val_loss: 1.2780 - val_accuracy: 0.7432\n",
            "Epoch 301/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2228 - accuracy: 0.9805\n",
            "Epoch 301: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.2240 - accuracy: 0.9796 - val_loss: 1.2614 - val_accuracy: 0.7297\n",
            "Epoch 302/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.9796\n",
            "Epoch 302: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.2221 - accuracy: 0.9796 - val_loss: 1.2259 - val_accuracy: 0.7297\n",
            "Epoch 303/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2144 - accuracy: 0.9766\n",
            "Epoch 303: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2198 - accuracy: 0.9864 - val_loss: 1.1963 - val_accuracy: 0.7297\n",
            "Epoch 304/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2218 - accuracy: 0.9688\n",
            "Epoch 304: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2183 - accuracy: 0.9796 - val_loss: 1.1813 - val_accuracy: 0.7432\n",
            "Epoch 305/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2083 - accuracy: 0.9922\n",
            "Epoch 305: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2151 - accuracy: 0.9796 - val_loss: 1.1705 - val_accuracy: 0.7432\n",
            "Epoch 306/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9796\n",
            "Epoch 306: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.2132 - accuracy: 0.9796 - val_loss: 1.1630 - val_accuracy: 0.7432\n",
            "Epoch 307/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2158 - accuracy: 0.9766\n",
            "Epoch 307: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2085 - accuracy: 0.9796 - val_loss: 1.1501 - val_accuracy: 0.7432\n",
            "Epoch 308/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2082 - accuracy: 0.9830\n",
            "Epoch 308: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.2082 - accuracy: 0.9830 - val_loss: 1.1547 - val_accuracy: 0.7568\n",
            "Epoch 309/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2192 - accuracy: 0.9844\n",
            "Epoch 309: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2108 - accuracy: 0.9830 - val_loss: 1.1606 - val_accuracy: 0.7703\n",
            "Epoch 310/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9762\n",
            "Epoch 310: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.2065 - accuracy: 0.9762 - val_loss: 1.1698 - val_accuracy: 0.7568\n",
            "Epoch 311/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1985 - accuracy: 0.9766\n",
            "Epoch 311: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2042 - accuracy: 0.9830 - val_loss: 1.1963 - val_accuracy: 0.7703\n",
            "Epoch 312/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1951 - accuracy: 0.9766\n",
            "Epoch 312: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2037 - accuracy: 0.9796 - val_loss: 1.2395 - val_accuracy: 0.7703\n",
            "Epoch 313/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1901 - accuracy: 0.9922\n",
            "Epoch 313: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2033 - accuracy: 0.9796 - val_loss: 1.2597 - val_accuracy: 0.7703\n",
            "Epoch 314/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1823 - accuracy: 0.9922\n",
            "Epoch 314: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2029 - accuracy: 0.9796 - val_loss: 1.2646 - val_accuracy: 0.7703\n",
            "Epoch 315/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1969 - accuracy: 0.9844\n",
            "Epoch 315: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1972 - accuracy: 0.9796 - val_loss: 1.2431 - val_accuracy: 0.7703\n",
            "Epoch 316/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1988 - accuracy: 0.9766\n",
            "Epoch 316: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1907 - accuracy: 0.9796 - val_loss: 1.2116 - val_accuracy: 0.7568\n",
            "Epoch 317/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1927 - accuracy: 0.9688\n",
            "Epoch 317: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1937 - accuracy: 0.9762 - val_loss: 1.1742 - val_accuracy: 0.7568\n",
            "Epoch 318/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.9830\n",
            "Epoch 318: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1897 - accuracy: 0.9830 - val_loss: 1.1639 - val_accuracy: 0.7703\n",
            "Epoch 319/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1719 - accuracy: 0.9922\n",
            "Epoch 319: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1872 - accuracy: 0.9796 - val_loss: 1.1623 - val_accuracy: 0.7703\n",
            "Epoch 320/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2040 - accuracy: 0.9766\n",
            "Epoch 320: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1864 - accuracy: 0.9830 - val_loss: 1.1618 - val_accuracy: 0.7703\n",
            "Epoch 321/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1908 - accuracy: 0.9922\n",
            "Epoch 321: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1841 - accuracy: 0.9796 - val_loss: 1.1785 - val_accuracy: 0.7568\n",
            "Epoch 322/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1689 - accuracy: 0.9844\n",
            "Epoch 322: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1914 - accuracy: 0.9762 - val_loss: 1.2709 - val_accuracy: 0.7297\n",
            "Epoch 323/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1765 - accuracy: 0.9688\n",
            "Epoch 323: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1774 - accuracy: 0.9796 - val_loss: 1.4228 - val_accuracy: 0.7027\n",
            "Epoch 324/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1849 - accuracy: 0.9766\n",
            "Epoch 324: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1815 - accuracy: 0.9796 - val_loss: 1.5520 - val_accuracy: 0.6892\n",
            "Epoch 325/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1666 - accuracy: 0.9844\n",
            "Epoch 325: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1783 - accuracy: 0.9796 - val_loss: 1.5854 - val_accuracy: 0.6892\n",
            "Epoch 326/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1677 - accuracy: 0.9922\n",
            "Epoch 326: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1793 - accuracy: 0.9830 - val_loss: 1.5659 - val_accuracy: 0.6757\n",
            "Epoch 327/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9796\n",
            "Epoch 327: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1746 - accuracy: 0.9796 - val_loss: 1.4963 - val_accuracy: 0.6757\n",
            "Epoch 328/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1798 - accuracy: 0.9609\n",
            "Epoch 328: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1736 - accuracy: 0.9830 - val_loss: 1.4026 - val_accuracy: 0.6892\n",
            "Epoch 329/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1931 - accuracy: 0.9688\n",
            "Epoch 329: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1727 - accuracy: 0.9830 - val_loss: 1.3404 - val_accuracy: 0.7027\n",
            "Epoch 330/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9864\n",
            "Epoch 330: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1724 - accuracy: 0.9864 - val_loss: 1.3092 - val_accuracy: 0.6892\n",
            "Epoch 331/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1731 - accuracy: 0.9922\n",
            "Epoch 331: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1682 - accuracy: 0.9864 - val_loss: 1.2799 - val_accuracy: 0.6892\n",
            "Epoch 332/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1667 - accuracy: 0.9688\n",
            "Epoch 332: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1649 - accuracy: 0.9830 - val_loss: 1.2792 - val_accuracy: 0.6892\n",
            "Epoch 333/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1636 - accuracy: 0.9844\n",
            "Epoch 333: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1658 - accuracy: 0.9796 - val_loss: 1.2979 - val_accuracy: 0.7027\n",
            "Epoch 334/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1542 - accuracy: 1.0000\n",
            "Epoch 334: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1631 - accuracy: 0.9796 - val_loss: 1.2994 - val_accuracy: 0.7027\n",
            "Epoch 335/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1509 - accuracy: 0.9844\n",
            "Epoch 335: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1612 - accuracy: 0.9796 - val_loss: 1.3080 - val_accuracy: 0.7162\n",
            "Epoch 336/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1704 - accuracy: 0.9844\n",
            "Epoch 336: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1617 - accuracy: 0.9796 - val_loss: 1.3139 - val_accuracy: 0.7297\n",
            "Epoch 337/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1561 - accuracy: 0.9766\n",
            "Epoch 337: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1573 - accuracy: 0.9796 - val_loss: 1.2913 - val_accuracy: 0.7432\n",
            "Epoch 338/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1516 - accuracy: 0.9922\n",
            "Epoch 338: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1541 - accuracy: 0.9796 - val_loss: 1.2676 - val_accuracy: 0.7703\n",
            "Epoch 339/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9796\n",
            "Epoch 339: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1572 - accuracy: 0.9796 - val_loss: 1.2080 - val_accuracy: 0.7703\n",
            "Epoch 340/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1502 - accuracy: 0.9766\n",
            "Epoch 340: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1552 - accuracy: 0.9796 - val_loss: 1.1601 - val_accuracy: 0.7432\n",
            "Epoch 341/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1552 - accuracy: 0.9796\n",
            "Epoch 341: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1552 - accuracy: 0.9796 - val_loss: 1.1307 - val_accuracy: 0.7432\n",
            "Epoch 342/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1592 - accuracy: 0.9922\n",
            "Epoch 342: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1524 - accuracy: 0.9830 - val_loss: 1.1168 - val_accuracy: 0.7432\n",
            "Epoch 343/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1400 - accuracy: 0.9844\n",
            "Epoch 343: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1526 - accuracy: 0.9864 - val_loss: 1.0964 - val_accuracy: 0.7703\n",
            "Epoch 344/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1558 - accuracy: 0.9766\n",
            "Epoch 344: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1489 - accuracy: 0.9796 - val_loss: 1.1490 - val_accuracy: 0.7027\n",
            "Epoch 345/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1428 - accuracy: 0.9766\n",
            "Epoch 345: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1474 - accuracy: 0.9830 - val_loss: 1.2241 - val_accuracy: 0.6622\n",
            "Epoch 346/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1542 - accuracy: 0.9766\n",
            "Epoch 346: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1461 - accuracy: 0.9796 - val_loss: 1.2628 - val_accuracy: 0.6757\n",
            "Epoch 347/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1488 - accuracy: 0.9922\n",
            "Epoch 347: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1465 - accuracy: 0.9864 - val_loss: 1.2271 - val_accuracy: 0.6757\n",
            "Epoch 348/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9796\n",
            "Epoch 348: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1454 - accuracy: 0.9796 - val_loss: 1.1802 - val_accuracy: 0.6757\n",
            "Epoch 349/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9830\n",
            "Epoch 349: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1437 - accuracy: 0.9830 - val_loss: 1.1498 - val_accuracy: 0.6892\n",
            "Epoch 350/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1248 - accuracy: 0.9922\n",
            "Epoch 350: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1403 - accuracy: 0.9830 - val_loss: 1.1448 - val_accuracy: 0.7162\n",
            "Epoch 351/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1556 - accuracy: 0.9688\n",
            "Epoch 351: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1410 - accuracy: 0.9796 - val_loss: 1.1493 - val_accuracy: 0.7162\n",
            "Epoch 352/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1358 - accuracy: 0.9844\n",
            "Epoch 352: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1381 - accuracy: 0.9796 - val_loss: 1.1516 - val_accuracy: 0.7162\n",
            "Epoch 353/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9796\n",
            "Epoch 353: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.1369 - accuracy: 0.9796 - val_loss: 1.1574 - val_accuracy: 0.7162\n",
            "Epoch 354/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1247 - accuracy: 0.9844\n",
            "Epoch 354: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1363 - accuracy: 0.9796 - val_loss: 1.1756 - val_accuracy: 0.7027\n",
            "Epoch 355/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1463 - accuracy: 0.9688\n",
            "Epoch 355: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1333 - accuracy: 0.9796 - val_loss: 1.2301 - val_accuracy: 0.6757\n",
            "Epoch 356/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9796\n",
            "Epoch 356: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1315 - accuracy: 0.9796 - val_loss: 1.2219 - val_accuracy: 0.6892\n",
            "Epoch 357/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1461 - accuracy: 0.9766\n",
            "Epoch 357: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1306 - accuracy: 0.9864 - val_loss: 1.1856 - val_accuracy: 0.6892\n",
            "Epoch 358/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9796\n",
            "Epoch 358: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.1335 - accuracy: 0.9796 - val_loss: 1.1498 - val_accuracy: 0.7027\n",
            "Epoch 359/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9830\n",
            "Epoch 359: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1306 - accuracy: 0.9830 - val_loss: 1.0976 - val_accuracy: 0.7162\n",
            "Epoch 360/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1121 - accuracy: 0.9922\n",
            "Epoch 360: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1300 - accuracy: 0.9864 - val_loss: 1.0764 - val_accuracy: 0.7432\n",
            "Epoch 361/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1329 - accuracy: 0.9766\n",
            "Epoch 361: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1263 - accuracy: 0.9796 - val_loss: 1.0687 - val_accuracy: 0.7703\n",
            "Epoch 362/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1287 - accuracy: 0.9805\n",
            "Epoch 362: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1261 - accuracy: 0.9830 - val_loss: 1.0917 - val_accuracy: 0.7568\n",
            "Epoch 363/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9796\n",
            "Epoch 363: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1242 - accuracy: 0.9796 - val_loss: 1.1156 - val_accuracy: 0.7703\n",
            "Epoch 364/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1325 - accuracy: 0.9766\n",
            "Epoch 364: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1230 - accuracy: 0.9796 - val_loss: 1.1375 - val_accuracy: 0.7703\n",
            "Epoch 365/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1253 - accuracy: 0.9805\n",
            "Epoch 365: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1248 - accuracy: 0.9796 - val_loss: 1.1437 - val_accuracy: 0.7568\n",
            "Epoch 366/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1240 - accuracy: 0.9766\n",
            "Epoch 366: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1234 - accuracy: 0.9830 - val_loss: 1.1544 - val_accuracy: 0.7568\n",
            "Epoch 367/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9864\n",
            "Epoch 367: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1242 - accuracy: 0.9864 - val_loss: 1.1649 - val_accuracy: 0.7568\n",
            "Epoch 368/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1114 - accuracy: 1.0000\n",
            "Epoch 368: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1241 - accuracy: 0.9796 - val_loss: 1.1721 - val_accuracy: 0.7568\n",
            "Epoch 369/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1227 - accuracy: 0.9766\n",
            "Epoch 369: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1192 - accuracy: 0.9796 - val_loss: 1.1946 - val_accuracy: 0.7297\n",
            "Epoch 370/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1347 - accuracy: 0.9766\n",
            "Epoch 370: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1209 - accuracy: 0.9796 - val_loss: 1.2029 - val_accuracy: 0.7297\n",
            "Epoch 371/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1182 - accuracy: 0.9688\n",
            "Epoch 371: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1195 - accuracy: 0.9796 - val_loss: 1.2100 - val_accuracy: 0.7432\n",
            "Epoch 372/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1339 - accuracy: 0.9688\n",
            "Epoch 372: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1175 - accuracy: 0.9796 - val_loss: 1.2180 - val_accuracy: 0.7432\n",
            "Epoch 373/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1139 - accuracy: 0.9805\n",
            "Epoch 373: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1130 - accuracy: 0.9796 - val_loss: 1.2103 - val_accuracy: 0.7432\n",
            "Epoch 374/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.9762\n",
            "Epoch 374: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1174 - accuracy: 0.9762 - val_loss: 1.1936 - val_accuracy: 0.7432\n",
            "Epoch 375/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1028 - accuracy: 0.9922\n",
            "Epoch 375: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1154 - accuracy: 0.9796 - val_loss: 1.1761 - val_accuracy: 0.7432\n",
            "Epoch 376/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1179 - accuracy: 0.9766\n",
            "Epoch 376: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1130 - accuracy: 0.9796 - val_loss: 1.1535 - val_accuracy: 0.7432\n",
            "Epoch 377/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1392 - accuracy: 0.9531\n",
            "Epoch 377: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1129 - accuracy: 0.9796 - val_loss: 1.1380 - val_accuracy: 0.7432\n",
            "Epoch 378/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1213 - accuracy: 0.9688\n",
            "Epoch 378: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1113 - accuracy: 0.9830 - val_loss: 1.1325 - val_accuracy: 0.7162\n",
            "Epoch 379/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1112 - accuracy: 0.9922\n",
            "Epoch 379: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1110 - accuracy: 0.9830 - val_loss: 1.1257 - val_accuracy: 0.7297\n",
            "Epoch 380/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9762\n",
            "Epoch 380: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1152 - accuracy: 0.9762 - val_loss: 1.1242 - val_accuracy: 0.7162\n",
            "Epoch 381/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1095 - accuracy: 0.9844\n",
            "Epoch 381: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1075 - accuracy: 0.9864 - val_loss: 1.1287 - val_accuracy: 0.7162\n",
            "Epoch 382/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1109 - accuracy: 0.9922\n",
            "Epoch 382: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1069 - accuracy: 0.9796 - val_loss: 1.1347 - val_accuracy: 0.7162\n",
            "Epoch 383/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0937 - accuracy: 0.9922\n",
            "Epoch 383: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1036 - accuracy: 0.9864 - val_loss: 1.1396 - val_accuracy: 0.7027\n",
            "Epoch 384/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0907 - accuracy: 1.0000\n",
            "Epoch 384: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1049 - accuracy: 0.9898 - val_loss: 1.1656 - val_accuracy: 0.7568\n",
            "Epoch 385/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0954 - accuracy: 0.9844\n",
            "Epoch 385: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1028 - accuracy: 0.9830 - val_loss: 1.2376 - val_accuracy: 0.7568\n",
            "Epoch 386/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1072 - accuracy: 0.9830\n",
            "Epoch 386: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1072 - accuracy: 0.9830 - val_loss: 1.3116 - val_accuracy: 0.7703\n",
            "Epoch 387/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0988 - accuracy: 0.9922\n",
            "Epoch 387: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1048 - accuracy: 0.9796 - val_loss: 1.3797 - val_accuracy: 0.7432\n",
            "Epoch 388/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9796\n",
            "Epoch 388: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1042 - accuracy: 0.9796 - val_loss: 1.4221 - val_accuracy: 0.7027\n",
            "Epoch 389/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0989 - accuracy: 0.9844\n",
            "Epoch 389: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1051 - accuracy: 0.9796 - val_loss: 1.4386 - val_accuracy: 0.7027\n",
            "Epoch 390/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0918 - accuracy: 0.9922\n",
            "Epoch 390: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1028 - accuracy: 0.9796 - val_loss: 1.4548 - val_accuracy: 0.6757\n",
            "Epoch 391/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0933 - accuracy: 0.9766\n",
            "Epoch 391: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1031 - accuracy: 0.9796 - val_loss: 1.5410 - val_accuracy: 0.6486\n",
            "Epoch 392/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0937 - accuracy: 0.9766\n",
            "Epoch 392: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1018 - accuracy: 0.9796 - val_loss: 1.6611 - val_accuracy: 0.6757\n",
            "Epoch 393/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9796\n",
            "Epoch 393: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.1014 - accuracy: 0.9796 - val_loss: 1.7391 - val_accuracy: 0.6757\n",
            "Epoch 394/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0870 - accuracy: 0.9844\n",
            "Epoch 394: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0998 - accuracy: 0.9796 - val_loss: 1.7490 - val_accuracy: 0.6892\n",
            "Epoch 395/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0891 - accuracy: 0.9922\n",
            "Epoch 395: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1025 - accuracy: 0.9796 - val_loss: 1.7174 - val_accuracy: 0.6892\n",
            "Epoch 396/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0892 - accuracy: 0.9844\n",
            "Epoch 396: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0976 - accuracy: 0.9796 - val_loss: 1.6245 - val_accuracy: 0.6892\n",
            "Epoch 397/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0874 - accuracy: 0.9844\n",
            "Epoch 397: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0971 - accuracy: 0.9796 - val_loss: 1.5516 - val_accuracy: 0.6757\n",
            "Epoch 398/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1000 - accuracy: 0.9766\n",
            "Epoch 398: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0986 - accuracy: 0.9796 - val_loss: 1.4730 - val_accuracy: 0.6622\n",
            "Epoch 399/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0978 - accuracy: 0.9922\n",
            "Epoch 399: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0956 - accuracy: 0.9830 - val_loss: 1.3779 - val_accuracy: 0.6757\n",
            "Epoch 400/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0762 - accuracy: 1.0000\n",
            "Epoch 400: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0955 - accuracy: 0.9830 - val_loss: 1.3122 - val_accuracy: 0.6892\n",
            "Epoch 401/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0799 - accuracy: 0.9844\n",
            "Epoch 401: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0975 - accuracy: 0.9762 - val_loss: 1.2697 - val_accuracy: 0.6892\n",
            "Epoch 402/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0998 - accuracy: 0.9844\n",
            "Epoch 402: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0915 - accuracy: 0.9864 - val_loss: 1.2302 - val_accuracy: 0.7162\n",
            "Epoch 403/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0820 - accuracy: 0.9766\n",
            "Epoch 403: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0906 - accuracy: 0.9830 - val_loss: 1.2192 - val_accuracy: 0.7162\n",
            "Epoch 404/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0997 - accuracy: 0.9844\n",
            "Epoch 404: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0922 - accuracy: 0.9796 - val_loss: 1.1912 - val_accuracy: 0.7297\n",
            "Epoch 405/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0790 - accuracy: 0.9922\n",
            "Epoch 405: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0890 - accuracy: 0.9898 - val_loss: 1.1919 - val_accuracy: 0.7297\n",
            "Epoch 406/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0757 - accuracy: 0.9922\n",
            "Epoch 406: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0902 - accuracy: 0.9864 - val_loss: 1.1978 - val_accuracy: 0.7297\n",
            "Epoch 407/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0886 - accuracy: 0.9766\n",
            "Epoch 407: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.0911 - accuracy: 0.9796 - val_loss: 1.2004 - val_accuracy: 0.7297\n",
            "Epoch 408/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0859 - accuracy: 0.9922\n",
            "Epoch 408: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0886 - accuracy: 0.9796 - val_loss: 1.1962 - val_accuracy: 0.7297\n",
            "Epoch 409/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0919 - accuracy: 0.9688\n",
            "Epoch 409: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0879 - accuracy: 0.9796 - val_loss: 1.1703 - val_accuracy: 0.7297\n",
            "Epoch 410/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0766 - accuracy: 0.9844\n",
            "Epoch 410: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0877 - accuracy: 0.9796 - val_loss: 1.1437 - val_accuracy: 0.7432\n",
            "Epoch 411/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0967 - accuracy: 0.9922\n",
            "Epoch 411: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0895 - accuracy: 0.9796 - val_loss: 1.1201 - val_accuracy: 0.7297\n",
            "Epoch 412/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9796\n",
            "Epoch 412: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0871 - accuracy: 0.9796 - val_loss: 1.1064 - val_accuracy: 0.7162\n",
            "Epoch 413/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0900 - accuracy: 0.9766\n",
            "Epoch 413: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0873 - accuracy: 0.9796 - val_loss: 1.0876 - val_accuracy: 0.7297\n",
            "Epoch 414/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9830\n",
            "Epoch 414: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0870 - accuracy: 0.9830 - val_loss: 1.0773 - val_accuracy: 0.7297\n",
            "Epoch 415/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9796\n",
            "Epoch 415: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0905 - accuracy: 0.9796 - val_loss: 1.0658 - val_accuracy: 0.7432\n",
            "Epoch 416/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0758 - accuracy: 0.9766\n",
            "Epoch 416: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0878 - accuracy: 0.9762 - val_loss: 1.0513 - val_accuracy: 0.7568\n",
            "Epoch 417/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0913 - accuracy: 0.9766\n",
            "Epoch 417: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0868 - accuracy: 0.9830 - val_loss: 1.0252 - val_accuracy: 0.7568\n",
            "Epoch 418/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0927 - accuracy: 0.9766\n",
            "Epoch 418: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0900 - accuracy: 0.9762 - val_loss: 1.0129 - val_accuracy: 0.7703\n",
            "Epoch 419/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0868 - accuracy: 0.9766\n",
            "Epoch 419: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0827 - accuracy: 0.9796 - val_loss: 1.0214 - val_accuracy: 0.7568\n",
            "Epoch 420/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0800 - accuracy: 0.9844\n",
            "Epoch 420: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0801 - accuracy: 0.9796 - val_loss: 1.0450 - val_accuracy: 0.7568\n",
            "Epoch 421/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0801 - accuracy: 0.9844\n",
            "Epoch 421: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0824 - accuracy: 0.9796 - val_loss: 1.0303 - val_accuracy: 0.7432\n",
            "Epoch 422/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0853 - accuracy: 0.9844\n",
            "Epoch 422: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0840 - accuracy: 0.9796 - val_loss: 1.0202 - val_accuracy: 0.7297\n",
            "Epoch 423/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9796\n",
            "Epoch 423: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0786 - accuracy: 0.9796 - val_loss: 1.0055 - val_accuracy: 0.7297\n",
            "Epoch 424/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0763 - accuracy: 0.9844\n",
            "Epoch 424: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0849 - accuracy: 0.9796 - val_loss: 0.9944 - val_accuracy: 0.7432\n",
            "Epoch 425/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0725 - accuracy: 0.9922\n",
            "Epoch 425: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0813 - accuracy: 0.9796 - val_loss: 0.9954 - val_accuracy: 0.7432\n",
            "Epoch 426/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0802 - accuracy: 0.9609\n",
            "Epoch 426: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0784 - accuracy: 0.9796 - val_loss: 0.9978 - val_accuracy: 0.7432\n",
            "Epoch 427/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0919 - accuracy: 0.9688\n",
            "Epoch 427: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0848 - accuracy: 0.9762 - val_loss: 0.9767 - val_accuracy: 0.7703\n",
            "Epoch 428/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0667 - accuracy: 0.9844\n",
            "Epoch 428: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0771 - accuracy: 0.9830 - val_loss: 0.9828 - val_accuracy: 0.7432\n",
            "Epoch 429/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0581 - accuracy: 1.0000\n",
            "Epoch 429: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0794 - accuracy: 0.9762 - val_loss: 1.0001 - val_accuracy: 0.7297\n",
            "Epoch 430/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0924 - accuracy: 0.9766\n",
            "Epoch 430: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0777 - accuracy: 0.9796 - val_loss: 1.0140 - val_accuracy: 0.7432\n",
            "Epoch 431/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0660 - accuracy: 0.9844\n",
            "Epoch 431: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0759 - accuracy: 0.9830 - val_loss: 1.0247 - val_accuracy: 0.7432\n",
            "Epoch 432/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0693 - accuracy: 0.9766\n",
            "Epoch 432: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0744 - accuracy: 0.9796 - val_loss: 1.0506 - val_accuracy: 0.7568\n",
            "Epoch 433/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0677 - accuracy: 0.9844\n",
            "Epoch 433: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0754 - accuracy: 0.9796 - val_loss: 1.0674 - val_accuracy: 0.7568\n",
            "Epoch 434/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0705 - accuracy: 0.9844\n",
            "Epoch 434: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0750 - accuracy: 0.9796 - val_loss: 1.0692 - val_accuracy: 0.7432\n",
            "Epoch 435/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0651 - accuracy: 0.9844\n",
            "Epoch 435: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0780 - accuracy: 0.9796 - val_loss: 1.0679 - val_accuracy: 0.7297\n",
            "Epoch 436/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0863 - accuracy: 0.9766\n",
            "Epoch 436: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0747 - accuracy: 0.9796 - val_loss: 1.0726 - val_accuracy: 0.7162\n",
            "Epoch 437/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0621 - accuracy: 0.9844\n",
            "Epoch 437: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0724 - accuracy: 0.9796 - val_loss: 1.0780 - val_accuracy: 0.7027\n",
            "Epoch 438/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0934 - accuracy: 0.9766\n",
            "Epoch 438: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0775 - accuracy: 0.9796 - val_loss: 1.0790 - val_accuracy: 0.7027\n",
            "Epoch 439/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0775 - accuracy: 0.9688\n",
            "Epoch 439: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0704 - accuracy: 0.9796 - val_loss: 1.0782 - val_accuracy: 0.7162\n",
            "Epoch 440/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0826 - accuracy: 0.9766\n",
            "Epoch 440: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0751 - accuracy: 0.9796 - val_loss: 1.0529 - val_accuracy: 0.7027\n",
            "Epoch 441/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0671 - accuracy: 0.9844\n",
            "Epoch 441: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0725 - accuracy: 0.9796 - val_loss: 1.0397 - val_accuracy: 0.7162\n",
            "Epoch 442/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0707 - accuracy: 0.9766\n",
            "Epoch 442: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0714 - accuracy: 0.9796 - val_loss: 1.0452 - val_accuracy: 0.7297\n",
            "Epoch 443/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0624 - accuracy: 0.9922\n",
            "Epoch 443: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0720 - accuracy: 0.9796 - val_loss: 1.0453 - val_accuracy: 0.7297\n",
            "Epoch 444/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0781 - accuracy: 0.9766\n",
            "Epoch 444: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0718 - accuracy: 0.9796 - val_loss: 1.0456 - val_accuracy: 0.7297\n",
            "Epoch 445/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0598 - accuracy: 0.9766\n",
            "Epoch 445: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0685 - accuracy: 0.9796 - val_loss: 1.0469 - val_accuracy: 0.7432\n",
            "Epoch 446/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0780 - accuracy: 0.9844\n",
            "Epoch 446: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0730 - accuracy: 0.9830 - val_loss: 1.0475 - val_accuracy: 0.7432\n",
            "Epoch 447/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0705 - accuracy: 0.9766\n",
            "Epoch 447: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0704 - accuracy: 0.9796 - val_loss: 1.0458 - val_accuracy: 0.7432\n",
            "Epoch 448/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0589 - accuracy: 0.9766\n",
            "Epoch 448: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0714 - accuracy: 0.9796 - val_loss: 1.0326 - val_accuracy: 0.7432\n",
            "Epoch 449/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0665 - accuracy: 0.9766\n",
            "Epoch 449: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0689 - accuracy: 0.9830 - val_loss: 1.0409 - val_accuracy: 0.7432\n",
            "Epoch 450/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0642 - accuracy: 0.9688\n",
            "Epoch 450: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0669 - accuracy: 0.9796 - val_loss: 1.0662 - val_accuracy: 0.7568\n",
            "Epoch 451/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0839 - accuracy: 0.9766\n",
            "Epoch 451: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0701 - accuracy: 0.9796 - val_loss: 1.0834 - val_accuracy: 0.7432\n",
            "Epoch 452/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9796\n",
            "Epoch 452: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0679 - accuracy: 0.9796 - val_loss: 1.0899 - val_accuracy: 0.7568\n",
            "Epoch 453/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0753 - accuracy: 0.9766\n",
            "Epoch 453: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0692 - accuracy: 0.9796 - val_loss: 1.1042 - val_accuracy: 0.7432\n",
            "Epoch 454/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0709 - accuracy: 0.9844\n",
            "Epoch 454: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0674 - accuracy: 0.9796 - val_loss: 1.1104 - val_accuracy: 0.7162\n",
            "Epoch 455/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0469 - accuracy: 1.0000\n",
            "Epoch 455: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0670 - accuracy: 0.9796 - val_loss: 1.1056 - val_accuracy: 0.7297\n",
            "Epoch 456/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0602 - accuracy: 0.9766\n",
            "Epoch 456: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0642 - accuracy: 0.9796 - val_loss: 1.0996 - val_accuracy: 0.7297\n",
            "Epoch 457/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9796\n",
            "Epoch 457: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0664 - accuracy: 0.9796 - val_loss: 1.0981 - val_accuracy: 0.7162\n",
            "Epoch 458/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0638 - accuracy: 0.9766\n",
            "Epoch 458: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0632 - accuracy: 0.9796 - val_loss: 1.1093 - val_accuracy: 0.7162\n",
            "Epoch 459/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0587 - accuracy: 0.9844\n",
            "Epoch 459: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0628 - accuracy: 0.9796 - val_loss: 1.0874 - val_accuracy: 0.7162\n",
            "Epoch 460/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0712 - accuracy: 0.9766\n",
            "Epoch 460: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0658 - accuracy: 0.9796 - val_loss: 1.0518 - val_accuracy: 0.7297\n",
            "Epoch 461/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0770 - accuracy: 0.9766\n",
            "Epoch 461: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0639 - accuracy: 0.9796 - val_loss: 1.0459 - val_accuracy: 0.7297\n",
            "Epoch 462/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0652 - accuracy: 0.9844\n",
            "Epoch 462: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0686 - accuracy: 0.9796 - val_loss: 1.0490 - val_accuracy: 0.7297\n",
            "Epoch 463/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0635 - accuracy: 0.9844\n",
            "Epoch 463: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0641 - accuracy: 0.9796 - val_loss: 1.0488 - val_accuracy: 0.7162\n",
            "Epoch 464/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0693 - accuracy: 0.9844\n",
            "Epoch 464: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0669 - accuracy: 0.9830 - val_loss: 1.0527 - val_accuracy: 0.7297\n",
            "Epoch 465/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0849 - accuracy: 0.9688\n",
            "Epoch 465: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0668 - accuracy: 0.9762 - val_loss: 1.0551 - val_accuracy: 0.7297\n",
            "Epoch 466/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - accuracy: 0.9844\n",
            "Epoch 466: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0634 - accuracy: 0.9796 - val_loss: 1.0389 - val_accuracy: 0.7162\n",
            "Epoch 467/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0462 - accuracy: 0.9922\n",
            "Epoch 467: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0630 - accuracy: 0.9796 - val_loss: 1.0375 - val_accuracy: 0.7297\n",
            "Epoch 468/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0809 - accuracy: 0.9609\n",
            "Epoch 468: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0641 - accuracy: 0.9796 - val_loss: 1.0434 - val_accuracy: 0.7162\n",
            "Epoch 469/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0581 - accuracy: 0.9922\n",
            "Epoch 469: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0616 - accuracy: 0.9796 - val_loss: 1.0263 - val_accuracy: 0.7432\n",
            "Epoch 470/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0607 - accuracy: 0.9844\n",
            "Epoch 470: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0632 - accuracy: 0.9796 - val_loss: 1.0113 - val_accuracy: 0.7568\n",
            "Epoch 471/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0731 - accuracy: 0.9688\n",
            "Epoch 471: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0623 - accuracy: 0.9796 - val_loss: 1.0066 - val_accuracy: 0.7432\n",
            "Epoch 472/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0785 - accuracy: 0.9766\n",
            "Epoch 472: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0634 - accuracy: 0.9796 - val_loss: 0.9946 - val_accuracy: 0.7432\n",
            "Epoch 473/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0483 - accuracy: 0.9844\n",
            "Epoch 473: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0613 - accuracy: 0.9796 - val_loss: 0.9801 - val_accuracy: 0.7432\n",
            "Epoch 474/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0493 - accuracy: 0.9844\n",
            "Epoch 474: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0581 - accuracy: 0.9796 - val_loss: 0.9676 - val_accuracy: 0.7432\n",
            "Epoch 475/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0625 - accuracy: 0.9766\n",
            "Epoch 475: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0587 - accuracy: 0.9796 - val_loss: 0.9631 - val_accuracy: 0.7432\n",
            "Epoch 476/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0558 - accuracy: 0.9688\n",
            "Epoch 476: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0660 - accuracy: 0.9762 - val_loss: 1.0146 - val_accuracy: 0.7432\n",
            "Epoch 477/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9844\n",
            "Epoch 477: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0584 - accuracy: 0.9796 - val_loss: 1.0769 - val_accuracy: 0.7568\n",
            "Epoch 478/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0584 - accuracy: 0.9844\n",
            "Epoch 478: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0603 - accuracy: 0.9796 - val_loss: 1.1103 - val_accuracy: 0.7568\n",
            "Epoch 479/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0549 - accuracy: 0.9766\n",
            "Epoch 479: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0603 - accuracy: 0.9796 - val_loss: 1.1249 - val_accuracy: 0.7568\n",
            "Epoch 480/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0469 - accuracy: 0.9844\n",
            "Epoch 480: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0607 - accuracy: 0.9796 - val_loss: 1.1234 - val_accuracy: 0.7432\n",
            "Epoch 481/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0458 - accuracy: 0.9844\n",
            "Epoch 481: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0589 - accuracy: 0.9796 - val_loss: 1.1193 - val_accuracy: 0.7432\n",
            "Epoch 482/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0587 - accuracy: 0.9844\n",
            "Epoch 482: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0580 - accuracy: 0.9796 - val_loss: 1.0887 - val_accuracy: 0.7568\n",
            "Epoch 483/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0608 - accuracy: 0.9844\n",
            "Epoch 483: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0583 - accuracy: 0.9796 - val_loss: 1.0567 - val_accuracy: 0.7568\n",
            "Epoch 484/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0591 - accuracy: 0.9766\n",
            "Epoch 484: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0578 - accuracy: 0.9796 - val_loss: 1.0176 - val_accuracy: 0.7568\n",
            "Epoch 485/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0659 - accuracy: 0.9688\n",
            "Epoch 485: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0596 - accuracy: 0.9762 - val_loss: 0.9829 - val_accuracy: 0.7568\n",
            "Epoch 486/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0667 - accuracy: 0.9766\n",
            "Epoch 486: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0569 - accuracy: 0.9796 - val_loss: 0.9621 - val_accuracy: 0.7568\n",
            "Epoch 487/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0423 - accuracy: 0.9922\n",
            "Epoch 487: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0566 - accuracy: 0.9796 - val_loss: 0.9626 - val_accuracy: 0.7568\n",
            "Epoch 488/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0629 - accuracy: 0.9609\n",
            "Epoch 488: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0568 - accuracy: 0.9796 - val_loss: 0.9782 - val_accuracy: 0.7297\n",
            "Epoch 489/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9796\n",
            "Epoch 489: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0566 - accuracy: 0.9796 - val_loss: 0.9883 - val_accuracy: 0.7297\n",
            "Epoch 490/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0467 - accuracy: 0.9844\n",
            "Epoch 490: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0558 - accuracy: 0.9796 - val_loss: 0.9769 - val_accuracy: 0.7432\n",
            "Epoch 491/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0533 - accuracy: 0.9922\n",
            "Epoch 491: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0573 - accuracy: 0.9796 - val_loss: 0.9634 - val_accuracy: 0.7432\n",
            "Epoch 492/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0661 - accuracy: 0.9844\n",
            "Epoch 492: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0585 - accuracy: 0.9796 - val_loss: 0.9537 - val_accuracy: 0.7432\n",
            "Epoch 493/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - accuracy: 0.9844\n",
            "Epoch 493: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0577 - accuracy: 0.9796 - val_loss: 0.9469 - val_accuracy: 0.7432\n",
            "Epoch 494/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0530 - accuracy: 0.9844\n",
            "Epoch 494: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0562 - accuracy: 0.9796 - val_loss: 0.9506 - val_accuracy: 0.7568\n",
            "Epoch 495/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9796\n",
            "Epoch 495: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0581 - accuracy: 0.9796 - val_loss: 0.9347 - val_accuracy: 0.7432\n",
            "Epoch 496/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0451 - accuracy: 1.0000\n",
            "Epoch 496: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0555 - accuracy: 0.9830 - val_loss: 0.9193 - val_accuracy: 0.7568\n",
            "Epoch 497/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0753 - accuracy: 0.9688\n",
            "Epoch 497: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0575 - accuracy: 0.9762 - val_loss: 0.9093 - val_accuracy: 0.7703\n",
            "Epoch 498/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9796\n",
            "Epoch 498: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0560 - accuracy: 0.9796 - val_loss: 0.9167 - val_accuracy: 0.7568\n",
            "Epoch 499/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0657 - accuracy: 0.9531\n",
            "Epoch 499: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0528 - accuracy: 0.9796 - val_loss: 0.9265 - val_accuracy: 0.7568\n",
            "Epoch 500/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0505 - accuracy: 0.9844\n",
            "Epoch 500: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0546 - accuracy: 0.9796 - val_loss: 0.9273 - val_accuracy: 0.7568\n",
            "Epoch 501/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0522 - accuracy: 0.9844\n",
            "Epoch 501: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0541 - accuracy: 0.9796 - val_loss: 0.9286 - val_accuracy: 0.7568\n",
            "Epoch 502/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0532 - accuracy: 0.9766\n",
            "Epoch 502: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0545 - accuracy: 0.9796 - val_loss: 0.9234 - val_accuracy: 0.7568\n",
            "Epoch 503/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0554 - accuracy: 0.9766\n",
            "Epoch 503: val_accuracy improved from 0.77027 to 0.78378, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0525 - accuracy: 0.9796 - val_loss: 0.9112 - val_accuracy: 0.7838\n",
            "Epoch 504/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0545 - accuracy: 0.9688\n",
            "Epoch 504: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0526 - accuracy: 0.9796 - val_loss: 0.9064 - val_accuracy: 0.7838\n",
            "Epoch 505/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0430 - accuracy: 0.9844\n",
            "Epoch 505: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0531 - accuracy: 0.9796 - val_loss: 0.9026 - val_accuracy: 0.7432\n",
            "Epoch 506/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0498 - accuracy: 0.9844\n",
            "Epoch 506: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0551 - accuracy: 0.9796 - val_loss: 0.9030 - val_accuracy: 0.7297\n",
            "Epoch 507/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0562 - accuracy: 0.9844\n",
            "Epoch 507: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0525 - accuracy: 0.9796 - val_loss: 0.9067 - val_accuracy: 0.7297\n",
            "Epoch 508/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0475 - accuracy: 0.9922\n",
            "Epoch 508: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0535 - accuracy: 0.9796 - val_loss: 0.9180 - val_accuracy: 0.7297\n",
            "Epoch 509/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0568 - accuracy: 0.9766\n",
            "Epoch 509: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0525 - accuracy: 0.9796 - val_loss: 0.9332 - val_accuracy: 0.7432\n",
            "Epoch 510/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000\n",
            "Epoch 510: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0540 - accuracy: 0.9796 - val_loss: 0.9497 - val_accuracy: 0.7297\n",
            "Epoch 511/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0553 - accuracy: 0.9844\n",
            "Epoch 511: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0493 - accuracy: 0.9796 - val_loss: 0.9701 - val_accuracy: 0.7162\n",
            "Epoch 512/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
            "Epoch 512: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0529 - accuracy: 0.9796 - val_loss: 0.9828 - val_accuracy: 0.7162\n",
            "Epoch 513/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0444 - accuracy: 0.9766\n",
            "Epoch 513: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0518 - accuracy: 0.9796 - val_loss: 1.0002 - val_accuracy: 0.7297\n",
            "Epoch 514/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0388 - accuracy: 0.9922\n",
            "Epoch 514: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0525 - accuracy: 0.9830 - val_loss: 1.0044 - val_accuracy: 0.7297\n",
            "Epoch 515/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0317 - accuracy: 1.0000\n",
            "Epoch 515: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0538 - accuracy: 0.9796 - val_loss: 0.9917 - val_accuracy: 0.7297\n",
            "Epoch 516/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0472 - accuracy: 0.9766\n",
            "Epoch 516: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0536 - accuracy: 0.9796 - val_loss: 0.9698 - val_accuracy: 0.7432\n",
            "Epoch 517/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0470 - accuracy: 1.0000\n",
            "Epoch 517: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0490 - accuracy: 0.9830 - val_loss: 0.9541 - val_accuracy: 0.7568\n",
            "Epoch 518/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0571 - accuracy: 0.9688\n",
            "Epoch 518: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0499 - accuracy: 0.9830 - val_loss: 0.9619 - val_accuracy: 0.7432\n",
            "Epoch 519/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000\n",
            "Epoch 519: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0528 - accuracy: 0.9830 - val_loss: 0.9589 - val_accuracy: 0.7568\n",
            "Epoch 520/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9844\n",
            "Epoch 520: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0492 - accuracy: 0.9830 - val_loss: 0.9598 - val_accuracy: 0.7432\n",
            "Epoch 521/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0517 - accuracy: 0.9766\n",
            "Epoch 521: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0504 - accuracy: 0.9830 - val_loss: 0.9719 - val_accuracy: 0.7162\n",
            "Epoch 522/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0679 - accuracy: 0.9766\n",
            "Epoch 522: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0513 - accuracy: 0.9796 - val_loss: 0.9855 - val_accuracy: 0.7297\n",
            "Epoch 523/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0581 - accuracy: 0.9688\n",
            "Epoch 523: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0514 - accuracy: 0.9796 - val_loss: 0.9745 - val_accuracy: 0.7432\n",
            "Epoch 524/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0329 - accuracy: 0.9922\n",
            "Epoch 524: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0493 - accuracy: 0.9898 - val_loss: 0.9604 - val_accuracy: 0.7703\n",
            "Epoch 525/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0514 - accuracy: 0.9766\n",
            "Epoch 525: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0507 - accuracy: 0.9864 - val_loss: 0.9735 - val_accuracy: 0.7703\n",
            "Epoch 526/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0757 - accuracy: 0.9688\n",
            "Epoch 526: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0529 - accuracy: 0.9864 - val_loss: 0.9919 - val_accuracy: 0.7568\n",
            "Epoch 527/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0774 - accuracy: 0.9609\n",
            "Epoch 527: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0528 - accuracy: 0.9796 - val_loss: 1.0073 - val_accuracy: 0.7568\n",
            "Epoch 528/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0596 - accuracy: 0.9766\n",
            "Epoch 528: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0520 - accuracy: 0.9762 - val_loss: 1.0228 - val_accuracy: 0.7568\n",
            "Epoch 529/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000\n",
            "Epoch 529: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0481 - accuracy: 0.9864 - val_loss: 1.0252 - val_accuracy: 0.7568\n",
            "Epoch 530/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0654 - accuracy: 0.9844\n",
            "Epoch 530: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0491 - accuracy: 0.9830 - val_loss: 1.0312 - val_accuracy: 0.7568\n",
            "Epoch 531/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0435 - accuracy: 0.9688\n",
            "Epoch 531: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0473 - accuracy: 0.9796 - val_loss: 1.0500 - val_accuracy: 0.7568\n",
            "Epoch 532/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0537 - accuracy: 0.9766\n",
            "Epoch 532: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0466 - accuracy: 0.9796 - val_loss: 1.0673 - val_accuracy: 0.7838\n",
            "Epoch 533/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0424 - accuracy: 0.9922\n",
            "Epoch 533: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0483 - accuracy: 0.9830 - val_loss: 1.0713 - val_accuracy: 0.7838\n",
            "Epoch 534/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0602 - accuracy: 0.9609\n",
            "Epoch 534: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0475 - accuracy: 0.9796 - val_loss: 1.0613 - val_accuracy: 0.7838\n",
            "Epoch 535/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0309 - accuracy: 0.9922\n",
            "Epoch 535: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0457 - accuracy: 0.9864 - val_loss: 1.0506 - val_accuracy: 0.7703\n",
            "Epoch 536/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9796\n",
            "Epoch 536: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0467 - accuracy: 0.9796 - val_loss: 1.0494 - val_accuracy: 0.7703\n",
            "Epoch 537/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0407 - accuracy: 0.9766\n",
            "Epoch 537: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0464 - accuracy: 0.9796 - val_loss: 1.0521 - val_accuracy: 0.7568\n",
            "Epoch 538/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0644 - accuracy: 0.9609\n",
            "Epoch 538: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0493 - accuracy: 0.9796 - val_loss: 1.0617 - val_accuracy: 0.7703\n",
            "Epoch 539/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0597 - accuracy: 0.9766\n",
            "Epoch 539: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0509 - accuracy: 0.9796 - val_loss: 1.0705 - val_accuracy: 0.7568\n",
            "Epoch 540/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0595 - accuracy: 0.9766\n",
            "Epoch 540: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0529 - accuracy: 0.9796 - val_loss: 1.0789 - val_accuracy: 0.7432\n",
            "Epoch 541/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0558 - accuracy: 0.9609\n",
            "Epoch 541: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0503 - accuracy: 0.9796 - val_loss: 1.0846 - val_accuracy: 0.7432\n",
            "Epoch 542/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9796\n",
            "Epoch 542: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0478 - accuracy: 0.9796 - val_loss: 1.0843 - val_accuracy: 0.7432\n",
            "Epoch 543/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0486 - accuracy: 0.9766\n",
            "Epoch 543: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0464 - accuracy: 0.9796 - val_loss: 1.0870 - val_accuracy: 0.7432\n",
            "Epoch 544/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0513 - accuracy: 0.9766\n",
            "Epoch 544: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0480 - accuracy: 0.9796 - val_loss: 1.0898 - val_accuracy: 0.7297\n",
            "Epoch 545/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0497 - accuracy: 0.9922\n",
            "Epoch 545: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0466 - accuracy: 0.9796 - val_loss: 1.1136 - val_accuracy: 0.7027\n",
            "Epoch 546/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0555 - accuracy: 0.9766\n",
            "Epoch 546: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0451 - accuracy: 0.9830 - val_loss: 1.1445 - val_accuracy: 0.6892\n",
            "Epoch 547/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9844\n",
            "Epoch 547: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0456 - accuracy: 0.9830 - val_loss: 1.1421 - val_accuracy: 0.6757\n",
            "Epoch 548/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0394 - accuracy: 0.9844\n",
            "Epoch 548: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0442 - accuracy: 0.9864 - val_loss: 1.1308 - val_accuracy: 0.6892\n",
            "Epoch 549/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0386 - accuracy: 1.0000\n",
            "Epoch 549: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0476 - accuracy: 0.9830 - val_loss: 1.1233 - val_accuracy: 0.7162\n",
            "Epoch 550/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0364 - accuracy: 0.9766\n",
            "Epoch 550: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0455 - accuracy: 0.9864 - val_loss: 1.1036 - val_accuracy: 0.7297\n",
            "Epoch 551/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0426 - accuracy: 0.9844\n",
            "Epoch 551: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0466 - accuracy: 0.9830 - val_loss: 1.0519 - val_accuracy: 0.7297\n",
            "Epoch 552/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0396 - accuracy: 1.0000\n",
            "Epoch 552: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0441 - accuracy: 0.9830 - val_loss: 0.9980 - val_accuracy: 0.7432\n",
            "Epoch 553/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0492 - accuracy: 0.9922\n",
            "Epoch 553: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0466 - accuracy: 0.9830 - val_loss: 0.9648 - val_accuracy: 0.7568\n",
            "Epoch 554/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0456 - accuracy: 0.9766\n",
            "Epoch 554: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0440 - accuracy: 0.9796 - val_loss: 0.9463 - val_accuracy: 0.7568\n",
            "Epoch 555/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0400 - accuracy: 0.9844\n",
            "Epoch 555: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 0.9428 - val_accuracy: 0.7703\n",
            "Epoch 556/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0475 - accuracy: 0.9844\n",
            "Epoch 556: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0482 - accuracy: 0.9830 - val_loss: 0.9730 - val_accuracy: 0.7703\n",
            "Epoch 557/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0559 - accuracy: 0.9688\n",
            "Epoch 557: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0457 - accuracy: 0.9830 - val_loss: 1.0410 - val_accuracy: 0.7027\n",
            "Epoch 558/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9762\n",
            "Epoch 558: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0480 - accuracy: 0.9762 - val_loss: 1.0937 - val_accuracy: 0.7027\n",
            "Epoch 559/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0435 - accuracy: 0.9844\n",
            "Epoch 559: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0447 - accuracy: 0.9796 - val_loss: 1.1349 - val_accuracy: 0.7027\n",
            "Epoch 560/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0480 - accuracy: 0.9688\n",
            "Epoch 560: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0497 - accuracy: 0.9796 - val_loss: 1.1625 - val_accuracy: 0.6892\n",
            "Epoch 561/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9922\n",
            "Epoch 561: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0479 - accuracy: 0.9762 - val_loss: 1.1714 - val_accuracy: 0.6757\n",
            "Epoch 562/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0483 - accuracy: 0.9766\n",
            "Epoch 562: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0463 - accuracy: 0.9796 - val_loss: 1.1671 - val_accuracy: 0.6757\n",
            "Epoch 563/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0320 - accuracy: 0.9922\n",
            "Epoch 563: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0452 - accuracy: 0.9796 - val_loss: 1.1588 - val_accuracy: 0.6757\n",
            "Epoch 564/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0387 - accuracy: 0.9766\n",
            "Epoch 564: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0455 - accuracy: 0.9796 - val_loss: 1.1257 - val_accuracy: 0.6892\n",
            "Epoch 565/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0453 - accuracy: 0.9844\n",
            "Epoch 565: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0461 - accuracy: 0.9796 - val_loss: 1.0863 - val_accuracy: 0.6892\n",
            "Epoch 566/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - accuracy: 0.9609\n",
            "Epoch 566: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0443 - accuracy: 0.9796 - val_loss: 1.0588 - val_accuracy: 0.7027\n",
            "Epoch 567/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0514 - accuracy: 0.9766\n",
            "Epoch 567: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0444 - accuracy: 0.9796 - val_loss: 1.0361 - val_accuracy: 0.7027\n",
            "Epoch 568/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0491 - accuracy: 0.9766\n",
            "Epoch 568: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0428 - accuracy: 0.9796 - val_loss: 1.0204 - val_accuracy: 0.7027\n",
            "Epoch 569/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0525 - accuracy: 0.9844\n",
            "Epoch 569: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0427 - accuracy: 0.9796 - val_loss: 1.0032 - val_accuracy: 0.7027\n",
            "Epoch 570/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0490 - accuracy: 0.9844\n",
            "Epoch 570: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0415 - accuracy: 0.9796 - val_loss: 0.9919 - val_accuracy: 0.7027\n",
            "Epoch 571/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0442 - accuracy: 0.9844\n",
            "Epoch 571: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0432 - accuracy: 0.9830 - val_loss: 0.9862 - val_accuracy: 0.7027\n",
            "Epoch 572/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0565 - accuracy: 0.9688\n",
            "Epoch 572: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0459 - accuracy: 0.9796 - val_loss: 0.9828 - val_accuracy: 0.7027\n",
            "Epoch 573/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0530 - accuracy: 0.9844\n",
            "Epoch 573: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0434 - accuracy: 0.9864 - val_loss: 0.9924 - val_accuracy: 0.7027\n",
            "Epoch 574/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 574: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0456 - accuracy: 0.9864 - val_loss: 1.0157 - val_accuracy: 0.7027\n",
            "Epoch 575/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9766\n",
            "Epoch 575: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0433 - accuracy: 0.9830 - val_loss: 1.0382 - val_accuracy: 0.7027\n",
            "Epoch 576/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0549 - accuracy: 0.9766\n",
            "Epoch 576: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0454 - accuracy: 0.9830 - val_loss: 1.0552 - val_accuracy: 0.7027\n",
            "Epoch 577/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0473 - accuracy: 0.9766\n",
            "Epoch 577: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0458 - accuracy: 0.9864 - val_loss: 1.0768 - val_accuracy: 0.7027\n",
            "Epoch 578/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0388 - accuracy: 0.9922\n",
            "Epoch 578: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0415 - accuracy: 0.9898 - val_loss: 1.0869 - val_accuracy: 0.7027\n",
            "Epoch 579/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0430 - accuracy: 0.9766\n",
            "Epoch 579: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0428 - accuracy: 0.9830 - val_loss: 1.0908 - val_accuracy: 0.7027\n",
            "Epoch 580/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0541 - accuracy: 0.9844\n",
            "Epoch 580: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0416 - accuracy: 0.9898 - val_loss: 1.0988 - val_accuracy: 0.7162\n",
            "Epoch 581/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0263 - accuracy: 0.9922\n",
            "Epoch 581: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0432 - accuracy: 0.9796 - val_loss: 1.0849 - val_accuracy: 0.7162\n",
            "Epoch 582/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0473 - accuracy: 0.9844\n",
            "Epoch 582: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0420 - accuracy: 0.9796 - val_loss: 1.0691 - val_accuracy: 0.7432\n",
            "Epoch 583/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0405 - accuracy: 0.9844\n",
            "Epoch 583: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0441 - accuracy: 0.9796 - val_loss: 1.0528 - val_accuracy: 0.7432\n",
            "Epoch 584/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0312 - accuracy: 0.9922\n",
            "Epoch 584: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0429 - accuracy: 0.9796 - val_loss: 1.0324 - val_accuracy: 0.7297\n",
            "Epoch 585/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0636 - accuracy: 0.9688\n",
            "Epoch 585: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0454 - accuracy: 0.9796 - val_loss: 1.0210 - val_accuracy: 0.7432\n",
            "Epoch 586/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0470 - accuracy: 0.9766\n",
            "Epoch 586: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0413 - accuracy: 0.9796 - val_loss: 1.0117 - val_accuracy: 0.7432\n",
            "Epoch 587/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0424 - accuracy: 0.9766\n",
            "Epoch 587: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0411 - accuracy: 0.9796 - val_loss: 0.9999 - val_accuracy: 0.7432\n",
            "Epoch 588/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0510 - accuracy: 0.9688\n",
            "Epoch 588: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0455 - accuracy: 0.9796 - val_loss: 1.0036 - val_accuracy: 0.7432\n",
            "Epoch 589/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0258 - accuracy: 0.9922\n",
            "Epoch 589: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0384 - accuracy: 0.9796 - val_loss: 1.0166 - val_accuracy: 0.7568\n",
            "Epoch 590/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0385 - accuracy: 0.9688\n",
            "Epoch 590: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0445 - accuracy: 0.9762 - val_loss: 1.0256 - val_accuracy: 0.7432\n",
            "Epoch 591/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0417 - accuracy: 0.9844\n",
            "Epoch 591: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0411 - accuracy: 0.9796 - val_loss: 1.0293 - val_accuracy: 0.7297\n",
            "Epoch 592/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0546 - accuracy: 0.9844\n",
            "Epoch 592: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0417 - accuracy: 0.9830 - val_loss: 1.0396 - val_accuracy: 0.7297\n",
            "Epoch 593/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0470 - accuracy: 0.9844\n",
            "Epoch 593: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0437 - accuracy: 0.9864 - val_loss: 1.0390 - val_accuracy: 0.7297\n",
            "Epoch 594/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0417 - accuracy: 0.9844\n",
            "Epoch 594: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0405 - accuracy: 0.9864 - val_loss: 1.0406 - val_accuracy: 0.7297\n",
            "Epoch 595/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0454 - accuracy: 0.9844\n",
            "Epoch 595: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0413 - accuracy: 0.9796 - val_loss: 1.0459 - val_accuracy: 0.7297\n",
            "Epoch 596/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0550 - accuracy: 0.9766\n",
            "Epoch 596: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0414 - accuracy: 0.9796 - val_loss: 1.0423 - val_accuracy: 0.7432\n",
            "Epoch 597/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0306 - accuracy: 1.0000\n",
            "Epoch 597: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0413 - accuracy: 0.9796 - val_loss: 1.0355 - val_accuracy: 0.7297\n",
            "Epoch 598/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0315 - accuracy: 0.9766\n",
            "Epoch 598: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0405 - accuracy: 0.9796 - val_loss: 1.0329 - val_accuracy: 0.7432\n",
            "Epoch 599/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0352 - accuracy: 0.9766\n",
            "Epoch 599: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0396 - accuracy: 0.9796 - val_loss: 1.0296 - val_accuracy: 0.7432\n",
            "Epoch 600/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0444 - accuracy: 0.9766\n",
            "Epoch 600: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0421 - accuracy: 0.9796 - val_loss: 1.0296 - val_accuracy: 0.7297\n",
            "Epoch 601/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0441 - accuracy: 0.9688\n",
            "Epoch 601: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0397 - accuracy: 0.9796 - val_loss: 1.0376 - val_accuracy: 0.7162\n",
            "Epoch 602/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0422 - accuracy: 0.9844\n",
            "Epoch 602: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0417 - accuracy: 0.9830 - val_loss: 1.0466 - val_accuracy: 0.7297\n",
            "Epoch 603/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0363 - accuracy: 0.9844\n",
            "Epoch 603: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0412 - accuracy: 0.9796 - val_loss: 1.0627 - val_accuracy: 0.7297\n",
            "Epoch 604/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0361 - accuracy: 0.9844\n",
            "Epoch 604: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0382 - accuracy: 0.9830 - val_loss: 1.0836 - val_accuracy: 0.7297\n",
            "Epoch 605/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0334 - accuracy: 0.9766\n",
            "Epoch 605: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0421 - accuracy: 0.9796 - val_loss: 1.1041 - val_accuracy: 0.7162\n",
            "Epoch 606/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0617 - accuracy: 0.9766\n",
            "Epoch 606: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0397 - accuracy: 0.9898 - val_loss: 1.1187 - val_accuracy: 0.7297\n",
            "Epoch 607/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0507 - accuracy: 0.9844\n",
            "Epoch 607: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0394 - accuracy: 0.9864 - val_loss: 1.1274 - val_accuracy: 0.7297\n",
            "Epoch 608/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0396 - accuracy: 0.9922\n",
            "Epoch 608: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0376 - accuracy: 0.9898 - val_loss: 1.1367 - val_accuracy: 0.7297\n",
            "Epoch 609/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0429 - accuracy: 0.9922\n",
            "Epoch 609: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0405 - accuracy: 0.9864 - val_loss: 1.1370 - val_accuracy: 0.7297\n",
            "Epoch 610/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0299 - accuracy: 0.9844\n",
            "Epoch 610: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0373 - accuracy: 0.9830 - val_loss: 1.1332 - val_accuracy: 0.7162\n",
            "Epoch 611/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0460 - accuracy: 0.9922\n",
            "Epoch 611: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0361 - accuracy: 0.9932 - val_loss: 1.1321 - val_accuracy: 0.7297\n",
            "Epoch 612/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9830\n",
            "Epoch 612: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0381 - accuracy: 0.9830 - val_loss: 1.1343 - val_accuracy: 0.7162\n",
            "Epoch 613/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0274 - accuracy: 1.0000\n",
            "Epoch 613: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0392 - accuracy: 0.9898 - val_loss: 1.1430 - val_accuracy: 0.7297\n",
            "Epoch 614/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0377 - accuracy: 0.9922\n",
            "Epoch 614: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0371 - accuracy: 0.9932 - val_loss: 1.1411 - val_accuracy: 0.7027\n",
            "Epoch 615/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000\n",
            "Epoch 615: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0385 - accuracy: 0.9898 - val_loss: 1.1302 - val_accuracy: 0.7162\n",
            "Epoch 616/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0625 - accuracy: 0.9766\n",
            "Epoch 616: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0453 - accuracy: 0.9762 - val_loss: 1.1053 - val_accuracy: 0.7297\n",
            "Epoch 617/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0397 - accuracy: 0.9688\n",
            "Epoch 617: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0394 - accuracy: 0.9796 - val_loss: 1.0682 - val_accuracy: 0.7432\n",
            "Epoch 618/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0288 - accuracy: 1.0000\n",
            "Epoch 618: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0406 - accuracy: 0.9830 - val_loss: 1.0628 - val_accuracy: 0.7432\n",
            "Epoch 619/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0386 - accuracy: 0.9766\n",
            "Epoch 619: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0408 - accuracy: 0.9796 - val_loss: 1.0550 - val_accuracy: 0.7568\n",
            "Epoch 620/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9796\n",
            "Epoch 620: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0403 - accuracy: 0.9796 - val_loss: 1.0381 - val_accuracy: 0.7568\n",
            "Epoch 621/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0339 - accuracy: 0.9766\n",
            "Epoch 621: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0393 - accuracy: 0.9796 - val_loss: 1.0409 - val_accuracy: 0.7568\n",
            "Epoch 622/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0352 - accuracy: 0.9844\n",
            "Epoch 622: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0431 - accuracy: 0.9796 - val_loss: 1.0490 - val_accuracy: 0.7432\n",
            "Epoch 623/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0584 - accuracy: 0.9688\n",
            "Epoch 623: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0434 - accuracy: 0.9796 - val_loss: 1.0400 - val_accuracy: 0.7432\n",
            "Epoch 624/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9796\n",
            "Epoch 624: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0396 - accuracy: 0.9796 - val_loss: 1.0143 - val_accuracy: 0.7568\n",
            "Epoch 625/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0362 - accuracy: 0.9766\n",
            "Epoch 625: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0398 - accuracy: 0.9796 - val_loss: 1.0048 - val_accuracy: 0.7703\n",
            "Epoch 626/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0399 - accuracy: 0.9688\n",
            "Epoch 626: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0423 - accuracy: 0.9796 - val_loss: 1.0021 - val_accuracy: 0.7568\n",
            "Epoch 627/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0313 - accuracy: 0.9922\n",
            "Epoch 627: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0383 - accuracy: 0.9796 - val_loss: 1.0024 - val_accuracy: 0.7703\n",
            "Epoch 628/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9762\n",
            "Epoch 628: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0428 - accuracy: 0.9762 - val_loss: 1.0053 - val_accuracy: 0.7432\n",
            "Epoch 629/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9844\n",
            "Epoch 629: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0389 - accuracy: 0.9830 - val_loss: 0.9921 - val_accuracy: 0.7568\n",
            "Epoch 630/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0504 - accuracy: 0.9609\n",
            "Epoch 630: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0398 - accuracy: 0.9796 - val_loss: 0.9768 - val_accuracy: 0.7703\n",
            "Epoch 631/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0561 - accuracy: 0.9688\n",
            "Epoch 631: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0412 - accuracy: 0.9830 - val_loss: 0.9805 - val_accuracy: 0.7838\n",
            "Epoch 632/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0537 - accuracy: 0.9688\n",
            "Epoch 632: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0377 - accuracy: 0.9830 - val_loss: 0.9831 - val_accuracy: 0.7838\n",
            "Epoch 633/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - accuracy: 0.9766\n",
            "Epoch 633: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0425 - accuracy: 0.9796 - val_loss: 0.9730 - val_accuracy: 0.7838\n",
            "Epoch 634/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0296 - accuracy: 0.9844\n",
            "Epoch 634: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0414 - accuracy: 0.9796 - val_loss: 0.9657 - val_accuracy: 0.7703\n",
            "Epoch 635/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0376 - accuracy: 0.9766\n",
            "Epoch 635: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0445 - accuracy: 0.9762 - val_loss: 0.9545 - val_accuracy: 0.7703\n",
            "Epoch 636/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0552 - accuracy: 0.9766\n",
            "Epoch 636: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0400 - accuracy: 0.9796 - val_loss: 0.9563 - val_accuracy: 0.7703\n",
            "Epoch 637/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0305 - accuracy: 0.9922\n",
            "Epoch 637: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0376 - accuracy: 0.9932 - val_loss: 0.9651 - val_accuracy: 0.7703\n",
            "Epoch 638/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0421 - accuracy: 0.9922\n",
            "Epoch 638: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.0401 - accuracy: 0.9898 - val_loss: 0.9780 - val_accuracy: 0.7703\n",
            "Epoch 639/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0506 - accuracy: 0.9844\n",
            "Epoch 639: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0374 - accuracy: 0.9830 - val_loss: 0.9912 - val_accuracy: 0.7432\n",
            "Epoch 640/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0463 - accuracy: 0.9844\n",
            "Epoch 640: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0382 - accuracy: 0.9830 - val_loss: 0.9965 - val_accuracy: 0.7297\n",
            "Epoch 641/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0334 - accuracy: 0.9844\n",
            "Epoch 641: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0348 - accuracy: 0.9796 - val_loss: 0.9923 - val_accuracy: 0.7432\n",
            "Epoch 642/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0418 - accuracy: 0.9922\n",
            "Epoch 642: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0401 - accuracy: 0.9830 - val_loss: 0.9956 - val_accuracy: 0.7297\n",
            "Epoch 643/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0525 - accuracy: 0.9688\n",
            "Epoch 643: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0374 - accuracy: 0.9796 - val_loss: 1.0032 - val_accuracy: 0.7297\n",
            "Epoch 644/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0379 - accuracy: 0.9844\n",
            "Epoch 644: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0389 - accuracy: 0.9796 - val_loss: 1.0081 - val_accuracy: 0.7162\n",
            "Epoch 645/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0189 - accuracy: 1.0000\n",
            "Epoch 645: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0426 - accuracy: 0.9796 - val_loss: 1.0244 - val_accuracy: 0.7162\n",
            "Epoch 646/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0362 - accuracy: 0.9844\n",
            "Epoch 646: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0362 - accuracy: 0.9864 - val_loss: 1.0454 - val_accuracy: 0.6757\n",
            "Epoch 647/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0347 - accuracy: 1.0000\n",
            "Epoch 647: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0371 - accuracy: 0.9830 - val_loss: 1.0413 - val_accuracy: 0.6892\n",
            "Epoch 648/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - accuracy: 0.9766\n",
            "Epoch 648: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0411 - accuracy: 0.9796 - val_loss: 1.0356 - val_accuracy: 0.6892\n",
            "Epoch 649/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0352 - accuracy: 0.9922\n",
            "Epoch 649: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0375 - accuracy: 0.9864 - val_loss: 1.0373 - val_accuracy: 0.7027\n",
            "Epoch 650/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0411 - accuracy: 0.9688\n",
            "Epoch 650: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0396 - accuracy: 0.9864 - val_loss: 1.0399 - val_accuracy: 0.7027\n",
            "Epoch 651/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0281 - accuracy: 0.9844\n",
            "Epoch 651: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0379 - accuracy: 0.9830 - val_loss: 1.0586 - val_accuracy: 0.7027\n",
            "Epoch 652/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0299 - accuracy: 1.0000\n",
            "Epoch 652: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 1.0686 - val_accuracy: 0.7027\n",
            "Epoch 653/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0384 - accuracy: 0.9922\n",
            "Epoch 653: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0397 - accuracy: 0.9830 - val_loss: 1.0708 - val_accuracy: 0.7162\n",
            "Epoch 654/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0518 - accuracy: 0.9688\n",
            "Epoch 654: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0396 - accuracy: 0.9796 - val_loss: 1.0983 - val_accuracy: 0.7162\n",
            "Epoch 655/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0256 - accuracy: 0.9922\n",
            "Epoch 655: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0354 - accuracy: 0.9796 - val_loss: 1.1258 - val_accuracy: 0.7162\n",
            "Epoch 656/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0488 - accuracy: 0.9688\n",
            "Epoch 656: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0376 - accuracy: 0.9796 - val_loss: 1.1461 - val_accuracy: 0.6892\n",
            "Epoch 657/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0329 - accuracy: 0.9844\n",
            "Epoch 657: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0393 - accuracy: 0.9830 - val_loss: 1.1488 - val_accuracy: 0.6892\n",
            "Epoch 658/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0376 - accuracy: 0.9844\n",
            "Epoch 658: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0410 - accuracy: 0.9796 - val_loss: 1.1527 - val_accuracy: 0.6892\n",
            "Epoch 659/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0617 - accuracy: 0.9688\n",
            "Epoch 659: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0417 - accuracy: 0.9830 - val_loss: 1.1360 - val_accuracy: 0.7162\n",
            "Epoch 660/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0459 - accuracy: 0.9688\n",
            "Epoch 660: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0391 - accuracy: 0.9830 - val_loss: 1.1231 - val_accuracy: 0.7162\n",
            "Epoch 661/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0386 - accuracy: 0.9844\n",
            "Epoch 661: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0413 - accuracy: 0.9830 - val_loss: 1.1298 - val_accuracy: 0.7297\n",
            "Epoch 662/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0620 - accuracy: 0.9609\n",
            "Epoch 662: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0427 - accuracy: 0.9762 - val_loss: 1.1081 - val_accuracy: 0.7432\n",
            "Epoch 663/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0586 - accuracy: 0.9688\n",
            "Epoch 663: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0409 - accuracy: 0.9796 - val_loss: 1.0835 - val_accuracy: 0.7432\n",
            "Epoch 664/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0424 - accuracy: 0.9844\n",
            "Epoch 664: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0411 - accuracy: 0.9796 - val_loss: 1.0536 - val_accuracy: 0.7432\n",
            "Epoch 665/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0284 - accuracy: 0.9844\n",
            "Epoch 665: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0392 - accuracy: 0.9796 - val_loss: 1.0158 - val_accuracy: 0.7568\n",
            "Epoch 666/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0399 - accuracy: 0.9766\n",
            "Epoch 666: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0372 - accuracy: 0.9796 - val_loss: 0.9838 - val_accuracy: 0.7838\n",
            "Epoch 667/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0340 - accuracy: 0.9922\n",
            "Epoch 667: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0377 - accuracy: 0.9796 - val_loss: 0.9672 - val_accuracy: 0.7838\n",
            "Epoch 668/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0347 - accuracy: 0.9766\n",
            "Epoch 668: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0389 - accuracy: 0.9796 - val_loss: 0.9468 - val_accuracy: 0.7703\n",
            "Epoch 669/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0511 - accuracy: 0.9609\n",
            "Epoch 669: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0379 - accuracy: 0.9796 - val_loss: 0.9233 - val_accuracy: 0.7838\n",
            "Epoch 670/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0212 - accuracy: 0.9922\n",
            "Epoch 670: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0368 - accuracy: 0.9796 - val_loss: 0.9146 - val_accuracy: 0.7838\n",
            "Epoch 671/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000\n",
            "Epoch 671: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0390 - accuracy: 0.9864 - val_loss: 0.9114 - val_accuracy: 0.7838\n",
            "Epoch 672/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0296 - accuracy: 0.9922\n",
            "Epoch 672: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0376 - accuracy: 0.9898 - val_loss: 0.9264 - val_accuracy: 0.7703\n",
            "Epoch 673/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0491 - accuracy: 0.9766\n",
            "Epoch 673: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0373 - accuracy: 0.9796 - val_loss: 0.9443 - val_accuracy: 0.7703\n",
            "Epoch 674/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0329 - accuracy: 0.9688\n",
            "Epoch 674: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0400 - accuracy: 0.9830 - val_loss: 0.9630 - val_accuracy: 0.7568\n",
            "Epoch 675/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0438 - accuracy: 0.9609\n",
            "Epoch 675: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0378 - accuracy: 0.9796 - val_loss: 0.9734 - val_accuracy: 0.7568\n",
            "Epoch 676/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0377 - accuracy: 0.9844\n",
            "Epoch 676: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0401 - accuracy: 0.9796 - val_loss: 0.9832 - val_accuracy: 0.7568\n",
            "Epoch 677/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0426 - accuracy: 0.9766\n",
            "Epoch 677: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0392 - accuracy: 0.9796 - val_loss: 0.9943 - val_accuracy: 0.7703\n",
            "Epoch 678/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9922\n",
            "Epoch 678: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0403 - accuracy: 0.9796 - val_loss: 0.9951 - val_accuracy: 0.7703\n",
            "Epoch 679/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0306 - accuracy: 0.9844\n",
            "Epoch 679: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0349 - accuracy: 0.9796 - val_loss: 0.9842 - val_accuracy: 0.7568\n",
            "Epoch 680/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0425 - accuracy: 0.9766\n",
            "Epoch 680: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0374 - accuracy: 0.9796 - val_loss: 0.9686 - val_accuracy: 0.7568\n",
            "Epoch 681/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9830\n",
            "Epoch 681: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0356 - accuracy: 0.9830 - val_loss: 0.9561 - val_accuracy: 0.7568\n",
            "Epoch 682/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9762\n",
            "Epoch 682: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0411 - accuracy: 0.9762 - val_loss: 0.9359 - val_accuracy: 0.7703\n",
            "Epoch 683/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0279 - accuracy: 0.9844\n",
            "Epoch 683: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0374 - accuracy: 0.9864 - val_loss: 0.9391 - val_accuracy: 0.7703\n",
            "Epoch 684/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0270 - accuracy: 0.9766\n",
            "Epoch 684: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0388 - accuracy: 0.9830 - val_loss: 0.9486 - val_accuracy: 0.7703\n",
            "Epoch 685/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0426 - accuracy: 0.9766\n",
            "Epoch 685: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0395 - accuracy: 0.9796 - val_loss: 0.9575 - val_accuracy: 0.7703\n",
            "Epoch 686/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0331 - accuracy: 0.9688\n",
            "Epoch 686: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0381 - accuracy: 0.9830 - val_loss: 0.9813 - val_accuracy: 0.7568\n",
            "Epoch 687/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0432 - accuracy: 0.9844\n",
            "Epoch 687: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0374 - accuracy: 0.9796 - val_loss: 1.0162 - val_accuracy: 0.7703\n",
            "Epoch 688/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0446 - accuracy: 0.9609\n",
            "Epoch 688: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0376 - accuracy: 0.9796 - val_loss: 1.0079 - val_accuracy: 0.7703\n",
            "Epoch 689/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0388 - accuracy: 0.9844\n",
            "Epoch 689: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0418 - accuracy: 0.9796 - val_loss: 0.9921 - val_accuracy: 0.7703\n",
            "Epoch 690/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0412 - accuracy: 0.9688\n",
            "Epoch 690: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0394 - accuracy: 0.9796 - val_loss: 0.9736 - val_accuracy: 0.7703\n",
            "Epoch 691/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0380 - accuracy: 0.9688\n",
            "Epoch 691: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0345 - accuracy: 0.9796 - val_loss: 0.9714 - val_accuracy: 0.7568\n",
            "Epoch 692/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0289 - accuracy: 0.9844\n",
            "Epoch 692: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0395 - accuracy: 0.9796 - val_loss: 0.9503 - val_accuracy: 0.7568\n",
            "Epoch 693/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0445 - accuracy: 0.9766\n",
            "Epoch 693: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0404 - accuracy: 0.9796 - val_loss: 0.9382 - val_accuracy: 0.7568\n",
            "Epoch 694/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9922\n",
            "Epoch 694: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0383 - accuracy: 0.9796 - val_loss: 0.9387 - val_accuracy: 0.7568\n",
            "Epoch 695/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0365 - accuracy: 0.9688\n",
            "Epoch 695: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0389 - accuracy: 0.9796 - val_loss: 0.9447 - val_accuracy: 0.7432\n",
            "Epoch 696/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0393 - accuracy: 0.9766\n",
            "Epoch 696: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0351 - accuracy: 0.9830 - val_loss: 0.9384 - val_accuracy: 0.7568\n",
            "Epoch 697/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9766\n",
            "Epoch 697: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0383 - accuracy: 0.9796 - val_loss: 0.9390 - val_accuracy: 0.7568\n",
            "Epoch 698/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0260 - accuracy: 0.9844\n",
            "Epoch 698: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0391 - accuracy: 0.9796 - val_loss: 0.9395 - val_accuracy: 0.7703\n",
            "Epoch 699/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0460 - accuracy: 0.9844\n",
            "Epoch 699: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0371 - accuracy: 0.9796 - val_loss: 0.9221 - val_accuracy: 0.7703\n",
            "Epoch 700/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0318 - accuracy: 0.9766\n",
            "Epoch 700: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0387 - accuracy: 0.9796 - val_loss: 0.9026 - val_accuracy: 0.7703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_CNN = CNN_model.predict(feature_valid)\n",
        "\n",
        "# convert the validation vector\n",
        "valid_y_CNN = y_valid_CNN.copy()\n",
        "for i in range(len(y_valid_CNN)):\n",
        "    j = np.where(y_valid_CNN[i] == np.amax(y_valid_CNN[i]))\n",
        "    valid_y_CNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOvZu2r_z1M-",
        "outputId": "a8445bef-23c0-4864-aa93-9b2e0dd6c4b5"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 4ms/step\n",
            "0.7702702702702703\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.60      0.72        15\n",
            "           1       0.73      0.89      0.80        36\n",
            "           2       0.80      0.70      0.74        23\n",
            "\n",
            "   micro avg       0.77      0.77      0.77        74\n",
            "   macro avg       0.81      0.73      0.75        74\n",
            "weighted avg       0.78      0.77      0.77        74\n",
            " samples avg       0.77      0.77      0.77        74\n",
            "\n",
            "auc score:  0.7955618440043882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_CNN = CNN_model.predict(feature_test)\n",
        "# convert the test vector\n",
        "test_y_CNN = y_test_CNN.copy()\n",
        "for i in range(len(y_test_CNN)):\n",
        "    j = np.where(y_test_CNN[i] == np.amax(y_test_CNN[i]))\n",
        "    test_y_CNN[i] = [0, 0, 0]\n",
        "    test_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_test_y,test_y_CNN))\n",
        "print(classification_report(label_test_y,test_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_test_y,test_y_CNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw0FKVhZz3ik",
        "outputId": "7634652c-f343-4a5b-9ca1-c2431fac926c"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 4ms/step\n",
            "0.7096774193548387\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.53      0.69        17\n",
            "           1       0.70      0.81      0.75        43\n",
            "           2       0.65      0.67      0.66        33\n",
            "\n",
            "   micro avg       0.71      0.71      0.71        93\n",
            "   macro avg       0.78      0.67      0.70        93\n",
            "weighted avg       0.74      0.71      0.71        93\n",
            " samples avg       0.71      0.71      0.71        93\n",
            "\n",
            "auc score:  0.751671986624107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss values\n",
        "plt.plot(CNN_history.history['loss'])\n",
        "plt.title('CNN Model loss with class=3')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "HSMxYgA4kl5V",
        "outputId": "28ea7000-5524-42f9-91fe-1b891af39d94"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(CNN_history.history['accuracy'])\n",
        "plt.title('CNN Model accuracy with class=3')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "WkNvEgUf6YI3",
        "outputId": "e4503d47-67b6-4506-b4c8-293a20172506"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tI8S4alF6afA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VLQol4zX6aha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#不好使的"
      ],
      "metadata": {
        "id": "Ye3Uy3jh6akF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_RNN_model():\n",
        "    RNN = Sequential()\n",
        "    RNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False))\n",
        "\n",
        "    RNN.add(Bidirectional(LSTM(word_dimension)))\n",
        "    RNN.add(Dense(word_dimension, activation='relu'))\n",
        "    RNN.add(Dense(3, activation='softmax'))\n",
        "    RNN.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "    return RNN\n",
        "\n",
        "RNN_model = create_RNN_model()\n",
        "RNN_history = RNN_model.fit(feature_train, label_train_y, epochs=800, batch_size=128, validation_data=(feature_valid, label_valid_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OO0KerungT4",
        "outputId": "d5571db1-3c60-4cd2-ca13-e7d071daf03b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "3/3 [==============================] - 4s 405ms/step - loss: 1.0818 - accuracy: 0.4150 - val_loss: 1.0438 - val_accuracy: 0.4865\n",
            "Epoch 2/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9848 - accuracy: 0.5714 - val_loss: 1.0760 - val_accuracy: 0.4865\n",
            "Epoch 3/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9833 - accuracy: 0.5714 - val_loss: 1.0695 - val_accuracy: 0.4865\n",
            "Epoch 4/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9751 - accuracy: 0.5714 - val_loss: 1.0536 - val_accuracy: 0.4865\n",
            "Epoch 5/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9748 - accuracy: 0.5714 - val_loss: 1.0464 - val_accuracy: 0.4865\n",
            "Epoch 6/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9744 - accuracy: 0.5714 - val_loss: 1.0396 - val_accuracy: 0.4865\n",
            "Epoch 7/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9762 - accuracy: 0.5714 - val_loss: 1.0414 - val_accuracy: 0.4865\n",
            "Epoch 8/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.9736 - accuracy: 0.5714 - val_loss: 1.0448 - val_accuracy: 0.4865\n",
            "Epoch 9/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9701 - accuracy: 0.5714 - val_loss: 1.0545 - val_accuracy: 0.4865\n",
            "Epoch 10/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.9717 - accuracy: 0.5714 - val_loss: 1.0592 - val_accuracy: 0.4865\n",
            "Epoch 11/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9766 - accuracy: 0.5714 - val_loss: 1.0527 - val_accuracy: 0.4865\n",
            "Epoch 12/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9674 - accuracy: 0.5714 - val_loss: 1.0395 - val_accuracy: 0.4865\n",
            "Epoch 13/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9700 - accuracy: 0.5714 - val_loss: 1.0428 - val_accuracy: 0.4865\n",
            "Epoch 14/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9756 - accuracy: 0.5714 - val_loss: 1.0414 - val_accuracy: 0.4865\n",
            "Epoch 15/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9736 - accuracy: 0.5714 - val_loss: 1.0384 - val_accuracy: 0.4865\n",
            "Epoch 16/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9668 - accuracy: 0.5714 - val_loss: 1.0349 - val_accuracy: 0.4865\n",
            "Epoch 17/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9683 - accuracy: 0.5714 - val_loss: 1.0361 - val_accuracy: 0.4865\n",
            "Epoch 18/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9693 - accuracy: 0.5714 - val_loss: 1.0359 - val_accuracy: 0.4865\n",
            "Epoch 19/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9683 - accuracy: 0.5714 - val_loss: 1.0496 - val_accuracy: 0.4865\n",
            "Epoch 20/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9665 - accuracy: 0.5714 - val_loss: 1.0462 - val_accuracy: 0.4865\n",
            "Epoch 21/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9635 - accuracy: 0.5714 - val_loss: 1.0312 - val_accuracy: 0.4865\n",
            "Epoch 22/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9644 - accuracy: 0.5714 - val_loss: 1.0290 - val_accuracy: 0.4865\n",
            "Epoch 23/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9636 - accuracy: 0.5714 - val_loss: 1.0334 - val_accuracy: 0.4865\n",
            "Epoch 24/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9605 - accuracy: 0.5714 - val_loss: 1.0414 - val_accuracy: 0.4865\n",
            "Epoch 25/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9608 - accuracy: 0.5714 - val_loss: 1.0447 - val_accuracy: 0.4865\n",
            "Epoch 26/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9613 - accuracy: 0.5714 - val_loss: 1.0375 - val_accuracy: 0.4865\n",
            "Epoch 27/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9595 - accuracy: 0.5714 - val_loss: 1.0277 - val_accuracy: 0.4865\n",
            "Epoch 28/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9591 - accuracy: 0.5714 - val_loss: 1.0224 - val_accuracy: 0.4865\n",
            "Epoch 29/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9599 - accuracy: 0.5714 - val_loss: 1.0295 - val_accuracy: 0.4865\n",
            "Epoch 30/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9566 - accuracy: 0.5714 - val_loss: 1.0259 - val_accuracy: 0.4865\n",
            "Epoch 31/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9552 - accuracy: 0.5714 - val_loss: 1.0199 - val_accuracy: 0.4865\n",
            "Epoch 32/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9561 - accuracy: 0.5714 - val_loss: 1.0203 - val_accuracy: 0.4865\n",
            "Epoch 33/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9531 - accuracy: 0.5714 - val_loss: 1.0309 - val_accuracy: 0.4865\n",
            "Epoch 34/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9531 - accuracy: 0.5714 - val_loss: 1.0277 - val_accuracy: 0.4865\n",
            "Epoch 35/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9528 - accuracy: 0.5714 - val_loss: 1.0248 - val_accuracy: 0.4865\n",
            "Epoch 36/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9544 - accuracy: 0.5714 - val_loss: 1.0482 - val_accuracy: 0.4865\n",
            "Epoch 37/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9569 - accuracy: 0.5714 - val_loss: 1.0287 - val_accuracy: 0.4865\n",
            "Epoch 38/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9495 - accuracy: 0.5714 - val_loss: 1.0053 - val_accuracy: 0.4865\n",
            "Epoch 39/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9507 - accuracy: 0.5714 - val_loss: 1.0148 - val_accuracy: 0.4865\n",
            "Epoch 40/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9457 - accuracy: 0.5714 - val_loss: 1.0328 - val_accuracy: 0.4865\n",
            "Epoch 41/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9495 - accuracy: 0.5714 - val_loss: 1.0172 - val_accuracy: 0.4865\n",
            "Epoch 42/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9611 - accuracy: 0.5714 - val_loss: 1.0134 - val_accuracy: 0.4865\n",
            "Epoch 43/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9550 - accuracy: 0.5714 - val_loss: 1.0329 - val_accuracy: 0.4865\n",
            "Epoch 44/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9472 - accuracy: 0.5714 - val_loss: 1.0120 - val_accuracy: 0.4865\n",
            "Epoch 45/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9463 - accuracy: 0.5714 - val_loss: 0.9965 - val_accuracy: 0.4865\n",
            "Epoch 46/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9481 - accuracy: 0.5714 - val_loss: 1.0056 - val_accuracy: 0.4865\n",
            "Epoch 47/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9441 - accuracy: 0.5714 - val_loss: 1.0300 - val_accuracy: 0.4865\n",
            "Epoch 48/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9466 - accuracy: 0.5714 - val_loss: 1.0153 - val_accuracy: 0.4865\n",
            "Epoch 49/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9406 - accuracy: 0.5748 - val_loss: 1.0021 - val_accuracy: 0.5000\n",
            "Epoch 50/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9401 - accuracy: 0.5986 - val_loss: 1.0070 - val_accuracy: 0.5000\n",
            "Epoch 51/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9339 - accuracy: 0.5884 - val_loss: 1.0311 - val_accuracy: 0.4865\n",
            "Epoch 52/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9404 - accuracy: 0.5816 - val_loss: 0.9848 - val_accuracy: 0.5000\n",
            "Epoch 53/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9446 - accuracy: 0.5952 - val_loss: 0.9817 - val_accuracy: 0.5000\n",
            "Epoch 54/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9396 - accuracy: 0.5884 - val_loss: 1.0175 - val_accuracy: 0.4865\n",
            "Epoch 55/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9463 - accuracy: 0.5782 - val_loss: 1.0517 - val_accuracy: 0.4865\n",
            "Epoch 56/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9410 - accuracy: 0.5850 - val_loss: 0.9869 - val_accuracy: 0.5000\n",
            "Epoch 57/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9535 - accuracy: 0.5952 - val_loss: 0.9889 - val_accuracy: 0.5000\n",
            "Epoch 58/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9448 - accuracy: 0.5918 - val_loss: 1.0111 - val_accuracy: 0.4865\n",
            "Epoch 59/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9350 - accuracy: 0.6020 - val_loss: 1.0391 - val_accuracy: 0.4865\n",
            "Epoch 60/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9423 - accuracy: 0.5952 - val_loss: 1.0062 - val_accuracy: 0.5000\n",
            "Epoch 61/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9313 - accuracy: 0.5952 - val_loss: 0.9835 - val_accuracy: 0.5000\n",
            "Epoch 62/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9300 - accuracy: 0.5884 - val_loss: 0.9929 - val_accuracy: 0.5000\n",
            "Epoch 63/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9308 - accuracy: 0.5986 - val_loss: 1.0171 - val_accuracy: 0.4865\n",
            "Epoch 64/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9315 - accuracy: 0.5986 - val_loss: 0.9973 - val_accuracy: 0.5000\n",
            "Epoch 65/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9238 - accuracy: 0.6020 - val_loss: 0.9834 - val_accuracy: 0.5000\n",
            "Epoch 66/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9285 - accuracy: 0.5986 - val_loss: 0.9791 - val_accuracy: 0.5000\n",
            "Epoch 67/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9258 - accuracy: 0.5952 - val_loss: 0.9950 - val_accuracy: 0.5000\n",
            "Epoch 68/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9214 - accuracy: 0.6020 - val_loss: 1.0020 - val_accuracy: 0.4865\n",
            "Epoch 69/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9206 - accuracy: 0.5986 - val_loss: 0.9990 - val_accuracy: 0.5000\n",
            "Epoch 70/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9181 - accuracy: 0.6020 - val_loss: 0.9840 - val_accuracy: 0.5000\n",
            "Epoch 71/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9162 - accuracy: 0.6020 - val_loss: 0.9875 - val_accuracy: 0.5000\n",
            "Epoch 72/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9125 - accuracy: 0.6020 - val_loss: 1.0031 - val_accuracy: 0.4865\n",
            "Epoch 73/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9157 - accuracy: 0.5952 - val_loss: 0.9776 - val_accuracy: 0.5000\n",
            "Epoch 74/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9144 - accuracy: 0.6054 - val_loss: 0.9726 - val_accuracy: 0.5000\n",
            "Epoch 75/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9082 - accuracy: 0.5986 - val_loss: 1.0463 - val_accuracy: 0.5000\n",
            "Epoch 76/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9280 - accuracy: 0.6020 - val_loss: 0.9925 - val_accuracy: 0.5000\n",
            "Epoch 77/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9236 - accuracy: 0.5986 - val_loss: 0.9806 - val_accuracy: 0.5000\n",
            "Epoch 78/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9109 - accuracy: 0.6054 - val_loss: 1.0334 - val_accuracy: 0.4865\n",
            "Epoch 79/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9277 - accuracy: 0.5952 - val_loss: 1.0385 - val_accuracy: 0.4865\n",
            "Epoch 80/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9189 - accuracy: 0.5952 - val_loss: 0.9826 - val_accuracy: 0.5000\n",
            "Epoch 81/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9065 - accuracy: 0.6054 - val_loss: 0.9640 - val_accuracy: 0.5676\n",
            "Epoch 82/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9058 - accuracy: 0.5986 - val_loss: 1.0012 - val_accuracy: 0.5000\n",
            "Epoch 83/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9048 - accuracy: 0.5986 - val_loss: 1.0307 - val_accuracy: 0.4865\n",
            "Epoch 84/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9090 - accuracy: 0.5918 - val_loss: 0.9928 - val_accuracy: 0.5000\n",
            "Epoch 85/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9003 - accuracy: 0.6054 - val_loss: 0.9772 - val_accuracy: 0.5000\n",
            "Epoch 86/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8995 - accuracy: 0.6054 - val_loss: 0.9983 - val_accuracy: 0.5000\n",
            "Epoch 87/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8926 - accuracy: 0.6020 - val_loss: 1.0246 - val_accuracy: 0.5000\n",
            "Epoch 88/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8959 - accuracy: 0.6020 - val_loss: 1.0255 - val_accuracy: 0.5135\n",
            "Epoch 89/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8870 - accuracy: 0.5952 - val_loss: 1.0645 - val_accuracy: 0.5135\n",
            "Epoch 90/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9021 - accuracy: 0.5986 - val_loss: 1.0049 - val_accuracy: 0.4865\n",
            "Epoch 91/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8943 - accuracy: 0.5986 - val_loss: 1.0169 - val_accuracy: 0.5000\n",
            "Epoch 92/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9019 - accuracy: 0.6020 - val_loss: 0.9995 - val_accuracy: 0.5000\n",
            "Epoch 93/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8982 - accuracy: 0.5986 - val_loss: 0.9896 - val_accuracy: 0.5000\n",
            "Epoch 94/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8993 - accuracy: 0.5986 - val_loss: 1.0062 - val_accuracy: 0.5000\n",
            "Epoch 95/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8946 - accuracy: 0.5986 - val_loss: 1.0107 - val_accuracy: 0.5000\n",
            "Epoch 96/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8965 - accuracy: 0.5986 - val_loss: 1.0096 - val_accuracy: 0.5000\n",
            "Epoch 97/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8949 - accuracy: 0.6054 - val_loss: 1.0041 - val_accuracy: 0.5000\n",
            "Epoch 98/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9085 - accuracy: 0.6054 - val_loss: 0.9893 - val_accuracy: 0.5000\n",
            "Epoch 99/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8866 - accuracy: 0.5952 - val_loss: 0.9835 - val_accuracy: 0.5270\n",
            "Epoch 100/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9035 - accuracy: 0.6054 - val_loss: 1.0028 - val_accuracy: 0.5000\n",
            "Epoch 101/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9089 - accuracy: 0.6054 - val_loss: 1.0718 - val_accuracy: 0.5000\n",
            "Epoch 102/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9040 - accuracy: 0.6020 - val_loss: 0.9957 - val_accuracy: 0.5000\n",
            "Epoch 103/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8900 - accuracy: 0.6020 - val_loss: 0.9923 - val_accuracy: 0.5405\n",
            "Epoch 104/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8826 - accuracy: 0.6088 - val_loss: 1.0109 - val_accuracy: 0.5000\n",
            "Epoch 105/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8842 - accuracy: 0.6054 - val_loss: 1.0315 - val_accuracy: 0.5000\n",
            "Epoch 106/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8861 - accuracy: 0.6088 - val_loss: 0.9970 - val_accuracy: 0.5000\n",
            "Epoch 107/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8857 - accuracy: 0.5986 - val_loss: 0.9985 - val_accuracy: 0.5135\n",
            "Epoch 108/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8720 - accuracy: 0.6020 - val_loss: 1.0222 - val_accuracy: 0.5000\n",
            "Epoch 109/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8882 - accuracy: 0.6088 - val_loss: 1.0819 - val_accuracy: 0.5000\n",
            "Epoch 110/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8877 - accuracy: 0.6020 - val_loss: 1.0163 - val_accuracy: 0.5000\n",
            "Epoch 111/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8632 - accuracy: 0.6088 - val_loss: 0.9688 - val_accuracy: 0.5541\n",
            "Epoch 112/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8628 - accuracy: 0.6020 - val_loss: 1.0449 - val_accuracy: 0.3514\n",
            "Epoch 113/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9266 - accuracy: 0.5136 - val_loss: 0.9894 - val_accuracy: 0.5135\n",
            "Epoch 114/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9055 - accuracy: 0.5952 - val_loss: 1.0056 - val_accuracy: 0.5000\n",
            "Epoch 115/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9068 - accuracy: 0.6054 - val_loss: 0.9881 - val_accuracy: 0.5000\n",
            "Epoch 116/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8966 - accuracy: 0.6088 - val_loss: 1.0335 - val_accuracy: 0.5000\n",
            "Epoch 117/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9077 - accuracy: 0.6020 - val_loss: 1.0465 - val_accuracy: 0.5000\n",
            "Epoch 118/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8967 - accuracy: 0.6020 - val_loss: 1.0191 - val_accuracy: 0.5000\n",
            "Epoch 119/800\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.8796 - accuracy: 0.6054 - val_loss: 1.0070 - val_accuracy: 0.5541\n",
            "Epoch 120/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8799 - accuracy: 0.6020 - val_loss: 1.0146 - val_accuracy: 0.5405\n",
            "Epoch 121/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8745 - accuracy: 0.5986 - val_loss: 1.0429 - val_accuracy: 0.5135\n",
            "Epoch 122/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8701 - accuracy: 0.6088 - val_loss: 1.0306 - val_accuracy: 0.5541\n",
            "Epoch 123/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8658 - accuracy: 0.6020 - val_loss: 1.0347 - val_accuracy: 0.5541\n",
            "Epoch 124/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8650 - accuracy: 0.6054 - val_loss: 1.0474 - val_accuracy: 0.5541\n",
            "Epoch 125/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8617 - accuracy: 0.6020 - val_loss: 1.0524 - val_accuracy: 0.5541\n",
            "Epoch 126/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8576 - accuracy: 0.6020 - val_loss: 1.0646 - val_accuracy: 0.5135\n",
            "Epoch 127/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8558 - accuracy: 0.6088 - val_loss: 1.0390 - val_accuracy: 0.5541\n",
            "Epoch 128/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8448 - accuracy: 0.6122 - val_loss: 1.0478 - val_accuracy: 0.5135\n",
            "Epoch 129/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8431 - accuracy: 0.5986 - val_loss: 1.0801 - val_accuracy: 0.5000\n",
            "Epoch 130/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8804 - accuracy: 0.6190 - val_loss: 1.0552 - val_accuracy: 0.5000\n",
            "Epoch 131/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8740 - accuracy: 0.6054 - val_loss: 1.0915 - val_accuracy: 0.4865\n",
            "Epoch 132/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8803 - accuracy: 0.6020 - val_loss: 1.0087 - val_accuracy: 0.5000\n",
            "Epoch 133/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8660 - accuracy: 0.6088 - val_loss: 0.9989 - val_accuracy: 0.5000\n",
            "Epoch 134/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8536 - accuracy: 0.6156 - val_loss: 1.0384 - val_accuracy: 0.5000\n",
            "Epoch 135/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8521 - accuracy: 0.6122 - val_loss: 1.0419 - val_accuracy: 0.5000\n",
            "Epoch 136/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8326 - accuracy: 0.6088 - val_loss: 1.0567 - val_accuracy: 0.5676\n",
            "Epoch 137/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8275 - accuracy: 0.6497 - val_loss: 1.0435 - val_accuracy: 0.5000\n",
            "Epoch 138/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8507 - accuracy: 0.6088 - val_loss: 1.0273 - val_accuracy: 0.5000\n",
            "Epoch 139/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8431 - accuracy: 0.6088 - val_loss: 1.0127 - val_accuracy: 0.5270\n",
            "Epoch 140/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8114 - accuracy: 0.6361 - val_loss: 1.1007 - val_accuracy: 0.5000\n",
            "Epoch 141/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8518 - accuracy: 0.6122 - val_loss: 1.1185 - val_accuracy: 0.5135\n",
            "Epoch 142/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8310 - accuracy: 0.5986 - val_loss: 1.0632 - val_accuracy: 0.5135\n",
            "Epoch 143/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8377 - accuracy: 0.6565 - val_loss: 1.0410 - val_accuracy: 0.5405\n",
            "Epoch 144/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8044 - accuracy: 0.6259 - val_loss: 1.0809 - val_accuracy: 0.5000\n",
            "Epoch 145/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7963 - accuracy: 0.6156 - val_loss: 1.1042 - val_accuracy: 0.5270\n",
            "Epoch 146/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8116 - accuracy: 0.6259 - val_loss: 1.0210 - val_accuracy: 0.5135\n",
            "Epoch 147/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8311 - accuracy: 0.6054 - val_loss: 1.0909 - val_accuracy: 0.5135\n",
            "Epoch 148/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8916 - accuracy: 0.6020 - val_loss: 0.9770 - val_accuracy: 0.5000\n",
            "Epoch 149/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8734 - accuracy: 0.6088 - val_loss: 0.9478 - val_accuracy: 0.5405\n",
            "Epoch 150/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8295 - accuracy: 0.5986 - val_loss: 1.0425 - val_accuracy: 0.5000\n",
            "Epoch 151/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8723 - accuracy: 0.6054 - val_loss: 0.9757 - val_accuracy: 0.5135\n",
            "Epoch 152/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8490 - accuracy: 0.6054 - val_loss: 0.9712 - val_accuracy: 0.5946\n",
            "Epoch 153/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.8697 - accuracy: 0.6088 - val_loss: 1.0061 - val_accuracy: 0.5135\n",
            "Epoch 154/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8563 - accuracy: 0.6156 - val_loss: 1.0524 - val_accuracy: 0.4865\n",
            "Epoch 155/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8722 - accuracy: 0.6054 - val_loss: 1.0267 - val_accuracy: 0.5000\n",
            "Epoch 156/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8524 - accuracy: 0.5986 - val_loss: 1.0160 - val_accuracy: 0.5000\n",
            "Epoch 157/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8656 - accuracy: 0.6190 - val_loss: 1.0409 - val_accuracy: 0.5000\n",
            "Epoch 158/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8663 - accuracy: 0.6054 - val_loss: 1.0546 - val_accuracy: 0.5000\n",
            "Epoch 159/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8667 - accuracy: 0.5952 - val_loss: 1.0464 - val_accuracy: 0.4865\n",
            "Epoch 160/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8473 - accuracy: 0.6020 - val_loss: 1.0359 - val_accuracy: 0.5000\n",
            "Epoch 161/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8464 - accuracy: 0.5986 - val_loss: 1.0566 - val_accuracy: 0.5000\n",
            "Epoch 162/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8546 - accuracy: 0.6020 - val_loss: 1.0074 - val_accuracy: 0.5405\n",
            "Epoch 163/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8311 - accuracy: 0.6122 - val_loss: 1.0089 - val_accuracy: 0.5135\n",
            "Epoch 164/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8138 - accuracy: 0.6531 - val_loss: 1.0557 - val_accuracy: 0.4054\n",
            "Epoch 165/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8032 - accuracy: 0.6667 - val_loss: 1.0775 - val_accuracy: 0.5000\n",
            "Epoch 166/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7863 - accuracy: 0.6224 - val_loss: 1.1172 - val_accuracy: 0.3919\n",
            "Epoch 167/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8552 - accuracy: 0.6259 - val_loss: 1.0186 - val_accuracy: 0.4865\n",
            "Epoch 168/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8471 - accuracy: 0.5986 - val_loss: 1.0355 - val_accuracy: 0.5000\n",
            "Epoch 169/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8405 - accuracy: 0.6088 - val_loss: 1.0121 - val_accuracy: 0.5135\n",
            "Epoch 170/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8412 - accuracy: 0.6361 - val_loss: 1.0385 - val_accuracy: 0.5000\n",
            "Epoch 171/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8340 - accuracy: 0.6156 - val_loss: 1.0238 - val_accuracy: 0.5135\n",
            "Epoch 172/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8136 - accuracy: 0.6259 - val_loss: 1.0392 - val_accuracy: 0.5000\n",
            "Epoch 173/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7946 - accuracy: 0.6327 - val_loss: 1.0992 - val_accuracy: 0.4459\n",
            "Epoch 174/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7898 - accuracy: 0.6361 - val_loss: 1.1169 - val_accuracy: 0.5135\n",
            "Epoch 175/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7843 - accuracy: 0.6361 - val_loss: 1.1080 - val_accuracy: 0.5135\n",
            "Epoch 176/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7639 - accuracy: 0.6531 - val_loss: 1.1173 - val_accuracy: 0.3784\n",
            "Epoch 177/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7790 - accuracy: 0.6803 - val_loss: 1.0840 - val_accuracy: 0.5541\n",
            "Epoch 178/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7834 - accuracy: 0.6293 - val_loss: 1.1547 - val_accuracy: 0.4324\n",
            "Epoch 179/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8274 - accuracy: 0.6020 - val_loss: 1.0590 - val_accuracy: 0.5541\n",
            "Epoch 180/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7951 - accuracy: 0.6259 - val_loss: 1.0350 - val_accuracy: 0.5405\n",
            "Epoch 181/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8130 - accuracy: 0.6020 - val_loss: 1.0012 - val_accuracy: 0.5135\n",
            "Epoch 182/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8134 - accuracy: 0.6395 - val_loss: 1.0048 - val_accuracy: 0.5541\n",
            "Epoch 183/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7928 - accuracy: 0.6020 - val_loss: 1.0644 - val_accuracy: 0.5000\n",
            "Epoch 184/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8074 - accuracy: 0.5986 - val_loss: 1.0335 - val_accuracy: 0.5270\n",
            "Epoch 185/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7793 - accuracy: 0.6599 - val_loss: 1.0443 - val_accuracy: 0.5135\n",
            "Epoch 186/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7657 - accuracy: 0.6735 - val_loss: 1.0574 - val_accuracy: 0.5270\n",
            "Epoch 187/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7601 - accuracy: 0.6259 - val_loss: 1.0741 - val_accuracy: 0.5000\n",
            "Epoch 188/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7415 - accuracy: 0.6429 - val_loss: 1.1189 - val_accuracy: 0.4459\n",
            "Epoch 189/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7313 - accuracy: 0.6803 - val_loss: 1.1319 - val_accuracy: 0.4324\n",
            "Epoch 190/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7313 - accuracy: 0.6633 - val_loss: 1.1251 - val_accuracy: 0.5000\n",
            "Epoch 191/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7250 - accuracy: 0.6769 - val_loss: 1.1225 - val_accuracy: 0.5000\n",
            "Epoch 192/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7369 - accuracy: 0.6735 - val_loss: 1.0859 - val_accuracy: 0.5135\n",
            "Epoch 193/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7655 - accuracy: 0.6939 - val_loss: 1.0591 - val_accuracy: 0.5270\n",
            "Epoch 194/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7572 - accuracy: 0.6429 - val_loss: 1.0681 - val_accuracy: 0.5000\n",
            "Epoch 195/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8013 - accuracy: 0.6395 - val_loss: 1.0197 - val_accuracy: 0.5270\n",
            "Epoch 196/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7890 - accuracy: 0.6395 - val_loss: 1.0207 - val_accuracy: 0.5270\n",
            "Epoch 197/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7858 - accuracy: 0.6224 - val_loss: 1.0359 - val_accuracy: 0.5405\n",
            "Epoch 198/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7809 - accuracy: 0.6293 - val_loss: 1.0076 - val_accuracy: 0.4865\n",
            "Epoch 199/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7686 - accuracy: 0.6293 - val_loss: 1.0211 - val_accuracy: 0.4459\n",
            "Epoch 200/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7485 - accuracy: 0.6497 - val_loss: 1.1291 - val_accuracy: 0.4324\n",
            "Epoch 201/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7255 - accuracy: 0.6735 - val_loss: 1.2427 - val_accuracy: 0.5000\n",
            "Epoch 202/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7910 - accuracy: 0.6599 - val_loss: 1.1504 - val_accuracy: 0.4054\n",
            "Epoch 203/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7283 - accuracy: 0.6803 - val_loss: 1.1345 - val_accuracy: 0.5405\n",
            "Epoch 204/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8647 - accuracy: 0.6259 - val_loss: 0.9479 - val_accuracy: 0.5811\n",
            "Epoch 205/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7716 - accuracy: 0.6735 - val_loss: 1.0066 - val_accuracy: 0.5541\n",
            "Epoch 206/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7933 - accuracy: 0.6565 - val_loss: 1.0572 - val_accuracy: 0.5000\n",
            "Epoch 207/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8007 - accuracy: 0.6224 - val_loss: 1.0997 - val_accuracy: 0.5135\n",
            "Epoch 208/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7893 - accuracy: 0.6361 - val_loss: 1.0991 - val_accuracy: 0.4865\n",
            "Epoch 209/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7658 - accuracy: 0.6395 - val_loss: 1.0900 - val_accuracy: 0.4730\n",
            "Epoch 210/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7675 - accuracy: 0.6497 - val_loss: 1.1167 - val_accuracy: 0.5270\n",
            "Epoch 211/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7511 - accuracy: 0.6361 - val_loss: 1.1555 - val_accuracy: 0.5270\n",
            "Epoch 212/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7566 - accuracy: 0.6463 - val_loss: 1.1409 - val_accuracy: 0.5541\n",
            "Epoch 213/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7169 - accuracy: 0.6973 - val_loss: 1.1830 - val_accuracy: 0.4324\n",
            "Epoch 214/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7334 - accuracy: 0.6871 - val_loss: 1.1867 - val_accuracy: 0.4730\n",
            "Epoch 215/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7212 - accuracy: 0.6735 - val_loss: 1.1363 - val_accuracy: 0.5811\n",
            "Epoch 216/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.7232 - accuracy: 0.6871 - val_loss: 1.1381 - val_accuracy: 0.5270\n",
            "Epoch 217/800\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6987 - accuracy: 0.6939 - val_loss: 1.1158 - val_accuracy: 0.5000\n",
            "Epoch 218/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6957 - accuracy: 0.7007 - val_loss: 1.1075 - val_accuracy: 0.4730\n",
            "Epoch 219/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6720 - accuracy: 0.7109 - val_loss: 1.1246 - val_accuracy: 0.4865\n",
            "Epoch 220/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6722 - accuracy: 0.7109 - val_loss: 1.1664 - val_accuracy: 0.5541\n",
            "Epoch 221/800\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.6821 - accuracy: 0.6701 - val_loss: 1.1679 - val_accuracy: 0.5000\n",
            "Epoch 222/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6756 - accuracy: 0.6973 - val_loss: 1.1040 - val_accuracy: 0.5541\n",
            "Epoch 223/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.6864 - accuracy: 0.7007 - val_loss: 1.1847 - val_accuracy: 0.3919\n",
            "Epoch 224/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7566 - accuracy: 0.6565 - val_loss: 1.1694 - val_accuracy: 0.4189\n",
            "Epoch 225/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7258 - accuracy: 0.6803 - val_loss: 1.3373 - val_accuracy: 0.4189\n",
            "Epoch 226/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7487 - accuracy: 0.6463 - val_loss: 1.2055 - val_accuracy: 0.5135\n",
            "Epoch 227/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7891 - accuracy: 0.6429 - val_loss: 1.0325 - val_accuracy: 0.5676\n",
            "Epoch 228/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7549 - accuracy: 0.6599 - val_loss: 1.0271 - val_accuracy: 0.5541\n",
            "Epoch 229/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7309 - accuracy: 0.6565 - val_loss: 1.0543 - val_accuracy: 0.5541\n",
            "Epoch 230/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7044 - accuracy: 0.6837 - val_loss: 1.1184 - val_accuracy: 0.4865\n",
            "Epoch 231/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6853 - accuracy: 0.7041 - val_loss: 1.2404 - val_accuracy: 0.4730\n",
            "Epoch 232/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6869 - accuracy: 0.7211 - val_loss: 1.1935 - val_accuracy: 0.5270\n",
            "Epoch 233/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6724 - accuracy: 0.7177 - val_loss: 1.1380 - val_accuracy: 0.4459\n",
            "Epoch 234/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6714 - accuracy: 0.7177 - val_loss: 1.1447 - val_accuracy: 0.5676\n",
            "Epoch 235/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6654 - accuracy: 0.7143 - val_loss: 1.1879 - val_accuracy: 0.5135\n",
            "Epoch 236/800\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.6527 - accuracy: 0.7279 - val_loss: 1.2116 - val_accuracy: 0.4459\n",
            "Epoch 237/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6476 - accuracy: 0.7245 - val_loss: 1.1918 - val_accuracy: 0.5541\n",
            "Epoch 238/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7212 - accuracy: 0.6633 - val_loss: 1.0899 - val_accuracy: 0.5946\n",
            "Epoch 239/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6678 - accuracy: 0.7177 - val_loss: 1.1771 - val_accuracy: 0.4595\n",
            "Epoch 240/800\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.6581 - accuracy: 0.7313 - val_loss: 1.1824 - val_accuracy: 0.5811\n",
            "Epoch 241/800\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.6559 - accuracy: 0.6939 - val_loss: 1.2661 - val_accuracy: 0.4865\n",
            "Epoch 242/800\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.6352 - accuracy: 0.7313 - val_loss: 1.2143 - val_accuracy: 0.4865\n",
            "Epoch 243/800\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.6355 - accuracy: 0.7415 - val_loss: 1.1705 - val_accuracy: 0.4730\n",
            "Epoch 244/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6813 - accuracy: 0.7007 - val_loss: 1.2693 - val_accuracy: 0.4730\n",
            "Epoch 245/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6799 - accuracy: 0.7245 - val_loss: 1.2229 - val_accuracy: 0.5405\n",
            "Epoch 246/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6962 - accuracy: 0.6973 - val_loss: 1.1827 - val_accuracy: 0.4459\n",
            "Epoch 247/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6312 - accuracy: 0.7381 - val_loss: 1.1171 - val_accuracy: 0.6081\n",
            "Epoch 248/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6430 - accuracy: 0.7585 - val_loss: 1.1843 - val_accuracy: 0.4730\n",
            "Epoch 249/800\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.6652 - accuracy: 0.7041 - val_loss: 1.2823 - val_accuracy: 0.3514\n",
            "Epoch 250/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6728 - accuracy: 0.7075 - val_loss: 1.1598 - val_accuracy: 0.5405\n",
            "Epoch 251/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6901 - accuracy: 0.6871 - val_loss: 1.0674 - val_accuracy: 0.5676\n",
            "Epoch 252/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6547 - accuracy: 0.7483 - val_loss: 1.1416 - val_accuracy: 0.5000\n",
            "Epoch 253/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6480 - accuracy: 0.7415 - val_loss: 1.2202 - val_accuracy: 0.4595\n",
            "Epoch 254/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6133 - accuracy: 0.7585 - val_loss: 1.2002 - val_accuracy: 0.5676\n",
            "Epoch 255/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.6647 - accuracy: 0.7177 - val_loss: 1.0046 - val_accuracy: 0.5946\n",
            "Epoch 256/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6433 - accuracy: 0.7415 - val_loss: 1.1428 - val_accuracy: 0.5000\n",
            "Epoch 257/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6236 - accuracy: 0.7619 - val_loss: 1.2334 - val_accuracy: 0.4865\n",
            "Epoch 258/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6519 - accuracy: 0.7245 - val_loss: 1.1666 - val_accuracy: 0.4459\n",
            "Epoch 259/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6314 - accuracy: 0.7551 - val_loss: 1.1875 - val_accuracy: 0.4189\n",
            "Epoch 260/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6292 - accuracy: 0.7415 - val_loss: 1.1921 - val_accuracy: 0.5541\n",
            "Epoch 261/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6008 - accuracy: 0.7483 - val_loss: 1.1211 - val_accuracy: 0.5811\n",
            "Epoch 262/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5819 - accuracy: 0.7721 - val_loss: 1.2613 - val_accuracy: 0.4459\n",
            "Epoch 263/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6018 - accuracy: 0.7721 - val_loss: 1.2511 - val_accuracy: 0.5676\n",
            "Epoch 264/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6424 - accuracy: 0.7483 - val_loss: 1.1026 - val_accuracy: 0.5946\n",
            "Epoch 265/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6955 - accuracy: 0.7211 - val_loss: 1.0642 - val_accuracy: 0.5135\n",
            "Epoch 266/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7446 - accuracy: 0.7109 - val_loss: 1.0772 - val_accuracy: 0.5541\n",
            "Epoch 267/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.7121 - accuracy: 0.6939 - val_loss: 1.0862 - val_accuracy: 0.5676\n",
            "Epoch 268/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6800 - accuracy: 0.7007 - val_loss: 1.1224 - val_accuracy: 0.5405\n",
            "Epoch 269/800\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.6670 - accuracy: 0.7415 - val_loss: 1.2047 - val_accuracy: 0.5000\n",
            "Epoch 270/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6564 - accuracy: 0.7177 - val_loss: 1.1914 - val_accuracy: 0.5135\n",
            "Epoch 271/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6099 - accuracy: 0.7347 - val_loss: 1.1572 - val_accuracy: 0.5541\n",
            "Epoch 272/800\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.6149 - accuracy: 0.7653 - val_loss: 1.1735 - val_accuracy: 0.4730\n",
            "Epoch 273/800\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.5925 - accuracy: 0.7313 - val_loss: 1.1872 - val_accuracy: 0.4459\n",
            "Epoch 274/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5901 - accuracy: 0.7517 - val_loss: 1.1625 - val_accuracy: 0.5000\n",
            "Epoch 275/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5826 - accuracy: 0.7653 - val_loss: 1.1394 - val_accuracy: 0.5676\n",
            "Epoch 276/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.5829 - accuracy: 0.7925 - val_loss: 1.1519 - val_accuracy: 0.5270\n",
            "Epoch 277/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6134 - accuracy: 0.7483 - val_loss: 1.2143 - val_accuracy: 0.5405\n",
            "Epoch 278/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5711 - accuracy: 0.7823 - val_loss: 1.2976 - val_accuracy: 0.4459\n",
            "Epoch 279/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5720 - accuracy: 0.7517 - val_loss: 1.1260 - val_accuracy: 0.5541\n",
            "Epoch 280/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5451 - accuracy: 0.7653 - val_loss: 1.2824 - val_accuracy: 0.5541\n",
            "Epoch 281/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5452 - accuracy: 0.7619 - val_loss: 1.2859 - val_accuracy: 0.5676\n",
            "Epoch 282/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5632 - accuracy: 0.7857 - val_loss: 1.1643 - val_accuracy: 0.5676\n",
            "Epoch 283/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5785 - accuracy: 0.7925 - val_loss: 1.1637 - val_accuracy: 0.5676\n",
            "Epoch 284/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5886 - accuracy: 0.7755 - val_loss: 1.1543 - val_accuracy: 0.5676\n",
            "Epoch 285/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5758 - accuracy: 0.7721 - val_loss: 1.1354 - val_accuracy: 0.5405\n",
            "Epoch 286/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5487 - accuracy: 0.7755 - val_loss: 1.2115 - val_accuracy: 0.4730\n",
            "Epoch 287/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5318 - accuracy: 0.7925 - val_loss: 1.3221 - val_accuracy: 0.5000\n",
            "Epoch 288/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5230 - accuracy: 0.7891 - val_loss: 1.3083 - val_accuracy: 0.4865\n",
            "Epoch 289/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5222 - accuracy: 0.7755 - val_loss: 1.3447 - val_accuracy: 0.4865\n",
            "Epoch 290/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5099 - accuracy: 0.8061 - val_loss: 1.3088 - val_accuracy: 0.5000\n",
            "Epoch 291/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5176 - accuracy: 0.7925 - val_loss: 1.2619 - val_accuracy: 0.5811\n",
            "Epoch 292/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5172 - accuracy: 0.7925 - val_loss: 1.3146 - val_accuracy: 0.5135\n",
            "Epoch 293/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5110 - accuracy: 0.8061 - val_loss: 1.2622 - val_accuracy: 0.5811\n",
            "Epoch 294/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5092 - accuracy: 0.7585 - val_loss: 1.2755 - val_accuracy: 0.5541\n",
            "Epoch 295/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4943 - accuracy: 0.8095 - val_loss: 1.4952 - val_accuracy: 0.5405\n",
            "Epoch 296/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4871 - accuracy: 0.7925 - val_loss: 1.4301 - val_accuracy: 0.4459\n",
            "Epoch 297/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5571 - accuracy: 0.8027 - val_loss: 1.3528 - val_accuracy: 0.5270\n",
            "Epoch 298/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5321 - accuracy: 0.7857 - val_loss: 1.1782 - val_accuracy: 0.5676\n",
            "Epoch 299/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5451 - accuracy: 0.7619 - val_loss: 1.3300 - val_accuracy: 0.5270\n",
            "Epoch 300/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5354 - accuracy: 0.7925 - val_loss: 1.2726 - val_accuracy: 0.5270\n",
            "Epoch 301/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4971 - accuracy: 0.7959 - val_loss: 1.3465 - val_accuracy: 0.5135\n",
            "Epoch 302/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4907 - accuracy: 0.8163 - val_loss: 1.4753 - val_accuracy: 0.4459\n",
            "Epoch 303/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4992 - accuracy: 0.8027 - val_loss: 1.2383 - val_accuracy: 0.5811\n",
            "Epoch 304/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5130 - accuracy: 0.7993 - val_loss: 1.1331 - val_accuracy: 0.5811\n",
            "Epoch 305/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5094 - accuracy: 0.8163 - val_loss: 1.2072 - val_accuracy: 0.6081\n",
            "Epoch 306/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5049 - accuracy: 0.8061 - val_loss: 1.3235 - val_accuracy: 0.5676\n",
            "Epoch 307/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4629 - accuracy: 0.8095 - val_loss: 1.3168 - val_accuracy: 0.5676\n",
            "Epoch 308/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4723 - accuracy: 0.8231 - val_loss: 1.4012 - val_accuracy: 0.5405\n",
            "Epoch 309/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4357 - accuracy: 0.8401 - val_loss: 1.2634 - val_accuracy: 0.5270\n",
            "Epoch 310/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4697 - accuracy: 0.7959 - val_loss: 1.3178 - val_accuracy: 0.5541\n",
            "Epoch 311/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4454 - accuracy: 0.8469 - val_loss: 1.5499 - val_accuracy: 0.4865\n",
            "Epoch 312/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4909 - accuracy: 0.7857 - val_loss: 1.2788 - val_accuracy: 0.5676\n",
            "Epoch 313/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4784 - accuracy: 0.8061 - val_loss: 1.2588 - val_accuracy: 0.5811\n",
            "Epoch 314/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4371 - accuracy: 0.8401 - val_loss: 1.4137 - val_accuracy: 0.4595\n",
            "Epoch 315/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4678 - accuracy: 0.8401 - val_loss: 1.4366 - val_accuracy: 0.5676\n",
            "Epoch 316/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4388 - accuracy: 0.8163 - val_loss: 1.4353 - val_accuracy: 0.5946\n",
            "Epoch 317/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4596 - accuracy: 0.8401 - val_loss: 1.3750 - val_accuracy: 0.4865\n",
            "Epoch 318/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4835 - accuracy: 0.7823 - val_loss: 1.3356 - val_accuracy: 0.4865\n",
            "Epoch 319/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5068 - accuracy: 0.7585 - val_loss: 1.2908 - val_accuracy: 0.5135\n",
            "Epoch 320/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4883 - accuracy: 0.7959 - val_loss: 1.1965 - val_accuracy: 0.5946\n",
            "Epoch 321/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4766 - accuracy: 0.7891 - val_loss: 1.2853 - val_accuracy: 0.5811\n",
            "Epoch 322/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4820 - accuracy: 0.8027 - val_loss: 1.3762 - val_accuracy: 0.5541\n",
            "Epoch 323/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4617 - accuracy: 0.7993 - val_loss: 1.4644 - val_accuracy: 0.5811\n",
            "Epoch 324/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4378 - accuracy: 0.8129 - val_loss: 1.2796 - val_accuracy: 0.5676\n",
            "Epoch 325/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4483 - accuracy: 0.8333 - val_loss: 1.3341 - val_accuracy: 0.5676\n",
            "Epoch 326/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4294 - accuracy: 0.8367 - val_loss: 1.4988 - val_accuracy: 0.5811\n",
            "Epoch 327/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4122 - accuracy: 0.8469 - val_loss: 1.3018 - val_accuracy: 0.5811\n",
            "Epoch 328/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4051 - accuracy: 0.8435 - val_loss: 1.4511 - val_accuracy: 0.5405\n",
            "Epoch 329/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4082 - accuracy: 0.8367 - val_loss: 1.3969 - val_accuracy: 0.5946\n",
            "Epoch 330/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4240 - accuracy: 0.8129 - val_loss: 1.5581 - val_accuracy: 0.4730\n",
            "Epoch 331/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4301 - accuracy: 0.8265 - val_loss: 1.4577 - val_accuracy: 0.5270\n",
            "Epoch 332/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4789 - accuracy: 0.7789 - val_loss: 1.6495 - val_accuracy: 0.5270\n",
            "Epoch 333/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5502 - accuracy: 0.7279 - val_loss: 1.3258 - val_accuracy: 0.5811\n",
            "Epoch 334/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5141 - accuracy: 0.7959 - val_loss: 1.5923 - val_accuracy: 0.5135\n",
            "Epoch 335/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5482 - accuracy: 0.7755 - val_loss: 1.5648 - val_accuracy: 0.5000\n",
            "Epoch 336/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4944 - accuracy: 0.7959 - val_loss: 1.3642 - val_accuracy: 0.5811\n",
            "Epoch 337/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4409 - accuracy: 0.8401 - val_loss: 1.4212 - val_accuracy: 0.6351\n",
            "Epoch 338/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4176 - accuracy: 0.8299 - val_loss: 1.5378 - val_accuracy: 0.5405\n",
            "Epoch 339/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3860 - accuracy: 0.8639 - val_loss: 1.5332 - val_accuracy: 0.5405\n",
            "Epoch 340/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4979 - accuracy: 0.8129 - val_loss: 1.3725 - val_accuracy: 0.5135\n",
            "Epoch 341/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4656 - accuracy: 0.8027 - val_loss: 1.2098 - val_accuracy: 0.5811\n",
            "Epoch 342/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4554 - accuracy: 0.8299 - val_loss: 1.2124 - val_accuracy: 0.5405\n",
            "Epoch 343/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4787 - accuracy: 0.7925 - val_loss: 1.3897 - val_accuracy: 0.5811\n",
            "Epoch 344/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4477 - accuracy: 0.8231 - val_loss: 1.3395 - val_accuracy: 0.6486\n",
            "Epoch 345/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4005 - accuracy: 0.8673 - val_loss: 1.3970 - val_accuracy: 0.6081\n",
            "Epoch 346/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4011 - accuracy: 0.8537 - val_loss: 1.4008 - val_accuracy: 0.6351\n",
            "Epoch 347/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3986 - accuracy: 0.8707 - val_loss: 1.5560 - val_accuracy: 0.5405\n",
            "Epoch 348/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3598 - accuracy: 0.8707 - val_loss: 1.4180 - val_accuracy: 0.5676\n",
            "Epoch 349/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3512 - accuracy: 0.8673 - val_loss: 1.4856 - val_accuracy: 0.5405\n",
            "Epoch 350/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3414 - accuracy: 0.8810 - val_loss: 1.4684 - val_accuracy: 0.5811\n",
            "Epoch 351/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3685 - accuracy: 0.8673 - val_loss: 1.4288 - val_accuracy: 0.5676\n",
            "Epoch 352/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3526 - accuracy: 0.8741 - val_loss: 1.3618 - val_accuracy: 0.5811\n",
            "Epoch 353/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3485 - accuracy: 0.8673 - val_loss: 1.5102 - val_accuracy: 0.5811\n",
            "Epoch 354/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3231 - accuracy: 0.8673 - val_loss: 1.4689 - val_accuracy: 0.5811\n",
            "Epoch 355/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3496 - accuracy: 0.8707 - val_loss: 1.4556 - val_accuracy: 0.5946\n",
            "Epoch 356/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3489 - accuracy: 0.8639 - val_loss: 1.6016 - val_accuracy: 0.6081\n",
            "Epoch 357/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3275 - accuracy: 0.8639 - val_loss: 1.6216 - val_accuracy: 0.5811\n",
            "Epoch 358/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3051 - accuracy: 0.9014 - val_loss: 1.5465 - val_accuracy: 0.5676\n",
            "Epoch 359/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3449 - accuracy: 0.8639 - val_loss: 1.6684 - val_accuracy: 0.5676\n",
            "Epoch 360/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3527 - accuracy: 0.8503 - val_loss: 1.7313 - val_accuracy: 0.5946\n",
            "Epoch 361/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3260 - accuracy: 0.8605 - val_loss: 1.5533 - val_accuracy: 0.6081\n",
            "Epoch 362/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3526 - accuracy: 0.8571 - val_loss: 1.5272 - val_accuracy: 0.5811\n",
            "Epoch 363/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3702 - accuracy: 0.8503 - val_loss: 1.6583 - val_accuracy: 0.5405\n",
            "Epoch 364/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3190 - accuracy: 0.8741 - val_loss: 1.5752 - val_accuracy: 0.5946\n",
            "Epoch 365/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2996 - accuracy: 0.9048 - val_loss: 1.7010 - val_accuracy: 0.5541\n",
            "Epoch 366/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3155 - accuracy: 0.8810 - val_loss: 1.5530 - val_accuracy: 0.5811\n",
            "Epoch 367/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2734 - accuracy: 0.9082 - val_loss: 1.4949 - val_accuracy: 0.5811\n",
            "Epoch 368/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2921 - accuracy: 0.9082 - val_loss: 1.6174 - val_accuracy: 0.5676\n",
            "Epoch 369/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2906 - accuracy: 0.8878 - val_loss: 1.6035 - val_accuracy: 0.5270\n",
            "Epoch 370/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2712 - accuracy: 0.8912 - val_loss: 1.5448 - val_accuracy: 0.5541\n",
            "Epoch 371/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2989 - accuracy: 0.8912 - val_loss: 1.6539 - val_accuracy: 0.5946\n",
            "Epoch 372/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3259 - accuracy: 0.8367 - val_loss: 1.7894 - val_accuracy: 0.5270\n",
            "Epoch 373/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3300 - accuracy: 0.8673 - val_loss: 1.5991 - val_accuracy: 0.5811\n",
            "Epoch 374/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3133 - accuracy: 0.8878 - val_loss: 1.7476 - val_accuracy: 0.6216\n",
            "Epoch 375/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2942 - accuracy: 0.8673 - val_loss: 1.6409 - val_accuracy: 0.5541\n",
            "Epoch 376/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3261 - accuracy: 0.8707 - val_loss: 1.7110 - val_accuracy: 0.6081\n",
            "Epoch 377/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3030 - accuracy: 0.8741 - val_loss: 1.6554 - val_accuracy: 0.5811\n",
            "Epoch 378/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3080 - accuracy: 0.8946 - val_loss: 1.5731 - val_accuracy: 0.5946\n",
            "Epoch 379/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2601 - accuracy: 0.9116 - val_loss: 1.6456 - val_accuracy: 0.5405\n",
            "Epoch 380/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2620 - accuracy: 0.8912 - val_loss: 1.6453 - val_accuracy: 0.6216\n",
            "Epoch 381/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2492 - accuracy: 0.9048 - val_loss: 1.9135 - val_accuracy: 0.5946\n",
            "Epoch 382/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2473 - accuracy: 0.9116 - val_loss: 1.8570 - val_accuracy: 0.5676\n",
            "Epoch 383/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2566 - accuracy: 0.9184 - val_loss: 1.7604 - val_accuracy: 0.6081\n",
            "Epoch 384/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2488 - accuracy: 0.9252 - val_loss: 1.7315 - val_accuracy: 0.5946\n",
            "Epoch 385/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2691 - accuracy: 0.8946 - val_loss: 1.9834 - val_accuracy: 0.5811\n",
            "Epoch 386/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2436 - accuracy: 0.9116 - val_loss: 1.9062 - val_accuracy: 0.6081\n",
            "Epoch 387/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2254 - accuracy: 0.9218 - val_loss: 1.7520 - val_accuracy: 0.5946\n",
            "Epoch 388/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2682 - accuracy: 0.9082 - val_loss: 1.8253 - val_accuracy: 0.5270\n",
            "Epoch 389/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2524 - accuracy: 0.9082 - val_loss: 1.8495 - val_accuracy: 0.5811\n",
            "Epoch 390/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2573 - accuracy: 0.9150 - val_loss: 1.6640 - val_accuracy: 0.5676\n",
            "Epoch 391/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2246 - accuracy: 0.9048 - val_loss: 1.6876 - val_accuracy: 0.5946\n",
            "Epoch 392/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2082 - accuracy: 0.9320 - val_loss: 1.8425 - val_accuracy: 0.5946\n",
            "Epoch 393/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2201 - accuracy: 0.9422 - val_loss: 1.8551 - val_accuracy: 0.5946\n",
            "Epoch 394/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2166 - accuracy: 0.9150 - val_loss: 2.0043 - val_accuracy: 0.6081\n",
            "Epoch 395/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2127 - accuracy: 0.9048 - val_loss: 1.8601 - val_accuracy: 0.6081\n",
            "Epoch 396/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2459 - accuracy: 0.9048 - val_loss: 1.9180 - val_accuracy: 0.5811\n",
            "Epoch 397/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2818 - accuracy: 0.8946 - val_loss: 2.0215 - val_accuracy: 0.5946\n",
            "Epoch 398/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3158 - accuracy: 0.8537 - val_loss: 1.5809 - val_accuracy: 0.6081\n",
            "Epoch 399/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3762 - accuracy: 0.8401 - val_loss: 1.8195 - val_accuracy: 0.6216\n",
            "Epoch 400/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3249 - accuracy: 0.8673 - val_loss: 1.8610 - val_accuracy: 0.5811\n",
            "Epoch 401/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3205 - accuracy: 0.8741 - val_loss: 1.5625 - val_accuracy: 0.6216\n",
            "Epoch 402/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4100 - accuracy: 0.8367 - val_loss: 1.7784 - val_accuracy: 0.6081\n",
            "Epoch 403/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2999 - accuracy: 0.8878 - val_loss: 1.6574 - val_accuracy: 0.5541\n",
            "Epoch 404/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2777 - accuracy: 0.9116 - val_loss: 1.6426 - val_accuracy: 0.6081\n",
            "Epoch 405/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2522 - accuracy: 0.9048 - val_loss: 1.7647 - val_accuracy: 0.5946\n",
            "Epoch 406/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2346 - accuracy: 0.9286 - val_loss: 1.8645 - val_accuracy: 0.5946\n",
            "Epoch 407/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2271 - accuracy: 0.9014 - val_loss: 1.9963 - val_accuracy: 0.5946\n",
            "Epoch 408/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2330 - accuracy: 0.9184 - val_loss: 2.0548 - val_accuracy: 0.6216\n",
            "Epoch 409/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2030 - accuracy: 0.9354 - val_loss: 2.0194 - val_accuracy: 0.5811\n",
            "Epoch 410/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2007 - accuracy: 0.9252 - val_loss: 1.8656 - val_accuracy: 0.5676\n",
            "Epoch 411/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2133 - accuracy: 0.9150 - val_loss: 1.7931 - val_accuracy: 0.5946\n",
            "Epoch 412/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1895 - accuracy: 0.9456 - val_loss: 2.0620 - val_accuracy: 0.6081\n",
            "Epoch 413/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2077 - accuracy: 0.9252 - val_loss: 2.1370 - val_accuracy: 0.6081\n",
            "Epoch 414/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2229 - accuracy: 0.9116 - val_loss: 1.8236 - val_accuracy: 0.5405\n",
            "Epoch 415/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2412 - accuracy: 0.8980 - val_loss: 1.6635 - val_accuracy: 0.5946\n",
            "Epoch 416/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2411 - accuracy: 0.9082 - val_loss: 1.8035 - val_accuracy: 0.5946\n",
            "Epoch 417/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2158 - accuracy: 0.9320 - val_loss: 2.0967 - val_accuracy: 0.5946\n",
            "Epoch 418/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2340 - accuracy: 0.9286 - val_loss: 2.0239 - val_accuracy: 0.5811\n",
            "Epoch 419/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1878 - accuracy: 0.9286 - val_loss: 1.8172 - val_accuracy: 0.6081\n",
            "Epoch 420/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2040 - accuracy: 0.9184 - val_loss: 1.8183 - val_accuracy: 0.6081\n",
            "Epoch 421/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2304 - accuracy: 0.9014 - val_loss: 1.8521 - val_accuracy: 0.6351\n",
            "Epoch 422/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1970 - accuracy: 0.9286 - val_loss: 1.9606 - val_accuracy: 0.5946\n",
            "Epoch 423/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2240 - accuracy: 0.9218 - val_loss: 1.9962 - val_accuracy: 0.5946\n",
            "Epoch 424/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1649 - accuracy: 0.9456 - val_loss: 2.0500 - val_accuracy: 0.5946\n",
            "Epoch 425/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1923 - accuracy: 0.9252 - val_loss: 2.1696 - val_accuracy: 0.5811\n",
            "Epoch 426/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1621 - accuracy: 0.9490 - val_loss: 2.2978 - val_accuracy: 0.5676\n",
            "Epoch 427/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1781 - accuracy: 0.9388 - val_loss: 2.3887 - val_accuracy: 0.6081\n",
            "Epoch 428/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2318 - accuracy: 0.9252 - val_loss: 2.0967 - val_accuracy: 0.5541\n",
            "Epoch 429/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1674 - accuracy: 0.9490 - val_loss: 1.9296 - val_accuracy: 0.5541\n",
            "Epoch 430/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2106 - accuracy: 0.9218 - val_loss: 1.9525 - val_accuracy: 0.5811\n",
            "Epoch 431/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1724 - accuracy: 0.9286 - val_loss: 1.8903 - val_accuracy: 0.5946\n",
            "Epoch 432/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2053 - accuracy: 0.9150 - val_loss: 1.8730 - val_accuracy: 0.6081\n",
            "Epoch 433/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1535 - accuracy: 0.9490 - val_loss: 2.0291 - val_accuracy: 0.6081\n",
            "Epoch 434/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1405 - accuracy: 0.9524 - val_loss: 2.0718 - val_accuracy: 0.5946\n",
            "Epoch 435/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2078 - accuracy: 0.9286 - val_loss: 2.0905 - val_accuracy: 0.6351\n",
            "Epoch 436/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2153 - accuracy: 0.9116 - val_loss: 1.9915 - val_accuracy: 0.5676\n",
            "Epoch 437/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1783 - accuracy: 0.9456 - val_loss: 1.9104 - val_accuracy: 0.5811\n",
            "Epoch 438/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1495 - accuracy: 0.9660 - val_loss: 2.0432 - val_accuracy: 0.5946\n",
            "Epoch 439/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1332 - accuracy: 0.9626 - val_loss: 2.2563 - val_accuracy: 0.6216\n",
            "Epoch 440/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1281 - accuracy: 0.9490 - val_loss: 2.4348 - val_accuracy: 0.6216\n",
            "Epoch 441/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1252 - accuracy: 0.9660 - val_loss: 2.2324 - val_accuracy: 0.6351\n",
            "Epoch 442/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1141 - accuracy: 0.9728 - val_loss: 2.1425 - val_accuracy: 0.6216\n",
            "Epoch 443/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1021 - accuracy: 0.9728 - val_loss: 2.1813 - val_accuracy: 0.6216\n",
            "Epoch 444/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1042 - accuracy: 0.9660 - val_loss: 2.2831 - val_accuracy: 0.6081\n",
            "Epoch 445/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1111 - accuracy: 0.9626 - val_loss: 2.2554 - val_accuracy: 0.6351\n",
            "Epoch 446/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0989 - accuracy: 0.9728 - val_loss: 2.2704 - val_accuracy: 0.6081\n",
            "Epoch 447/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0869 - accuracy: 0.9830 - val_loss: 2.2711 - val_accuracy: 0.6351\n",
            "Epoch 448/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0851 - accuracy: 0.9898 - val_loss: 2.3711 - val_accuracy: 0.5946\n",
            "Epoch 449/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0967 - accuracy: 0.9762 - val_loss: 2.3665 - val_accuracy: 0.6216\n",
            "Epoch 450/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0816 - accuracy: 0.9796 - val_loss: 2.2745 - val_accuracy: 0.6216\n",
            "Epoch 451/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0724 - accuracy: 0.9864 - val_loss: 2.3351 - val_accuracy: 0.6216\n",
            "Epoch 452/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0705 - accuracy: 0.9864 - val_loss: 2.4482 - val_accuracy: 0.6216\n",
            "Epoch 453/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0748 - accuracy: 0.9796 - val_loss: 2.5156 - val_accuracy: 0.6081\n",
            "Epoch 454/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0914 - accuracy: 0.9762 - val_loss: 2.4960 - val_accuracy: 0.5946\n",
            "Epoch 455/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0778 - accuracy: 0.9830 - val_loss: 2.5487 - val_accuracy: 0.6081\n",
            "Epoch 456/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0937 - accuracy: 0.9728 - val_loss: 2.6153 - val_accuracy: 0.6081\n",
            "Epoch 457/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2444 - accuracy: 0.9082 - val_loss: 2.3773 - val_accuracy: 0.6351\n",
            "Epoch 458/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2520 - accuracy: 0.9116 - val_loss: 2.4173 - val_accuracy: 0.5541\n",
            "Epoch 459/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1763 - accuracy: 0.9388 - val_loss: 2.5567 - val_accuracy: 0.6081\n",
            "Epoch 460/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2481 - accuracy: 0.8980 - val_loss: 2.0636 - val_accuracy: 0.6216\n",
            "Epoch 461/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1929 - accuracy: 0.9252 - val_loss: 2.0527 - val_accuracy: 0.6216\n",
            "Epoch 462/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2253 - accuracy: 0.9150 - val_loss: 2.0505 - val_accuracy: 0.6486\n",
            "Epoch 463/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1709 - accuracy: 0.9388 - val_loss: 2.1552 - val_accuracy: 0.6622\n",
            "Epoch 464/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1596 - accuracy: 0.9558 - val_loss: 2.2139 - val_accuracy: 0.5405\n",
            "Epoch 465/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1616 - accuracy: 0.9422 - val_loss: 2.4194 - val_accuracy: 0.5811\n",
            "Epoch 466/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1372 - accuracy: 0.9626 - val_loss: 2.5529 - val_accuracy: 0.5811\n",
            "Epoch 467/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1693 - accuracy: 0.9286 - val_loss: 2.4250 - val_accuracy: 0.6216\n",
            "Epoch 468/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1150 - accuracy: 0.9626 - val_loss: 2.1435 - val_accuracy: 0.5676\n",
            "Epoch 469/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1135 - accuracy: 0.9660 - val_loss: 2.1965 - val_accuracy: 0.5541\n",
            "Epoch 470/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0984 - accuracy: 0.9796 - val_loss: 2.3839 - val_accuracy: 0.6081\n",
            "Epoch 471/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0883 - accuracy: 0.9830 - val_loss: 2.4518 - val_accuracy: 0.6351\n",
            "Epoch 472/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0832 - accuracy: 0.9898 - val_loss: 2.5377 - val_accuracy: 0.6081\n",
            "Epoch 473/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0870 - accuracy: 0.9762 - val_loss: 2.4579 - val_accuracy: 0.5946\n",
            "Epoch 474/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0897 - accuracy: 0.9728 - val_loss: 2.3939 - val_accuracy: 0.5676\n",
            "Epoch 475/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0852 - accuracy: 0.9762 - val_loss: 2.3779 - val_accuracy: 0.6216\n",
            "Epoch 476/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1263 - accuracy: 0.9524 - val_loss: 2.6739 - val_accuracy: 0.5541\n",
            "Epoch 477/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1091 - accuracy: 0.9694 - val_loss: 2.6596 - val_accuracy: 0.5811\n",
            "Epoch 478/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1077 - accuracy: 0.9694 - val_loss: 2.4131 - val_accuracy: 0.5676\n",
            "Epoch 479/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0986 - accuracy: 0.9728 - val_loss: 2.4247 - val_accuracy: 0.6081\n",
            "Epoch 480/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1236 - accuracy: 0.9490 - val_loss: 2.6112 - val_accuracy: 0.6486\n",
            "Epoch 481/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0864 - accuracy: 0.9762 - val_loss: 2.6408 - val_accuracy: 0.5676\n",
            "Epoch 482/800\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0970 - accuracy: 0.9728 - val_loss: 2.5701 - val_accuracy: 0.6216\n",
            "Epoch 483/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1261 - accuracy: 0.9558 - val_loss: 2.9307 - val_accuracy: 0.6081\n",
            "Epoch 484/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2686 - accuracy: 0.8946 - val_loss: 2.9354 - val_accuracy: 0.5270\n",
            "Epoch 485/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2518 - accuracy: 0.9082 - val_loss: 2.5120 - val_accuracy: 0.6216\n",
            "Epoch 486/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3062 - accuracy: 0.8980 - val_loss: 2.7503 - val_accuracy: 0.5270\n",
            "Epoch 487/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2699 - accuracy: 0.8912 - val_loss: 2.3950 - val_accuracy: 0.5946\n",
            "Epoch 488/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3774 - accuracy: 0.8537 - val_loss: 2.2917 - val_accuracy: 0.6216\n",
            "Epoch 489/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3459 - accuracy: 0.8571 - val_loss: 2.0214 - val_accuracy: 0.5676\n",
            "Epoch 490/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3258 - accuracy: 0.8537 - val_loss: 1.9641 - val_accuracy: 0.5405\n",
            "Epoch 491/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2678 - accuracy: 0.8878 - val_loss: 2.1216 - val_accuracy: 0.5135\n",
            "Epoch 492/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2615 - accuracy: 0.8980 - val_loss: 2.1721 - val_accuracy: 0.5676\n",
            "Epoch 493/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2393 - accuracy: 0.8980 - val_loss: 2.1833 - val_accuracy: 0.5811\n",
            "Epoch 494/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2156 - accuracy: 0.9184 - val_loss: 2.1517 - val_accuracy: 0.6216\n",
            "Epoch 495/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2397 - accuracy: 0.9354 - val_loss: 2.2704 - val_accuracy: 0.5811\n",
            "Epoch 496/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1772 - accuracy: 0.9184 - val_loss: 2.3336 - val_accuracy: 0.5541\n",
            "Epoch 497/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1741 - accuracy: 0.9252 - val_loss: 2.3710 - val_accuracy: 0.5946\n",
            "Epoch 498/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1563 - accuracy: 0.9524 - val_loss: 2.2339 - val_accuracy: 0.6351\n",
            "Epoch 499/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1241 - accuracy: 0.9524 - val_loss: 2.2129 - val_accuracy: 0.5676\n",
            "Epoch 500/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1237 - accuracy: 0.9592 - val_loss: 2.3911 - val_accuracy: 0.6486\n",
            "Epoch 501/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0978 - accuracy: 0.9830 - val_loss: 2.5028 - val_accuracy: 0.5946\n",
            "Epoch 502/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0962 - accuracy: 0.9796 - val_loss: 2.4974 - val_accuracy: 0.6486\n",
            "Epoch 503/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0857 - accuracy: 0.9796 - val_loss: 2.4907 - val_accuracy: 0.6216\n",
            "Epoch 504/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0797 - accuracy: 0.9864 - val_loss: 2.3878 - val_accuracy: 0.6216\n",
            "Epoch 505/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0729 - accuracy: 0.9830 - val_loss: 2.4722 - val_accuracy: 0.6351\n",
            "Epoch 506/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0750 - accuracy: 0.9796 - val_loss: 2.4309 - val_accuracy: 0.5946\n",
            "Epoch 507/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0590 - accuracy: 0.9898 - val_loss: 2.4652 - val_accuracy: 0.6216\n",
            "Epoch 508/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0639 - accuracy: 0.9830 - val_loss: 2.5581 - val_accuracy: 0.5946\n",
            "Epoch 509/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0762 - accuracy: 0.9728 - val_loss: 2.6386 - val_accuracy: 0.6081\n",
            "Epoch 510/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0601 - accuracy: 0.9898 - val_loss: 2.6322 - val_accuracy: 0.6216\n",
            "Epoch 511/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0586 - accuracy: 0.9898 - val_loss: 2.6157 - val_accuracy: 0.6081\n",
            "Epoch 512/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0547 - accuracy: 0.9898 - val_loss: 2.7503 - val_accuracy: 0.6216\n",
            "Epoch 513/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0572 - accuracy: 0.9864 - val_loss: 2.8463 - val_accuracy: 0.6081\n",
            "Epoch 514/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0559 - accuracy: 0.9898 - val_loss: 2.7883 - val_accuracy: 0.6216\n",
            "Epoch 515/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0513 - accuracy: 0.9898 - val_loss: 2.7315 - val_accuracy: 0.6081\n",
            "Epoch 516/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0520 - accuracy: 0.9932 - val_loss: 2.7136 - val_accuracy: 0.6216\n",
            "Epoch 517/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0445 - accuracy: 0.9898 - val_loss: 2.7588 - val_accuracy: 0.6351\n",
            "Epoch 518/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0512 - accuracy: 0.9898 - val_loss: 2.7825 - val_accuracy: 0.6351\n",
            "Epoch 519/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0516 - accuracy: 0.9898 - val_loss: 2.7036 - val_accuracy: 0.6216\n",
            "Epoch 520/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0532 - accuracy: 0.9898 - val_loss: 2.7108 - val_accuracy: 0.6081\n",
            "Epoch 521/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0429 - accuracy: 0.9898 - val_loss: 2.7920 - val_accuracy: 0.6081\n",
            "Epoch 522/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0446 - accuracy: 0.9898 - val_loss: 2.8726 - val_accuracy: 0.5946\n",
            "Epoch 523/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0379 - accuracy: 0.9932 - val_loss: 2.9082 - val_accuracy: 0.5946\n",
            "Epoch 524/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0391 - accuracy: 0.9932 - val_loss: 2.9603 - val_accuracy: 0.5811\n",
            "Epoch 525/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0367 - accuracy: 0.9932 - val_loss: 2.9866 - val_accuracy: 0.6081\n",
            "Epoch 526/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0455 - accuracy: 0.9830 - val_loss: 2.9015 - val_accuracy: 0.6081\n",
            "Epoch 527/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0486 - accuracy: 0.9898 - val_loss: 2.6467 - val_accuracy: 0.6216\n",
            "Epoch 528/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0538 - accuracy: 0.9898 - val_loss: 2.5019 - val_accuracy: 0.6486\n",
            "Epoch 529/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0619 - accuracy: 0.9762 - val_loss: 2.6115 - val_accuracy: 0.6081\n",
            "Epoch 530/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0416 - accuracy: 0.9898 - val_loss: 2.9910 - val_accuracy: 0.6081\n",
            "Epoch 531/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0554 - accuracy: 0.9898 - val_loss: 3.1585 - val_accuracy: 0.5946\n",
            "Epoch 532/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0602 - accuracy: 0.9830 - val_loss: 2.9856 - val_accuracy: 0.6216\n",
            "Epoch 533/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0710 - accuracy: 0.9728 - val_loss: 2.8824 - val_accuracy: 0.5946\n",
            "Epoch 534/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1025 - accuracy: 0.9660 - val_loss: 2.7053 - val_accuracy: 0.5946\n",
            "Epoch 535/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0680 - accuracy: 0.9762 - val_loss: 2.6986 - val_accuracy: 0.6351\n",
            "Epoch 536/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0809 - accuracy: 0.9762 - val_loss: 2.8642 - val_accuracy: 0.6216\n",
            "Epoch 537/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1227 - accuracy: 0.9626 - val_loss: 3.1492 - val_accuracy: 0.6081\n",
            "Epoch 538/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1177 - accuracy: 0.9592 - val_loss: 2.9420 - val_accuracy: 0.5946\n",
            "Epoch 539/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1056 - accuracy: 0.9592 - val_loss: 2.7368 - val_accuracy: 0.5946\n",
            "Epoch 540/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0851 - accuracy: 0.9626 - val_loss: 3.1340 - val_accuracy: 0.5946\n",
            "Epoch 541/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0906 - accuracy: 0.9592 - val_loss: 3.1992 - val_accuracy: 0.6081\n",
            "Epoch 542/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1194 - accuracy: 0.9694 - val_loss: 3.0357 - val_accuracy: 0.5946\n",
            "Epoch 543/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1127 - accuracy: 0.9728 - val_loss: 2.8338 - val_accuracy: 0.5946\n",
            "Epoch 544/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1046 - accuracy: 0.9694 - val_loss: 2.7039 - val_accuracy: 0.6216\n",
            "Epoch 545/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1123 - accuracy: 0.9592 - val_loss: 3.0069 - val_accuracy: 0.6351\n",
            "Epoch 546/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0733 - accuracy: 0.9830 - val_loss: 3.1204 - val_accuracy: 0.6351\n",
            "Epoch 547/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0654 - accuracy: 0.9864 - val_loss: 3.0708 - val_accuracy: 0.6216\n",
            "Epoch 548/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0582 - accuracy: 0.9932 - val_loss: 2.9848 - val_accuracy: 0.6486\n",
            "Epoch 549/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0642 - accuracy: 0.9864 - val_loss: 2.9157 - val_accuracy: 0.6351\n",
            "Epoch 550/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0452 - accuracy: 0.9898 - val_loss: 2.8323 - val_accuracy: 0.6216\n",
            "Epoch 551/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0574 - accuracy: 0.9830 - val_loss: 2.9166 - val_accuracy: 0.6351\n",
            "Epoch 552/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0605 - accuracy: 0.9864 - val_loss: 2.8621 - val_accuracy: 0.6486\n",
            "Epoch 553/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0413 - accuracy: 0.9898 - val_loss: 2.9020 - val_accuracy: 0.6081\n",
            "Epoch 554/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0529 - accuracy: 0.9864 - val_loss: 2.9858 - val_accuracy: 0.6216\n",
            "Epoch 555/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0579 - accuracy: 0.9830 - val_loss: 2.9743 - val_accuracy: 0.6081\n",
            "Epoch 556/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0520 - accuracy: 0.9830 - val_loss: 2.9717 - val_accuracy: 0.5676\n",
            "Epoch 557/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0489 - accuracy: 0.9932 - val_loss: 2.9613 - val_accuracy: 0.6081\n",
            "Epoch 558/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0471 - accuracy: 0.9898 - val_loss: 3.0602 - val_accuracy: 0.6081\n",
            "Epoch 559/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0487 - accuracy: 0.9864 - val_loss: 3.0161 - val_accuracy: 0.6081\n",
            "Epoch 560/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0456 - accuracy: 0.9932 - val_loss: 2.9473 - val_accuracy: 0.6216\n",
            "Epoch 561/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0463 - accuracy: 0.9898 - val_loss: 3.0290 - val_accuracy: 0.6351\n",
            "Epoch 562/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0559 - accuracy: 0.9864 - val_loss: 3.0049 - val_accuracy: 0.6486\n",
            "Epoch 563/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 3.0848 - val_accuracy: 0.5811\n",
            "Epoch 564/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0545 - accuracy: 0.9864 - val_loss: 3.1163 - val_accuracy: 0.6351\n",
            "Epoch 565/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0582 - accuracy: 0.9864 - val_loss: 3.1245 - val_accuracy: 0.6216\n",
            "Epoch 566/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0448 - accuracy: 0.9864 - val_loss: 3.1716 - val_accuracy: 0.5946\n",
            "Epoch 567/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0615 - accuracy: 0.9830 - val_loss: 2.8058 - val_accuracy: 0.6216\n",
            "Epoch 568/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0669 - accuracy: 0.9796 - val_loss: 2.8669 - val_accuracy: 0.6081\n",
            "Epoch 569/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0861 - accuracy: 0.9762 - val_loss: 2.9102 - val_accuracy: 0.6081\n",
            "Epoch 570/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0657 - accuracy: 0.9728 - val_loss: 3.0421 - val_accuracy: 0.6216\n",
            "Epoch 571/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0715 - accuracy: 0.9864 - val_loss: 3.0099 - val_accuracy: 0.6351\n",
            "Epoch 572/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0750 - accuracy: 0.9626 - val_loss: 2.8858 - val_accuracy: 0.5946\n",
            "Epoch 573/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1339 - accuracy: 0.9592 - val_loss: 2.7133 - val_accuracy: 0.6081\n",
            "Epoch 574/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0936 - accuracy: 0.9762 - val_loss: 3.0206 - val_accuracy: 0.5811\n",
            "Epoch 575/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1100 - accuracy: 0.9660 - val_loss: 3.0299 - val_accuracy: 0.6351\n",
            "Epoch 576/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0834 - accuracy: 0.9694 - val_loss: 3.0980 - val_accuracy: 0.6216\n",
            "Epoch 577/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1138 - accuracy: 0.9524 - val_loss: 2.7263 - val_accuracy: 0.6486\n",
            "Epoch 578/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0777 - accuracy: 0.9796 - val_loss: 2.6280 - val_accuracy: 0.6081\n",
            "Epoch 579/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0733 - accuracy: 0.9796 - val_loss: 2.6729 - val_accuracy: 0.6081\n",
            "Epoch 580/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0743 - accuracy: 0.9762 - val_loss: 2.7728 - val_accuracy: 0.6351\n",
            "Epoch 581/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0543 - accuracy: 0.9898 - val_loss: 2.8968 - val_accuracy: 0.6081\n",
            "Epoch 582/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0748 - accuracy: 0.9762 - val_loss: 2.9854 - val_accuracy: 0.5946\n",
            "Epoch 583/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0560 - accuracy: 0.9830 - val_loss: 3.0541 - val_accuracy: 0.6351\n",
            "Epoch 584/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0469 - accuracy: 0.9898 - val_loss: 3.0098 - val_accuracy: 0.6351\n",
            "Epoch 585/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0546 - accuracy: 0.9864 - val_loss: 2.9345 - val_accuracy: 0.6351\n",
            "Epoch 586/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0546 - accuracy: 0.9796 - val_loss: 2.9624 - val_accuracy: 0.6351\n",
            "Epoch 587/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0393 - accuracy: 0.9932 - val_loss: 2.9124 - val_accuracy: 0.6486\n",
            "Epoch 588/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0483 - accuracy: 0.9864 - val_loss: 2.9313 - val_accuracy: 0.6216\n",
            "Epoch 589/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0409 - accuracy: 0.9932 - val_loss: 2.8549 - val_accuracy: 0.6622\n",
            "Epoch 590/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0408 - accuracy: 0.9932 - val_loss: 2.8321 - val_accuracy: 0.6486\n",
            "Epoch 591/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0322 - accuracy: 0.9966 - val_loss: 2.9317 - val_accuracy: 0.6351\n",
            "Epoch 592/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0341 - accuracy: 0.9932 - val_loss: 3.0705 - val_accuracy: 0.6216\n",
            "Epoch 593/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0330 - accuracy: 0.9932 - val_loss: 3.1289 - val_accuracy: 0.6081\n",
            "Epoch 594/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0332 - accuracy: 0.9932 - val_loss: 3.1585 - val_accuracy: 0.6351\n",
            "Epoch 595/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0279 - accuracy: 0.9932 - val_loss: 3.1548 - val_accuracy: 0.6216\n",
            "Epoch 596/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0283 - accuracy: 0.9898 - val_loss: 3.0607 - val_accuracy: 0.6351\n",
            "Epoch 597/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0335 - accuracy: 0.9932 - val_loss: 3.0817 - val_accuracy: 0.6486\n",
            "Epoch 598/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0254 - accuracy: 0.9932 - val_loss: 3.1415 - val_accuracy: 0.6351\n",
            "Epoch 599/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0311 - accuracy: 0.9932 - val_loss: 3.2607 - val_accuracy: 0.6486\n",
            "Epoch 600/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0256 - accuracy: 0.9932 - val_loss: 3.2429 - val_accuracy: 0.6351\n",
            "Epoch 601/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0309 - accuracy: 0.9932 - val_loss: 3.2182 - val_accuracy: 0.6351\n",
            "Epoch 602/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0308 - accuracy: 0.9932 - val_loss: 3.2161 - val_accuracy: 0.6081\n",
            "Epoch 603/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0702 - accuracy: 0.9694 - val_loss: 3.1438 - val_accuracy: 0.6486\n",
            "Epoch 604/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0387 - accuracy: 0.9864 - val_loss: 3.1742 - val_accuracy: 0.6081\n",
            "Epoch 605/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0383 - accuracy: 0.9932 - val_loss: 3.3371 - val_accuracy: 0.6216\n",
            "Epoch 606/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0338 - accuracy: 0.9966 - val_loss: 3.3706 - val_accuracy: 0.6216\n",
            "Epoch 607/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0330 - accuracy: 0.9898 - val_loss: 3.2863 - val_accuracy: 0.6081\n",
            "Epoch 608/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0324 - accuracy: 0.9898 - val_loss: 3.1788 - val_accuracy: 0.6081\n",
            "Epoch 609/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0256 - accuracy: 0.9932 - val_loss: 3.1509 - val_accuracy: 0.6216\n",
            "Epoch 610/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0282 - accuracy: 0.9932 - val_loss: 3.1637 - val_accuracy: 0.6351\n",
            "Epoch 611/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0254 - accuracy: 0.9966 - val_loss: 3.2305 - val_accuracy: 0.6486\n",
            "Epoch 612/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0311 - accuracy: 0.9864 - val_loss: 3.2253 - val_accuracy: 0.6486\n",
            "Epoch 613/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0225 - accuracy: 0.9932 - val_loss: 3.2381 - val_accuracy: 0.6216\n",
            "Epoch 614/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0252 - accuracy: 0.9966 - val_loss: 3.3084 - val_accuracy: 0.6351\n",
            "Epoch 615/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0203 - accuracy: 0.9932 - val_loss: 3.3656 - val_accuracy: 0.6351\n",
            "Epoch 616/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0223 - accuracy: 0.9932 - val_loss: 3.3489 - val_accuracy: 0.6351\n",
            "Epoch 617/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0223 - accuracy: 0.9966 - val_loss: 3.3504 - val_accuracy: 0.6351\n",
            "Epoch 618/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0179 - accuracy: 0.9932 - val_loss: 3.3575 - val_accuracy: 0.6351\n",
            "Epoch 619/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0312 - accuracy: 0.9898 - val_loss: 3.3696 - val_accuracy: 0.6486\n",
            "Epoch 620/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0217 - accuracy: 0.9898 - val_loss: 3.3727 - val_accuracy: 0.6216\n",
            "Epoch 621/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 3.4225 - val_accuracy: 0.6216\n",
            "Epoch 622/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0267 - accuracy: 0.9898 - val_loss: 3.4916 - val_accuracy: 0.6216\n",
            "Epoch 623/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0230 - accuracy: 0.9966 - val_loss: 3.5092 - val_accuracy: 0.6081\n",
            "Epoch 624/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0440 - accuracy: 0.9796 - val_loss: 3.3439 - val_accuracy: 0.6351\n",
            "Epoch 625/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0313 - accuracy: 0.9932 - val_loss: 3.3770 - val_accuracy: 0.6351\n",
            "Epoch 626/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0296 - accuracy: 0.9898 - val_loss: 3.5647 - val_accuracy: 0.5811\n",
            "Epoch 627/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0530 - accuracy: 0.9796 - val_loss: 3.2668 - val_accuracy: 0.6351\n",
            "Epoch 628/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0702 - accuracy: 0.9762 - val_loss: 3.0833 - val_accuracy: 0.6622\n",
            "Epoch 629/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0438 - accuracy: 0.9898 - val_loss: 3.0664 - val_accuracy: 0.6081\n",
            "Epoch 630/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0479 - accuracy: 0.9796 - val_loss: 3.2147 - val_accuracy: 0.5946\n",
            "Epoch 631/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0425 - accuracy: 0.9898 - val_loss: 3.3000 - val_accuracy: 0.6081\n",
            "Epoch 632/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0438 - accuracy: 0.9898 - val_loss: 3.2126 - val_accuracy: 0.6081\n",
            "Epoch 633/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0347 - accuracy: 0.9966 - val_loss: 3.3334 - val_accuracy: 0.6351\n",
            "Epoch 634/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0540 - accuracy: 0.9762 - val_loss: 3.3199 - val_accuracy: 0.6216\n",
            "Epoch 635/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0482 - accuracy: 0.9864 - val_loss: 3.0620 - val_accuracy: 0.6216\n",
            "Epoch 636/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0399 - accuracy: 0.9932 - val_loss: 2.9765 - val_accuracy: 0.6216\n",
            "Epoch 637/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0450 - accuracy: 0.9898 - val_loss: 3.1028 - val_accuracy: 0.6081\n",
            "Epoch 638/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1911 - accuracy: 0.9456 - val_loss: 3.6089 - val_accuracy: 0.5811\n",
            "Epoch 639/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1977 - accuracy: 0.9286 - val_loss: 3.0346 - val_accuracy: 0.5405\n",
            "Epoch 640/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3124 - accuracy: 0.9014 - val_loss: 3.0202 - val_accuracy: 0.6216\n",
            "Epoch 641/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2649 - accuracy: 0.9218 - val_loss: 3.8130 - val_accuracy: 0.6081\n",
            "Epoch 642/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2180 - accuracy: 0.9218 - val_loss: 3.2854 - val_accuracy: 0.6081\n",
            "Epoch 643/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3023 - accuracy: 0.8810 - val_loss: 2.8616 - val_accuracy: 0.6486\n",
            "Epoch 644/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3051 - accuracy: 0.8810 - val_loss: 3.0800 - val_accuracy: 0.5405\n",
            "Epoch 645/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1599 - accuracy: 0.9218 - val_loss: 3.4552 - val_accuracy: 0.5946\n",
            "Epoch 646/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3207 - accuracy: 0.9014 - val_loss: 2.6532 - val_accuracy: 0.5676\n",
            "Epoch 647/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2316 - accuracy: 0.9116 - val_loss: 2.8900 - val_accuracy: 0.5676\n",
            "Epoch 648/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1805 - accuracy: 0.9490 - val_loss: 2.9453 - val_accuracy: 0.6081\n",
            "Epoch 649/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1239 - accuracy: 0.9626 - val_loss: 3.0892 - val_accuracy: 0.6081\n",
            "Epoch 650/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1222 - accuracy: 0.9694 - val_loss: 3.1077 - val_accuracy: 0.6216\n",
            "Epoch 651/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0945 - accuracy: 0.9728 - val_loss: 3.1344 - val_accuracy: 0.5946\n",
            "Epoch 652/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0956 - accuracy: 0.9660 - val_loss: 3.0061 - val_accuracy: 0.6486\n",
            "Epoch 653/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0709 - accuracy: 0.9830 - val_loss: 2.7434 - val_accuracy: 0.6216\n",
            "Epoch 654/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0811 - accuracy: 0.9762 - val_loss: 2.7549 - val_accuracy: 0.6351\n",
            "Epoch 655/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0633 - accuracy: 0.9864 - val_loss: 2.9837 - val_accuracy: 0.6351\n",
            "Epoch 656/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0490 - accuracy: 0.9932 - val_loss: 3.1090 - val_accuracy: 0.6351\n",
            "Epoch 657/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0517 - accuracy: 0.9830 - val_loss: 3.1511 - val_accuracy: 0.5811\n",
            "Epoch 658/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0544 - accuracy: 0.9898 - val_loss: 3.2175 - val_accuracy: 0.6351\n",
            "Epoch 659/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0465 - accuracy: 0.9898 - val_loss: 3.2624 - val_accuracy: 0.6351\n",
            "Epoch 660/800\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0527 - accuracy: 0.9830 - val_loss: 3.1080 - val_accuracy: 0.6081\n",
            "Epoch 661/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0469 - accuracy: 0.9898 - val_loss: 3.0104 - val_accuracy: 0.6081\n",
            "Epoch 662/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0401 - accuracy: 0.9932 - val_loss: 2.9972 - val_accuracy: 0.6351\n",
            "Epoch 663/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0364 - accuracy: 0.9966 - val_loss: 2.9704 - val_accuracy: 0.6486\n",
            "Epoch 664/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0310 - accuracy: 0.9966 - val_loss: 2.9940 - val_accuracy: 0.6486\n",
            "Epoch 665/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0347 - accuracy: 0.9898 - val_loss: 3.0066 - val_accuracy: 0.6486\n",
            "Epoch 666/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0319 - accuracy: 0.9932 - val_loss: 3.0530 - val_accuracy: 0.6081\n",
            "Epoch 667/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0339 - accuracy: 0.9966 - val_loss: 3.1230 - val_accuracy: 0.6081\n",
            "Epoch 668/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0318 - accuracy: 0.9966 - val_loss: 3.1924 - val_accuracy: 0.6216\n",
            "Epoch 669/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0273 - accuracy: 0.9966 - val_loss: 3.2140 - val_accuracy: 0.6351\n",
            "Epoch 670/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0247 - accuracy: 0.9966 - val_loss: 3.1897 - val_accuracy: 0.6486\n",
            "Epoch 671/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0250 - accuracy: 0.9966 - val_loss: 3.0583 - val_accuracy: 0.6216\n",
            "Epoch 672/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0496 - accuracy: 0.9830 - val_loss: 3.1974 - val_accuracy: 0.6757\n",
            "Epoch 673/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0561 - accuracy: 0.9830 - val_loss: 3.2010 - val_accuracy: 0.6486\n",
            "Epoch 674/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0390 - accuracy: 0.9898 - val_loss: 3.2485 - val_accuracy: 0.6216\n",
            "Epoch 675/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0636 - accuracy: 0.9796 - val_loss: 3.1574 - val_accuracy: 0.6351\n",
            "Epoch 676/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0472 - accuracy: 0.9898 - val_loss: 3.1182 - val_accuracy: 0.6486\n",
            "Epoch 677/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1032 - accuracy: 0.9694 - val_loss: 3.1363 - val_accuracy: 0.6351\n",
            "Epoch 678/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0439 - accuracy: 0.9864 - val_loss: 3.1325 - val_accuracy: 0.6351\n",
            "Epoch 679/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0424 - accuracy: 0.9864 - val_loss: 3.3620 - val_accuracy: 0.6081\n",
            "Epoch 680/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0609 - accuracy: 0.9864 - val_loss: 3.3531 - val_accuracy: 0.6081\n",
            "Epoch 681/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0394 - accuracy: 0.9932 - val_loss: 3.2923 - val_accuracy: 0.6351\n",
            "Epoch 682/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0343 - accuracy: 0.9864 - val_loss: 3.2114 - val_accuracy: 0.6486\n",
            "Epoch 683/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0269 - accuracy: 0.9932 - val_loss: 3.0777 - val_accuracy: 0.6216\n",
            "Epoch 684/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0295 - accuracy: 0.9966 - val_loss: 3.1054 - val_accuracy: 0.6486\n",
            "Epoch 685/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0339 - accuracy: 0.9932 - val_loss: 3.2142 - val_accuracy: 0.6486\n",
            "Epoch 686/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0298 - accuracy: 0.9966 - val_loss: 3.2871 - val_accuracy: 0.6486\n",
            "Epoch 687/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0265 - accuracy: 0.9966 - val_loss: 3.3899 - val_accuracy: 0.6486\n",
            "Epoch 688/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0246 - accuracy: 0.9932 - val_loss: 3.3404 - val_accuracy: 0.6081\n",
            "Epoch 689/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0332 - accuracy: 0.9932 - val_loss: 3.3121 - val_accuracy: 0.6351\n",
            "Epoch 690/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 3.3350 - val_accuracy: 0.6486\n",
            "Epoch 691/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0240 - accuracy: 0.9966 - val_loss: 3.3909 - val_accuracy: 0.6486\n",
            "Epoch 692/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0251 - accuracy: 0.9898 - val_loss: 3.4390 - val_accuracy: 0.6351\n",
            "Epoch 693/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 3.4427 - val_accuracy: 0.6351\n",
            "Epoch 694/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0202 - accuracy: 0.9966 - val_loss: 3.4454 - val_accuracy: 0.6216\n",
            "Epoch 695/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 3.4888 - val_accuracy: 0.6351\n",
            "Epoch 696/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0203 - accuracy: 0.9932 - val_loss: 3.5255 - val_accuracy: 0.6351\n",
            "Epoch 697/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0189 - accuracy: 0.9966 - val_loss: 3.5152 - val_accuracy: 0.6351\n",
            "Epoch 698/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0180 - accuracy: 0.9966 - val_loss: 3.4547 - val_accuracy: 0.6486\n",
            "Epoch 699/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0191 - accuracy: 0.9966 - val_loss: 3.4481 - val_accuracy: 0.6486\n",
            "Epoch 700/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0166 - accuracy: 0.9966 - val_loss: 3.4604 - val_accuracy: 0.6486\n",
            "Epoch 701/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0197 - accuracy: 0.9932 - val_loss: 3.4669 - val_accuracy: 0.6486\n",
            "Epoch 702/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0176 - accuracy: 0.9932 - val_loss: 3.4669 - val_accuracy: 0.6216\n",
            "Epoch 703/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0174 - accuracy: 0.9966 - val_loss: 3.4914 - val_accuracy: 0.6351\n",
            "Epoch 704/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0163 - accuracy: 0.9966 - val_loss: 3.5254 - val_accuracy: 0.6486\n",
            "Epoch 705/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0165 - accuracy: 0.9966 - val_loss: 3.5717 - val_accuracy: 0.6486\n",
            "Epoch 706/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0162 - accuracy: 0.9932 - val_loss: 3.5823 - val_accuracy: 0.6216\n",
            "Epoch 707/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0155 - accuracy: 0.9966 - val_loss: 3.5735 - val_accuracy: 0.6216\n",
            "Epoch 708/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0246 - accuracy: 0.9898 - val_loss: 3.5055 - val_accuracy: 0.6351\n",
            "Epoch 709/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0142 - accuracy: 0.9966 - val_loss: 3.4409 - val_accuracy: 0.6486\n",
            "Epoch 710/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0234 - accuracy: 0.9932 - val_loss: 3.3418 - val_accuracy: 0.6351\n",
            "Epoch 711/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0470 - accuracy: 0.9830 - val_loss: 3.4453 - val_accuracy: 0.6486\n",
            "Epoch 712/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0498 - accuracy: 0.9694 - val_loss: 3.6380 - val_accuracy: 0.6486\n",
            "Epoch 713/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0426 - accuracy: 0.9898 - val_loss: 3.4737 - val_accuracy: 0.6486\n",
            "Epoch 714/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0445 - accuracy: 0.9830 - val_loss: 3.2821 - val_accuracy: 0.6081\n",
            "Epoch 715/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0360 - accuracy: 0.9932 - val_loss: 3.4022 - val_accuracy: 0.5946\n",
            "Epoch 716/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0435 - accuracy: 0.9864 - val_loss: 3.6724 - val_accuracy: 0.6486\n",
            "Epoch 717/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0329 - accuracy: 0.9932 - val_loss: 3.5427 - val_accuracy: 0.6351\n",
            "Epoch 718/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0286 - accuracy: 0.9898 - val_loss: 3.4259 - val_accuracy: 0.6216\n",
            "Epoch 719/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0478 - accuracy: 0.9898 - val_loss: 3.8669 - val_accuracy: 0.6081\n",
            "Epoch 720/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0759 - accuracy: 0.9626 - val_loss: 3.5349 - val_accuracy: 0.5946\n",
            "Epoch 721/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0672 - accuracy: 0.9762 - val_loss: 3.3425 - val_accuracy: 0.6216\n",
            "Epoch 722/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1011 - accuracy: 0.9592 - val_loss: 3.3746 - val_accuracy: 0.6757\n",
            "Epoch 723/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0690 - accuracy: 0.9728 - val_loss: 3.5600 - val_accuracy: 0.6351\n",
            "Epoch 724/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0671 - accuracy: 0.9728 - val_loss: 3.5032 - val_accuracy: 0.6486\n",
            "Epoch 725/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0516 - accuracy: 0.9898 - val_loss: 3.3672 - val_accuracy: 0.6216\n",
            "Epoch 726/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0836 - accuracy: 0.9626 - val_loss: 3.3328 - val_accuracy: 0.6486\n",
            "Epoch 727/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0509 - accuracy: 0.9830 - val_loss: 3.3657 - val_accuracy: 0.6351\n",
            "Epoch 728/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0386 - accuracy: 0.9864 - val_loss: 3.2561 - val_accuracy: 0.6351\n",
            "Epoch 729/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0502 - accuracy: 0.9864 - val_loss: 3.3894 - val_accuracy: 0.6081\n",
            "Epoch 730/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0318 - accuracy: 0.9966 - val_loss: 3.4341 - val_accuracy: 0.6486\n",
            "Epoch 731/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0346 - accuracy: 0.9864 - val_loss: 3.3934 - val_accuracy: 0.6486\n",
            "Epoch 732/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0344 - accuracy: 0.9898 - val_loss: 3.4288 - val_accuracy: 0.6216\n",
            "Epoch 733/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0247 - accuracy: 0.9966 - val_loss: 3.3805 - val_accuracy: 0.6351\n",
            "Epoch 734/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0267 - accuracy: 0.9898 - val_loss: 3.3530 - val_accuracy: 0.6216\n",
            "Epoch 735/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0212 - accuracy: 0.9966 - val_loss: 3.4113 - val_accuracy: 0.6351\n",
            "Epoch 736/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 3.5086 - val_accuracy: 0.6216\n",
            "Epoch 737/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0182 - accuracy: 0.9932 - val_loss: 3.6136 - val_accuracy: 0.6081\n",
            "Epoch 738/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0254 - accuracy: 0.9932 - val_loss: 3.6992 - val_accuracy: 0.6081\n",
            "Epoch 739/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0208 - accuracy: 0.9966 - val_loss: 3.7620 - val_accuracy: 0.5946\n",
            "Epoch 740/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0392 - accuracy: 0.9864 - val_loss: 3.6380 - val_accuracy: 0.6081\n",
            "Epoch 741/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0307 - accuracy: 0.9898 - val_loss: 3.6133 - val_accuracy: 0.5946\n",
            "Epoch 742/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0270 - accuracy: 0.9898 - val_loss: 3.5660 - val_accuracy: 0.6081\n",
            "Epoch 743/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0215 - accuracy: 0.9898 - val_loss: 3.5732 - val_accuracy: 0.6216\n",
            "Epoch 744/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0241 - accuracy: 0.9932 - val_loss: 3.6411 - val_accuracy: 0.6351\n",
            "Epoch 745/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0297 - accuracy: 0.9932 - val_loss: 3.5524 - val_accuracy: 0.6351\n",
            "Epoch 746/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0225 - accuracy: 0.9932 - val_loss: 3.3883 - val_accuracy: 0.6486\n",
            "Epoch 747/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0272 - accuracy: 0.9898 - val_loss: 3.4104 - val_accuracy: 0.6351\n",
            "Epoch 748/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0166 - accuracy: 0.9966 - val_loss: 3.6595 - val_accuracy: 0.6081\n",
            "Epoch 749/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0246 - accuracy: 0.9966 - val_loss: 3.6193 - val_accuracy: 0.6622\n",
            "Epoch 750/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0315 - accuracy: 0.9932 - val_loss: 3.4827 - val_accuracy: 0.6486\n",
            "Epoch 751/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0517 - accuracy: 0.9694 - val_loss: 3.2426 - val_accuracy: 0.6351\n",
            "Epoch 752/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0267 - accuracy: 0.9932 - val_loss: 3.3311 - val_accuracy: 0.6216\n",
            "Epoch 753/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0291 - accuracy: 0.9932 - val_loss: 3.5023 - val_accuracy: 0.6216\n",
            "Epoch 754/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0550 - accuracy: 0.9864 - val_loss: 3.5740 - val_accuracy: 0.6351\n",
            "Epoch 755/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0310 - accuracy: 0.9932 - val_loss: 3.5359 - val_accuracy: 0.6351\n",
            "Epoch 756/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0423 - accuracy: 0.9864 - val_loss: 3.3915 - val_accuracy: 0.6486\n",
            "Epoch 757/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0271 - accuracy: 0.9966 - val_loss: 3.3866 - val_accuracy: 0.6216\n",
            "Epoch 758/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0309 - accuracy: 0.9898 - val_loss: 3.4432 - val_accuracy: 0.6351\n",
            "Epoch 759/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0227 - accuracy: 0.9966 - val_loss: 3.4497 - val_accuracy: 0.6351\n",
            "Epoch 760/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0252 - accuracy: 0.9932 - val_loss: 3.4910 - val_accuracy: 0.6216\n",
            "Epoch 761/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0220 - accuracy: 0.9966 - val_loss: 3.5457 - val_accuracy: 0.6216\n",
            "Epoch 762/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0225 - accuracy: 0.9932 - val_loss: 3.4920 - val_accuracy: 0.6216\n",
            "Epoch 763/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0223 - accuracy: 0.9966 - val_loss: 3.4382 - val_accuracy: 0.6216\n",
            "Epoch 764/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0256 - accuracy: 0.9898 - val_loss: 3.3418 - val_accuracy: 0.6351\n",
            "Epoch 765/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0193 - accuracy: 0.9932 - val_loss: 3.4239 - val_accuracy: 0.6351\n",
            "Epoch 766/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 3.5429 - val_accuracy: 0.6081\n",
            "Epoch 767/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0254 - accuracy: 0.9932 - val_loss: 3.5603 - val_accuracy: 0.6081\n",
            "Epoch 768/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0207 - accuracy: 0.9932 - val_loss: 3.6321 - val_accuracy: 0.6216\n",
            "Epoch 769/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0176 - accuracy: 0.9966 - val_loss: 3.7010 - val_accuracy: 0.6081\n",
            "Epoch 770/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0196 - accuracy: 0.9966 - val_loss: 3.6918 - val_accuracy: 0.6081\n",
            "Epoch 771/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0173 - accuracy: 0.9966 - val_loss: 3.6342 - val_accuracy: 0.6081\n",
            "Epoch 772/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0159 - accuracy: 0.9966 - val_loss: 3.5287 - val_accuracy: 0.6216\n",
            "Epoch 773/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0255 - accuracy: 0.9898 - val_loss: 3.5619 - val_accuracy: 0.6351\n",
            "Epoch 774/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0169 - accuracy: 0.9898 - val_loss: 3.6222 - val_accuracy: 0.6351\n",
            "Epoch 775/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0142 - accuracy: 0.9966 - val_loss: 3.7268 - val_accuracy: 0.6351\n",
            "Epoch 776/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0175 - accuracy: 0.9966 - val_loss: 3.8156 - val_accuracy: 0.6216\n",
            "Epoch 777/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0181 - accuracy: 0.9966 - val_loss: 3.8577 - val_accuracy: 0.6216\n",
            "Epoch 778/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0176 - accuracy: 0.9932 - val_loss: 3.8360 - val_accuracy: 0.6216\n",
            "Epoch 779/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0160 - accuracy: 0.9932 - val_loss: 3.7572 - val_accuracy: 0.6486\n",
            "Epoch 780/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0144 - accuracy: 0.9932 - val_loss: 3.6827 - val_accuracy: 0.6351\n",
            "Epoch 781/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 3.6700 - val_accuracy: 0.6216\n",
            "Epoch 782/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 3.7188 - val_accuracy: 0.6351\n",
            "Epoch 783/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0148 - accuracy: 0.9932 - val_loss: 3.7122 - val_accuracy: 0.6351\n",
            "Epoch 784/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0216 - accuracy: 0.9898 - val_loss: 3.6668 - val_accuracy: 0.6351\n",
            "Epoch 785/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0125 - accuracy: 0.9966 - val_loss: 3.7729 - val_accuracy: 0.6486\n",
            "Epoch 786/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0348 - accuracy: 0.9898 - val_loss: 3.7119 - val_accuracy: 0.6486\n",
            "Epoch 787/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0234 - accuracy: 0.9932 - val_loss: 3.6400 - val_accuracy: 0.6351\n",
            "Epoch 788/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0226 - accuracy: 0.9966 - val_loss: 3.6135 - val_accuracy: 0.5946\n",
            "Epoch 789/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0178 - accuracy: 0.9932 - val_loss: 3.5338 - val_accuracy: 0.6081\n",
            "Epoch 790/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0213 - accuracy: 0.9932 - val_loss: 3.7175 - val_accuracy: 0.6081\n",
            "Epoch 791/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0249 - accuracy: 0.9932 - val_loss: 3.8375 - val_accuracy: 0.6216\n",
            "Epoch 792/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0140 - accuracy: 0.9966 - val_loss: 3.8616 - val_accuracy: 0.6081\n",
            "Epoch 793/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0333 - accuracy: 0.9830 - val_loss: 3.9114 - val_accuracy: 0.5946\n",
            "Epoch 794/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0208 - accuracy: 0.9966 - val_loss: 3.8599 - val_accuracy: 0.6351\n",
            "Epoch 795/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0288 - accuracy: 0.9898 - val_loss: 3.7649 - val_accuracy: 0.6622\n",
            "Epoch 796/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0201 - accuracy: 0.9932 - val_loss: 3.7515 - val_accuracy: 0.6351\n",
            "Epoch 797/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0250 - accuracy: 0.9932 - val_loss: 4.1245 - val_accuracy: 0.6351\n",
            "Epoch 798/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1141 - accuracy: 0.9524 - val_loss: 3.5796 - val_accuracy: 0.6216\n",
            "Epoch 799/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1611 - accuracy: 0.9422 - val_loss: 3.3700 - val_accuracy: 0.5676\n",
            "Epoch 800/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1537 - accuracy: 0.9456 - val_loss: 3.5352 - val_accuracy: 0.6081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_RNN_model():\n",
        "    RNN = Sequential()\n",
        "    RNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False))\n",
        "\n",
        "    RNN.add(Bidirectional(LSTM(word_dimension)))\n",
        "    RNN.add(Dense(word_dimension, activation='relu'))\n",
        "    RNN.add(Dense(3, activation='softmax'))\n",
        "    RNN.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "    return RNN\n",
        "\n",
        "RNN_model = create_RNN_model()\n",
        "RNN_history = RNN_model.fit(feature_train, label_train_y, epochs=800, batch_size=128, validation_data=(feature_valid, label_valid_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RONOSfBzoqGI",
        "outputId": "32415d1d-6bab-494f-f5ea-7568de7e5fe2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "3/3 [==============================] - 4s 432ms/step - loss: 1.0432 - accuracy: 0.4388 - val_loss: 1.0469 - val_accuracy: 0.4865\n",
            "Epoch 2/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.9820 - accuracy: 0.5714 - val_loss: 1.0850 - val_accuracy: 0.4865\n",
            "Epoch 3/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9896 - accuracy: 0.5714 - val_loss: 1.0943 - val_accuracy: 0.4865\n",
            "Epoch 4/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9840 - accuracy: 0.5714 - val_loss: 1.0665 - val_accuracy: 0.4865\n",
            "Epoch 5/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9853 - accuracy: 0.5714 - val_loss: 1.0526 - val_accuracy: 0.4865\n",
            "Epoch 6/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9785 - accuracy: 0.5714 - val_loss: 1.0431 - val_accuracy: 0.4865\n",
            "Epoch 7/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9764 - accuracy: 0.5714 - val_loss: 1.0456 - val_accuracy: 0.4865\n",
            "Epoch 8/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9764 - accuracy: 0.5714 - val_loss: 1.0466 - val_accuracy: 0.4865\n",
            "Epoch 9/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9742 - accuracy: 0.5714 - val_loss: 1.0524 - val_accuracy: 0.4865\n",
            "Epoch 10/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9723 - accuracy: 0.5714 - val_loss: 1.0578 - val_accuracy: 0.4865\n",
            "Epoch 11/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9730 - accuracy: 0.5714 - val_loss: 1.0584 - val_accuracy: 0.4865\n",
            "Epoch 12/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9754 - accuracy: 0.5714 - val_loss: 1.0543 - val_accuracy: 0.4865\n",
            "Epoch 13/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9722 - accuracy: 0.5714 - val_loss: 1.0584 - val_accuracy: 0.4865\n",
            "Epoch 14/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9720 - accuracy: 0.5714 - val_loss: 1.0562 - val_accuracy: 0.4865\n",
            "Epoch 15/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9708 - accuracy: 0.5714 - val_loss: 1.0507 - val_accuracy: 0.4865\n",
            "Epoch 16/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9709 - accuracy: 0.5714 - val_loss: 1.0509 - val_accuracy: 0.4865\n",
            "Epoch 17/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9708 - accuracy: 0.5714 - val_loss: 1.0520 - val_accuracy: 0.4865\n",
            "Epoch 18/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9704 - accuracy: 0.5714 - val_loss: 1.0520 - val_accuracy: 0.4865\n",
            "Epoch 19/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9730 - accuracy: 0.5714 - val_loss: 1.0496 - val_accuracy: 0.4865\n",
            "Epoch 20/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9717 - accuracy: 0.5714 - val_loss: 1.0437 - val_accuracy: 0.4865\n",
            "Epoch 21/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9700 - accuracy: 0.5714 - val_loss: 1.0426 - val_accuracy: 0.4865\n",
            "Epoch 22/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9689 - accuracy: 0.5714 - val_loss: 1.0464 - val_accuracy: 0.4865\n",
            "Epoch 23/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9702 - accuracy: 0.5714 - val_loss: 1.0541 - val_accuracy: 0.4865\n",
            "Epoch 24/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9655 - accuracy: 0.5714 - val_loss: 1.0472 - val_accuracy: 0.4865\n",
            "Epoch 25/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9683 - accuracy: 0.5714 - val_loss: 1.0421 - val_accuracy: 0.4865\n",
            "Epoch 26/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9683 - accuracy: 0.5714 - val_loss: 1.0437 - val_accuracy: 0.4865\n",
            "Epoch 27/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9659 - accuracy: 0.5714 - val_loss: 1.0554 - val_accuracy: 0.4865\n",
            "Epoch 28/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9661 - accuracy: 0.5714 - val_loss: 1.0505 - val_accuracy: 0.4865\n",
            "Epoch 29/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9635 - accuracy: 0.5714 - val_loss: 1.0357 - val_accuracy: 0.4865\n",
            "Epoch 30/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9676 - accuracy: 0.5714 - val_loss: 1.0292 - val_accuracy: 0.4865\n",
            "Epoch 31/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9671 - accuracy: 0.5714 - val_loss: 1.0482 - val_accuracy: 0.4865\n",
            "Epoch 32/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9636 - accuracy: 0.5714 - val_loss: 1.0694 - val_accuracy: 0.4865\n",
            "Epoch 33/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9674 - accuracy: 0.5714 - val_loss: 1.0599 - val_accuracy: 0.4865\n",
            "Epoch 34/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9626 - accuracy: 0.5714 - val_loss: 1.0391 - val_accuracy: 0.4865\n",
            "Epoch 35/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9594 - accuracy: 0.5714 - val_loss: 1.0281 - val_accuracy: 0.4865\n",
            "Epoch 36/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9618 - accuracy: 0.5714 - val_loss: 1.0294 - val_accuracy: 0.4865\n",
            "Epoch 37/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9589 - accuracy: 0.5714 - val_loss: 1.0415 - val_accuracy: 0.4865\n",
            "Epoch 38/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9586 - accuracy: 0.5714 - val_loss: 1.0431 - val_accuracy: 0.4865\n",
            "Epoch 39/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9562 - accuracy: 0.5714 - val_loss: 1.0430 - val_accuracy: 0.4865\n",
            "Epoch 40/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9533 - accuracy: 0.5714 - val_loss: 1.0244 - val_accuracy: 0.4865\n",
            "Epoch 41/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9541 - accuracy: 0.5714 - val_loss: 1.0214 - val_accuracy: 0.4865\n",
            "Epoch 42/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9459 - accuracy: 0.5714 - val_loss: 1.0440 - val_accuracy: 0.4865\n",
            "Epoch 43/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9509 - accuracy: 0.5714 - val_loss: 1.0249 - val_accuracy: 0.4865\n",
            "Epoch 44/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9446 - accuracy: 0.5714 - val_loss: 1.0041 - val_accuracy: 0.4865\n",
            "Epoch 45/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9439 - accuracy: 0.5714 - val_loss: 1.0170 - val_accuracy: 0.4865\n",
            "Epoch 46/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9555 - accuracy: 0.5714 - val_loss: 1.0413 - val_accuracy: 0.4865\n",
            "Epoch 47/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9478 - accuracy: 0.5714 - val_loss: 1.0012 - val_accuracy: 0.4865\n",
            "Epoch 48/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9508 - accuracy: 0.5816 - val_loss: 1.0041 - val_accuracy: 0.5000\n",
            "Epoch 49/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9466 - accuracy: 0.5884 - val_loss: 1.0198 - val_accuracy: 0.4865\n",
            "Epoch 50/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9399 - accuracy: 0.5714 - val_loss: 1.0405 - val_accuracy: 0.4865\n",
            "Epoch 51/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9425 - accuracy: 0.5714 - val_loss: 1.0180 - val_accuracy: 0.4865\n",
            "Epoch 52/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9338 - accuracy: 0.5986 - val_loss: 1.0107 - val_accuracy: 0.5000\n",
            "Epoch 53/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9314 - accuracy: 0.5986 - val_loss: 1.0124 - val_accuracy: 0.5000\n",
            "Epoch 54/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9308 - accuracy: 0.5986 - val_loss: 1.0089 - val_accuracy: 0.5000\n",
            "Epoch 55/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9265 - accuracy: 0.5986 - val_loss: 0.9862 - val_accuracy: 0.5000\n",
            "Epoch 56/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9341 - accuracy: 0.5986 - val_loss: 1.0027 - val_accuracy: 0.5000\n",
            "Epoch 57/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9368 - accuracy: 0.5952 - val_loss: 1.0480 - val_accuracy: 0.4865\n",
            "Epoch 58/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9354 - accuracy: 0.5952 - val_loss: 0.9920 - val_accuracy: 0.5000\n",
            "Epoch 59/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9219 - accuracy: 0.5952 - val_loss: 0.9785 - val_accuracy: 0.5135\n",
            "Epoch 60/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9306 - accuracy: 0.6054 - val_loss: 0.9931 - val_accuracy: 0.5000\n",
            "Epoch 61/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9204 - accuracy: 0.6054 - val_loss: 1.0309 - val_accuracy: 0.5000\n",
            "Epoch 62/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9216 - accuracy: 0.5986 - val_loss: 1.0071 - val_accuracy: 0.5000\n",
            "Epoch 63/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9106 - accuracy: 0.6054 - val_loss: 0.9837 - val_accuracy: 0.5270\n",
            "Epoch 64/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9203 - accuracy: 0.5986 - val_loss: 0.9861 - val_accuracy: 0.5270\n",
            "Epoch 65/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9222 - accuracy: 0.6020 - val_loss: 1.0163 - val_accuracy: 0.5000\n",
            "Epoch 66/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9111 - accuracy: 0.6054 - val_loss: 1.0082 - val_accuracy: 0.5000\n",
            "Epoch 67/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9098 - accuracy: 0.5986 - val_loss: 1.0082 - val_accuracy: 0.5000\n",
            "Epoch 68/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9070 - accuracy: 0.6020 - val_loss: 1.0079 - val_accuracy: 0.5000\n",
            "Epoch 69/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9019 - accuracy: 0.6122 - val_loss: 0.9963 - val_accuracy: 0.5135\n",
            "Epoch 70/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9082 - accuracy: 0.5986 - val_loss: 1.0143 - val_accuracy: 0.5000\n",
            "Epoch 71/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9110 - accuracy: 0.6054 - val_loss: 0.9986 - val_accuracy: 0.5000\n",
            "Epoch 72/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9028 - accuracy: 0.6054 - val_loss: 0.9722 - val_accuracy: 0.5135\n",
            "Epoch 73/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9055 - accuracy: 0.5918 - val_loss: 0.9785 - val_accuracy: 0.5135\n",
            "Epoch 74/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8996 - accuracy: 0.5952 - val_loss: 0.9979 - val_accuracy: 0.5135\n",
            "Epoch 75/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9000 - accuracy: 0.5986 - val_loss: 0.9983 - val_accuracy: 0.5135\n",
            "Epoch 76/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8984 - accuracy: 0.6020 - val_loss: 0.9918 - val_accuracy: 0.5000\n",
            "Epoch 77/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8953 - accuracy: 0.5986 - val_loss: 0.9916 - val_accuracy: 0.5000\n",
            "Epoch 78/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8953 - accuracy: 0.6088 - val_loss: 1.0112 - val_accuracy: 0.5000\n",
            "Epoch 79/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8939 - accuracy: 0.6088 - val_loss: 0.9983 - val_accuracy: 0.5135\n",
            "Epoch 80/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8859 - accuracy: 0.5952 - val_loss: 0.9856 - val_accuracy: 0.5270\n",
            "Epoch 81/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8865 - accuracy: 0.5986 - val_loss: 0.9990 - val_accuracy: 0.5270\n",
            "Epoch 82/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8789 - accuracy: 0.6088 - val_loss: 0.9951 - val_accuracy: 0.5135\n",
            "Epoch 83/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9060 - accuracy: 0.6054 - val_loss: 1.0138 - val_accuracy: 0.4865\n",
            "Epoch 84/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9148 - accuracy: 0.6020 - val_loss: 0.9836 - val_accuracy: 0.4865\n",
            "Epoch 85/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9166 - accuracy: 0.6020 - val_loss: 1.0060 - val_accuracy: 0.5000\n",
            "Epoch 86/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9094 - accuracy: 0.6054 - val_loss: 0.9950 - val_accuracy: 0.5000\n",
            "Epoch 87/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8961 - accuracy: 0.5986 - val_loss: 0.9705 - val_accuracy: 0.5135\n",
            "Epoch 88/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9087 - accuracy: 0.5748 - val_loss: 1.0059 - val_accuracy: 0.5000\n",
            "Epoch 89/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9057 - accuracy: 0.6020 - val_loss: 0.9959 - val_accuracy: 0.5135\n",
            "Epoch 90/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8929 - accuracy: 0.5986 - val_loss: 0.9806 - val_accuracy: 0.5270\n",
            "Epoch 91/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9061 - accuracy: 0.5850 - val_loss: 1.0047 - val_accuracy: 0.5000\n",
            "Epoch 92/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8969 - accuracy: 0.6054 - val_loss: 1.0475 - val_accuracy: 0.5000\n",
            "Epoch 93/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9035 - accuracy: 0.6054 - val_loss: 0.9956 - val_accuracy: 0.5000\n",
            "Epoch 94/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8948 - accuracy: 0.6054 - val_loss: 0.9880 - val_accuracy: 0.5000\n",
            "Epoch 95/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8858 - accuracy: 0.6054 - val_loss: 1.0043 - val_accuracy: 0.5000\n",
            "Epoch 96/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8864 - accuracy: 0.6054 - val_loss: 1.0008 - val_accuracy: 0.5135\n",
            "Epoch 97/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8861 - accuracy: 0.6020 - val_loss: 0.9874 - val_accuracy: 0.5135\n",
            "Epoch 98/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8819 - accuracy: 0.5952 - val_loss: 0.9796 - val_accuracy: 0.5135\n",
            "Epoch 99/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8801 - accuracy: 0.5918 - val_loss: 0.9772 - val_accuracy: 0.5135\n",
            "Epoch 100/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8766 - accuracy: 0.5986 - val_loss: 0.9945 - val_accuracy: 0.5135\n",
            "Epoch 101/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8750 - accuracy: 0.6020 - val_loss: 0.9667 - val_accuracy: 0.5270\n",
            "Epoch 102/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8747 - accuracy: 0.6054 - val_loss: 0.9744 - val_accuracy: 0.5135\n",
            "Epoch 103/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8708 - accuracy: 0.6054 - val_loss: 1.0170 - val_accuracy: 0.5000\n",
            "Epoch 104/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8814 - accuracy: 0.6054 - val_loss: 0.9899 - val_accuracy: 0.5135\n",
            "Epoch 105/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8686 - accuracy: 0.5986 - val_loss: 0.9866 - val_accuracy: 0.5000\n",
            "Epoch 106/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8677 - accuracy: 0.5918 - val_loss: 1.0230 - val_accuracy: 0.5000\n",
            "Epoch 107/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8787 - accuracy: 0.6054 - val_loss: 1.0052 - val_accuracy: 0.5000\n",
            "Epoch 108/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8624 - accuracy: 0.6054 - val_loss: 0.9820 - val_accuracy: 0.5541\n",
            "Epoch 109/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8771 - accuracy: 0.6020 - val_loss: 1.0092 - val_accuracy: 0.5270\n",
            "Epoch 110/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8626 - accuracy: 0.6054 - val_loss: 1.0470 - val_accuracy: 0.5135\n",
            "Epoch 111/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8632 - accuracy: 0.6156 - val_loss: 1.0391 - val_accuracy: 0.5135\n",
            "Epoch 112/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9137 - accuracy: 0.6054 - val_loss: 0.9595 - val_accuracy: 0.5135\n",
            "Epoch 113/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9006 - accuracy: 0.6156 - val_loss: 0.9947 - val_accuracy: 0.4865\n",
            "Epoch 114/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9175 - accuracy: 0.6020 - val_loss: 1.0310 - val_accuracy: 0.4865\n",
            "Epoch 115/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8932 - accuracy: 0.5918 - val_loss: 0.9701 - val_accuracy: 0.5000\n",
            "Epoch 116/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9111 - accuracy: 0.6088 - val_loss: 0.9791 - val_accuracy: 0.5135\n",
            "Epoch 117/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8976 - accuracy: 0.6054 - val_loss: 1.0072 - val_accuracy: 0.5000\n",
            "Epoch 118/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8885 - accuracy: 0.6054 - val_loss: 1.0435 - val_accuracy: 0.5135\n",
            "Epoch 119/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8856 - accuracy: 0.6054 - val_loss: 0.9788 - val_accuracy: 0.5000\n",
            "Epoch 120/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8757 - accuracy: 0.5918 - val_loss: 0.9679 - val_accuracy: 0.5270\n",
            "Epoch 121/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8818 - accuracy: 0.5918 - val_loss: 0.9777 - val_accuracy: 0.5000\n",
            "Epoch 122/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8633 - accuracy: 0.5952 - val_loss: 0.9969 - val_accuracy: 0.5135\n",
            "Epoch 123/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8685 - accuracy: 0.6054 - val_loss: 0.9853 - val_accuracy: 0.5270\n",
            "Epoch 124/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8615 - accuracy: 0.5986 - val_loss: 0.9702 - val_accuracy: 0.5541\n",
            "Epoch 125/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8575 - accuracy: 0.6054 - val_loss: 0.9830 - val_accuracy: 0.5270\n",
            "Epoch 126/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8551 - accuracy: 0.6020 - val_loss: 1.0032 - val_accuracy: 0.5270\n",
            "Epoch 127/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8579 - accuracy: 0.6054 - val_loss: 1.0007 - val_accuracy: 0.5135\n",
            "Epoch 128/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8447 - accuracy: 0.6020 - val_loss: 1.0114 - val_accuracy: 0.5135\n",
            "Epoch 129/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8529 - accuracy: 0.6088 - val_loss: 0.9850 - val_accuracy: 0.5405\n",
            "Epoch 130/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8618 - accuracy: 0.6054 - val_loss: 0.9924 - val_accuracy: 0.5541\n",
            "Epoch 131/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8571 - accuracy: 0.6088 - val_loss: 1.0396 - val_accuracy: 0.5270\n",
            "Epoch 132/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8461 - accuracy: 0.6020 - val_loss: 1.0074 - val_accuracy: 0.5135\n",
            "Epoch 133/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8391 - accuracy: 0.6088 - val_loss: 0.9944 - val_accuracy: 0.5135\n",
            "Epoch 134/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8332 - accuracy: 0.6190 - val_loss: 1.0064 - val_accuracy: 0.5405\n",
            "Epoch 135/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8411 - accuracy: 0.6088 - val_loss: 1.0160 - val_accuracy: 0.5270\n",
            "Epoch 136/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8580 - accuracy: 0.5952 - val_loss: 0.9731 - val_accuracy: 0.5135\n",
            "Epoch 137/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8316 - accuracy: 0.6122 - val_loss: 0.9921 - val_accuracy: 0.5000\n",
            "Epoch 138/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8366 - accuracy: 0.5952 - val_loss: 0.9833 - val_accuracy: 0.5000\n",
            "Epoch 139/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8516 - accuracy: 0.5986 - val_loss: 0.9893 - val_accuracy: 0.5135\n",
            "Epoch 140/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8361 - accuracy: 0.5952 - val_loss: 1.0044 - val_accuracy: 0.5405\n",
            "Epoch 141/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8311 - accuracy: 0.6054 - val_loss: 0.9702 - val_accuracy: 0.5135\n",
            "Epoch 142/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8433 - accuracy: 0.5952 - val_loss: 0.9632 - val_accuracy: 0.5270\n",
            "Epoch 143/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8755 - accuracy: 0.6361 - val_loss: 0.9797 - val_accuracy: 0.5270\n",
            "Epoch 144/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8242 - accuracy: 0.6224 - val_loss: 1.0895 - val_accuracy: 0.5000\n",
            "Epoch 145/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8586 - accuracy: 0.6156 - val_loss: 1.0870 - val_accuracy: 0.5000\n",
            "Epoch 146/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8801 - accuracy: 0.6327 - val_loss: 1.0038 - val_accuracy: 0.5270\n",
            "Epoch 147/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9297 - accuracy: 0.6224 - val_loss: 0.9879 - val_accuracy: 0.4865\n",
            "Epoch 148/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8489 - accuracy: 0.6122 - val_loss: 1.0618 - val_accuracy: 0.5135\n",
            "Epoch 149/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8743 - accuracy: 0.5952 - val_loss: 1.0733 - val_accuracy: 0.5135\n",
            "Epoch 150/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8682 - accuracy: 0.5952 - val_loss: 1.0110 - val_accuracy: 0.5135\n",
            "Epoch 151/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8403 - accuracy: 0.6054 - val_loss: 0.9652 - val_accuracy: 0.5000\n",
            "Epoch 152/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8369 - accuracy: 0.6122 - val_loss: 0.9593 - val_accuracy: 0.5135\n",
            "Epoch 153/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8275 - accuracy: 0.6293 - val_loss: 0.9829 - val_accuracy: 0.5135\n",
            "Epoch 154/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8196 - accuracy: 0.6293 - val_loss: 0.9798 - val_accuracy: 0.5270\n",
            "Epoch 155/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8221 - accuracy: 0.6361 - val_loss: 0.9768 - val_accuracy: 0.5405\n",
            "Epoch 156/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8156 - accuracy: 0.6327 - val_loss: 0.9868 - val_accuracy: 0.5135\n",
            "Epoch 157/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8083 - accuracy: 0.6429 - val_loss: 0.9702 - val_accuracy: 0.5135\n",
            "Epoch 158/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8168 - accuracy: 0.6497 - val_loss: 0.9526 - val_accuracy: 0.5135\n",
            "Epoch 159/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8165 - accuracy: 0.6361 - val_loss: 0.9611 - val_accuracy: 0.5270\n",
            "Epoch 160/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7994 - accuracy: 0.6395 - val_loss: 0.9864 - val_accuracy: 0.5270\n",
            "Epoch 161/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8143 - accuracy: 0.6224 - val_loss: 1.1493 - val_accuracy: 0.4324\n",
            "Epoch 162/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8758 - accuracy: 0.5952 - val_loss: 0.9862 - val_accuracy: 0.5135\n",
            "Epoch 163/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8143 - accuracy: 0.6361 - val_loss: 1.0601 - val_accuracy: 0.5000\n",
            "Epoch 164/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8559 - accuracy: 0.5918 - val_loss: 1.0172 - val_accuracy: 0.5135\n",
            "Epoch 165/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8261 - accuracy: 0.6054 - val_loss: 0.9462 - val_accuracy: 0.5270\n",
            "Epoch 166/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8292 - accuracy: 0.6293 - val_loss: 0.9579 - val_accuracy: 0.5405\n",
            "Epoch 167/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8042 - accuracy: 0.6054 - val_loss: 1.0283 - val_accuracy: 0.5135\n",
            "Epoch 168/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8278 - accuracy: 0.5952 - val_loss: 0.9730 - val_accuracy: 0.5135\n",
            "Epoch 169/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8073 - accuracy: 0.6088 - val_loss: 0.9566 - val_accuracy: 0.5270\n",
            "Epoch 170/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8230 - accuracy: 0.6497 - val_loss: 0.9716 - val_accuracy: 0.5405\n",
            "Epoch 171/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7892 - accuracy: 0.6361 - val_loss: 1.0430 - val_accuracy: 0.5405\n",
            "Epoch 172/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8067 - accuracy: 0.6327 - val_loss: 1.0057 - val_accuracy: 0.5405\n",
            "Epoch 173/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7732 - accuracy: 0.6667 - val_loss: 0.9771 - val_accuracy: 0.5135\n",
            "Epoch 174/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7911 - accuracy: 0.6224 - val_loss: 1.0466 - val_accuracy: 0.4865\n",
            "Epoch 175/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8011 - accuracy: 0.6054 - val_loss: 1.1392 - val_accuracy: 0.5000\n",
            "Epoch 176/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8220 - accuracy: 0.6190 - val_loss: 1.0327 - val_accuracy: 0.4865\n",
            "Epoch 177/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7910 - accuracy: 0.6497 - val_loss: 0.9773 - val_accuracy: 0.5405\n",
            "Epoch 178/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7785 - accuracy: 0.6701 - val_loss: 1.0254 - val_accuracy: 0.5676\n",
            "Epoch 179/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7727 - accuracy: 0.6565 - val_loss: 1.0507 - val_accuracy: 0.5405\n",
            "Epoch 180/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7739 - accuracy: 0.6531 - val_loss: 0.9970 - val_accuracy: 0.5270\n",
            "Epoch 181/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7674 - accuracy: 0.6497 - val_loss: 0.9785 - val_accuracy: 0.5270\n",
            "Epoch 182/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7658 - accuracy: 0.6701 - val_loss: 1.0380 - val_accuracy: 0.5135\n",
            "Epoch 183/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7538 - accuracy: 0.6531 - val_loss: 1.0196 - val_accuracy: 0.5135\n",
            "Epoch 184/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7569 - accuracy: 0.6905 - val_loss: 1.0354 - val_accuracy: 0.5135\n",
            "Epoch 185/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7305 - accuracy: 0.6837 - val_loss: 0.9813 - val_accuracy: 0.5135\n",
            "Epoch 186/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7372 - accuracy: 0.6803 - val_loss: 1.0212 - val_accuracy: 0.5000\n",
            "Epoch 187/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7078 - accuracy: 0.6667 - val_loss: 1.1374 - val_accuracy: 0.4865\n",
            "Epoch 188/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7770 - accuracy: 0.6531 - val_loss: 1.0199 - val_accuracy: 0.5135\n",
            "Epoch 189/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7748 - accuracy: 0.6599 - val_loss: 0.9285 - val_accuracy: 0.5270\n",
            "Epoch 190/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8200 - accuracy: 0.5952 - val_loss: 0.9826 - val_accuracy: 0.6081\n",
            "Epoch 191/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8185 - accuracy: 0.6361 - val_loss: 1.0290 - val_accuracy: 0.5541\n",
            "Epoch 192/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8264 - accuracy: 0.6361 - val_loss: 1.0612 - val_accuracy: 0.5541\n",
            "Epoch 193/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8423 - accuracy: 0.6395 - val_loss: 1.0558 - val_accuracy: 0.5676\n",
            "Epoch 194/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8249 - accuracy: 0.6395 - val_loss: 0.9672 - val_accuracy: 0.5541\n",
            "Epoch 195/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7828 - accuracy: 0.6735 - val_loss: 1.0096 - val_accuracy: 0.5000\n",
            "Epoch 196/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7843 - accuracy: 0.6327 - val_loss: 1.0220 - val_accuracy: 0.5135\n",
            "Epoch 197/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7618 - accuracy: 0.6633 - val_loss: 1.0254 - val_accuracy: 0.4865\n",
            "Epoch 198/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7718 - accuracy: 0.6667 - val_loss: 1.0378 - val_accuracy: 0.5000\n",
            "Epoch 199/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7511 - accuracy: 0.6735 - val_loss: 1.0559 - val_accuracy: 0.5135\n",
            "Epoch 200/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7568 - accuracy: 0.6769 - val_loss: 1.0180 - val_accuracy: 0.4865\n",
            "Epoch 201/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7479 - accuracy: 0.6667 - val_loss: 1.0302 - val_accuracy: 0.5000\n",
            "Epoch 202/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7423 - accuracy: 0.6633 - val_loss: 1.0121 - val_accuracy: 0.5135\n",
            "Epoch 203/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7240 - accuracy: 0.6973 - val_loss: 1.0531 - val_accuracy: 0.4595\n",
            "Epoch 204/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7380 - accuracy: 0.6633 - val_loss: 1.0432 - val_accuracy: 0.4865\n",
            "Epoch 205/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7067 - accuracy: 0.6667 - val_loss: 1.0293 - val_accuracy: 0.5135\n",
            "Epoch 206/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7380 - accuracy: 0.6599 - val_loss: 1.0160 - val_accuracy: 0.5000\n",
            "Epoch 207/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7059 - accuracy: 0.6667 - val_loss: 1.0461 - val_accuracy: 0.5000\n",
            "Epoch 208/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6904 - accuracy: 0.6769 - val_loss: 1.0633 - val_accuracy: 0.4865\n",
            "Epoch 209/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6792 - accuracy: 0.7279 - val_loss: 1.0052 - val_accuracy: 0.5135\n",
            "Epoch 210/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7034 - accuracy: 0.7075 - val_loss: 1.0791 - val_accuracy: 0.4865\n",
            "Epoch 211/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6646 - accuracy: 0.7313 - val_loss: 1.0383 - val_accuracy: 0.5270\n",
            "Epoch 212/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6812 - accuracy: 0.6905 - val_loss: 1.0006 - val_accuracy: 0.5135\n",
            "Epoch 213/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6727 - accuracy: 0.7177 - val_loss: 1.0745 - val_accuracy: 0.5135\n",
            "Epoch 214/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7051 - accuracy: 0.6769 - val_loss: 1.3419 - val_accuracy: 0.5000\n",
            "Epoch 215/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7211 - accuracy: 0.7041 - val_loss: 0.9885 - val_accuracy: 0.5270\n",
            "Epoch 216/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7094 - accuracy: 0.6939 - val_loss: 1.0043 - val_accuracy: 0.5676\n",
            "Epoch 217/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7365 - accuracy: 0.6803 - val_loss: 1.1424 - val_accuracy: 0.5135\n",
            "Epoch 218/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7388 - accuracy: 0.6735 - val_loss: 1.1156 - val_accuracy: 0.5000\n",
            "Epoch 219/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7379 - accuracy: 0.6565 - val_loss: 1.0162 - val_accuracy: 0.4730\n",
            "Epoch 220/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7074 - accuracy: 0.7075 - val_loss: 0.9930 - val_accuracy: 0.5135\n",
            "Epoch 221/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6934 - accuracy: 0.7041 - val_loss: 1.1220 - val_accuracy: 0.4865\n",
            "Epoch 222/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6635 - accuracy: 0.7313 - val_loss: 1.0994 - val_accuracy: 0.5000\n",
            "Epoch 223/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6578 - accuracy: 0.7279 - val_loss: 1.0309 - val_accuracy: 0.5000\n",
            "Epoch 224/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6608 - accuracy: 0.7245 - val_loss: 1.0399 - val_accuracy: 0.5000\n",
            "Epoch 225/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6585 - accuracy: 0.7279 - val_loss: 1.0048 - val_accuracy: 0.5135\n",
            "Epoch 226/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6500 - accuracy: 0.7415 - val_loss: 1.1793 - val_accuracy: 0.4865\n",
            "Epoch 227/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6947 - accuracy: 0.7109 - val_loss: 1.1383 - val_accuracy: 0.5000\n",
            "Epoch 228/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7205 - accuracy: 0.7007 - val_loss: 1.1067 - val_accuracy: 0.4865\n",
            "Epoch 229/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6707 - accuracy: 0.7381 - val_loss: 0.9971 - val_accuracy: 0.5270\n",
            "Epoch 230/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6901 - accuracy: 0.7143 - val_loss: 1.1009 - val_accuracy: 0.4865\n",
            "Epoch 231/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7253 - accuracy: 0.6973 - val_loss: 1.0982 - val_accuracy: 0.4865\n",
            "Epoch 232/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6991 - accuracy: 0.6837 - val_loss: 1.0425 - val_accuracy: 0.5270\n",
            "Epoch 233/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6868 - accuracy: 0.7177 - val_loss: 1.0540 - val_accuracy: 0.5135\n",
            "Epoch 234/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6716 - accuracy: 0.7381 - val_loss: 1.2149 - val_accuracy: 0.4865\n",
            "Epoch 235/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6938 - accuracy: 0.7109 - val_loss: 1.0754 - val_accuracy: 0.5135\n",
            "Epoch 236/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6515 - accuracy: 0.7347 - val_loss: 1.1130 - val_accuracy: 0.4595\n",
            "Epoch 237/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7930 - accuracy: 0.6701 - val_loss: 0.9765 - val_accuracy: 0.5541\n",
            "Epoch 238/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6939 - accuracy: 0.7211 - val_loss: 0.9412 - val_accuracy: 0.5541\n",
            "Epoch 239/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7480 - accuracy: 0.6701 - val_loss: 0.9313 - val_accuracy: 0.5676\n",
            "Epoch 240/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7531 - accuracy: 0.7007 - val_loss: 0.9685 - val_accuracy: 0.5270\n",
            "Epoch 241/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7444 - accuracy: 0.7041 - val_loss: 0.9868 - val_accuracy: 0.5541\n",
            "Epoch 242/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7755 - accuracy: 0.6327 - val_loss: 0.9780 - val_accuracy: 0.5405\n",
            "Epoch 243/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7516 - accuracy: 0.6599 - val_loss: 0.9653 - val_accuracy: 0.5270\n",
            "Epoch 244/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7329 - accuracy: 0.6735 - val_loss: 0.9973 - val_accuracy: 0.5541\n",
            "Epoch 245/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7297 - accuracy: 0.6633 - val_loss: 0.9864 - val_accuracy: 0.5405\n",
            "Epoch 246/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7112 - accuracy: 0.7177 - val_loss: 0.9991 - val_accuracy: 0.5405\n",
            "Epoch 247/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6943 - accuracy: 0.7245 - val_loss: 1.0265 - val_accuracy: 0.4730\n",
            "Epoch 248/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6747 - accuracy: 0.6973 - val_loss: 0.9937 - val_accuracy: 0.5135\n",
            "Epoch 249/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6615 - accuracy: 0.7347 - val_loss: 1.0286 - val_accuracy: 0.5000\n",
            "Epoch 250/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6378 - accuracy: 0.7449 - val_loss: 1.1295 - val_accuracy: 0.4865\n",
            "Epoch 251/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6363 - accuracy: 0.7517 - val_loss: 1.2219 - val_accuracy: 0.3919\n",
            "Epoch 252/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6699 - accuracy: 0.7381 - val_loss: 1.1129 - val_accuracy: 0.4459\n",
            "Epoch 253/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6720 - accuracy: 0.7177 - val_loss: 1.1040 - val_accuracy: 0.4595\n",
            "Epoch 254/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6590 - accuracy: 0.7347 - val_loss: 1.0982 - val_accuracy: 0.4189\n",
            "Epoch 255/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6202 - accuracy: 0.7415 - val_loss: 1.1555 - val_accuracy: 0.4595\n",
            "Epoch 256/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.6316 - accuracy: 0.7619 - val_loss: 1.2105 - val_accuracy: 0.4865\n",
            "Epoch 257/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6212 - accuracy: 0.7347 - val_loss: 1.1728 - val_accuracy: 0.4459\n",
            "Epoch 258/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6472 - accuracy: 0.7313 - val_loss: 1.0864 - val_accuracy: 0.4865\n",
            "Epoch 259/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6579 - accuracy: 0.7279 - val_loss: 1.0122 - val_accuracy: 0.5541\n",
            "Epoch 260/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6323 - accuracy: 0.7483 - val_loss: 1.0798 - val_accuracy: 0.5000\n",
            "Epoch 261/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6565 - accuracy: 0.7109 - val_loss: 1.0485 - val_accuracy: 0.5405\n",
            "Epoch 262/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6128 - accuracy: 0.7585 - val_loss: 1.1119 - val_accuracy: 0.4730\n",
            "Epoch 263/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6023 - accuracy: 0.7721 - val_loss: 1.1972 - val_accuracy: 0.5000\n",
            "Epoch 264/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6102 - accuracy: 0.7619 - val_loss: 1.1527 - val_accuracy: 0.5000\n",
            "Epoch 265/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5816 - accuracy: 0.7925 - val_loss: 1.2095 - val_accuracy: 0.4324\n",
            "Epoch 266/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6061 - accuracy: 0.7551 - val_loss: 1.1207 - val_accuracy: 0.4459\n",
            "Epoch 267/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6011 - accuracy: 0.7551 - val_loss: 1.0358 - val_accuracy: 0.5405\n",
            "Epoch 268/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6325 - accuracy: 0.7449 - val_loss: 1.0732 - val_accuracy: 0.5270\n",
            "Epoch 269/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6008 - accuracy: 0.7415 - val_loss: 1.1447 - val_accuracy: 0.4730\n",
            "Epoch 270/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6215 - accuracy: 0.7483 - val_loss: 1.0590 - val_accuracy: 0.5676\n",
            "Epoch 271/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6450 - accuracy: 0.7585 - val_loss: 1.1399 - val_accuracy: 0.4865\n",
            "Epoch 272/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5579 - accuracy: 0.7823 - val_loss: 1.2931 - val_accuracy: 0.4865\n",
            "Epoch 273/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6392 - accuracy: 0.7449 - val_loss: 1.1555 - val_accuracy: 0.5000\n",
            "Epoch 274/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5832 - accuracy: 0.7517 - val_loss: 1.0696 - val_accuracy: 0.5135\n",
            "Epoch 275/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5819 - accuracy: 0.7653 - val_loss: 1.1836 - val_accuracy: 0.5135\n",
            "Epoch 276/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5754 - accuracy: 0.7653 - val_loss: 1.1880 - val_accuracy: 0.5135\n",
            "Epoch 277/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5703 - accuracy: 0.7823 - val_loss: 1.2616 - val_accuracy: 0.5270\n",
            "Epoch 278/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5821 - accuracy: 0.7619 - val_loss: 1.2802 - val_accuracy: 0.4459\n",
            "Epoch 279/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5512 - accuracy: 0.7891 - val_loss: 1.1586 - val_accuracy: 0.4595\n",
            "Epoch 280/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5651 - accuracy: 0.7891 - val_loss: 1.1007 - val_accuracy: 0.4865\n",
            "Epoch 281/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5546 - accuracy: 0.7925 - val_loss: 1.3739 - val_accuracy: 0.5270\n",
            "Epoch 282/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5467 - accuracy: 0.7891 - val_loss: 1.2206 - val_accuracy: 0.4459\n",
            "Epoch 283/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5171 - accuracy: 0.7857 - val_loss: 1.1948 - val_accuracy: 0.5135\n",
            "Epoch 284/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5302 - accuracy: 0.7959 - val_loss: 1.3042 - val_accuracy: 0.4865\n",
            "Epoch 285/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5301 - accuracy: 0.7857 - val_loss: 1.2210 - val_accuracy: 0.5000\n",
            "Epoch 286/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5167 - accuracy: 0.7891 - val_loss: 1.1827 - val_accuracy: 0.5405\n",
            "Epoch 287/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5211 - accuracy: 0.7823 - val_loss: 1.2458 - val_accuracy: 0.4865\n",
            "Epoch 288/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5229 - accuracy: 0.7721 - val_loss: 1.1075 - val_accuracy: 0.4865\n",
            "Epoch 289/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5268 - accuracy: 0.7823 - val_loss: 1.1348 - val_accuracy: 0.5676\n",
            "Epoch 290/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5717 - accuracy: 0.7755 - val_loss: 1.1773 - val_accuracy: 0.4730\n",
            "Epoch 291/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5363 - accuracy: 0.7687 - val_loss: 1.1063 - val_accuracy: 0.6081\n",
            "Epoch 292/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6307 - accuracy: 0.7415 - val_loss: 1.1935 - val_accuracy: 0.5405\n",
            "Epoch 293/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7636 - accuracy: 0.7041 - val_loss: 1.0472 - val_accuracy: 0.5811\n",
            "Epoch 294/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6105 - accuracy: 0.7551 - val_loss: 1.0585 - val_accuracy: 0.5000\n",
            "Epoch 295/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6566 - accuracy: 0.7109 - val_loss: 1.1758 - val_accuracy: 0.5135\n",
            "Epoch 296/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6064 - accuracy: 0.7177 - val_loss: 1.2721 - val_accuracy: 0.5000\n",
            "Epoch 297/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6191 - accuracy: 0.7313 - val_loss: 1.0759 - val_accuracy: 0.5541\n",
            "Epoch 298/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5773 - accuracy: 0.7517 - val_loss: 1.0753 - val_accuracy: 0.6216\n",
            "Epoch 299/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5869 - accuracy: 0.7517 - val_loss: 1.0837 - val_accuracy: 0.5135\n",
            "Epoch 300/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.5575 - accuracy: 0.7551 - val_loss: 1.1283 - val_accuracy: 0.5270\n",
            "Epoch 301/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5450 - accuracy: 0.7925 - val_loss: 1.2458 - val_accuracy: 0.5270\n",
            "Epoch 302/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5948 - accuracy: 0.7551 - val_loss: 1.0795 - val_accuracy: 0.5405\n",
            "Epoch 303/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5575 - accuracy: 0.7687 - val_loss: 1.0565 - val_accuracy: 0.5541\n",
            "Epoch 304/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6120 - accuracy: 0.7449 - val_loss: 1.0845 - val_accuracy: 0.5405\n",
            "Epoch 305/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5895 - accuracy: 0.7585 - val_loss: 1.1003 - val_accuracy: 0.5676\n",
            "Epoch 306/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5559 - accuracy: 0.7483 - val_loss: 1.1912 - val_accuracy: 0.5541\n",
            "Epoch 307/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5297 - accuracy: 0.7891 - val_loss: 1.2125 - val_accuracy: 0.4730\n",
            "Epoch 308/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5437 - accuracy: 0.7755 - val_loss: 1.2052 - val_accuracy: 0.4595\n",
            "Epoch 309/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5321 - accuracy: 0.7449 - val_loss: 1.2047 - val_accuracy: 0.4865\n",
            "Epoch 310/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4991 - accuracy: 0.7789 - val_loss: 1.1986 - val_accuracy: 0.4865\n",
            "Epoch 311/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5062 - accuracy: 0.8163 - val_loss: 1.1869 - val_accuracy: 0.5541\n",
            "Epoch 312/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4991 - accuracy: 0.7823 - val_loss: 1.2737 - val_accuracy: 0.4595\n",
            "Epoch 313/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4876 - accuracy: 0.7857 - val_loss: 1.1746 - val_accuracy: 0.5405\n",
            "Epoch 314/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4926 - accuracy: 0.7755 - val_loss: 1.2765 - val_accuracy: 0.4865\n",
            "Epoch 315/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5198 - accuracy: 0.7993 - val_loss: 1.2046 - val_accuracy: 0.5270\n",
            "Epoch 316/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5199 - accuracy: 0.7891 - val_loss: 1.2369 - val_accuracy: 0.4865\n",
            "Epoch 317/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4776 - accuracy: 0.8027 - val_loss: 1.1119 - val_accuracy: 0.5676\n",
            "Epoch 318/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5440 - accuracy: 0.7925 - val_loss: 1.1365 - val_accuracy: 0.5541\n",
            "Epoch 319/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5399 - accuracy: 0.7789 - val_loss: 1.2541 - val_accuracy: 0.5135\n",
            "Epoch 320/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.5378 - accuracy: 0.7585 - val_loss: 1.2792 - val_accuracy: 0.5000\n",
            "Epoch 321/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5729 - accuracy: 0.7619 - val_loss: 1.1991 - val_accuracy: 0.5811\n",
            "Epoch 322/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5299 - accuracy: 0.7959 - val_loss: 1.2188 - val_accuracy: 0.5270\n",
            "Epoch 323/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5105 - accuracy: 0.7823 - val_loss: 1.2452 - val_accuracy: 0.5270\n",
            "Epoch 324/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5672 - accuracy: 0.7687 - val_loss: 1.2020 - val_accuracy: 0.5405\n",
            "Epoch 325/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5115 - accuracy: 0.8027 - val_loss: 1.1097 - val_accuracy: 0.5405\n",
            "Epoch 326/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5193 - accuracy: 0.7959 - val_loss: 1.1467 - val_accuracy: 0.5405\n",
            "Epoch 327/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4854 - accuracy: 0.8061 - val_loss: 1.2416 - val_accuracy: 0.5000\n",
            "Epoch 328/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4859 - accuracy: 0.7993 - val_loss: 1.1380 - val_accuracy: 0.5000\n",
            "Epoch 329/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4378 - accuracy: 0.8333 - val_loss: 1.1669 - val_accuracy: 0.5135\n",
            "Epoch 330/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4492 - accuracy: 0.7857 - val_loss: 1.2683 - val_accuracy: 0.5541\n",
            "Epoch 331/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4411 - accuracy: 0.8503 - val_loss: 1.3003 - val_accuracy: 0.5135\n",
            "Epoch 332/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4612 - accuracy: 0.7891 - val_loss: 1.2270 - val_accuracy: 0.5270\n",
            "Epoch 333/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4407 - accuracy: 0.8469 - val_loss: 1.4245 - val_accuracy: 0.4865\n",
            "Epoch 334/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4581 - accuracy: 0.8197 - val_loss: 1.1194 - val_accuracy: 0.5811\n",
            "Epoch 335/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4940 - accuracy: 0.8061 - val_loss: 1.2096 - val_accuracy: 0.5541\n",
            "Epoch 336/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5040 - accuracy: 0.8231 - val_loss: 1.3212 - val_accuracy: 0.4730\n",
            "Epoch 337/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4526 - accuracy: 0.8265 - val_loss: 1.2215 - val_accuracy: 0.4865\n",
            "Epoch 338/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5012 - accuracy: 0.7585 - val_loss: 1.1320 - val_accuracy: 0.5541\n",
            "Epoch 339/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4455 - accuracy: 0.8401 - val_loss: 1.1631 - val_accuracy: 0.5405\n",
            "Epoch 340/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5418 - accuracy: 0.7857 - val_loss: 1.2329 - val_accuracy: 0.5811\n",
            "Epoch 341/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5301 - accuracy: 0.7959 - val_loss: 1.2007 - val_accuracy: 0.5541\n",
            "Epoch 342/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6020 - accuracy: 0.7483 - val_loss: 1.0789 - val_accuracy: 0.6351\n",
            "Epoch 343/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4824 - accuracy: 0.8367 - val_loss: 1.1675 - val_accuracy: 0.5405\n",
            "Epoch 344/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4764 - accuracy: 0.8095 - val_loss: 1.2441 - val_accuracy: 0.5541\n",
            "Epoch 345/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4549 - accuracy: 0.7993 - val_loss: 1.1189 - val_accuracy: 0.5541\n",
            "Epoch 346/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4929 - accuracy: 0.7891 - val_loss: 1.1663 - val_accuracy: 0.5676\n",
            "Epoch 347/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4568 - accuracy: 0.8333 - val_loss: 1.2900 - val_accuracy: 0.4865\n",
            "Epoch 348/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4373 - accuracy: 0.7959 - val_loss: 1.2482 - val_accuracy: 0.5135\n",
            "Epoch 349/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4295 - accuracy: 0.8435 - val_loss: 1.3388 - val_accuracy: 0.5270\n",
            "Epoch 350/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4250 - accuracy: 0.8401 - val_loss: 1.3053 - val_accuracy: 0.5541\n",
            "Epoch 351/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4117 - accuracy: 0.8469 - val_loss: 1.2316 - val_accuracy: 0.5946\n",
            "Epoch 352/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3844 - accuracy: 0.8639 - val_loss: 1.2481 - val_accuracy: 0.5541\n",
            "Epoch 353/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4734 - accuracy: 0.7959 - val_loss: 1.4045 - val_accuracy: 0.4865\n",
            "Epoch 354/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5610 - accuracy: 0.7857 - val_loss: 1.2073 - val_accuracy: 0.5676\n",
            "Epoch 355/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5633 - accuracy: 0.7687 - val_loss: 1.2301 - val_accuracy: 0.5000\n",
            "Epoch 356/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4912 - accuracy: 0.7789 - val_loss: 1.1369 - val_accuracy: 0.5541\n",
            "Epoch 357/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5343 - accuracy: 0.7687 - val_loss: 1.1230 - val_accuracy: 0.5541\n",
            "Epoch 358/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5263 - accuracy: 0.7925 - val_loss: 1.2961 - val_accuracy: 0.4730\n",
            "Epoch 359/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4175 - accuracy: 0.8469 - val_loss: 1.3295 - val_accuracy: 0.5270\n",
            "Epoch 360/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4497 - accuracy: 0.7789 - val_loss: 1.3093 - val_accuracy: 0.5676\n",
            "Epoch 361/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4500 - accuracy: 0.8299 - val_loss: 1.1791 - val_accuracy: 0.5676\n",
            "Epoch 362/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4317 - accuracy: 0.8265 - val_loss: 1.1397 - val_accuracy: 0.5811\n",
            "Epoch 363/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4132 - accuracy: 0.8537 - val_loss: 1.2448 - val_accuracy: 0.5811\n",
            "Epoch 364/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4160 - accuracy: 0.8469 - val_loss: 1.3893 - val_accuracy: 0.5000\n",
            "Epoch 365/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3996 - accuracy: 0.8435 - val_loss: 1.2947 - val_accuracy: 0.5541\n",
            "Epoch 366/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3759 - accuracy: 0.8605 - val_loss: 1.2873 - val_accuracy: 0.5676\n",
            "Epoch 367/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3951 - accuracy: 0.8673 - val_loss: 1.2231 - val_accuracy: 0.5135\n",
            "Epoch 368/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3719 - accuracy: 0.8469 - val_loss: 1.4915 - val_accuracy: 0.4595\n",
            "Epoch 369/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4421 - accuracy: 0.8333 - val_loss: 1.2218 - val_accuracy: 0.6216\n",
            "Epoch 370/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4508 - accuracy: 0.8333 - val_loss: 1.2121 - val_accuracy: 0.5676\n",
            "Epoch 371/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4405 - accuracy: 0.8503 - val_loss: 1.2572 - val_accuracy: 0.5405\n",
            "Epoch 372/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3913 - accuracy: 0.8810 - val_loss: 1.3490 - val_accuracy: 0.5135\n",
            "Epoch 373/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3550 - accuracy: 0.8537 - val_loss: 1.3220 - val_accuracy: 0.5541\n",
            "Epoch 374/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3893 - accuracy: 0.8401 - val_loss: 1.3114 - val_accuracy: 0.5541\n",
            "Epoch 375/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3739 - accuracy: 0.8435 - val_loss: 1.3329 - val_accuracy: 0.5135\n",
            "Epoch 376/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3829 - accuracy: 0.8503 - val_loss: 1.2788 - val_accuracy: 0.5676\n",
            "Epoch 377/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3597 - accuracy: 0.8776 - val_loss: 1.2679 - val_accuracy: 0.6081\n",
            "Epoch 378/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3465 - accuracy: 0.8503 - val_loss: 1.4485 - val_accuracy: 0.5135\n",
            "Epoch 379/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3941 - accuracy: 0.8605 - val_loss: 1.1643 - val_accuracy: 0.5541\n",
            "Epoch 380/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4720 - accuracy: 0.7891 - val_loss: 1.2378 - val_accuracy: 0.5811\n",
            "Epoch 381/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4038 - accuracy: 0.8401 - val_loss: 1.2842 - val_accuracy: 0.6486\n",
            "Epoch 382/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3548 - accuracy: 0.8776 - val_loss: 1.1577 - val_accuracy: 0.6351\n",
            "Epoch 383/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3610 - accuracy: 0.8605 - val_loss: 1.3121 - val_accuracy: 0.6081\n",
            "Epoch 384/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4118 - accuracy: 0.8367 - val_loss: 1.1841 - val_accuracy: 0.5946\n",
            "Epoch 385/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4285 - accuracy: 0.8197 - val_loss: 1.2142 - val_accuracy: 0.6081\n",
            "Epoch 386/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3814 - accuracy: 0.8503 - val_loss: 1.3975 - val_accuracy: 0.5405\n",
            "Epoch 387/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3566 - accuracy: 0.8435 - val_loss: 1.1349 - val_accuracy: 0.6622\n",
            "Epoch 388/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3587 - accuracy: 0.8707 - val_loss: 1.2765 - val_accuracy: 0.6216\n",
            "Epoch 389/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3598 - accuracy: 0.8435 - val_loss: 1.4832 - val_accuracy: 0.5270\n",
            "Epoch 390/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3086 - accuracy: 0.8776 - val_loss: 1.2792 - val_accuracy: 0.5270\n",
            "Epoch 391/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3488 - accuracy: 0.8469 - val_loss: 1.3356 - val_accuracy: 0.6216\n",
            "Epoch 392/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3241 - accuracy: 0.8707 - val_loss: 1.3835 - val_accuracy: 0.6216\n",
            "Epoch 393/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3094 - accuracy: 0.8844 - val_loss: 1.4167 - val_accuracy: 0.5946\n",
            "Epoch 394/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2939 - accuracy: 0.8946 - val_loss: 1.4943 - val_accuracy: 0.6622\n",
            "Epoch 395/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3127 - accuracy: 0.8776 - val_loss: 1.5540 - val_accuracy: 0.5541\n",
            "Epoch 396/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3370 - accuracy: 0.8946 - val_loss: 1.3735 - val_accuracy: 0.6351\n",
            "Epoch 397/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3496 - accuracy: 0.8673 - val_loss: 1.5373 - val_accuracy: 0.5676\n",
            "Epoch 398/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3846 - accuracy: 0.8707 - val_loss: 1.4012 - val_accuracy: 0.6486\n",
            "Epoch 399/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3192 - accuracy: 0.8844 - val_loss: 1.4866 - val_accuracy: 0.6081\n",
            "Epoch 400/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2755 - accuracy: 0.9184 - val_loss: 1.5495 - val_accuracy: 0.5946\n",
            "Epoch 401/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2749 - accuracy: 0.9116 - val_loss: 1.5226 - val_accuracy: 0.5811\n",
            "Epoch 402/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2478 - accuracy: 0.9048 - val_loss: 1.4906 - val_accuracy: 0.6351\n",
            "Epoch 403/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2738 - accuracy: 0.8946 - val_loss: 1.6481 - val_accuracy: 0.5135\n",
            "Epoch 404/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3260 - accuracy: 0.8810 - val_loss: 1.4972 - val_accuracy: 0.5946\n",
            "Epoch 405/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3064 - accuracy: 0.8912 - val_loss: 1.4831 - val_accuracy: 0.6351\n",
            "Epoch 406/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3433 - accuracy: 0.8673 - val_loss: 1.3618 - val_accuracy: 0.5676\n",
            "Epoch 407/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5830 - accuracy: 0.7449 - val_loss: 1.3081 - val_accuracy: 0.5946\n",
            "Epoch 408/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4919 - accuracy: 0.7857 - val_loss: 1.6138 - val_accuracy: 0.5000\n",
            "Epoch 409/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4553 - accuracy: 0.7857 - val_loss: 1.3878 - val_accuracy: 0.5811\n",
            "Epoch 410/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5340 - accuracy: 0.7891 - val_loss: 1.8297 - val_accuracy: 0.4595\n",
            "Epoch 411/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5171 - accuracy: 0.7823 - val_loss: 1.4438 - val_accuracy: 0.5541\n",
            "Epoch 412/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6050 - accuracy: 0.7415 - val_loss: 1.4837 - val_accuracy: 0.5000\n",
            "Epoch 413/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4762 - accuracy: 0.8231 - val_loss: 1.2549 - val_accuracy: 0.5676\n",
            "Epoch 414/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4287 - accuracy: 0.8367 - val_loss: 1.3179 - val_accuracy: 0.5270\n",
            "Epoch 415/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3696 - accuracy: 0.8537 - val_loss: 1.4672 - val_accuracy: 0.5811\n",
            "Epoch 416/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3731 - accuracy: 0.8401 - val_loss: 1.3247 - val_accuracy: 0.5811\n",
            "Epoch 417/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3367 - accuracy: 0.8741 - val_loss: 1.2774 - val_accuracy: 0.5676\n",
            "Epoch 418/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3427 - accuracy: 0.8946 - val_loss: 1.3636 - val_accuracy: 0.5676\n",
            "Epoch 419/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3615 - accuracy: 0.8401 - val_loss: 1.3607 - val_accuracy: 0.5811\n",
            "Epoch 420/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3089 - accuracy: 0.8946 - val_loss: 1.4008 - val_accuracy: 0.5811\n",
            "Epoch 421/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3112 - accuracy: 0.9116 - val_loss: 1.3905 - val_accuracy: 0.5270\n",
            "Epoch 422/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2943 - accuracy: 0.9082 - val_loss: 1.3934 - val_accuracy: 0.5946\n",
            "Epoch 423/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2602 - accuracy: 0.9014 - val_loss: 1.5105 - val_accuracy: 0.5541\n",
            "Epoch 424/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2879 - accuracy: 0.8741 - val_loss: 1.5221 - val_accuracy: 0.5270\n",
            "Epoch 425/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3298 - accuracy: 0.8810 - val_loss: 1.6184 - val_accuracy: 0.5135\n",
            "Epoch 426/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3108 - accuracy: 0.8741 - val_loss: 1.4066 - val_accuracy: 0.5946\n",
            "Epoch 427/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3001 - accuracy: 0.8810 - val_loss: 1.6191 - val_accuracy: 0.5811\n",
            "Epoch 428/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2757 - accuracy: 0.9116 - val_loss: 1.4227 - val_accuracy: 0.6081\n",
            "Epoch 429/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3152 - accuracy: 0.8707 - val_loss: 1.5145 - val_accuracy: 0.5676\n",
            "Epoch 430/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3859 - accuracy: 0.8673 - val_loss: 1.2561 - val_accuracy: 0.5946\n",
            "Epoch 431/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3348 - accuracy: 0.8776 - val_loss: 1.3194 - val_accuracy: 0.6622\n",
            "Epoch 432/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2813 - accuracy: 0.9048 - val_loss: 1.5946 - val_accuracy: 0.6081\n",
            "Epoch 433/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2750 - accuracy: 0.9184 - val_loss: 1.4978 - val_accuracy: 0.5811\n",
            "Epoch 434/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2643 - accuracy: 0.9184 - val_loss: 1.4143 - val_accuracy: 0.5946\n",
            "Epoch 435/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2755 - accuracy: 0.9218 - val_loss: 1.6092 - val_accuracy: 0.5676\n",
            "Epoch 436/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2520 - accuracy: 0.9184 - val_loss: 1.5510 - val_accuracy: 0.6486\n",
            "Epoch 437/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2268 - accuracy: 0.9456 - val_loss: 1.7296 - val_accuracy: 0.5946\n",
            "Epoch 438/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2379 - accuracy: 0.9252 - val_loss: 1.5477 - val_accuracy: 0.6081\n",
            "Epoch 439/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2253 - accuracy: 0.9082 - val_loss: 1.6620 - val_accuracy: 0.6081\n",
            "Epoch 440/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3359 - accuracy: 0.8776 - val_loss: 1.6749 - val_accuracy: 0.5541\n",
            "Epoch 441/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3647 - accuracy: 0.8673 - val_loss: 1.4089 - val_accuracy: 0.5541\n",
            "Epoch 442/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3110 - accuracy: 0.8605 - val_loss: 1.5502 - val_accuracy: 0.5676\n",
            "Epoch 443/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3760 - accuracy: 0.8571 - val_loss: 1.3033 - val_accuracy: 0.6486\n",
            "Epoch 444/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3655 - accuracy: 0.8639 - val_loss: 1.5419 - val_accuracy: 0.5811\n",
            "Epoch 445/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4001 - accuracy: 0.8401 - val_loss: 1.4894 - val_accuracy: 0.6081\n",
            "Epoch 446/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2893 - accuracy: 0.8946 - val_loss: 1.5078 - val_accuracy: 0.5541\n",
            "Epoch 447/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3706 - accuracy: 0.8333 - val_loss: 1.5305 - val_accuracy: 0.6081\n",
            "Epoch 448/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2444 - accuracy: 0.9320 - val_loss: 1.7919 - val_accuracy: 0.5676\n",
            "Epoch 449/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2900 - accuracy: 0.8912 - val_loss: 1.5610 - val_accuracy: 0.5946\n",
            "Epoch 450/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2247 - accuracy: 0.9150 - val_loss: 1.5066 - val_accuracy: 0.6351\n",
            "Epoch 451/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2096 - accuracy: 0.9388 - val_loss: 1.5688 - val_accuracy: 0.6081\n",
            "Epoch 452/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2054 - accuracy: 0.9082 - val_loss: 1.5453 - val_accuracy: 0.5946\n",
            "Epoch 453/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2219 - accuracy: 0.9354 - val_loss: 1.5854 - val_accuracy: 0.6216\n",
            "Epoch 454/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1897 - accuracy: 0.9388 - val_loss: 1.7361 - val_accuracy: 0.5541\n",
            "Epoch 455/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2553 - accuracy: 0.8912 - val_loss: 1.5629 - val_accuracy: 0.6351\n",
            "Epoch 456/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2783 - accuracy: 0.8810 - val_loss: 1.6596 - val_accuracy: 0.5676\n",
            "Epoch 457/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3068 - accuracy: 0.8776 - val_loss: 1.4694 - val_accuracy: 0.6351\n",
            "Epoch 458/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3185 - accuracy: 0.8741 - val_loss: 1.9273 - val_accuracy: 0.5405\n",
            "Epoch 459/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4991 - accuracy: 0.8231 - val_loss: 1.3838 - val_accuracy: 0.5946\n",
            "Epoch 460/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4731 - accuracy: 0.7993 - val_loss: 1.3706 - val_accuracy: 0.5811\n",
            "Epoch 461/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3748 - accuracy: 0.8571 - val_loss: 1.5828 - val_accuracy: 0.5811\n",
            "Epoch 462/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2985 - accuracy: 0.8912 - val_loss: 1.7172 - val_accuracy: 0.5811\n",
            "Epoch 463/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2669 - accuracy: 0.8946 - val_loss: 1.5212 - val_accuracy: 0.6081\n",
            "Epoch 464/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2433 - accuracy: 0.9116 - val_loss: 1.5735 - val_accuracy: 0.6081\n",
            "Epoch 465/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2420 - accuracy: 0.9184 - val_loss: 1.4567 - val_accuracy: 0.6486\n",
            "Epoch 466/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2380 - accuracy: 0.9014 - val_loss: 1.4477 - val_accuracy: 0.6486\n",
            "Epoch 467/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2327 - accuracy: 0.9252 - val_loss: 1.5160 - val_accuracy: 0.6486\n",
            "Epoch 468/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2223 - accuracy: 0.9286 - val_loss: 1.7581 - val_accuracy: 0.6081\n",
            "Epoch 469/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2143 - accuracy: 0.9218 - val_loss: 1.9067 - val_accuracy: 0.5405\n",
            "Epoch 470/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2980 - accuracy: 0.8912 - val_loss: 1.7134 - val_accuracy: 0.5946\n",
            "Epoch 471/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3261 - accuracy: 0.8810 - val_loss: 1.8061 - val_accuracy: 0.5135\n",
            "Epoch 472/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2993 - accuracy: 0.8878 - val_loss: 1.5580 - val_accuracy: 0.5946\n",
            "Epoch 473/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2432 - accuracy: 0.9150 - val_loss: 1.7286 - val_accuracy: 0.6081\n",
            "Epoch 474/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2408 - accuracy: 0.9116 - val_loss: 1.9976 - val_accuracy: 0.5811\n",
            "Epoch 475/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2466 - accuracy: 0.9184 - val_loss: 1.6543 - val_accuracy: 0.6216\n",
            "Epoch 476/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2019 - accuracy: 0.9388 - val_loss: 1.5735 - val_accuracy: 0.6216\n",
            "Epoch 477/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2070 - accuracy: 0.9524 - val_loss: 1.6429 - val_accuracy: 0.6081\n",
            "Epoch 478/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1536 - accuracy: 0.9660 - val_loss: 1.7213 - val_accuracy: 0.6351\n",
            "Epoch 479/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1989 - accuracy: 0.9252 - val_loss: 1.6839 - val_accuracy: 0.5676\n",
            "Epoch 480/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1948 - accuracy: 0.9388 - val_loss: 1.6692 - val_accuracy: 0.5946\n",
            "Epoch 481/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2138 - accuracy: 0.9218 - val_loss: 1.5563 - val_accuracy: 0.5541\n",
            "Epoch 482/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2834 - accuracy: 0.9048 - val_loss: 1.6296 - val_accuracy: 0.6081\n",
            "Epoch 483/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2348 - accuracy: 0.9082 - val_loss: 1.9499 - val_accuracy: 0.5676\n",
            "Epoch 484/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2378 - accuracy: 0.9082 - val_loss: 1.9739 - val_accuracy: 0.5405\n",
            "Epoch 485/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2035 - accuracy: 0.9252 - val_loss: 1.6469 - val_accuracy: 0.6081\n",
            "Epoch 486/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2498 - accuracy: 0.9150 - val_loss: 1.6815 - val_accuracy: 0.5811\n",
            "Epoch 487/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2038 - accuracy: 0.9354 - val_loss: 1.8869 - val_accuracy: 0.5676\n",
            "Epoch 488/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2063 - accuracy: 0.9184 - val_loss: 2.0059 - val_accuracy: 0.5541\n",
            "Epoch 489/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2114 - accuracy: 0.9218 - val_loss: 1.7114 - val_accuracy: 0.6081\n",
            "Epoch 490/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2455 - accuracy: 0.8878 - val_loss: 2.0045 - val_accuracy: 0.5541\n",
            "Epoch 491/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2153 - accuracy: 0.9354 - val_loss: 1.7489 - val_accuracy: 0.5676\n",
            "Epoch 492/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1633 - accuracy: 0.9388 - val_loss: 1.8002 - val_accuracy: 0.6081\n",
            "Epoch 493/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1926 - accuracy: 0.9524 - val_loss: 1.7710 - val_accuracy: 0.6081\n",
            "Epoch 494/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1552 - accuracy: 0.9456 - val_loss: 1.7305 - val_accuracy: 0.5946\n",
            "Epoch 495/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1667 - accuracy: 0.9422 - val_loss: 1.8757 - val_accuracy: 0.5946\n",
            "Epoch 496/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1482 - accuracy: 0.9592 - val_loss: 1.9162 - val_accuracy: 0.6081\n",
            "Epoch 497/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1436 - accuracy: 0.9626 - val_loss: 1.8229 - val_accuracy: 0.5811\n",
            "Epoch 498/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1672 - accuracy: 0.9490 - val_loss: 1.8070 - val_accuracy: 0.5676\n",
            "Epoch 499/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1426 - accuracy: 0.9626 - val_loss: 1.7979 - val_accuracy: 0.5676\n",
            "Epoch 500/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3634 - accuracy: 0.8401 - val_loss: 1.7625 - val_accuracy: 0.5270\n",
            "Epoch 501/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4181 - accuracy: 0.8503 - val_loss: 1.8422 - val_accuracy: 0.5811\n",
            "Epoch 502/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2602 - accuracy: 0.8810 - val_loss: 1.7328 - val_accuracy: 0.5946\n",
            "Epoch 503/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2776 - accuracy: 0.9116 - val_loss: 1.7908 - val_accuracy: 0.5541\n",
            "Epoch 504/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4527 - accuracy: 0.8027 - val_loss: 1.8022 - val_accuracy: 0.5946\n",
            "Epoch 505/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3874 - accuracy: 0.8333 - val_loss: 1.7021 - val_accuracy: 0.5541\n",
            "Epoch 506/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2811 - accuracy: 0.9082 - val_loss: 1.6195 - val_accuracy: 0.5676\n",
            "Epoch 507/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2446 - accuracy: 0.8878 - val_loss: 1.8044 - val_accuracy: 0.5676\n",
            "Epoch 508/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1904 - accuracy: 0.9354 - val_loss: 1.6497 - val_accuracy: 0.5946\n",
            "Epoch 509/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1865 - accuracy: 0.9456 - val_loss: 1.7159 - val_accuracy: 0.5811\n",
            "Epoch 510/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1660 - accuracy: 0.9422 - val_loss: 1.6721 - val_accuracy: 0.6216\n",
            "Epoch 511/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1636 - accuracy: 0.9456 - val_loss: 1.6567 - val_accuracy: 0.6081\n",
            "Epoch 512/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1611 - accuracy: 0.9524 - val_loss: 1.7326 - val_accuracy: 0.5811\n",
            "Epoch 513/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1768 - accuracy: 0.9456 - val_loss: 1.8770 - val_accuracy: 0.5946\n",
            "Epoch 514/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2069 - accuracy: 0.9320 - val_loss: 1.8274 - val_accuracy: 0.6081\n",
            "Epoch 515/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2476 - accuracy: 0.9150 - val_loss: 1.6443 - val_accuracy: 0.5946\n",
            "Epoch 516/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1596 - accuracy: 0.9558 - val_loss: 1.7369 - val_accuracy: 0.5811\n",
            "Epoch 517/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1903 - accuracy: 0.9354 - val_loss: 1.8501 - val_accuracy: 0.5541\n",
            "Epoch 518/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2480 - accuracy: 0.9150 - val_loss: 1.7155 - val_accuracy: 0.6351\n",
            "Epoch 519/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1938 - accuracy: 0.9252 - val_loss: 1.8276 - val_accuracy: 0.5676\n",
            "Epoch 520/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1979 - accuracy: 0.9184 - val_loss: 1.8671 - val_accuracy: 0.5811\n",
            "Epoch 521/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1950 - accuracy: 0.9252 - val_loss: 1.9634 - val_accuracy: 0.5946\n",
            "Epoch 522/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3951 - accuracy: 0.8435 - val_loss: 1.8813 - val_accuracy: 0.5135\n",
            "Epoch 523/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3000 - accuracy: 0.8776 - val_loss: 1.6364 - val_accuracy: 0.5811\n",
            "Epoch 524/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3225 - accuracy: 0.8503 - val_loss: 1.7050 - val_accuracy: 0.5811\n",
            "Epoch 525/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2585 - accuracy: 0.8912 - val_loss: 1.7488 - val_accuracy: 0.6081\n",
            "Epoch 526/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1954 - accuracy: 0.9252 - val_loss: 1.9349 - val_accuracy: 0.6216\n",
            "Epoch 527/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2339 - accuracy: 0.9082 - val_loss: 1.9331 - val_accuracy: 0.5676\n",
            "Epoch 528/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1898 - accuracy: 0.9252 - val_loss: 1.9569 - val_accuracy: 0.5946\n",
            "Epoch 529/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1615 - accuracy: 0.9388 - val_loss: 1.9418 - val_accuracy: 0.5676\n",
            "Epoch 530/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1784 - accuracy: 0.9524 - val_loss: 1.7884 - val_accuracy: 0.5946\n",
            "Epoch 531/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1292 - accuracy: 0.9660 - val_loss: 2.0477 - val_accuracy: 0.5541\n",
            "Epoch 532/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1272 - accuracy: 0.9558 - val_loss: 1.9158 - val_accuracy: 0.6351\n",
            "Epoch 533/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1471 - accuracy: 0.9558 - val_loss: 1.9271 - val_accuracy: 0.6351\n",
            "Epoch 534/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1569 - accuracy: 0.9626 - val_loss: 1.8729 - val_accuracy: 0.6081\n",
            "Epoch 535/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1667 - accuracy: 0.9592 - val_loss: 1.7687 - val_accuracy: 0.6081\n",
            "Epoch 536/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1575 - accuracy: 0.9456 - val_loss: 2.0779 - val_accuracy: 0.5946\n",
            "Epoch 537/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1257 - accuracy: 0.9660 - val_loss: 2.0163 - val_accuracy: 0.5946\n",
            "Epoch 538/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1309 - accuracy: 0.9592 - val_loss: 1.9439 - val_accuracy: 0.6216\n",
            "Epoch 539/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1453 - accuracy: 0.9558 - val_loss: 2.2592 - val_accuracy: 0.5676\n",
            "Epoch 540/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1521 - accuracy: 0.9558 - val_loss: 1.8565 - val_accuracy: 0.5946\n",
            "Epoch 541/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1287 - accuracy: 0.9660 - val_loss: 1.9881 - val_accuracy: 0.5811\n",
            "Epoch 542/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1074 - accuracy: 0.9796 - val_loss: 2.1145 - val_accuracy: 0.6081\n",
            "Epoch 543/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1356 - accuracy: 0.9490 - val_loss: 2.0552 - val_accuracy: 0.6081\n",
            "Epoch 544/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1082 - accuracy: 0.9830 - val_loss: 2.0356 - val_accuracy: 0.5946\n",
            "Epoch 545/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1207 - accuracy: 0.9694 - val_loss: 2.1191 - val_accuracy: 0.5811\n",
            "Epoch 546/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1037 - accuracy: 0.9762 - val_loss: 2.1494 - val_accuracy: 0.5946\n",
            "Epoch 547/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0842 - accuracy: 0.9864 - val_loss: 1.9227 - val_accuracy: 0.5946\n",
            "Epoch 548/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0968 - accuracy: 0.9830 - val_loss: 2.0619 - val_accuracy: 0.6351\n",
            "Epoch 549/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0874 - accuracy: 0.9830 - val_loss: 2.0058 - val_accuracy: 0.6486\n",
            "Epoch 550/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0895 - accuracy: 0.9796 - val_loss: 2.2483 - val_accuracy: 0.5946\n",
            "Epoch 551/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1282 - accuracy: 0.9524 - val_loss: 2.2405 - val_accuracy: 0.5946\n",
            "Epoch 552/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1082 - accuracy: 0.9660 - val_loss: 2.0217 - val_accuracy: 0.6216\n",
            "Epoch 553/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1342 - accuracy: 0.9660 - val_loss: 2.4254 - val_accuracy: 0.5676\n",
            "Epoch 554/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1396 - accuracy: 0.9592 - val_loss: 2.0668 - val_accuracy: 0.6081\n",
            "Epoch 555/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1909 - accuracy: 0.9116 - val_loss: 2.4045 - val_accuracy: 0.4865\n",
            "Epoch 556/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2055 - accuracy: 0.9048 - val_loss: 2.4391 - val_accuracy: 0.5541\n",
            "Epoch 557/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2394 - accuracy: 0.9116 - val_loss: 1.8618 - val_accuracy: 0.5811\n",
            "Epoch 558/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2514 - accuracy: 0.9048 - val_loss: 2.1620 - val_accuracy: 0.5270\n",
            "Epoch 559/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2875 - accuracy: 0.8810 - val_loss: 1.8222 - val_accuracy: 0.6081\n",
            "Epoch 560/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2523 - accuracy: 0.8878 - val_loss: 2.1567 - val_accuracy: 0.5946\n",
            "Epoch 561/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2311 - accuracy: 0.9116 - val_loss: 2.0075 - val_accuracy: 0.5676\n",
            "Epoch 562/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1852 - accuracy: 0.9150 - val_loss: 1.7334 - val_accuracy: 0.5676\n",
            "Epoch 563/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2134 - accuracy: 0.9048 - val_loss: 1.9248 - val_accuracy: 0.5676\n",
            "Epoch 564/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1797 - accuracy: 0.9286 - val_loss: 2.0463 - val_accuracy: 0.6081\n",
            "Epoch 565/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2176 - accuracy: 0.9150 - val_loss: 2.0921 - val_accuracy: 0.6081\n",
            "Epoch 566/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1961 - accuracy: 0.9422 - val_loss: 1.9372 - val_accuracy: 0.5946\n",
            "Epoch 567/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1565 - accuracy: 0.9456 - val_loss: 2.0613 - val_accuracy: 0.5946\n",
            "Epoch 568/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1349 - accuracy: 0.9592 - val_loss: 2.1269 - val_accuracy: 0.5676\n",
            "Epoch 569/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1025 - accuracy: 0.9694 - val_loss: 2.2602 - val_accuracy: 0.5946\n",
            "Epoch 570/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1021 - accuracy: 0.9694 - val_loss: 2.1912 - val_accuracy: 0.5811\n",
            "Epoch 571/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0944 - accuracy: 0.9762 - val_loss: 2.1497 - val_accuracy: 0.5946\n",
            "Epoch 572/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0783 - accuracy: 0.9864 - val_loss: 2.1274 - val_accuracy: 0.6081\n",
            "Epoch 573/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0924 - accuracy: 0.9694 - val_loss: 2.0490 - val_accuracy: 0.6216\n",
            "Epoch 574/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0771 - accuracy: 0.9864 - val_loss: 2.2868 - val_accuracy: 0.5676\n",
            "Epoch 575/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0955 - accuracy: 0.9762 - val_loss: 2.2023 - val_accuracy: 0.6216\n",
            "Epoch 576/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0795 - accuracy: 0.9728 - val_loss: 2.2952 - val_accuracy: 0.5811\n",
            "Epoch 577/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0804 - accuracy: 0.9830 - val_loss: 2.0328 - val_accuracy: 0.6081\n",
            "Epoch 578/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1368 - accuracy: 0.9286 - val_loss: 2.0881 - val_accuracy: 0.6081\n",
            "Epoch 579/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0866 - accuracy: 0.9864 - val_loss: 2.0904 - val_accuracy: 0.5811\n",
            "Epoch 580/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0722 - accuracy: 0.9932 - val_loss: 2.1472 - val_accuracy: 0.6216\n",
            "Epoch 581/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0813 - accuracy: 0.9830 - val_loss: 2.4057 - val_accuracy: 0.5811\n",
            "Epoch 582/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0838 - accuracy: 0.9796 - val_loss: 2.1660 - val_accuracy: 0.5811\n",
            "Epoch 583/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0765 - accuracy: 0.9830 - val_loss: 2.2184 - val_accuracy: 0.6216\n",
            "Epoch 584/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0578 - accuracy: 0.9898 - val_loss: 2.2738 - val_accuracy: 0.6081\n",
            "Epoch 585/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0582 - accuracy: 0.9830 - val_loss: 2.1979 - val_accuracy: 0.5811\n",
            "Epoch 586/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0659 - accuracy: 0.9796 - val_loss: 2.3112 - val_accuracy: 0.6081\n",
            "Epoch 587/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0620 - accuracy: 0.9932 - val_loss: 2.2732 - val_accuracy: 0.6351\n",
            "Epoch 588/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0551 - accuracy: 0.9932 - val_loss: 2.2187 - val_accuracy: 0.6216\n",
            "Epoch 589/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0623 - accuracy: 0.9864 - val_loss: 2.2144 - val_accuracy: 0.6216\n",
            "Epoch 590/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0548 - accuracy: 0.9864 - val_loss: 2.2900 - val_accuracy: 0.5811\n",
            "Epoch 591/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0616 - accuracy: 0.9898 - val_loss: 2.3455 - val_accuracy: 0.5676\n",
            "Epoch 592/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0665 - accuracy: 0.9864 - val_loss: 2.2717 - val_accuracy: 0.6351\n",
            "Epoch 593/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0640 - accuracy: 0.9796 - val_loss: 2.3587 - val_accuracy: 0.6081\n",
            "Epoch 594/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0658 - accuracy: 0.9898 - val_loss: 2.2710 - val_accuracy: 0.6081\n",
            "Epoch 595/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1018 - accuracy: 0.9694 - val_loss: 2.3213 - val_accuracy: 0.5676\n",
            "Epoch 596/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0769 - accuracy: 0.9796 - val_loss: 2.4159 - val_accuracy: 0.5946\n",
            "Epoch 597/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0675 - accuracy: 0.9864 - val_loss: 2.3496 - val_accuracy: 0.6081\n",
            "Epoch 598/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0637 - accuracy: 0.9898 - val_loss: 2.4791 - val_accuracy: 0.5676\n",
            "Epoch 599/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0577 - accuracy: 0.9830 - val_loss: 2.3048 - val_accuracy: 0.6216\n",
            "Epoch 600/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0642 - accuracy: 0.9728 - val_loss: 2.3574 - val_accuracy: 0.5676\n",
            "Epoch 601/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0530 - accuracy: 0.9898 - val_loss: 2.4326 - val_accuracy: 0.5676\n",
            "Epoch 602/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0691 - accuracy: 0.9864 - val_loss: 2.3106 - val_accuracy: 0.6216\n",
            "Epoch 603/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0583 - accuracy: 0.9932 - val_loss: 2.3329 - val_accuracy: 0.5946\n",
            "Epoch 604/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0907 - accuracy: 0.9728 - val_loss: 2.2892 - val_accuracy: 0.6081\n",
            "Epoch 605/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1229 - accuracy: 0.9660 - val_loss: 2.3364 - val_accuracy: 0.6216\n",
            "Epoch 606/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1347 - accuracy: 0.9456 - val_loss: 2.3690 - val_accuracy: 0.5811\n",
            "Epoch 607/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1301 - accuracy: 0.9490 - val_loss: 2.2626 - val_accuracy: 0.5946\n",
            "Epoch 608/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1603 - accuracy: 0.9422 - val_loss: 2.2326 - val_accuracy: 0.6081\n",
            "Epoch 609/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1288 - accuracy: 0.9354 - val_loss: 2.6144 - val_accuracy: 0.5811\n",
            "Epoch 610/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1896 - accuracy: 0.9252 - val_loss: 2.2979 - val_accuracy: 0.6351\n",
            "Epoch 611/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1353 - accuracy: 0.9490 - val_loss: 2.0553 - val_accuracy: 0.6216\n",
            "Epoch 612/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2265 - accuracy: 0.9116 - val_loss: 2.1512 - val_accuracy: 0.6216\n",
            "Epoch 613/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1583 - accuracy: 0.9320 - val_loss: 2.6218 - val_accuracy: 0.5676\n",
            "Epoch 614/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1887 - accuracy: 0.9490 - val_loss: 2.1495 - val_accuracy: 0.6216\n",
            "Epoch 615/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1614 - accuracy: 0.9388 - val_loss: 2.3340 - val_accuracy: 0.5946\n",
            "Epoch 616/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1641 - accuracy: 0.9490 - val_loss: 2.3504 - val_accuracy: 0.5541\n",
            "Epoch 617/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1784 - accuracy: 0.9354 - val_loss: 2.6201 - val_accuracy: 0.5270\n",
            "Epoch 618/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1236 - accuracy: 0.9490 - val_loss: 2.3592 - val_accuracy: 0.6216\n",
            "Epoch 619/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0870 - accuracy: 0.9762 - val_loss: 2.2829 - val_accuracy: 0.6757\n",
            "Epoch 620/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0848 - accuracy: 0.9796 - val_loss: 2.2464 - val_accuracy: 0.6081\n",
            "Epoch 621/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1015 - accuracy: 0.9762 - val_loss: 2.1369 - val_accuracy: 0.5946\n",
            "Epoch 622/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0824 - accuracy: 0.9728 - val_loss: 2.2914 - val_accuracy: 0.6081\n",
            "Epoch 623/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1006 - accuracy: 0.9660 - val_loss: 2.1698 - val_accuracy: 0.5946\n",
            "Epoch 624/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0702 - accuracy: 0.9830 - val_loss: 2.1602 - val_accuracy: 0.6216\n",
            "Epoch 625/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0765 - accuracy: 0.9796 - val_loss: 2.2922 - val_accuracy: 0.6351\n",
            "Epoch 626/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0796 - accuracy: 0.9762 - val_loss: 2.4856 - val_accuracy: 0.5676\n",
            "Epoch 627/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0988 - accuracy: 0.9728 - val_loss: 2.2549 - val_accuracy: 0.5946\n",
            "Epoch 628/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0573 - accuracy: 0.9898 - val_loss: 2.2280 - val_accuracy: 0.5811\n",
            "Epoch 629/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0603 - accuracy: 0.9932 - val_loss: 2.3643 - val_accuracy: 0.6081\n",
            "Epoch 630/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0517 - accuracy: 0.9932 - val_loss: 2.4003 - val_accuracy: 0.6081\n",
            "Epoch 631/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0473 - accuracy: 0.9898 - val_loss: 2.3032 - val_accuracy: 0.6351\n",
            "Epoch 632/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0493 - accuracy: 0.9898 - val_loss: 2.2578 - val_accuracy: 0.6216\n",
            "Epoch 633/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0554 - accuracy: 0.9864 - val_loss: 2.3288 - val_accuracy: 0.6351\n",
            "Epoch 634/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0467 - accuracy: 0.9898 - val_loss: 2.4347 - val_accuracy: 0.5946\n",
            "Epoch 635/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0533 - accuracy: 0.9864 - val_loss: 2.4773 - val_accuracy: 0.5946\n",
            "Epoch 636/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0476 - accuracy: 0.9898 - val_loss: 2.4093 - val_accuracy: 0.6216\n",
            "Epoch 637/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0550 - accuracy: 0.9932 - val_loss: 2.3934 - val_accuracy: 0.6216\n",
            "Epoch 638/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0525 - accuracy: 0.9898 - val_loss: 2.5113 - val_accuracy: 0.5946\n",
            "Epoch 639/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0467 - accuracy: 0.9932 - val_loss: 2.4481 - val_accuracy: 0.6622\n",
            "Epoch 640/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0690 - accuracy: 0.9592 - val_loss: 2.4773 - val_accuracy: 0.6216\n",
            "Epoch 641/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0533 - accuracy: 0.9864 - val_loss: 2.4478 - val_accuracy: 0.6216\n",
            "Epoch 642/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0612 - accuracy: 0.9898 - val_loss: 2.2225 - val_accuracy: 0.6081\n",
            "Epoch 643/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0834 - accuracy: 0.9694 - val_loss: 2.4761 - val_accuracy: 0.5946\n",
            "Epoch 644/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0653 - accuracy: 0.9694 - val_loss: 2.5406 - val_accuracy: 0.6081\n",
            "Epoch 645/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0481 - accuracy: 0.9932 - val_loss: 2.5397 - val_accuracy: 0.5676\n",
            "Epoch 646/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0482 - accuracy: 0.9898 - val_loss: 2.4514 - val_accuracy: 0.6216\n",
            "Epoch 647/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0432 - accuracy: 0.9932 - val_loss: 2.5096 - val_accuracy: 0.6081\n",
            "Epoch 648/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0511 - accuracy: 0.9864 - val_loss: 2.3903 - val_accuracy: 0.5811\n",
            "Epoch 649/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0687 - accuracy: 0.9830 - val_loss: 2.5102 - val_accuracy: 0.5811\n",
            "Epoch 650/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0446 - accuracy: 0.9932 - val_loss: 2.7625 - val_accuracy: 0.6081\n",
            "Epoch 651/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0634 - accuracy: 0.9830 - val_loss: 2.5022 - val_accuracy: 0.5811\n",
            "Epoch 652/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0825 - accuracy: 0.9660 - val_loss: 2.5648 - val_accuracy: 0.5811\n",
            "Epoch 653/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0832 - accuracy: 0.9762 - val_loss: 2.6197 - val_accuracy: 0.6081\n",
            "Epoch 654/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1190 - accuracy: 0.9694 - val_loss: 2.5433 - val_accuracy: 0.5405\n",
            "Epoch 655/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1541 - accuracy: 0.9388 - val_loss: 2.4035 - val_accuracy: 0.5946\n",
            "Epoch 656/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0768 - accuracy: 0.9694 - val_loss: 2.5434 - val_accuracy: 0.5946\n",
            "Epoch 657/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0696 - accuracy: 0.9796 - val_loss: 2.5025 - val_accuracy: 0.5946\n",
            "Epoch 658/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1272 - accuracy: 0.9592 - val_loss: 2.3702 - val_accuracy: 0.5541\n",
            "Epoch 659/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1133 - accuracy: 0.9524 - val_loss: 2.2687 - val_accuracy: 0.6081\n",
            "Epoch 660/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1044 - accuracy: 0.9694 - val_loss: 2.3329 - val_accuracy: 0.6216\n",
            "Epoch 661/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0685 - accuracy: 0.9796 - val_loss: 2.5078 - val_accuracy: 0.5946\n",
            "Epoch 662/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1278 - accuracy: 0.9592 - val_loss: 2.4459 - val_accuracy: 0.6081\n",
            "Epoch 663/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0696 - accuracy: 0.9898 - val_loss: 2.2633 - val_accuracy: 0.6081\n",
            "Epoch 664/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0800 - accuracy: 0.9762 - val_loss: 2.4412 - val_accuracy: 0.5676\n",
            "Epoch 665/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0630 - accuracy: 0.9830 - val_loss: 2.4923 - val_accuracy: 0.6081\n",
            "Epoch 666/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1321 - accuracy: 0.9490 - val_loss: 2.3781 - val_accuracy: 0.6216\n",
            "Epoch 667/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0891 - accuracy: 0.9694 - val_loss: 2.5467 - val_accuracy: 0.5946\n",
            "Epoch 668/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0890 - accuracy: 0.9762 - val_loss: 2.4940 - val_accuracy: 0.5946\n",
            "Epoch 669/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0636 - accuracy: 0.9830 - val_loss: 2.4015 - val_accuracy: 0.6081\n",
            "Epoch 670/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0464 - accuracy: 0.9898 - val_loss: 2.4686 - val_accuracy: 0.6081\n",
            "Epoch 671/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0475 - accuracy: 0.9830 - val_loss: 2.3041 - val_accuracy: 0.6216\n",
            "Epoch 672/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1688 - accuracy: 0.9388 - val_loss: 2.6142 - val_accuracy: 0.5676\n",
            "Epoch 673/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1747 - accuracy: 0.9388 - val_loss: 2.8095 - val_accuracy: 0.5541\n",
            "Epoch 674/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1112 - accuracy: 0.9558 - val_loss: 2.5364 - val_accuracy: 0.5676\n",
            "Epoch 675/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1332 - accuracy: 0.9490 - val_loss: 2.5608 - val_accuracy: 0.5946\n",
            "Epoch 676/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0853 - accuracy: 0.9694 - val_loss: 2.5658 - val_accuracy: 0.5811\n",
            "Epoch 677/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0446 - accuracy: 0.9932 - val_loss: 2.4678 - val_accuracy: 0.5946\n",
            "Epoch 678/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1107 - accuracy: 0.9660 - val_loss: 2.6252 - val_accuracy: 0.5946\n",
            "Epoch 679/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0758 - accuracy: 0.9830 - val_loss: 2.8471 - val_accuracy: 0.5676\n",
            "Epoch 680/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2109 - accuracy: 0.9320 - val_loss: 2.2648 - val_accuracy: 0.6216\n",
            "Epoch 681/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2702 - accuracy: 0.8980 - val_loss: 2.4067 - val_accuracy: 0.6216\n",
            "Epoch 682/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1276 - accuracy: 0.9592 - val_loss: 2.7009 - val_accuracy: 0.5270\n",
            "Epoch 683/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1540 - accuracy: 0.9388 - val_loss: 2.4213 - val_accuracy: 0.5541\n",
            "Epoch 684/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2840 - accuracy: 0.8776 - val_loss: 2.4036 - val_accuracy: 0.6216\n",
            "Epoch 685/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2262 - accuracy: 0.9116 - val_loss: 2.5390 - val_accuracy: 0.5946\n",
            "Epoch 686/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2224 - accuracy: 0.9354 - val_loss: 2.2238 - val_accuracy: 0.6351\n",
            "Epoch 687/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1728 - accuracy: 0.9252 - val_loss: 2.2277 - val_accuracy: 0.5676\n",
            "Epoch 688/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1482 - accuracy: 0.9524 - val_loss: 2.6311 - val_accuracy: 0.5811\n",
            "Epoch 689/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1031 - accuracy: 0.9660 - val_loss: 2.8820 - val_accuracy: 0.5811\n",
            "Epoch 690/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1136 - accuracy: 0.9660 - val_loss: 2.4959 - val_accuracy: 0.5811\n",
            "Epoch 691/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1725 - accuracy: 0.9252 - val_loss: 2.1594 - val_accuracy: 0.5946\n",
            "Epoch 692/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1959 - accuracy: 0.9252 - val_loss: 2.2631 - val_accuracy: 0.5676\n",
            "Epoch 693/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1539 - accuracy: 0.9456 - val_loss: 2.4582 - val_accuracy: 0.5676\n",
            "Epoch 694/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1692 - accuracy: 0.9388 - val_loss: 2.5292 - val_accuracy: 0.5676\n",
            "Epoch 695/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1396 - accuracy: 0.9626 - val_loss: 2.4065 - val_accuracy: 0.5946\n",
            "Epoch 696/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1038 - accuracy: 0.9694 - val_loss: 2.1607 - val_accuracy: 0.6081\n",
            "Epoch 697/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1277 - accuracy: 0.9490 - val_loss: 2.2124 - val_accuracy: 0.6216\n",
            "Epoch 698/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0887 - accuracy: 0.9762 - val_loss: 2.4994 - val_accuracy: 0.5811\n",
            "Epoch 699/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1214 - accuracy: 0.9524 - val_loss: 2.4291 - val_accuracy: 0.5811\n",
            "Epoch 700/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0780 - accuracy: 0.9762 - val_loss: 2.3494 - val_accuracy: 0.5135\n",
            "Epoch 701/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0682 - accuracy: 0.9830 - val_loss: 2.3702 - val_accuracy: 0.5541\n",
            "Epoch 702/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0603 - accuracy: 0.9898 - val_loss: 2.5456 - val_accuracy: 0.5541\n",
            "Epoch 703/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0558 - accuracy: 0.9898 - val_loss: 2.5551 - val_accuracy: 0.5946\n",
            "Epoch 704/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0448 - accuracy: 0.9932 - val_loss: 2.4415 - val_accuracy: 0.6351\n",
            "Epoch 705/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0458 - accuracy: 0.9864 - val_loss: 2.4319 - val_accuracy: 0.6081\n",
            "Epoch 706/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0454 - accuracy: 0.9864 - val_loss: 2.3594 - val_accuracy: 0.6081\n",
            "Epoch 707/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0378 - accuracy: 0.9932 - val_loss: 2.4241 - val_accuracy: 0.6081\n",
            "Epoch 708/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0339 - accuracy: 0.9932 - val_loss: 2.5083 - val_accuracy: 0.5946\n",
            "Epoch 709/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0371 - accuracy: 0.9932 - val_loss: 2.4865 - val_accuracy: 0.6216\n",
            "Epoch 710/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0312 - accuracy: 0.9932 - val_loss: 2.5027 - val_accuracy: 0.6351\n",
            "Epoch 711/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0321 - accuracy: 0.9932 - val_loss: 2.4761 - val_accuracy: 0.6216\n",
            "Epoch 712/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0311 - accuracy: 0.9932 - val_loss: 2.4517 - val_accuracy: 0.6216\n",
            "Epoch 713/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0298 - accuracy: 0.9932 - val_loss: 2.5188 - val_accuracy: 0.6351\n",
            "Epoch 714/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0276 - accuracy: 0.9932 - val_loss: 2.5949 - val_accuracy: 0.6351\n",
            "Epoch 715/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0284 - accuracy: 0.9932 - val_loss: 2.5782 - val_accuracy: 0.6351\n",
            "Epoch 716/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0271 - accuracy: 0.9932 - val_loss: 2.5428 - val_accuracy: 0.6486\n",
            "Epoch 717/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0264 - accuracy: 0.9932 - val_loss: 2.5184 - val_accuracy: 0.6216\n",
            "Epoch 718/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0262 - accuracy: 0.9932 - val_loss: 2.5089 - val_accuracy: 0.6216\n",
            "Epoch 719/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0267 - accuracy: 0.9898 - val_loss: 2.5095 - val_accuracy: 0.6216\n",
            "Epoch 720/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0242 - accuracy: 0.9932 - val_loss: 2.6295 - val_accuracy: 0.6216\n",
            "Epoch 721/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0278 - accuracy: 0.9932 - val_loss: 2.6199 - val_accuracy: 0.6351\n",
            "Epoch 722/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0250 - accuracy: 0.9932 - val_loss: 2.4724 - val_accuracy: 0.6351\n",
            "Epoch 723/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0275 - accuracy: 0.9864 - val_loss: 2.4832 - val_accuracy: 0.6486\n",
            "Epoch 724/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0286 - accuracy: 0.9864 - val_loss: 2.6255 - val_accuracy: 0.6081\n",
            "Epoch 725/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0255 - accuracy: 0.9898 - val_loss: 2.6727 - val_accuracy: 0.6081\n",
            "Epoch 726/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0255 - accuracy: 0.9898 - val_loss: 2.6406 - val_accuracy: 0.6081\n",
            "Epoch 727/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0296 - accuracy: 0.9864 - val_loss: 2.5603 - val_accuracy: 0.6351\n",
            "Epoch 728/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0252 - accuracy: 0.9932 - val_loss: 2.5518 - val_accuracy: 0.6351\n",
            "Epoch 729/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0291 - accuracy: 0.9932 - val_loss: 2.5006 - val_accuracy: 0.6216\n",
            "Epoch 730/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0289 - accuracy: 0.9966 - val_loss: 2.4211 - val_accuracy: 0.6351\n",
            "Epoch 731/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0320 - accuracy: 0.9898 - val_loss: 2.5241 - val_accuracy: 0.6081\n",
            "Epoch 732/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0334 - accuracy: 0.9898 - val_loss: 2.5762 - val_accuracy: 0.6081\n",
            "Epoch 733/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0251 - accuracy: 0.9932 - val_loss: 2.6451 - val_accuracy: 0.6081\n",
            "Epoch 734/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0273 - accuracy: 0.9932 - val_loss: 2.6694 - val_accuracy: 0.6081\n",
            "Epoch 735/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0283 - accuracy: 0.9932 - val_loss: 2.7182 - val_accuracy: 0.5811\n",
            "Epoch 736/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0288 - accuracy: 0.9932 - val_loss: 2.5898 - val_accuracy: 0.6216\n",
            "Epoch 737/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0236 - accuracy: 0.9932 - val_loss: 2.5495 - val_accuracy: 0.6757\n",
            "Epoch 738/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0285 - accuracy: 0.9898 - val_loss: 2.5927 - val_accuracy: 0.6486\n",
            "Epoch 739/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0256 - accuracy: 0.9932 - val_loss: 2.6412 - val_accuracy: 0.6216\n",
            "Epoch 740/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0240 - accuracy: 0.9932 - val_loss: 2.5359 - val_accuracy: 0.5946\n",
            "Epoch 741/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0309 - accuracy: 0.9898 - val_loss: 2.6172 - val_accuracy: 0.5946\n",
            "Epoch 742/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0255 - accuracy: 0.9898 - val_loss: 2.7626 - val_accuracy: 0.5946\n",
            "Epoch 743/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0288 - accuracy: 0.9932 - val_loss: 2.7230 - val_accuracy: 0.6081\n",
            "Epoch 744/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0427 - accuracy: 0.9830 - val_loss: 2.5779 - val_accuracy: 0.6216\n",
            "Epoch 745/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0267 - accuracy: 0.9932 - val_loss: 2.5023 - val_accuracy: 0.5811\n",
            "Epoch 746/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0390 - accuracy: 0.9932 - val_loss: 2.4277 - val_accuracy: 0.6216\n",
            "Epoch 747/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0310 - accuracy: 0.9932 - val_loss: 2.6773 - val_accuracy: 0.6081\n",
            "Epoch 748/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0351 - accuracy: 0.9864 - val_loss: 2.5152 - val_accuracy: 0.6216\n",
            "Epoch 749/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0377 - accuracy: 0.9898 - val_loss: 2.7113 - val_accuracy: 0.6081\n",
            "Epoch 750/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0485 - accuracy: 0.9864 - val_loss: 2.6820 - val_accuracy: 0.6216\n",
            "Epoch 751/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0486 - accuracy: 0.9864 - val_loss: 2.5795 - val_accuracy: 0.6216\n",
            "Epoch 752/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0360 - accuracy: 0.9898 - val_loss: 2.6497 - val_accuracy: 0.5676\n",
            "Epoch 753/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0495 - accuracy: 0.9864 - val_loss: 2.5481 - val_accuracy: 0.6351\n",
            "Epoch 754/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0457 - accuracy: 0.9762 - val_loss: 2.5238 - val_accuracy: 0.6351\n",
            "Epoch 755/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0455 - accuracy: 0.9830 - val_loss: 2.6922 - val_accuracy: 0.5946\n",
            "Epoch 756/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0416 - accuracy: 0.9864 - val_loss: 2.6225 - val_accuracy: 0.6351\n",
            "Epoch 757/800\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0268 - accuracy: 0.9932 - val_loss: 2.5067 - val_accuracy: 0.6486\n",
            "Epoch 758/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0446 - accuracy: 0.9898 - val_loss: 2.6298 - val_accuracy: 0.6216\n",
            "Epoch 759/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0304 - accuracy: 0.9932 - val_loss: 2.7293 - val_accuracy: 0.6216\n",
            "Epoch 760/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0292 - accuracy: 0.9932 - val_loss: 2.7594 - val_accuracy: 0.5946\n",
            "Epoch 761/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0329 - accuracy: 0.9932 - val_loss: 2.7387 - val_accuracy: 0.5811\n",
            "Epoch 762/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0273 - accuracy: 0.9932 - val_loss: 2.7321 - val_accuracy: 0.5946\n",
            "Epoch 763/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0264 - accuracy: 0.9932 - val_loss: 2.7249 - val_accuracy: 0.6081\n",
            "Epoch 764/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0259 - accuracy: 0.9932 - val_loss: 2.7339 - val_accuracy: 0.6081\n",
            "Epoch 765/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0265 - accuracy: 0.9932 - val_loss: 2.6709 - val_accuracy: 0.6216\n",
            "Epoch 766/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0242 - accuracy: 0.9898 - val_loss: 2.7492 - val_accuracy: 0.5946\n",
            "Epoch 767/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0271 - accuracy: 0.9932 - val_loss: 2.8058 - val_accuracy: 0.6081\n",
            "Epoch 768/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0242 - accuracy: 0.9932 - val_loss: 2.8319 - val_accuracy: 0.6081\n",
            "Epoch 769/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0284 - accuracy: 0.9932 - val_loss: 2.8642 - val_accuracy: 0.5946\n",
            "Epoch 770/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 2.7111 - val_accuracy: 0.6216\n",
            "Epoch 771/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0284 - accuracy: 0.9864 - val_loss: 2.7037 - val_accuracy: 0.6216\n",
            "Epoch 772/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0220 - accuracy: 0.9966 - val_loss: 2.7835 - val_accuracy: 0.6081\n",
            "Epoch 773/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0252 - accuracy: 0.9932 - val_loss: 2.7361 - val_accuracy: 0.6351\n",
            "Epoch 774/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0356 - accuracy: 0.9898 - val_loss: 2.8052 - val_accuracy: 0.6351\n",
            "Epoch 775/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0243 - accuracy: 0.9932 - val_loss: 2.8530 - val_accuracy: 0.6081\n",
            "Epoch 776/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0230 - accuracy: 0.9932 - val_loss: 2.8118 - val_accuracy: 0.6081\n",
            "Epoch 777/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0257 - accuracy: 0.9932 - val_loss: 2.7274 - val_accuracy: 0.6486\n",
            "Epoch 778/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0303 - accuracy: 0.9830 - val_loss: 2.6521 - val_accuracy: 0.6486\n",
            "Epoch 779/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0230 - accuracy: 0.9864 - val_loss: 2.6350 - val_accuracy: 0.6351\n",
            "Epoch 780/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0279 - accuracy: 0.9932 - val_loss: 2.6669 - val_accuracy: 0.6216\n",
            "Epoch 781/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0263 - accuracy: 0.9932 - val_loss: 2.6947 - val_accuracy: 0.6216\n",
            "Epoch 782/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 2.7106 - val_accuracy: 0.6216\n",
            "Epoch 783/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0199 - accuracy: 0.9932 - val_loss: 2.7856 - val_accuracy: 0.5946\n",
            "Epoch 784/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0204 - accuracy: 0.9898 - val_loss: 2.8971 - val_accuracy: 0.5676\n",
            "Epoch 785/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0224 - accuracy: 0.9932 - val_loss: 2.9318 - val_accuracy: 0.5541\n",
            "Epoch 786/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0261 - accuracy: 0.9932 - val_loss: 2.8163 - val_accuracy: 0.6081\n",
            "Epoch 787/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0215 - accuracy: 0.9932 - val_loss: 2.7899 - val_accuracy: 0.6081\n",
            "Epoch 788/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0201 - accuracy: 0.9932 - val_loss: 2.8518 - val_accuracy: 0.6216\n",
            "Epoch 789/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0236 - accuracy: 0.9932 - val_loss: 2.8123 - val_accuracy: 0.6216\n",
            "Epoch 790/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0305 - accuracy: 0.9898 - val_loss: 2.8147 - val_accuracy: 0.5946\n",
            "Epoch 791/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0310 - accuracy: 0.9898 - val_loss: 2.9636 - val_accuracy: 0.5270\n",
            "Epoch 792/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0893 - accuracy: 0.9660 - val_loss: 2.8020 - val_accuracy: 0.5811\n",
            "Epoch 793/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1094 - accuracy: 0.9558 - val_loss: 3.0180 - val_accuracy: 0.5135\n",
            "Epoch 794/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5340 - accuracy: 0.8503 - val_loss: 1.8658 - val_accuracy: 0.6351\n",
            "Epoch 795/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8582 - accuracy: 0.6973 - val_loss: 2.1403 - val_accuracy: 0.5405\n",
            "Epoch 796/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.0371 - accuracy: 0.7279 - val_loss: 2.7843 - val_accuracy: 0.6081\n",
            "Epoch 797/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.2311 - accuracy: 0.7211 - val_loss: 2.3330 - val_accuracy: 0.5270\n",
            "Epoch 798/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8592 - accuracy: 0.7313 - val_loss: 1.5316 - val_accuracy: 0.5000\n",
            "Epoch 799/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6334 - accuracy: 0.7483 - val_loss: 1.2387 - val_accuracy: 0.5946\n",
            "Epoch 800/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6510 - accuracy: 0.7245 - val_loss: 1.1962 - val_accuracy: 0.6216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation with the RNN\n",
        "y_valid_RNN = RNN_model.predict(feature_valid)\n",
        "valid_y_RNN = y_valid_RNN.copy()\n",
        "for i in range(len(y_valid_RNN)):\n",
        "    j = np.where(y_valid_RNN[i] == np.amax(y_valid_RNN[i]))\n",
        "    valid_y_RNN[i] = [0, 0, 0]\n",
        "    valid_y_RNN[i][j] = 1\n",
        "\n",
        "# print acc and report\n",
        "print(accuracy_score(label_valid_y,valid_y_RNN))\n",
        "print(classification_report(label_valid_y,valid_y_RNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_RNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH_wAo-aj7pp",
        "outputId": "b997e874-3bac-48f5-aad5-ce5414e694e4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 7ms/step\n",
            "0.6081081081081081\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.53      0.64        15\n",
            "           1       0.61      0.75      0.67        36\n",
            "           2       0.50      0.43      0.47        23\n",
            "\n",
            "   micro avg       0.61      0.61      0.61        74\n",
            "   macro avg       0.64      0.57      0.59        74\n",
            "weighted avg       0.62      0.61      0.60        74\n",
            " samples avg       0.61      0.61      0.61        74\n",
            "\n",
            "auc score:  0.6734617974198431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_RNN = RNN_model.predict(feature_test)\n",
        "# convert the test vector\n",
        "test_y_RNN = y_test_RNN.copy()\n",
        "for i in range(len(y_test_RNN)):\n",
        "    j = np.where(y_test_RNN[i] == np.amax(y_test_RNN[i]))\n",
        "    test_y_RNN[i] = [0, 0, 0]\n",
        "    test_y_RNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_test_y,test_y_RNN))\n",
        "print(classification_report(label_test_y,test_y_RNN))\n",
        "print(\"auc score: \",roc_auc_score(label_test_y,test_y_RNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE9_m0yUj9e-",
        "outputId": "0a5d6b09-8220-4320-a934-88f8b524ba9c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 7ms/step\n",
            "0.6451612903225806\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.47      0.53        17\n",
            "           1       0.67      0.86      0.76        43\n",
            "           2       0.60      0.45      0.52        33\n",
            "\n",
            "   micro avg       0.65      0.65      0.65        93\n",
            "   macro avg       0.63      0.60      0.60        93\n",
            "weighted avg       0.64      0.65      0.63        93\n",
            " samples avg       0.65      0.65      0.65        93\n",
            "\n",
            "auc score:  0.6988571109612941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Bidirectional, GRU, LSTM, Attention, GlobalMaxPooling1D, Dense, Dropout, Input, Concatenate, Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "\n",
        "def create_CNN_RNN_model():\n",
        "    inputs = Input(shape=(maxlen,))\n",
        "    embeddings = Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False)(inputs)\n",
        "\n",
        "    conv1 = Conv1D(128, 3, activation='relu')(embeddings)\n",
        "    pool1 = MaxPooling1D(3)(conv1)\n",
        "    conv2 = Conv1D(128, 3, activation='relu')(pool1)\n",
        "    pool2 = MaxPooling1D(3)(conv2)\n",
        "\n",
        "    gru1 = Bidirectional(GRU(128, return_sequences=True))(pool2)\n",
        "    gru2 = Bidirectional(GRU(64, return_sequences=True))(gru1)\n",
        "\n",
        "    lstm1 = Bidirectional(LSTM(128, return_sequences=True))(pool2)\n",
        "    lstm2 = Bidirectional(LSTM(64, return_sequences=True))(lstm1)\n",
        "\n",
        "    concat = Concatenate(axis=-1)([gru2, lstm2])\n",
        "\n",
        "    attention = Attention()([concat, concat])\n",
        "\n",
        "    pool = GlobalMaxPooling1D()(attention)\n",
        "\n",
        "    dense1 = Dense(128, activation='relu')(pool)\n",
        "    dropout1 = Dropout(0.5)(dense1)\n",
        "\n",
        "    dense2 = Dense(64, activation='relu')(dropout1)\n",
        "    dropout2 = Dropout(0.5)(dense2)\n",
        "\n",
        "    outputs = Dense(3, activation='softmax')(dropout2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "J_ylya-Sj_8a"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_RNN_model = create_CNN_RNN_model()\n",
        "CNN_RNN_history = CNN_RNN_model.fit(feature_train, label_train_y, epochs=750, batch_size=128,validation_data=(feature_valid, label_valid_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETMdc-cvkHBR",
        "outputId": "56981ea4-b156-4f5f-ae6c-b7874ab10520"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/750\n",
            "3/3 [==============================] - 16s 1s/step - loss: 1.0204 - accuracy: 0.5374 - val_loss: 1.0792 - val_accuracy: 0.4865\n",
            "Epoch 2/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.0279 - accuracy: 0.5306 - val_loss: 1.0389 - val_accuracy: 0.4865\n",
            "Epoch 3/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.0366 - accuracy: 0.5170 - val_loss: 1.0408 - val_accuracy: 0.4865\n",
            "Epoch 4/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.0300 - accuracy: 0.5578 - val_loss: 1.0419 - val_accuracy: 0.4865\n",
            "Epoch 5/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.0017 - accuracy: 0.5714 - val_loss: 1.0471 - val_accuracy: 0.4865\n",
            "Epoch 6/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9766 - accuracy: 0.5612 - val_loss: 1.0561 - val_accuracy: 0.4865\n",
            "Epoch 7/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.0067 - accuracy: 0.5748 - val_loss: 1.0432 - val_accuracy: 0.4865\n",
            "Epoch 8/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9928 - accuracy: 0.5714 - val_loss: 1.0420 - val_accuracy: 0.4865\n",
            "Epoch 9/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9869 - accuracy: 0.5510 - val_loss: 1.0414 - val_accuracy: 0.4865\n",
            "Epoch 10/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9727 - accuracy: 0.5748 - val_loss: 1.0675 - val_accuracy: 0.4865\n",
            "Epoch 11/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9978 - accuracy: 0.5680 - val_loss: 1.0457 - val_accuracy: 0.4865\n",
            "Epoch 12/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.0129 - accuracy: 0.5646 - val_loss: 1.0438 - val_accuracy: 0.4865\n",
            "Epoch 13/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.0149 - accuracy: 0.5646 - val_loss: 1.0446 - val_accuracy: 0.4865\n",
            "Epoch 14/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.9784 - accuracy: 0.5714 - val_loss: 1.0463 - val_accuracy: 0.4865\n",
            "Epoch 15/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.0012 - accuracy: 0.5680 - val_loss: 1.0552 - val_accuracy: 0.4865\n",
            "Epoch 16/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9892 - accuracy: 0.5714 - val_loss: 1.0459 - val_accuracy: 0.4865\n",
            "Epoch 17/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9783 - accuracy: 0.5680 - val_loss: 1.0437 - val_accuracy: 0.4865\n",
            "Epoch 18/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 1.0118 - accuracy: 0.5646 - val_loss: 1.0459 - val_accuracy: 0.4865\n",
            "Epoch 19/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9871 - accuracy: 0.5748 - val_loss: 1.0555 - val_accuracy: 0.4865\n",
            "Epoch 20/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9906 - accuracy: 0.5714 - val_loss: 1.0739 - val_accuracy: 0.4865\n",
            "Epoch 21/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9608 - accuracy: 0.5748 - val_loss: 1.0706 - val_accuracy: 0.4865\n",
            "Epoch 22/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9932 - accuracy: 0.5646 - val_loss: 1.0554 - val_accuracy: 0.4865\n",
            "Epoch 23/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9723 - accuracy: 0.5714 - val_loss: 1.0524 - val_accuracy: 0.4865\n",
            "Epoch 24/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9707 - accuracy: 0.5714 - val_loss: 1.0730 - val_accuracy: 0.4865\n",
            "Epoch 25/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9587 - accuracy: 0.5680 - val_loss: 1.0776 - val_accuracy: 0.4865\n",
            "Epoch 26/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9609 - accuracy: 0.5714 - val_loss: 1.0710 - val_accuracy: 0.4865\n",
            "Epoch 27/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9574 - accuracy: 0.5714 - val_loss: 1.0517 - val_accuracy: 0.4865\n",
            "Epoch 28/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9480 - accuracy: 0.5714 - val_loss: 1.0857 - val_accuracy: 0.4865\n",
            "Epoch 29/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9538 - accuracy: 0.5714 - val_loss: 1.0960 - val_accuracy: 0.4865\n",
            "Epoch 30/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.9370 - accuracy: 0.5714 - val_loss: 1.0511 - val_accuracy: 0.4865\n",
            "Epoch 31/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9717 - accuracy: 0.5782 - val_loss: 1.0497 - val_accuracy: 0.4865\n",
            "Epoch 32/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9476 - accuracy: 0.5714 - val_loss: 1.1042 - val_accuracy: 0.4865\n",
            "Epoch 33/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9392 - accuracy: 0.5680 - val_loss: 1.0642 - val_accuracy: 0.4865\n",
            "Epoch 34/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9300 - accuracy: 0.5714 - val_loss: 1.0447 - val_accuracy: 0.4865\n",
            "Epoch 35/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9225 - accuracy: 0.5816 - val_loss: 1.0762 - val_accuracy: 0.4865\n",
            "Epoch 36/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9272 - accuracy: 0.5782 - val_loss: 1.0879 - val_accuracy: 0.4865\n",
            "Epoch 37/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9345 - accuracy: 0.5782 - val_loss: 1.0480 - val_accuracy: 0.4189\n",
            "Epoch 38/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9138 - accuracy: 0.5884 - val_loss: 1.0837 - val_accuracy: 0.4865\n",
            "Epoch 39/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.8789 - accuracy: 0.6088 - val_loss: 1.0742 - val_accuracy: 0.4730\n",
            "Epoch 40/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.8957 - accuracy: 0.6020 - val_loss: 1.1379 - val_accuracy: 0.4865\n",
            "Epoch 41/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.8844 - accuracy: 0.6122 - val_loss: 1.0783 - val_accuracy: 0.4865\n",
            "Epoch 42/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.8755 - accuracy: 0.6088 - val_loss: 1.0454 - val_accuracy: 0.4595\n",
            "Epoch 43/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.8508 - accuracy: 0.6327 - val_loss: 1.1070 - val_accuracy: 0.5000\n",
            "Epoch 44/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.8799 - accuracy: 0.6224 - val_loss: 1.0818 - val_accuracy: 0.4865\n",
            "Epoch 45/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.8434 - accuracy: 0.6190 - val_loss: 1.0704 - val_accuracy: 0.4865\n",
            "Epoch 46/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8353 - accuracy: 0.6224 - val_loss: 1.0529 - val_accuracy: 0.4730\n",
            "Epoch 47/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8346 - accuracy: 0.6190 - val_loss: 1.0704 - val_accuracy: 0.4865\n",
            "Epoch 48/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8057 - accuracy: 0.6224 - val_loss: 1.0625 - val_accuracy: 0.4459\n",
            "Epoch 49/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8170 - accuracy: 0.6293 - val_loss: 1.1283 - val_accuracy: 0.5000\n",
            "Epoch 50/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8523 - accuracy: 0.6224 - val_loss: 1.0323 - val_accuracy: 0.4595\n",
            "Epoch 51/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.8084 - accuracy: 0.6463 - val_loss: 1.0320 - val_accuracy: 0.5000\n",
            "Epoch 52/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7942 - accuracy: 0.6327 - val_loss: 1.0552 - val_accuracy: 0.4459\n",
            "Epoch 53/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.7891 - accuracy: 0.6361 - val_loss: 1.1050 - val_accuracy: 0.5000\n",
            "Epoch 54/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.8031 - accuracy: 0.6259 - val_loss: 1.0397 - val_accuracy: 0.4865\n",
            "Epoch 55/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7640 - accuracy: 0.6463 - val_loss: 1.0809 - val_accuracy: 0.5000\n",
            "Epoch 56/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7735 - accuracy: 0.6361 - val_loss: 1.1346 - val_accuracy: 0.4865\n",
            "Epoch 57/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8158 - accuracy: 0.6293 - val_loss: 1.1467 - val_accuracy: 0.4730\n",
            "Epoch 58/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.8872 - accuracy: 0.5850 - val_loss: 0.9947 - val_accuracy: 0.4865\n",
            "Epoch 59/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.8114 - accuracy: 0.6463 - val_loss: 1.0133 - val_accuracy: 0.4459\n",
            "Epoch 60/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.8073 - accuracy: 0.6633 - val_loss: 1.1163 - val_accuracy: 0.5000\n",
            "Epoch 61/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7571 - accuracy: 0.6599 - val_loss: 1.0700 - val_accuracy: 0.4595\n",
            "Epoch 62/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7465 - accuracy: 0.6395 - val_loss: 1.1080 - val_accuracy: 0.5135\n",
            "Epoch 63/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7403 - accuracy: 0.6463 - val_loss: 1.1145 - val_accuracy: 0.4324\n",
            "Epoch 64/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7559 - accuracy: 0.6497 - val_loss: 1.2282 - val_accuracy: 0.5000\n",
            "Epoch 65/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8097 - accuracy: 0.6429 - val_loss: 1.0690 - val_accuracy: 0.4459\n",
            "Epoch 66/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7812 - accuracy: 0.6395 - val_loss: 1.0692 - val_accuracy: 0.5135\n",
            "Epoch 67/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.7360 - accuracy: 0.6361 - val_loss: 1.0695 - val_accuracy: 0.5135\n",
            "Epoch 68/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7370 - accuracy: 0.6633 - val_loss: 1.0530 - val_accuracy: 0.5000\n",
            "Epoch 69/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7378 - accuracy: 0.6599 - val_loss: 1.0857 - val_accuracy: 0.5135\n",
            "Epoch 70/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7046 - accuracy: 0.6599 - val_loss: 1.0466 - val_accuracy: 0.5135\n",
            "Epoch 71/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7003 - accuracy: 0.6871 - val_loss: 1.1218 - val_accuracy: 0.5135\n",
            "Epoch 72/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7001 - accuracy: 0.6667 - val_loss: 1.1018 - val_accuracy: 0.4865\n",
            "Epoch 73/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7004 - accuracy: 0.7007 - val_loss: 1.2045 - val_accuracy: 0.5270\n",
            "Epoch 74/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7203 - accuracy: 0.6701 - val_loss: 1.2507 - val_accuracy: 0.4324\n",
            "Epoch 75/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7162 - accuracy: 0.7041 - val_loss: 1.1803 - val_accuracy: 0.5000\n",
            "Epoch 76/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7261 - accuracy: 0.6905 - val_loss: 1.0117 - val_accuracy: 0.5000\n",
            "Epoch 77/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7086 - accuracy: 0.7245 - val_loss: 1.1398 - val_accuracy: 0.4865\n",
            "Epoch 78/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6946 - accuracy: 0.6837 - val_loss: 1.1980 - val_accuracy: 0.4865\n",
            "Epoch 79/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6534 - accuracy: 0.7211 - val_loss: 1.1379 - val_accuracy: 0.4865\n",
            "Epoch 80/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6512 - accuracy: 0.7041 - val_loss: 1.1359 - val_accuracy: 0.5000\n",
            "Epoch 81/750\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6586 - accuracy: 0.7041 - val_loss: 1.1012 - val_accuracy: 0.4865\n",
            "Epoch 82/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6657 - accuracy: 0.7143 - val_loss: 1.1284 - val_accuracy: 0.4865\n",
            "Epoch 83/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6548 - accuracy: 0.6973 - val_loss: 1.1268 - val_accuracy: 0.5676\n",
            "Epoch 84/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6524 - accuracy: 0.7177 - val_loss: 1.1029 - val_accuracy: 0.5270\n",
            "Epoch 85/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5946 - accuracy: 0.7347 - val_loss: 1.2101 - val_accuracy: 0.5270\n",
            "Epoch 86/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6056 - accuracy: 0.7211 - val_loss: 1.2803 - val_accuracy: 0.5000\n",
            "Epoch 87/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5903 - accuracy: 0.7449 - val_loss: 1.2470 - val_accuracy: 0.5541\n",
            "Epoch 88/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5718 - accuracy: 0.7483 - val_loss: 1.1522 - val_accuracy: 0.4595\n",
            "Epoch 89/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6222 - accuracy: 0.7551 - val_loss: 1.2024 - val_accuracy: 0.4865\n",
            "Epoch 90/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5691 - accuracy: 0.7687 - val_loss: 1.2898 - val_accuracy: 0.5405\n",
            "Epoch 91/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5349 - accuracy: 0.7823 - val_loss: 1.5999 - val_accuracy: 0.5135\n",
            "Epoch 92/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6109 - accuracy: 0.7483 - val_loss: 1.5680 - val_accuracy: 0.5405\n",
            "Epoch 93/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6755 - accuracy: 0.7211 - val_loss: 1.2028 - val_accuracy: 0.4730\n",
            "Epoch 94/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6159 - accuracy: 0.7415 - val_loss: 1.1539 - val_accuracy: 0.5000\n",
            "Epoch 95/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5793 - accuracy: 0.7483 - val_loss: 1.1864 - val_accuracy: 0.5270\n",
            "Epoch 96/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5747 - accuracy: 0.7483 - val_loss: 1.1234 - val_accuracy: 0.5270\n",
            "Epoch 97/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5559 - accuracy: 0.7653 - val_loss: 1.2429 - val_accuracy: 0.5405\n",
            "Epoch 98/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5203 - accuracy: 0.7823 - val_loss: 1.2115 - val_accuracy: 0.5135\n",
            "Epoch 99/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5077 - accuracy: 0.7823 - val_loss: 1.4274 - val_accuracy: 0.5405\n",
            "Epoch 100/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5505 - accuracy: 0.7857 - val_loss: 1.2446 - val_accuracy: 0.5405\n",
            "Epoch 101/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5256 - accuracy: 0.7891 - val_loss: 1.5216 - val_accuracy: 0.5270\n",
            "Epoch 102/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5405 - accuracy: 0.7517 - val_loss: 1.2400 - val_accuracy: 0.5135\n",
            "Epoch 103/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5078 - accuracy: 0.7891 - val_loss: 1.4549 - val_accuracy: 0.5135\n",
            "Epoch 104/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5193 - accuracy: 0.7755 - val_loss: 1.3224 - val_accuracy: 0.5135\n",
            "Epoch 105/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5007 - accuracy: 0.8061 - val_loss: 1.2759 - val_accuracy: 0.5541\n",
            "Epoch 106/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4582 - accuracy: 0.8095 - val_loss: 1.3804 - val_accuracy: 0.5270\n",
            "Epoch 107/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4565 - accuracy: 0.7993 - val_loss: 1.3828 - val_accuracy: 0.5000\n",
            "Epoch 108/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4296 - accuracy: 0.7925 - val_loss: 1.3697 - val_accuracy: 0.5405\n",
            "Epoch 109/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4511 - accuracy: 0.8061 - val_loss: 1.4273 - val_accuracy: 0.5811\n",
            "Epoch 110/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4576 - accuracy: 0.8265 - val_loss: 1.3682 - val_accuracy: 0.5541\n",
            "Epoch 111/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4340 - accuracy: 0.8197 - val_loss: 1.5888 - val_accuracy: 0.5541\n",
            "Epoch 112/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4945 - accuracy: 0.7857 - val_loss: 1.4442 - val_accuracy: 0.5270\n",
            "Epoch 113/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5082 - accuracy: 0.7857 - val_loss: 1.5402 - val_accuracy: 0.5270\n",
            "Epoch 114/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4854 - accuracy: 0.7789 - val_loss: 1.3497 - val_accuracy: 0.5676\n",
            "Epoch 115/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4231 - accuracy: 0.8333 - val_loss: 1.3653 - val_accuracy: 0.5811\n",
            "Epoch 116/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4187 - accuracy: 0.8333 - val_loss: 1.5556 - val_accuracy: 0.5946\n",
            "Epoch 117/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4185 - accuracy: 0.8061 - val_loss: 1.5209 - val_accuracy: 0.5676\n",
            "Epoch 118/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3749 - accuracy: 0.8469 - val_loss: 1.4296 - val_accuracy: 0.6351\n",
            "Epoch 119/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3948 - accuracy: 0.8401 - val_loss: 1.4850 - val_accuracy: 0.5811\n",
            "Epoch 120/750\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4209 - accuracy: 0.8299 - val_loss: 1.4558 - val_accuracy: 0.5946\n",
            "Epoch 121/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3542 - accuracy: 0.8537 - val_loss: 1.5492 - val_accuracy: 0.5541\n",
            "Epoch 122/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3580 - accuracy: 0.8367 - val_loss: 1.5376 - val_accuracy: 0.5676\n",
            "Epoch 123/750\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3421 - accuracy: 0.8605 - val_loss: 1.5743 - val_accuracy: 0.6486\n",
            "Epoch 124/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3403 - accuracy: 0.8639 - val_loss: 1.7216 - val_accuracy: 0.6081\n",
            "Epoch 125/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3250 - accuracy: 0.8673 - val_loss: 1.9663 - val_accuracy: 0.5811\n",
            "Epoch 126/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3426 - accuracy: 0.8605 - val_loss: 1.9432 - val_accuracy: 0.5946\n",
            "Epoch 127/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3883 - accuracy: 0.8367 - val_loss: 1.9546 - val_accuracy: 0.5676\n",
            "Epoch 128/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3995 - accuracy: 0.8299 - val_loss: 1.8973 - val_accuracy: 0.5405\n",
            "Epoch 129/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3953 - accuracy: 0.8333 - val_loss: 1.7840 - val_accuracy: 0.5946\n",
            "Epoch 130/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3505 - accuracy: 0.8469 - val_loss: 1.8012 - val_accuracy: 0.5811\n",
            "Epoch 131/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4184 - accuracy: 0.8129 - val_loss: 1.6750 - val_accuracy: 0.6081\n",
            "Epoch 132/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3459 - accuracy: 0.8469 - val_loss: 1.7346 - val_accuracy: 0.5946\n",
            "Epoch 133/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3761 - accuracy: 0.8435 - val_loss: 1.6896 - val_accuracy: 0.6081\n",
            "Epoch 134/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3661 - accuracy: 0.8537 - val_loss: 1.7470 - val_accuracy: 0.5811\n",
            "Epoch 135/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3462 - accuracy: 0.8503 - val_loss: 1.7317 - val_accuracy: 0.5676\n",
            "Epoch 136/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3259 - accuracy: 0.8639 - val_loss: 1.6212 - val_accuracy: 0.5811\n",
            "Epoch 137/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3146 - accuracy: 0.8673 - val_loss: 1.6491 - val_accuracy: 0.5946\n",
            "Epoch 138/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3009 - accuracy: 0.8673 - val_loss: 1.9307 - val_accuracy: 0.5811\n",
            "Epoch 139/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2852 - accuracy: 0.8707 - val_loss: 1.8639 - val_accuracy: 0.5946\n",
            "Epoch 140/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3054 - accuracy: 0.8673 - val_loss: 1.5717 - val_accuracy: 0.5811\n",
            "Epoch 141/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3253 - accuracy: 0.8605 - val_loss: 1.6813 - val_accuracy: 0.5946\n",
            "Epoch 142/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2638 - accuracy: 0.8912 - val_loss: 1.8702 - val_accuracy: 0.6216\n",
            "Epoch 143/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2947 - accuracy: 0.8707 - val_loss: 1.9348 - val_accuracy: 0.6216\n",
            "Epoch 144/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2690 - accuracy: 0.8810 - val_loss: 1.9258 - val_accuracy: 0.6081\n",
            "Epoch 145/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2811 - accuracy: 0.8810 - val_loss: 1.9374 - val_accuracy: 0.5946\n",
            "Epoch 146/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2665 - accuracy: 0.8741 - val_loss: 1.9843 - val_accuracy: 0.6081\n",
            "Epoch 147/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2757 - accuracy: 0.8605 - val_loss: 1.9682 - val_accuracy: 0.5946\n",
            "Epoch 148/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2513 - accuracy: 0.8946 - val_loss: 1.9801 - val_accuracy: 0.6351\n",
            "Epoch 149/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2568 - accuracy: 0.8741 - val_loss: 2.0127 - val_accuracy: 0.6216\n",
            "Epoch 150/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2536 - accuracy: 0.8912 - val_loss: 2.1036 - val_accuracy: 0.5676\n",
            "Epoch 151/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2920 - accuracy: 0.8810 - val_loss: 2.0063 - val_accuracy: 0.6216\n",
            "Epoch 152/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2401 - accuracy: 0.8912 - val_loss: 2.1111 - val_accuracy: 0.6351\n",
            "Epoch 153/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2567 - accuracy: 0.8810 - val_loss: 2.3459 - val_accuracy: 0.6216\n",
            "Epoch 154/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2546 - accuracy: 0.8878 - val_loss: 2.4384 - val_accuracy: 0.6081\n",
            "Epoch 155/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2341 - accuracy: 0.8946 - val_loss: 2.3519 - val_accuracy: 0.6081\n",
            "Epoch 156/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2248 - accuracy: 0.8912 - val_loss: 2.3700 - val_accuracy: 0.6351\n",
            "Epoch 157/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2184 - accuracy: 0.9014 - val_loss: 2.4375 - val_accuracy: 0.6216\n",
            "Epoch 158/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2363 - accuracy: 0.9048 - val_loss: 2.4773 - val_accuracy: 0.6081\n",
            "Epoch 159/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2178 - accuracy: 0.9048 - val_loss: 2.6568 - val_accuracy: 0.5541\n",
            "Epoch 160/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2457 - accuracy: 0.8844 - val_loss: 2.6085 - val_accuracy: 0.5946\n",
            "Epoch 161/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2471 - accuracy: 0.8980 - val_loss: 2.6233 - val_accuracy: 0.6081\n",
            "Epoch 162/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2301 - accuracy: 0.8980 - val_loss: 2.6038 - val_accuracy: 0.5946\n",
            "Epoch 163/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2429 - accuracy: 0.9014 - val_loss: 2.6635 - val_accuracy: 0.5811\n",
            "Epoch 164/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2613 - accuracy: 0.8946 - val_loss: 2.9192 - val_accuracy: 0.6081\n",
            "Epoch 165/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3132 - accuracy: 0.8776 - val_loss: 2.5534 - val_accuracy: 0.5811\n",
            "Epoch 166/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2805 - accuracy: 0.8707 - val_loss: 2.5657 - val_accuracy: 0.6081\n",
            "Epoch 167/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2528 - accuracy: 0.8810 - val_loss: 2.6835 - val_accuracy: 0.5946\n",
            "Epoch 168/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2361 - accuracy: 0.8946 - val_loss: 2.8216 - val_accuracy: 0.5811\n",
            "Epoch 169/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2493 - accuracy: 0.8980 - val_loss: 2.6499 - val_accuracy: 0.5811\n",
            "Epoch 170/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2474 - accuracy: 0.8980 - val_loss: 2.5506 - val_accuracy: 0.5946\n",
            "Epoch 171/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2317 - accuracy: 0.9048 - val_loss: 2.5663 - val_accuracy: 0.5946\n",
            "Epoch 172/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2212 - accuracy: 0.9082 - val_loss: 2.6010 - val_accuracy: 0.5946\n",
            "Epoch 173/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2369 - accuracy: 0.9014 - val_loss: 2.7087 - val_accuracy: 0.6081\n",
            "Epoch 174/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2089 - accuracy: 0.9048 - val_loss: 2.7850 - val_accuracy: 0.5946\n",
            "Epoch 175/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2294 - accuracy: 0.9014 - val_loss: 2.6604 - val_accuracy: 0.6216\n",
            "Epoch 176/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2190 - accuracy: 0.9014 - val_loss: 2.7673 - val_accuracy: 0.5676\n",
            "Epoch 177/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2536 - accuracy: 0.8912 - val_loss: 3.3037 - val_accuracy: 0.5676\n",
            "Epoch 178/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2584 - accuracy: 0.8844 - val_loss: 3.0124 - val_accuracy: 0.5676\n",
            "Epoch 179/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2217 - accuracy: 0.8912 - val_loss: 2.8709 - val_accuracy: 0.5946\n",
            "Epoch 180/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2179 - accuracy: 0.8980 - val_loss: 2.8940 - val_accuracy: 0.5405\n",
            "Epoch 181/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2384 - accuracy: 0.8946 - val_loss: 3.0250 - val_accuracy: 0.5270\n",
            "Epoch 182/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2208 - accuracy: 0.8980 - val_loss: 2.9745 - val_accuracy: 0.5541\n",
            "Epoch 183/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2310 - accuracy: 0.8980 - val_loss: 2.9356 - val_accuracy: 0.5811\n",
            "Epoch 184/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2369 - accuracy: 0.9014 - val_loss: 2.8440 - val_accuracy: 0.5676\n",
            "Epoch 185/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2225 - accuracy: 0.9014 - val_loss: 2.8272 - val_accuracy: 0.5270\n",
            "Epoch 186/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2395 - accuracy: 0.8980 - val_loss: 2.7948 - val_accuracy: 0.5541\n",
            "Epoch 187/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2217 - accuracy: 0.8980 - val_loss: 2.7962 - val_accuracy: 0.5811\n",
            "Epoch 188/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2248 - accuracy: 0.8878 - val_loss: 2.8794 - val_accuracy: 0.5676\n",
            "Epoch 189/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2080 - accuracy: 0.8912 - val_loss: 2.9262 - val_accuracy: 0.5541\n",
            "Epoch 190/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2239 - accuracy: 0.9082 - val_loss: 2.8606 - val_accuracy: 0.5541\n",
            "Epoch 191/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2173 - accuracy: 0.9048 - val_loss: 2.8654 - val_accuracy: 0.6081\n",
            "Epoch 192/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1962 - accuracy: 0.9082 - val_loss: 2.9214 - val_accuracy: 0.6081\n",
            "Epoch 193/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1984 - accuracy: 0.9048 - val_loss: 2.9901 - val_accuracy: 0.6081\n",
            "Epoch 194/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2024 - accuracy: 0.9116 - val_loss: 3.0710 - val_accuracy: 0.6216\n",
            "Epoch 195/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2095 - accuracy: 0.9048 - val_loss: 3.1127 - val_accuracy: 0.6081\n",
            "Epoch 196/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1884 - accuracy: 0.9116 - val_loss: 3.1053 - val_accuracy: 0.5946\n",
            "Epoch 197/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2130 - accuracy: 0.9048 - val_loss: 3.1763 - val_accuracy: 0.5946\n",
            "Epoch 198/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2077 - accuracy: 0.9082 - val_loss: 3.3341 - val_accuracy: 0.5946\n",
            "Epoch 199/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1908 - accuracy: 0.9082 - val_loss: 3.4133 - val_accuracy: 0.6351\n",
            "Epoch 200/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2233 - accuracy: 0.8980 - val_loss: 3.2746 - val_accuracy: 0.6081\n",
            "Epoch 201/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1881 - accuracy: 0.9184 - val_loss: 3.1013 - val_accuracy: 0.5946\n",
            "Epoch 202/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2307 - accuracy: 0.8980 - val_loss: 3.2158 - val_accuracy: 0.5811\n",
            "Epoch 203/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2028 - accuracy: 0.9014 - val_loss: 2.9454 - val_accuracy: 0.5946\n",
            "Epoch 204/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2181 - accuracy: 0.9082 - val_loss: 2.9066 - val_accuracy: 0.6351\n",
            "Epoch 205/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2061 - accuracy: 0.9014 - val_loss: 2.8963 - val_accuracy: 0.5811\n",
            "Epoch 206/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2150 - accuracy: 0.8980 - val_loss: 3.0627 - val_accuracy: 0.5405\n",
            "Epoch 207/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2181 - accuracy: 0.9048 - val_loss: 3.0289 - val_accuracy: 0.5405\n",
            "Epoch 208/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2230 - accuracy: 0.9014 - val_loss: 2.9845 - val_accuracy: 0.5405\n",
            "Epoch 209/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2122 - accuracy: 0.9014 - val_loss: 2.9720 - val_accuracy: 0.5811\n",
            "Epoch 210/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1993 - accuracy: 0.9048 - val_loss: 2.9534 - val_accuracy: 0.5676\n",
            "Epoch 211/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1997 - accuracy: 0.9048 - val_loss: 2.9330 - val_accuracy: 0.5541\n",
            "Epoch 212/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2043 - accuracy: 0.9048 - val_loss: 2.9778 - val_accuracy: 0.5405\n",
            "Epoch 213/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2037 - accuracy: 0.9048 - val_loss: 3.0373 - val_accuracy: 0.5676\n",
            "Epoch 214/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2285 - accuracy: 0.9048 - val_loss: 3.0090 - val_accuracy: 0.5946\n",
            "Epoch 215/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2044 - accuracy: 0.9082 - val_loss: 2.9487 - val_accuracy: 0.6081\n",
            "Epoch 216/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2069 - accuracy: 0.9150 - val_loss: 3.0229 - val_accuracy: 0.5946\n",
            "Epoch 217/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2053 - accuracy: 0.9014 - val_loss: 2.9323 - val_accuracy: 0.5946\n",
            "Epoch 218/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2042 - accuracy: 0.9014 - val_loss: 2.9033 - val_accuracy: 0.6081\n",
            "Epoch 219/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2070 - accuracy: 0.9048 - val_loss: 2.9432 - val_accuracy: 0.6351\n",
            "Epoch 220/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2068 - accuracy: 0.9048 - val_loss: 2.9875 - val_accuracy: 0.6486\n",
            "Epoch 221/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2122 - accuracy: 0.9014 - val_loss: 3.1274 - val_accuracy: 0.6216\n",
            "Epoch 222/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1870 - accuracy: 0.9116 - val_loss: 3.2971 - val_accuracy: 0.5811\n",
            "Epoch 223/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2162 - accuracy: 0.9014 - val_loss: 3.2620 - val_accuracy: 0.6216\n",
            "Epoch 224/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2023 - accuracy: 0.9116 - val_loss: 3.2912 - val_accuracy: 0.5405\n",
            "Epoch 225/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2013 - accuracy: 0.9116 - val_loss: 3.2685 - val_accuracy: 0.5811\n",
            "Epoch 226/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1882 - accuracy: 0.9082 - val_loss: 3.1667 - val_accuracy: 0.6216\n",
            "Epoch 227/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1972 - accuracy: 0.9082 - val_loss: 3.0512 - val_accuracy: 0.6486\n",
            "Epoch 228/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2249 - accuracy: 0.9014 - val_loss: 3.1564 - val_accuracy: 0.5946\n",
            "Epoch 229/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2005 - accuracy: 0.9116 - val_loss: 3.1929 - val_accuracy: 0.5946\n",
            "Epoch 230/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2561 - accuracy: 0.8912 - val_loss: 3.1135 - val_accuracy: 0.5946\n",
            "Epoch 231/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2181 - accuracy: 0.8980 - val_loss: 3.2692 - val_accuracy: 0.5946\n",
            "Epoch 232/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2110 - accuracy: 0.9014 - val_loss: 3.5261 - val_accuracy: 0.5946\n",
            "Epoch 233/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2191 - accuracy: 0.8980 - val_loss: 3.4485 - val_accuracy: 0.5405\n",
            "Epoch 234/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2104 - accuracy: 0.9014 - val_loss: 3.5026 - val_accuracy: 0.5811\n",
            "Epoch 235/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2015 - accuracy: 0.9150 - val_loss: 3.7294 - val_accuracy: 0.5541\n",
            "Epoch 236/750\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2371 - accuracy: 0.8980 - val_loss: 3.6234 - val_accuracy: 0.5541\n",
            "Epoch 237/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2313 - accuracy: 0.8946 - val_loss: 3.5047 - val_accuracy: 0.5270\n",
            "Epoch 238/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2188 - accuracy: 0.9116 - val_loss: 3.3032 - val_accuracy: 0.5811\n",
            "Epoch 239/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2189 - accuracy: 0.9048 - val_loss: 3.2429 - val_accuracy: 0.5811\n",
            "Epoch 240/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2041 - accuracy: 0.9048 - val_loss: 3.0200 - val_accuracy: 0.6216\n",
            "Epoch 241/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2211 - accuracy: 0.9048 - val_loss: 3.0331 - val_accuracy: 0.5946\n",
            "Epoch 242/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1995 - accuracy: 0.9048 - val_loss: 3.1319 - val_accuracy: 0.5946\n",
            "Epoch 243/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1903 - accuracy: 0.9048 - val_loss: 3.2225 - val_accuracy: 0.6081\n",
            "Epoch 244/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1990 - accuracy: 0.9014 - val_loss: 3.2441 - val_accuracy: 0.5541\n",
            "Epoch 245/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2154 - accuracy: 0.9014 - val_loss: 3.2029 - val_accuracy: 0.5405\n",
            "Epoch 246/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1983 - accuracy: 0.9082 - val_loss: 3.2813 - val_accuracy: 0.5541\n",
            "Epoch 247/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2008 - accuracy: 0.9116 - val_loss: 3.4678 - val_accuracy: 0.5946\n",
            "Epoch 248/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1980 - accuracy: 0.9116 - val_loss: 3.5948 - val_accuracy: 0.5946\n",
            "Epoch 249/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1963 - accuracy: 0.9150 - val_loss: 3.7200 - val_accuracy: 0.5946\n",
            "Epoch 250/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1977 - accuracy: 0.9082 - val_loss: 3.7958 - val_accuracy: 0.5946\n",
            "Epoch 251/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1921 - accuracy: 0.9116 - val_loss: 3.7828 - val_accuracy: 0.5811\n",
            "Epoch 252/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1951 - accuracy: 0.9218 - val_loss: 3.7342 - val_accuracy: 0.5811\n",
            "Epoch 253/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2019 - accuracy: 0.9048 - val_loss: 3.6830 - val_accuracy: 0.5541\n",
            "Epoch 254/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1817 - accuracy: 0.9184 - val_loss: 3.6827 - val_accuracy: 0.5676\n",
            "Epoch 255/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1821 - accuracy: 0.9150 - val_loss: 3.7054 - val_accuracy: 0.5811\n",
            "Epoch 256/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2004 - accuracy: 0.9116 - val_loss: 3.7662 - val_accuracy: 0.5676\n",
            "Epoch 257/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1955 - accuracy: 0.9082 - val_loss: 3.7518 - val_accuracy: 0.5811\n",
            "Epoch 258/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1952 - accuracy: 0.9082 - val_loss: 3.7676 - val_accuracy: 0.5946\n",
            "Epoch 259/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1983 - accuracy: 0.9082 - val_loss: 3.8185 - val_accuracy: 0.5946\n",
            "Epoch 260/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2046 - accuracy: 0.9014 - val_loss: 3.7397 - val_accuracy: 0.5541\n",
            "Epoch 261/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1974 - accuracy: 0.9048 - val_loss: 3.8298 - val_accuracy: 0.5811\n",
            "Epoch 262/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2065 - accuracy: 0.9048 - val_loss: 3.8047 - val_accuracy: 0.5946\n",
            "Epoch 263/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1938 - accuracy: 0.9082 - val_loss: 3.6744 - val_accuracy: 0.5811\n",
            "Epoch 264/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2173 - accuracy: 0.9048 - val_loss: 3.4887 - val_accuracy: 0.5811\n",
            "Epoch 265/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2081 - accuracy: 0.9014 - val_loss: 3.5634 - val_accuracy: 0.5676\n",
            "Epoch 266/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2234 - accuracy: 0.8946 - val_loss: 3.2742 - val_accuracy: 0.6216\n",
            "Epoch 267/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2129 - accuracy: 0.9048 - val_loss: 2.7938 - val_accuracy: 0.6216\n",
            "Epoch 268/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1956 - accuracy: 0.9048 - val_loss: 2.6887 - val_accuracy: 0.6081\n",
            "Epoch 269/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1965 - accuracy: 0.9116 - val_loss: 2.8291 - val_accuracy: 0.6081\n",
            "Epoch 270/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2078 - accuracy: 0.9082 - val_loss: 2.9205 - val_accuracy: 0.5946\n",
            "Epoch 271/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2028 - accuracy: 0.9116 - val_loss: 2.9765 - val_accuracy: 0.5676\n",
            "Epoch 272/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2070 - accuracy: 0.9082 - val_loss: 3.0841 - val_accuracy: 0.5270\n",
            "Epoch 273/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2104 - accuracy: 0.9014 - val_loss: 3.3263 - val_accuracy: 0.5135\n",
            "Epoch 274/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2007 - accuracy: 0.9116 - val_loss: 3.4357 - val_accuracy: 0.5811\n",
            "Epoch 275/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2021 - accuracy: 0.9116 - val_loss: 3.5283 - val_accuracy: 0.5946\n",
            "Epoch 276/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1907 - accuracy: 0.9116 - val_loss: 3.5169 - val_accuracy: 0.5811\n",
            "Epoch 277/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1916 - accuracy: 0.9116 - val_loss: 3.4496 - val_accuracy: 0.5811\n",
            "Epoch 278/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2007 - accuracy: 0.9150 - val_loss: 3.3899 - val_accuracy: 0.5811\n",
            "Epoch 279/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1890 - accuracy: 0.9082 - val_loss: 3.3007 - val_accuracy: 0.5676\n",
            "Epoch 280/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1860 - accuracy: 0.9116 - val_loss: 3.2361 - val_accuracy: 0.5676\n",
            "Epoch 281/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1836 - accuracy: 0.9150 - val_loss: 3.2917 - val_accuracy: 0.5811\n",
            "Epoch 282/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2049 - accuracy: 0.9048 - val_loss: 3.4301 - val_accuracy: 0.5676\n",
            "Epoch 283/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1969 - accuracy: 0.9082 - val_loss: 3.4584 - val_accuracy: 0.5405\n",
            "Epoch 284/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1892 - accuracy: 0.9116 - val_loss: 3.4479 - val_accuracy: 0.5811\n",
            "Epoch 285/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1964 - accuracy: 0.9082 - val_loss: 3.4973 - val_accuracy: 0.5541\n",
            "Epoch 286/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1925 - accuracy: 0.9116 - val_loss: 3.4703 - val_accuracy: 0.6081\n",
            "Epoch 287/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2446 - accuracy: 0.8878 - val_loss: 3.5349 - val_accuracy: 0.5811\n",
            "Epoch 288/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3062 - accuracy: 0.8980 - val_loss: 2.8350 - val_accuracy: 0.5946\n",
            "Epoch 289/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4073 - accuracy: 0.8741 - val_loss: 2.7655 - val_accuracy: 0.5000\n",
            "Epoch 290/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2493 - accuracy: 0.8912 - val_loss: 2.9092 - val_accuracy: 0.5676\n",
            "Epoch 291/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2528 - accuracy: 0.8946 - val_loss: 2.6692 - val_accuracy: 0.5811\n",
            "Epoch 292/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2492 - accuracy: 0.8980 - val_loss: 2.1750 - val_accuracy: 0.6351\n",
            "Epoch 293/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3726 - accuracy: 0.8673 - val_loss: 1.7946 - val_accuracy: 0.5811\n",
            "Epoch 294/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5209 - accuracy: 0.8197 - val_loss: 1.3247 - val_accuracy: 0.6216\n",
            "Epoch 295/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6177 - accuracy: 0.7789 - val_loss: 1.3355 - val_accuracy: 0.5541\n",
            "Epoch 296/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.5586 - accuracy: 0.7823 - val_loss: 1.1596 - val_accuracy: 0.6081\n",
            "Epoch 297/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4711 - accuracy: 0.8129 - val_loss: 1.0612 - val_accuracy: 0.5676\n",
            "Epoch 298/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4372 - accuracy: 0.8197 - val_loss: 1.0087 - val_accuracy: 0.6081\n",
            "Epoch 299/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3913 - accuracy: 0.8367 - val_loss: 1.2572 - val_accuracy: 0.6081\n",
            "Epoch 300/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3718 - accuracy: 0.8435 - val_loss: 1.2398 - val_accuracy: 0.6081\n",
            "Epoch 301/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3289 - accuracy: 0.8503 - val_loss: 1.2357 - val_accuracy: 0.5946\n",
            "Epoch 302/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3122 - accuracy: 0.8741 - val_loss: 1.3172 - val_accuracy: 0.5946\n",
            "Epoch 303/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2877 - accuracy: 0.8707 - val_loss: 1.5763 - val_accuracy: 0.5946\n",
            "Epoch 304/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2917 - accuracy: 0.8844 - val_loss: 1.5247 - val_accuracy: 0.5811\n",
            "Epoch 305/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2559 - accuracy: 0.8878 - val_loss: 1.5541 - val_accuracy: 0.6081\n",
            "Epoch 306/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2642 - accuracy: 0.8878 - val_loss: 1.7070 - val_accuracy: 0.5946\n",
            "Epoch 307/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2326 - accuracy: 0.8980 - val_loss: 1.9168 - val_accuracy: 0.6081\n",
            "Epoch 308/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2203 - accuracy: 0.8946 - val_loss: 2.0037 - val_accuracy: 0.6081\n",
            "Epoch 309/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2095 - accuracy: 0.9082 - val_loss: 2.0832 - val_accuracy: 0.5811\n",
            "Epoch 310/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2086 - accuracy: 0.9116 - val_loss: 2.2359 - val_accuracy: 0.5676\n",
            "Epoch 311/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2227 - accuracy: 0.9048 - val_loss: 2.3112 - val_accuracy: 0.5811\n",
            "Epoch 312/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2113 - accuracy: 0.9048 - val_loss: 2.3211 - val_accuracy: 0.5811\n",
            "Epoch 313/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2094 - accuracy: 0.9048 - val_loss: 2.3067 - val_accuracy: 0.5811\n",
            "Epoch 314/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2110 - accuracy: 0.9048 - val_loss: 2.3136 - val_accuracy: 0.5811\n",
            "Epoch 315/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2024 - accuracy: 0.9048 - val_loss: 2.3314 - val_accuracy: 0.5946\n",
            "Epoch 316/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1935 - accuracy: 0.9048 - val_loss: 2.4013 - val_accuracy: 0.5946\n",
            "Epoch 317/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1842 - accuracy: 0.9116 - val_loss: 2.4702 - val_accuracy: 0.5946\n",
            "Epoch 318/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2046 - accuracy: 0.9048 - val_loss: 2.5028 - val_accuracy: 0.5946\n",
            "Epoch 319/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1955 - accuracy: 0.8980 - val_loss: 2.4933 - val_accuracy: 0.6081\n",
            "Epoch 320/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1923 - accuracy: 0.9150 - val_loss: 2.5493 - val_accuracy: 0.5405\n",
            "Epoch 321/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2051 - accuracy: 0.9048 - val_loss: 2.6073 - val_accuracy: 0.5946\n",
            "Epoch 322/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2112 - accuracy: 0.9048 - val_loss: 2.6816 - val_accuracy: 0.5946\n",
            "Epoch 323/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2026 - accuracy: 0.9048 - val_loss: 2.6264 - val_accuracy: 0.5946\n",
            "Epoch 324/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2036 - accuracy: 0.9048 - val_loss: 2.5379 - val_accuracy: 0.6216\n",
            "Epoch 325/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2011 - accuracy: 0.9048 - val_loss: 2.5372 - val_accuracy: 0.6081\n",
            "Epoch 326/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1945 - accuracy: 0.9082 - val_loss: 2.5806 - val_accuracy: 0.5811\n",
            "Epoch 327/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2022 - accuracy: 0.9082 - val_loss: 2.5955 - val_accuracy: 0.5811\n",
            "Epoch 328/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2006 - accuracy: 0.9116 - val_loss: 2.5953 - val_accuracy: 0.5946\n",
            "Epoch 329/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1955 - accuracy: 0.9116 - val_loss: 2.6091 - val_accuracy: 0.5946\n",
            "Epoch 330/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1899 - accuracy: 0.9082 - val_loss: 2.6232 - val_accuracy: 0.5946\n",
            "Epoch 331/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1925 - accuracy: 0.9116 - val_loss: 2.6467 - val_accuracy: 0.5811\n",
            "Epoch 332/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1860 - accuracy: 0.9150 - val_loss: 2.6853 - val_accuracy: 0.5811\n",
            "Epoch 333/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1964 - accuracy: 0.9082 - val_loss: 2.7191 - val_accuracy: 0.5811\n",
            "Epoch 334/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1940 - accuracy: 0.9116 - val_loss: 2.7373 - val_accuracy: 0.5811\n",
            "Epoch 335/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1946 - accuracy: 0.9116 - val_loss: 2.7476 - val_accuracy: 0.5811\n",
            "Epoch 336/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2038 - accuracy: 0.9116 - val_loss: 2.7426 - val_accuracy: 0.5811\n",
            "Epoch 337/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1907 - accuracy: 0.9116 - val_loss: 2.7297 - val_accuracy: 0.5811\n",
            "Epoch 338/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1902 - accuracy: 0.9116 - val_loss: 2.7202 - val_accuracy: 0.5946\n",
            "Epoch 339/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1860 - accuracy: 0.9116 - val_loss: 2.7228 - val_accuracy: 0.6081\n",
            "Epoch 340/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1991 - accuracy: 0.9116 - val_loss: 2.7515 - val_accuracy: 0.6081\n",
            "Epoch 341/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1848 - accuracy: 0.9116 - val_loss: 2.7837 - val_accuracy: 0.6081\n",
            "Epoch 342/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1850 - accuracy: 0.9150 - val_loss: 2.8027 - val_accuracy: 0.6081\n",
            "Epoch 343/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1879 - accuracy: 0.9116 - val_loss: 2.8189 - val_accuracy: 0.6216\n",
            "Epoch 344/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1891 - accuracy: 0.9150 - val_loss: 2.8435 - val_accuracy: 0.6216\n",
            "Epoch 345/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1913 - accuracy: 0.9116 - val_loss: 2.8562 - val_accuracy: 0.6216\n",
            "Epoch 346/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1909 - accuracy: 0.9116 - val_loss: 2.8590 - val_accuracy: 0.6216\n",
            "Epoch 347/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1933 - accuracy: 0.9116 - val_loss: 2.8674 - val_accuracy: 0.6216\n",
            "Epoch 348/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1935 - accuracy: 0.9116 - val_loss: 2.8856 - val_accuracy: 0.6351\n",
            "Epoch 349/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1969 - accuracy: 0.9082 - val_loss: 2.8967 - val_accuracy: 0.6351\n",
            "Epoch 350/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1884 - accuracy: 0.9116 - val_loss: 2.9073 - val_accuracy: 0.6351\n",
            "Epoch 351/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1910 - accuracy: 0.9116 - val_loss: 2.9152 - val_accuracy: 0.6081\n",
            "Epoch 352/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2036 - accuracy: 0.9048 - val_loss: 2.9087 - val_accuracy: 0.5811\n",
            "Epoch 353/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1925 - accuracy: 0.9048 - val_loss: 2.9241 - val_accuracy: 0.5811\n",
            "Epoch 354/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1928 - accuracy: 0.9082 - val_loss: 2.9831 - val_accuracy: 0.5946\n",
            "Epoch 355/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1936 - accuracy: 0.9082 - val_loss: 3.0465 - val_accuracy: 0.5946\n",
            "Epoch 356/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1848 - accuracy: 0.9082 - val_loss: 3.0359 - val_accuracy: 0.5946\n",
            "Epoch 357/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1899 - accuracy: 0.9150 - val_loss: 3.0378 - val_accuracy: 0.5946\n",
            "Epoch 358/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1963 - accuracy: 0.9116 - val_loss: 3.0117 - val_accuracy: 0.6351\n",
            "Epoch 359/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1946 - accuracy: 0.9082 - val_loss: 3.0225 - val_accuracy: 0.6081\n",
            "Epoch 360/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1912 - accuracy: 0.9048 - val_loss: 3.0148 - val_accuracy: 0.6216\n",
            "Epoch 361/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1874 - accuracy: 0.9116 - val_loss: 3.0215 - val_accuracy: 0.6216\n",
            "Epoch 362/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1904 - accuracy: 0.9082 - val_loss: 3.0241 - val_accuracy: 0.6081\n",
            "Epoch 363/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1923 - accuracy: 0.9116 - val_loss: 3.0298 - val_accuracy: 0.6081\n",
            "Epoch 364/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1899 - accuracy: 0.9150 - val_loss: 3.0365 - val_accuracy: 0.6081\n",
            "Epoch 365/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1870 - accuracy: 0.9116 - val_loss: 3.0496 - val_accuracy: 0.6081\n",
            "Epoch 366/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1946 - accuracy: 0.9048 - val_loss: 3.0792 - val_accuracy: 0.6081\n",
            "Epoch 367/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1843 - accuracy: 0.9150 - val_loss: 3.1101 - val_accuracy: 0.6081\n",
            "Epoch 368/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1874 - accuracy: 0.9116 - val_loss: 3.1628 - val_accuracy: 0.5946\n",
            "Epoch 369/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1898 - accuracy: 0.9082 - val_loss: 3.2516 - val_accuracy: 0.6216\n",
            "Epoch 370/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1836 - accuracy: 0.9116 - val_loss: 3.2927 - val_accuracy: 0.6216\n",
            "Epoch 371/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2015 - accuracy: 0.9116 - val_loss: 3.2873 - val_accuracy: 0.6081\n",
            "Epoch 372/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1908 - accuracy: 0.9116 - val_loss: 3.2436 - val_accuracy: 0.6081\n",
            "Epoch 373/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1866 - accuracy: 0.9116 - val_loss: 3.2501 - val_accuracy: 0.6081\n",
            "Epoch 374/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1912 - accuracy: 0.9116 - val_loss: 3.2740 - val_accuracy: 0.5946\n",
            "Epoch 375/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1952 - accuracy: 0.9048 - val_loss: 3.3113 - val_accuracy: 0.5946\n",
            "Epoch 376/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1849 - accuracy: 0.9116 - val_loss: 3.3417 - val_accuracy: 0.5946\n",
            "Epoch 377/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1879 - accuracy: 0.9150 - val_loss: 3.3631 - val_accuracy: 0.5946\n",
            "Epoch 378/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1862 - accuracy: 0.9116 - val_loss: 3.3677 - val_accuracy: 0.5811\n",
            "Epoch 379/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1953 - accuracy: 0.9116 - val_loss: 3.3683 - val_accuracy: 0.5811\n",
            "Epoch 380/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1831 - accuracy: 0.9150 - val_loss: 3.3524 - val_accuracy: 0.5811\n",
            "Epoch 381/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1847 - accuracy: 0.9116 - val_loss: 3.3410 - val_accuracy: 0.5946\n",
            "Epoch 382/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1862 - accuracy: 0.9116 - val_loss: 3.3410 - val_accuracy: 0.6081\n",
            "Epoch 383/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1873 - accuracy: 0.9116 - val_loss: 3.3571 - val_accuracy: 0.6216\n",
            "Epoch 384/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1866 - accuracy: 0.9116 - val_loss: 3.3718 - val_accuracy: 0.6216\n",
            "Epoch 385/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1841 - accuracy: 0.9150 - val_loss: 3.3935 - val_accuracy: 0.6216\n",
            "Epoch 386/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1870 - accuracy: 0.9082 - val_loss: 3.4170 - val_accuracy: 0.6216\n",
            "Epoch 387/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1832 - accuracy: 0.9116 - val_loss: 3.4672 - val_accuracy: 0.6081\n",
            "Epoch 388/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1858 - accuracy: 0.9116 - val_loss: 3.5130 - val_accuracy: 0.5946\n",
            "Epoch 389/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1825 - accuracy: 0.9116 - val_loss: 3.5287 - val_accuracy: 0.5946\n",
            "Epoch 390/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1858 - accuracy: 0.9116 - val_loss: 3.5389 - val_accuracy: 0.5946\n",
            "Epoch 391/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1878 - accuracy: 0.9150 - val_loss: 3.5373 - val_accuracy: 0.5811\n",
            "Epoch 392/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1924 - accuracy: 0.9116 - val_loss: 3.5148 - val_accuracy: 0.6081\n",
            "Epoch 393/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1855 - accuracy: 0.9116 - val_loss: 3.5023 - val_accuracy: 0.6081\n",
            "Epoch 394/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1998 - accuracy: 0.9082 - val_loss: 3.4956 - val_accuracy: 0.5946\n",
            "Epoch 395/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1899 - accuracy: 0.9048 - val_loss: 3.4831 - val_accuracy: 0.5946\n",
            "Epoch 396/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1776 - accuracy: 0.9116 - val_loss: 3.4735 - val_accuracy: 0.5946\n",
            "Epoch 397/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1904 - accuracy: 0.9116 - val_loss: 3.4788 - val_accuracy: 0.5946\n",
            "Epoch 398/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1874 - accuracy: 0.9082 - val_loss: 3.4818 - val_accuracy: 0.5946\n",
            "Epoch 399/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1846 - accuracy: 0.9116 - val_loss: 3.4826 - val_accuracy: 0.5811\n",
            "Epoch 400/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1832 - accuracy: 0.9116 - val_loss: 3.4865 - val_accuracy: 0.5811\n",
            "Epoch 401/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1961 - accuracy: 0.9048 - val_loss: 3.4986 - val_accuracy: 0.5811\n",
            "Epoch 402/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1896 - accuracy: 0.9082 - val_loss: 3.5053 - val_accuracy: 0.5811\n",
            "Epoch 403/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1902 - accuracy: 0.9116 - val_loss: 3.4965 - val_accuracy: 0.5811\n",
            "Epoch 404/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1942 - accuracy: 0.9082 - val_loss: 3.4872 - val_accuracy: 0.5676\n",
            "Epoch 405/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1828 - accuracy: 0.9150 - val_loss: 3.4775 - val_accuracy: 0.5676\n",
            "Epoch 406/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1873 - accuracy: 0.9184 - val_loss: 3.4802 - val_accuracy: 0.5676\n",
            "Epoch 407/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1922 - accuracy: 0.9082 - val_loss: 3.4962 - val_accuracy: 0.5676\n",
            "Epoch 408/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1907 - accuracy: 0.9116 - val_loss: 3.5282 - val_accuracy: 0.5811\n",
            "Epoch 409/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1820 - accuracy: 0.9116 - val_loss: 3.5732 - val_accuracy: 0.5811\n",
            "Epoch 410/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1914 - accuracy: 0.9116 - val_loss: 3.6003 - val_accuracy: 0.5811\n",
            "Epoch 411/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1894 - accuracy: 0.9116 - val_loss: 3.6894 - val_accuracy: 0.5676\n",
            "Epoch 412/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1957 - accuracy: 0.9116 - val_loss: 3.5988 - val_accuracy: 0.6081\n",
            "Epoch 413/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2094 - accuracy: 0.9116 - val_loss: 3.4247 - val_accuracy: 0.5811\n",
            "Epoch 414/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1919 - accuracy: 0.9116 - val_loss: 3.3534 - val_accuracy: 0.5946\n",
            "Epoch 415/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1893 - accuracy: 0.9116 - val_loss: 3.4734 - val_accuracy: 0.5946\n",
            "Epoch 416/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2003 - accuracy: 0.9116 - val_loss: 3.4349 - val_accuracy: 0.5676\n",
            "Epoch 417/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1949 - accuracy: 0.9116 - val_loss: 3.5929 - val_accuracy: 0.5676\n",
            "Epoch 418/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2138 - accuracy: 0.9048 - val_loss: 3.4671 - val_accuracy: 0.6081\n",
            "Epoch 419/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1934 - accuracy: 0.9116 - val_loss: 3.7213 - val_accuracy: 0.6081\n",
            "Epoch 420/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2064 - accuracy: 0.9082 - val_loss: 3.5904 - val_accuracy: 0.5946\n",
            "Epoch 421/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2014 - accuracy: 0.9082 - val_loss: 3.2321 - val_accuracy: 0.6216\n",
            "Epoch 422/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1896 - accuracy: 0.9116 - val_loss: 3.1391 - val_accuracy: 0.5676\n",
            "Epoch 423/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1902 - accuracy: 0.9048 - val_loss: 3.2089 - val_accuracy: 0.6081\n",
            "Epoch 424/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1961 - accuracy: 0.9082 - val_loss: 3.2922 - val_accuracy: 0.5811\n",
            "Epoch 425/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1866 - accuracy: 0.9116 - val_loss: 3.4258 - val_accuracy: 0.5811\n",
            "Epoch 426/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1898 - accuracy: 0.9082 - val_loss: 3.5362 - val_accuracy: 0.5676\n",
            "Epoch 427/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1853 - accuracy: 0.9082 - val_loss: 3.6267 - val_accuracy: 0.5676\n",
            "Epoch 428/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1931 - accuracy: 0.9116 - val_loss: 3.6944 - val_accuracy: 0.5676\n",
            "Epoch 429/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1954 - accuracy: 0.9048 - val_loss: 3.7342 - val_accuracy: 0.5676\n",
            "Epoch 430/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1911 - accuracy: 0.9082 - val_loss: 3.6257 - val_accuracy: 0.5676\n",
            "Epoch 431/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1813 - accuracy: 0.9150 - val_loss: 3.6079 - val_accuracy: 0.5676\n",
            "Epoch 432/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1855 - accuracy: 0.9116 - val_loss: 3.6372 - val_accuracy: 0.5676\n",
            "Epoch 433/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1952 - accuracy: 0.9116 - val_loss: 3.6680 - val_accuracy: 0.5811\n",
            "Epoch 434/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1874 - accuracy: 0.9150 - val_loss: 3.6957 - val_accuracy: 0.5811\n",
            "Epoch 435/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1839 - accuracy: 0.9116 - val_loss: 3.7186 - val_accuracy: 0.5946\n",
            "Epoch 436/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1839 - accuracy: 0.9150 - val_loss: 3.7474 - val_accuracy: 0.6081\n",
            "Epoch 437/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1949 - accuracy: 0.9116 - val_loss: 3.7975 - val_accuracy: 0.5676\n",
            "Epoch 438/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1924 - accuracy: 0.9116 - val_loss: 3.9283 - val_accuracy: 0.5541\n",
            "Epoch 439/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2087 - accuracy: 0.9082 - val_loss: 3.6965 - val_accuracy: 0.5676\n",
            "Epoch 440/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1963 - accuracy: 0.9116 - val_loss: 3.4550 - val_accuracy: 0.5811\n",
            "Epoch 441/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1947 - accuracy: 0.9082 - val_loss: 3.3863 - val_accuracy: 0.6081\n",
            "Epoch 442/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1884 - accuracy: 0.9116 - val_loss: 3.3198 - val_accuracy: 0.6351\n",
            "Epoch 443/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2112 - accuracy: 0.9082 - val_loss: 3.1001 - val_accuracy: 0.6216\n",
            "Epoch 444/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1887 - accuracy: 0.9116 - val_loss: 3.2515 - val_accuracy: 0.5946\n",
            "Epoch 445/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2360 - accuracy: 0.9048 - val_loss: 2.9670 - val_accuracy: 0.6081\n",
            "Epoch 446/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1869 - accuracy: 0.9116 - val_loss: 2.6678 - val_accuracy: 0.6351\n",
            "Epoch 447/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2019 - accuracy: 0.9082 - val_loss: 2.6315 - val_accuracy: 0.6216\n",
            "Epoch 448/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2154 - accuracy: 0.9014 - val_loss: 2.6898 - val_accuracy: 0.6216\n",
            "Epoch 449/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1923 - accuracy: 0.9150 - val_loss: 2.8352 - val_accuracy: 0.5811\n",
            "Epoch 450/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2468 - accuracy: 0.9014 - val_loss: 2.7467 - val_accuracy: 0.5676\n",
            "Epoch 451/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2194 - accuracy: 0.9014 - val_loss: 2.6577 - val_accuracy: 0.5946\n",
            "Epoch 452/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2195 - accuracy: 0.8946 - val_loss: 2.7398 - val_accuracy: 0.5946\n",
            "Epoch 453/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2012 - accuracy: 0.9082 - val_loss: 2.8188 - val_accuracy: 0.5946\n",
            "Epoch 454/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1984 - accuracy: 0.9082 - val_loss: 2.9084 - val_accuracy: 0.5946\n",
            "Epoch 455/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2005 - accuracy: 0.9048 - val_loss: 2.9073 - val_accuracy: 0.5946\n",
            "Epoch 456/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2007 - accuracy: 0.9048 - val_loss: 2.8924 - val_accuracy: 0.5811\n",
            "Epoch 457/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1898 - accuracy: 0.9116 - val_loss: 2.8806 - val_accuracy: 0.5676\n",
            "Epoch 458/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1913 - accuracy: 0.9116 - val_loss: 2.8893 - val_accuracy: 0.5811\n",
            "Epoch 459/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1952 - accuracy: 0.9116 - val_loss: 2.9137 - val_accuracy: 0.5811\n",
            "Epoch 460/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1929 - accuracy: 0.9116 - val_loss: 2.9465 - val_accuracy: 0.5946\n",
            "Epoch 461/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1915 - accuracy: 0.9116 - val_loss: 2.9932 - val_accuracy: 0.5946\n",
            "Epoch 462/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1877 - accuracy: 0.9116 - val_loss: 3.0428 - val_accuracy: 0.5946\n",
            "Epoch 463/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1916 - accuracy: 0.9116 - val_loss: 3.0872 - val_accuracy: 0.5946\n",
            "Epoch 464/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1870 - accuracy: 0.9116 - val_loss: 3.1272 - val_accuracy: 0.5946\n",
            "Epoch 465/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1883 - accuracy: 0.9116 - val_loss: 3.1477 - val_accuracy: 0.5946\n",
            "Epoch 466/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1904 - accuracy: 0.9116 - val_loss: 3.1333 - val_accuracy: 0.5946\n",
            "Epoch 467/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1859 - accuracy: 0.9116 - val_loss: 3.1357 - val_accuracy: 0.6216\n",
            "Epoch 468/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1891 - accuracy: 0.9150 - val_loss: 3.1484 - val_accuracy: 0.5811\n",
            "Epoch 469/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1853 - accuracy: 0.9116 - val_loss: 3.1642 - val_accuracy: 0.5676\n",
            "Epoch 470/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1884 - accuracy: 0.9150 - val_loss: 3.1791 - val_accuracy: 0.5676\n",
            "Epoch 471/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1820 - accuracy: 0.9150 - val_loss: 3.2074 - val_accuracy: 0.5676\n",
            "Epoch 472/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1848 - accuracy: 0.9116 - val_loss: 3.2403 - val_accuracy: 0.5676\n",
            "Epoch 473/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1941 - accuracy: 0.9116 - val_loss: 3.2328 - val_accuracy: 0.5946\n",
            "Epoch 474/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1916 - accuracy: 0.9082 - val_loss: 3.2426 - val_accuracy: 0.6081\n",
            "Epoch 475/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1837 - accuracy: 0.9116 - val_loss: 3.2644 - val_accuracy: 0.5946\n",
            "Epoch 476/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1855 - accuracy: 0.9150 - val_loss: 3.3296 - val_accuracy: 0.5811\n",
            "Epoch 477/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1842 - accuracy: 0.9116 - val_loss: 3.3412 - val_accuracy: 0.5811\n",
            "Epoch 478/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1982 - accuracy: 0.9082 - val_loss: 3.3325 - val_accuracy: 0.5811\n",
            "Epoch 479/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1901 - accuracy: 0.9150 - val_loss: 3.3230 - val_accuracy: 0.5811\n",
            "Epoch 480/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1851 - accuracy: 0.9082 - val_loss: 3.3313 - val_accuracy: 0.6081\n",
            "Epoch 481/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1848 - accuracy: 0.9150 - val_loss: 3.3753 - val_accuracy: 0.6081\n",
            "Epoch 482/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1874 - accuracy: 0.9082 - val_loss: 3.4260 - val_accuracy: 0.6081\n",
            "Epoch 483/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1910 - accuracy: 0.9116 - val_loss: 3.4645 - val_accuracy: 0.6081\n",
            "Epoch 484/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1921 - accuracy: 0.9082 - val_loss: 3.5576 - val_accuracy: 0.5946\n",
            "Epoch 485/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1852 - accuracy: 0.9116 - val_loss: 3.6233 - val_accuracy: 0.5946\n",
            "Epoch 486/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1963 - accuracy: 0.9048 - val_loss: 3.6394 - val_accuracy: 0.6081\n",
            "Epoch 487/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1819 - accuracy: 0.9116 - val_loss: 3.6541 - val_accuracy: 0.6216\n",
            "Epoch 488/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1800 - accuracy: 0.9184 - val_loss: 3.6476 - val_accuracy: 0.6081\n",
            "Epoch 489/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1884 - accuracy: 0.9150 - val_loss: 3.6542 - val_accuracy: 0.6081\n",
            "Epoch 490/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1996 - accuracy: 0.8980 - val_loss: 3.6207 - val_accuracy: 0.6081\n",
            "Epoch 491/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1890 - accuracy: 0.9082 - val_loss: 3.5755 - val_accuracy: 0.5811\n",
            "Epoch 492/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1949 - accuracy: 0.9116 - val_loss: 3.5480 - val_accuracy: 0.5811\n",
            "Epoch 493/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1934 - accuracy: 0.9116 - val_loss: 3.5249 - val_accuracy: 0.5811\n",
            "Epoch 494/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1969 - accuracy: 0.9116 - val_loss: 3.4994 - val_accuracy: 0.5676\n",
            "Epoch 495/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1881 - accuracy: 0.9116 - val_loss: 3.4699 - val_accuracy: 0.5811\n",
            "Epoch 496/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1862 - accuracy: 0.9150 - val_loss: 3.4619 - val_accuracy: 0.5811\n",
            "Epoch 497/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1882 - accuracy: 0.9116 - val_loss: 3.4671 - val_accuracy: 0.5811\n",
            "Epoch 498/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1885 - accuracy: 0.9116 - val_loss: 3.4878 - val_accuracy: 0.5811\n",
            "Epoch 499/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1796 - accuracy: 0.9150 - val_loss: 3.5088 - val_accuracy: 0.5811\n",
            "Epoch 500/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1909 - accuracy: 0.9082 - val_loss: 3.5365 - val_accuracy: 0.5811\n",
            "Epoch 501/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1884 - accuracy: 0.9150 - val_loss: 3.5692 - val_accuracy: 0.5676\n",
            "Epoch 502/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1911 - accuracy: 0.9048 - val_loss: 3.5949 - val_accuracy: 0.5676\n",
            "Epoch 503/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1926 - accuracy: 0.9082 - val_loss: 3.6329 - val_accuracy: 0.5676\n",
            "Epoch 504/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1846 - accuracy: 0.9116 - val_loss: 3.6620 - val_accuracy: 0.5676\n",
            "Epoch 505/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1864 - accuracy: 0.9116 - val_loss: 3.6838 - val_accuracy: 0.5811\n",
            "Epoch 506/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1869 - accuracy: 0.9116 - val_loss: 3.6684 - val_accuracy: 0.5811\n",
            "Epoch 507/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2076 - accuracy: 0.9116 - val_loss: 3.5181 - val_accuracy: 0.5811\n",
            "Epoch 508/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2183 - accuracy: 0.9116 - val_loss: 3.4201 - val_accuracy: 0.5946\n",
            "Epoch 509/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1994 - accuracy: 0.9116 - val_loss: 3.4300 - val_accuracy: 0.5946\n",
            "Epoch 510/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1874 - accuracy: 0.9116 - val_loss: 3.4312 - val_accuracy: 0.5946\n",
            "Epoch 511/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1892 - accuracy: 0.9082 - val_loss: 3.4471 - val_accuracy: 0.6081\n",
            "Epoch 512/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1866 - accuracy: 0.9116 - val_loss: 3.4630 - val_accuracy: 0.5946\n",
            "Epoch 513/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1927 - accuracy: 0.9116 - val_loss: 3.4945 - val_accuracy: 0.5946\n",
            "Epoch 514/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1868 - accuracy: 0.9116 - val_loss: 3.5286 - val_accuracy: 0.5946\n",
            "Epoch 515/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1865 - accuracy: 0.9116 - val_loss: 3.5613 - val_accuracy: 0.5946\n",
            "Epoch 516/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1902 - accuracy: 0.9082 - val_loss: 3.5872 - val_accuracy: 0.5946\n",
            "Epoch 517/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1857 - accuracy: 0.9150 - val_loss: 3.6102 - val_accuracy: 0.5946\n",
            "Epoch 518/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1837 - accuracy: 0.9150 - val_loss: 3.6488 - val_accuracy: 0.5946\n",
            "Epoch 519/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1848 - accuracy: 0.9150 - val_loss: 3.6903 - val_accuracy: 0.5946\n",
            "Epoch 520/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2036 - accuracy: 0.9082 - val_loss: 3.7066 - val_accuracy: 0.5946\n",
            "Epoch 521/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1836 - accuracy: 0.9116 - val_loss: 3.7153 - val_accuracy: 0.5946\n",
            "Epoch 522/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1886 - accuracy: 0.9116 - val_loss: 3.6893 - val_accuracy: 0.5946\n",
            "Epoch 523/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1975 - accuracy: 0.9048 - val_loss: 3.6483 - val_accuracy: 0.6081\n",
            "Epoch 524/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1879 - accuracy: 0.9082 - val_loss: 3.6537 - val_accuracy: 0.5946\n",
            "Epoch 525/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1901 - accuracy: 0.9082 - val_loss: 3.6690 - val_accuracy: 0.5946\n",
            "Epoch 526/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1885 - accuracy: 0.9116 - val_loss: 3.6470 - val_accuracy: 0.5946\n",
            "Epoch 527/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1824 - accuracy: 0.9082 - val_loss: 3.6292 - val_accuracy: 0.5946\n",
            "Epoch 528/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1831 - accuracy: 0.9150 - val_loss: 3.6024 - val_accuracy: 0.6081\n",
            "Epoch 529/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1854 - accuracy: 0.9116 - val_loss: 3.6069 - val_accuracy: 0.6081\n",
            "Epoch 530/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1837 - accuracy: 0.9116 - val_loss: 3.6161 - val_accuracy: 0.6081\n",
            "Epoch 531/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1924 - accuracy: 0.9116 - val_loss: 3.6301 - val_accuracy: 0.5946\n",
            "Epoch 532/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1815 - accuracy: 0.9116 - val_loss: 3.6583 - val_accuracy: 0.5946\n",
            "Epoch 533/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1918 - accuracy: 0.9116 - val_loss: 3.6903 - val_accuracy: 0.5946\n",
            "Epoch 534/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1804 - accuracy: 0.9184 - val_loss: 3.7417 - val_accuracy: 0.5946\n",
            "Epoch 535/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1830 - accuracy: 0.9116 - val_loss: 3.7561 - val_accuracy: 0.5946\n",
            "Epoch 536/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1947 - accuracy: 0.9116 - val_loss: 3.7732 - val_accuracy: 0.5946\n",
            "Epoch 537/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1990 - accuracy: 0.9116 - val_loss: 3.8034 - val_accuracy: 0.5946\n",
            "Epoch 538/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1858 - accuracy: 0.9116 - val_loss: 3.8292 - val_accuracy: 0.5946\n",
            "Epoch 539/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1844 - accuracy: 0.9116 - val_loss: 3.8401 - val_accuracy: 0.5946\n",
            "Epoch 540/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1915 - accuracy: 0.9082 - val_loss: 3.8371 - val_accuracy: 0.5811\n",
            "Epoch 541/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1851 - accuracy: 0.9116 - val_loss: 3.8365 - val_accuracy: 0.5946\n",
            "Epoch 542/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1948 - accuracy: 0.9082 - val_loss: 3.8420 - val_accuracy: 0.5946\n",
            "Epoch 543/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1852 - accuracy: 0.9116 - val_loss: 3.8464 - val_accuracy: 0.5946\n",
            "Epoch 544/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1834 - accuracy: 0.9150 - val_loss: 3.8591 - val_accuracy: 0.5946\n",
            "Epoch 545/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1923 - accuracy: 0.9048 - val_loss: 3.8752 - val_accuracy: 0.6081\n",
            "Epoch 546/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1816 - accuracy: 0.9184 - val_loss: 3.8987 - val_accuracy: 0.6081\n",
            "Epoch 547/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1805 - accuracy: 0.9150 - val_loss: 3.9252 - val_accuracy: 0.5946\n",
            "Epoch 548/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1911 - accuracy: 0.9082 - val_loss: 3.9582 - val_accuracy: 0.5946\n",
            "Epoch 549/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1874 - accuracy: 0.9082 - val_loss: 3.9958 - val_accuracy: 0.5811\n",
            "Epoch 550/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1835 - accuracy: 0.9116 - val_loss: 4.0234 - val_accuracy: 0.5811\n",
            "Epoch 551/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1878 - accuracy: 0.9082 - val_loss: 4.0483 - val_accuracy: 0.5811\n",
            "Epoch 552/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1867 - accuracy: 0.9116 - val_loss: 4.0613 - val_accuracy: 0.5811\n",
            "Epoch 553/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1835 - accuracy: 0.9116 - val_loss: 4.0824 - val_accuracy: 0.5811\n",
            "Epoch 554/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1845 - accuracy: 0.9150 - val_loss: 4.0879 - val_accuracy: 0.5811\n",
            "Epoch 555/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1871 - accuracy: 0.9082 - val_loss: 4.1064 - val_accuracy: 0.5946\n",
            "Epoch 556/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1824 - accuracy: 0.9116 - val_loss: 4.1241 - val_accuracy: 0.5946\n",
            "Epoch 557/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1887 - accuracy: 0.9150 - val_loss: 4.1344 - val_accuracy: 0.6081\n",
            "Epoch 558/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1856 - accuracy: 0.9116 - val_loss: 4.1080 - val_accuracy: 0.5946\n",
            "Epoch 559/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1825 - accuracy: 0.9150 - val_loss: 4.1074 - val_accuracy: 0.6081\n",
            "Epoch 560/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1814 - accuracy: 0.9150 - val_loss: 4.1269 - val_accuracy: 0.6081\n",
            "Epoch 561/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1846 - accuracy: 0.9116 - val_loss: 4.1391 - val_accuracy: 0.6081\n",
            "Epoch 562/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1957 - accuracy: 0.9048 - val_loss: 4.1534 - val_accuracy: 0.6081\n",
            "Epoch 563/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1930 - accuracy: 0.9082 - val_loss: 3.9382 - val_accuracy: 0.6216\n",
            "Epoch 564/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2023 - accuracy: 0.9082 - val_loss: 3.7307 - val_accuracy: 0.6081\n",
            "Epoch 565/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1799 - accuracy: 0.9116 - val_loss: 3.6064 - val_accuracy: 0.6216\n",
            "Epoch 566/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1909 - accuracy: 0.9116 - val_loss: 3.5962 - val_accuracy: 0.6216\n",
            "Epoch 567/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1899 - accuracy: 0.9116 - val_loss: 3.6832 - val_accuracy: 0.6216\n",
            "Epoch 568/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1885 - accuracy: 0.9116 - val_loss: 3.7208 - val_accuracy: 0.6216\n",
            "Epoch 569/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1928 - accuracy: 0.9116 - val_loss: 3.8177 - val_accuracy: 0.5946\n",
            "Epoch 570/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.1875 - accuracy: 0.9116 - val_loss: 3.9246 - val_accuracy: 0.5946\n",
            "Epoch 571/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1836 - accuracy: 0.9116 - val_loss: 4.0244 - val_accuracy: 0.5946\n",
            "Epoch 572/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1985 - accuracy: 0.9048 - val_loss: 4.0204 - val_accuracy: 0.5811\n",
            "Epoch 573/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2006 - accuracy: 0.9082 - val_loss: 3.8401 - val_accuracy: 0.5946\n",
            "Epoch 574/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1866 - accuracy: 0.9116 - val_loss: 3.6497 - val_accuracy: 0.5946\n",
            "Epoch 575/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1778 - accuracy: 0.9150 - val_loss: 3.5179 - val_accuracy: 0.6081\n",
            "Epoch 576/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1953 - accuracy: 0.9082 - val_loss: 3.4566 - val_accuracy: 0.6216\n",
            "Epoch 577/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1821 - accuracy: 0.9116 - val_loss: 3.4224 - val_accuracy: 0.6216\n",
            "Epoch 578/750\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1907 - accuracy: 0.9116 - val_loss: 3.4131 - val_accuracy: 0.6081\n",
            "Epoch 579/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.1890 - accuracy: 0.9082 - val_loss: 3.4069 - val_accuracy: 0.6081\n",
            "Epoch 580/750\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.1858 - accuracy: 0.9150 - val_loss: 3.3896 - val_accuracy: 0.6081\n",
            "Epoch 581/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1908 - accuracy: 0.9116 - val_loss: 3.3877 - val_accuracy: 0.6081\n",
            "Epoch 582/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1852 - accuracy: 0.9116 - val_loss: 3.4041 - val_accuracy: 0.6081\n",
            "Epoch 583/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1949 - accuracy: 0.9082 - val_loss: 3.5048 - val_accuracy: 0.5946\n",
            "Epoch 584/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1864 - accuracy: 0.9150 - val_loss: 3.6025 - val_accuracy: 0.6081\n",
            "Epoch 585/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1918 - accuracy: 0.9116 - val_loss: 3.6481 - val_accuracy: 0.6081\n",
            "Epoch 586/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1872 - accuracy: 0.9116 - val_loss: 3.6576 - val_accuracy: 0.5946\n",
            "Epoch 587/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1824 - accuracy: 0.9082 - val_loss: 3.6771 - val_accuracy: 0.5946\n",
            "Epoch 588/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1877 - accuracy: 0.9116 - val_loss: 3.6999 - val_accuracy: 0.6081\n",
            "Epoch 589/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1847 - accuracy: 0.9150 - val_loss: 3.7272 - val_accuracy: 0.6216\n",
            "Epoch 590/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1888 - accuracy: 0.9116 - val_loss: 3.7471 - val_accuracy: 0.6216\n",
            "Epoch 591/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1832 - accuracy: 0.9150 - val_loss: 3.7587 - val_accuracy: 0.6216\n",
            "Epoch 592/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1808 - accuracy: 0.9150 - val_loss: 3.7699 - val_accuracy: 0.6081\n",
            "Epoch 593/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1877 - accuracy: 0.9150 - val_loss: 3.7828 - val_accuracy: 0.5946\n",
            "Epoch 594/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1914 - accuracy: 0.9116 - val_loss: 3.8106 - val_accuracy: 0.6081\n",
            "Epoch 595/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1940 - accuracy: 0.9116 - val_loss: 3.8307 - val_accuracy: 0.6081\n",
            "Epoch 596/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1902 - accuracy: 0.9116 - val_loss: 3.8349 - val_accuracy: 0.5946\n",
            "Epoch 597/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1876 - accuracy: 0.9150 - val_loss: 3.8357 - val_accuracy: 0.5946\n",
            "Epoch 598/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1880 - accuracy: 0.9048 - val_loss: 3.8415 - val_accuracy: 0.6081\n",
            "Epoch 599/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1815 - accuracy: 0.9150 - val_loss: 3.8626 - val_accuracy: 0.5946\n",
            "Epoch 600/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1834 - accuracy: 0.9116 - val_loss: 3.8638 - val_accuracy: 0.6081\n",
            "Epoch 601/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1777 - accuracy: 0.9150 - val_loss: 3.8815 - val_accuracy: 0.6216\n",
            "Epoch 602/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1814 - accuracy: 0.9150 - val_loss: 3.9223 - val_accuracy: 0.6081\n",
            "Epoch 603/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1886 - accuracy: 0.9116 - val_loss: 3.9431 - val_accuracy: 0.6081\n",
            "Epoch 604/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1788 - accuracy: 0.9150 - val_loss: 3.9440 - val_accuracy: 0.5946\n",
            "Epoch 605/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1854 - accuracy: 0.9116 - val_loss: 3.9330 - val_accuracy: 0.5946\n",
            "Epoch 606/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1815 - accuracy: 0.9150 - val_loss: 3.9708 - val_accuracy: 0.6081\n",
            "Epoch 607/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1850 - accuracy: 0.9116 - val_loss: 3.9906 - val_accuracy: 0.6081\n",
            "Epoch 608/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1783 - accuracy: 0.9150 - val_loss: 3.9948 - val_accuracy: 0.6081\n",
            "Epoch 609/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1842 - accuracy: 0.9150 - val_loss: 4.0054 - val_accuracy: 0.6081\n",
            "Epoch 610/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1835 - accuracy: 0.9150 - val_loss: 4.0391 - val_accuracy: 0.6216\n",
            "Epoch 611/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1862 - accuracy: 0.9150 - val_loss: 4.0444 - val_accuracy: 0.5946\n",
            "Epoch 612/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1755 - accuracy: 0.9150 - val_loss: 4.0530 - val_accuracy: 0.6081\n",
            "Epoch 613/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1879 - accuracy: 0.9116 - val_loss: 4.0952 - val_accuracy: 0.6081\n",
            "Epoch 614/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1870 - accuracy: 0.9116 - val_loss: 4.0876 - val_accuracy: 0.5946\n",
            "Epoch 615/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1851 - accuracy: 0.9150 - val_loss: 4.0847 - val_accuracy: 0.6081\n",
            "Epoch 616/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1747 - accuracy: 0.9184 - val_loss: 4.1666 - val_accuracy: 0.6081\n",
            "Epoch 617/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1882 - accuracy: 0.9116 - val_loss: 4.1658 - val_accuracy: 0.5946\n",
            "Epoch 618/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1863 - accuracy: 0.9116 - val_loss: 4.1371 - val_accuracy: 0.5811\n",
            "Epoch 619/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1807 - accuracy: 0.9150 - val_loss: 4.1775 - val_accuracy: 0.6081\n",
            "Epoch 620/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1860 - accuracy: 0.9082 - val_loss: 4.1091 - val_accuracy: 0.5946\n",
            "Epoch 621/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1831 - accuracy: 0.9116 - val_loss: 4.2188 - val_accuracy: 0.5946\n",
            "Epoch 622/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2033 - accuracy: 0.9116 - val_loss: 4.0317 - val_accuracy: 0.5946\n",
            "Epoch 623/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2186 - accuracy: 0.9048 - val_loss: 4.2940 - val_accuracy: 0.5811\n",
            "Epoch 624/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1976 - accuracy: 0.9048 - val_loss: 3.8082 - val_accuracy: 0.6081\n",
            "Epoch 625/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2382 - accuracy: 0.9014 - val_loss: 2.8707 - val_accuracy: 0.6216\n",
            "Epoch 626/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3062 - accuracy: 0.8912 - val_loss: 3.1675 - val_accuracy: 0.6351\n",
            "Epoch 627/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4445 - accuracy: 0.8605 - val_loss: 2.5490 - val_accuracy: 0.6216\n",
            "Epoch 628/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2693 - accuracy: 0.8946 - val_loss: 2.3353 - val_accuracy: 0.6216\n",
            "Epoch 629/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2242 - accuracy: 0.9048 - val_loss: 2.4416 - val_accuracy: 0.6216\n",
            "Epoch 630/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2408 - accuracy: 0.9014 - val_loss: 2.3499 - val_accuracy: 0.6351\n",
            "Epoch 631/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1961 - accuracy: 0.9082 - val_loss: 2.3536 - val_accuracy: 0.6081\n",
            "Epoch 632/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2060 - accuracy: 0.9082 - val_loss: 2.4835 - val_accuracy: 0.6081\n",
            "Epoch 633/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2032 - accuracy: 0.9014 - val_loss: 2.5832 - val_accuracy: 0.5946\n",
            "Epoch 634/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2238 - accuracy: 0.9014 - val_loss: 2.6029 - val_accuracy: 0.5946\n",
            "Epoch 635/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2041 - accuracy: 0.9048 - val_loss: 2.6428 - val_accuracy: 0.6081\n",
            "Epoch 636/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2284 - accuracy: 0.9048 - val_loss: 2.6609 - val_accuracy: 0.6081\n",
            "Epoch 637/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1953 - accuracy: 0.9116 - val_loss: 2.7290 - val_accuracy: 0.6081\n",
            "Epoch 638/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1851 - accuracy: 0.9116 - val_loss: 2.8248 - val_accuracy: 0.6216\n",
            "Epoch 639/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1920 - accuracy: 0.9082 - val_loss: 2.9024 - val_accuracy: 0.5946\n",
            "Epoch 640/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1902 - accuracy: 0.9082 - val_loss: 2.9415 - val_accuracy: 0.6081\n",
            "Epoch 641/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1907 - accuracy: 0.9116 - val_loss: 2.9823 - val_accuracy: 0.5811\n",
            "Epoch 642/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1917 - accuracy: 0.9082 - val_loss: 3.0196 - val_accuracy: 0.5811\n",
            "Epoch 643/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1949 - accuracy: 0.9116 - val_loss: 3.0167 - val_accuracy: 0.5811\n",
            "Epoch 644/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1860 - accuracy: 0.9116 - val_loss: 3.0366 - val_accuracy: 0.5946\n",
            "Epoch 645/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1905 - accuracy: 0.9116 - val_loss: 3.0907 - val_accuracy: 0.6081\n",
            "Epoch 646/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1859 - accuracy: 0.9116 - val_loss: 3.1497 - val_accuracy: 0.6081\n",
            "Epoch 647/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1937 - accuracy: 0.9116 - val_loss: 3.1977 - val_accuracy: 0.6081\n",
            "Epoch 648/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1821 - accuracy: 0.9116 - val_loss: 3.2036 - val_accuracy: 0.6216\n",
            "Epoch 649/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1874 - accuracy: 0.9116 - val_loss: 3.2020 - val_accuracy: 0.6216\n",
            "Epoch 650/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1904 - accuracy: 0.9082 - val_loss: 3.2069 - val_accuracy: 0.6216\n",
            "Epoch 651/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1896 - accuracy: 0.9116 - val_loss: 3.2029 - val_accuracy: 0.6081\n",
            "Epoch 652/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1841 - accuracy: 0.9150 - val_loss: 3.1837 - val_accuracy: 0.5946\n",
            "Epoch 653/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1869 - accuracy: 0.9082 - val_loss: 3.1639 - val_accuracy: 0.6081\n",
            "Epoch 654/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1816 - accuracy: 0.9116 - val_loss: 3.1518 - val_accuracy: 0.6216\n",
            "Epoch 655/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1868 - accuracy: 0.9116 - val_loss: 3.1511 - val_accuracy: 0.6081\n",
            "Epoch 656/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1862 - accuracy: 0.9082 - val_loss: 3.1631 - val_accuracy: 0.6216\n",
            "Epoch 657/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1864 - accuracy: 0.9116 - val_loss: 3.1794 - val_accuracy: 0.6216\n",
            "Epoch 658/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1887 - accuracy: 0.9116 - val_loss: 3.2004 - val_accuracy: 0.6216\n",
            "Epoch 659/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1870 - accuracy: 0.9116 - val_loss: 3.2436 - val_accuracy: 0.6081\n",
            "Epoch 660/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1761 - accuracy: 0.9184 - val_loss: 3.2906 - val_accuracy: 0.6216\n",
            "Epoch 661/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1878 - accuracy: 0.9082 - val_loss: 3.3228 - val_accuracy: 0.6216\n",
            "Epoch 662/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1877 - accuracy: 0.9116 - val_loss: 3.3580 - val_accuracy: 0.6216\n",
            "Epoch 663/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1916 - accuracy: 0.9116 - val_loss: 3.3964 - val_accuracy: 0.6216\n",
            "Epoch 664/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1852 - accuracy: 0.9150 - val_loss: 3.4454 - val_accuracy: 0.6351\n",
            "Epoch 665/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1810 - accuracy: 0.9116 - val_loss: 3.4882 - val_accuracy: 0.6216\n",
            "Epoch 666/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1886 - accuracy: 0.9150 - val_loss: 3.5217 - val_accuracy: 0.6081\n",
            "Epoch 667/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1853 - accuracy: 0.9116 - val_loss: 3.5119 - val_accuracy: 0.6081\n",
            "Epoch 668/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1821 - accuracy: 0.9150 - val_loss: 3.5105 - val_accuracy: 0.5946\n",
            "Epoch 669/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1876 - accuracy: 0.9116 - val_loss: 3.5061 - val_accuracy: 0.5946\n",
            "Epoch 670/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1862 - accuracy: 0.9150 - val_loss: 3.5164 - val_accuracy: 0.5946\n",
            "Epoch 671/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1925 - accuracy: 0.9116 - val_loss: 3.5162 - val_accuracy: 0.5946\n",
            "Epoch 672/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1812 - accuracy: 0.9116 - val_loss: 3.4936 - val_accuracy: 0.5946\n",
            "Epoch 673/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1903 - accuracy: 0.9014 - val_loss: 3.4348 - val_accuracy: 0.5811\n",
            "Epoch 674/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1821 - accuracy: 0.9116 - val_loss: 3.4376 - val_accuracy: 0.5946\n",
            "Epoch 675/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1862 - accuracy: 0.9116 - val_loss: 3.4267 - val_accuracy: 0.5946\n",
            "Epoch 676/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1926 - accuracy: 0.9116 - val_loss: 3.4164 - val_accuracy: 0.5946\n",
            "Epoch 677/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1946 - accuracy: 0.9116 - val_loss: 3.4023 - val_accuracy: 0.5946\n",
            "Epoch 678/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1920 - accuracy: 0.9116 - val_loss: 3.3761 - val_accuracy: 0.5811\n",
            "Epoch 679/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1885 - accuracy: 0.9116 - val_loss: 3.3508 - val_accuracy: 0.6081\n",
            "Epoch 680/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1818 - accuracy: 0.9116 - val_loss: 3.3552 - val_accuracy: 0.5946\n",
            "Epoch 681/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1816 - accuracy: 0.9150 - val_loss: 3.3665 - val_accuracy: 0.6081\n",
            "Epoch 682/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1906 - accuracy: 0.9048 - val_loss: 3.3801 - val_accuracy: 0.6081\n",
            "Epoch 683/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1880 - accuracy: 0.9082 - val_loss: 3.3951 - val_accuracy: 0.6081\n",
            "Epoch 684/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1903 - accuracy: 0.9082 - val_loss: 3.4188 - val_accuracy: 0.5946\n",
            "Epoch 685/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1893 - accuracy: 0.9116 - val_loss: 3.4793 - val_accuracy: 0.5946\n",
            "Epoch 686/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1919 - accuracy: 0.9116 - val_loss: 3.5402 - val_accuracy: 0.5811\n",
            "Epoch 687/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2035 - accuracy: 0.9116 - val_loss: 3.5536 - val_accuracy: 0.5811\n",
            "Epoch 688/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1880 - accuracy: 0.9116 - val_loss: 3.5379 - val_accuracy: 0.5811\n",
            "Epoch 689/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1857 - accuracy: 0.9116 - val_loss: 3.5332 - val_accuracy: 0.5811\n",
            "Epoch 690/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1854 - accuracy: 0.9116 - val_loss: 3.5357 - val_accuracy: 0.5811\n",
            "Epoch 691/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1843 - accuracy: 0.9082 - val_loss: 3.5468 - val_accuracy: 0.5946\n",
            "Epoch 692/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1837 - accuracy: 0.9116 - val_loss: 3.5643 - val_accuracy: 0.5946\n",
            "Epoch 693/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1808 - accuracy: 0.9184 - val_loss: 3.5817 - val_accuracy: 0.5946\n",
            "Epoch 694/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1873 - accuracy: 0.9116 - val_loss: 3.6074 - val_accuracy: 0.5946\n",
            "Epoch 695/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1890 - accuracy: 0.9116 - val_loss: 3.6552 - val_accuracy: 0.5811\n",
            "Epoch 696/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1823 - accuracy: 0.9150 - val_loss: 3.6915 - val_accuracy: 0.5811\n",
            "Epoch 697/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1846 - accuracy: 0.9150 - val_loss: 3.7580 - val_accuracy: 0.5811\n",
            "Epoch 698/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1898 - accuracy: 0.9150 - val_loss: 3.8536 - val_accuracy: 0.5676\n",
            "Epoch 699/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1878 - accuracy: 0.9116 - val_loss: 3.8619 - val_accuracy: 0.5811\n",
            "Epoch 700/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1864 - accuracy: 0.9116 - val_loss: 3.8528 - val_accuracy: 0.5811\n",
            "Epoch 701/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1882 - accuracy: 0.9150 - val_loss: 3.8275 - val_accuracy: 0.5811\n",
            "Epoch 702/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1918 - accuracy: 0.9082 - val_loss: 3.9740 - val_accuracy: 0.5676\n",
            "Epoch 703/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1843 - accuracy: 0.9116 - val_loss: 3.8911 - val_accuracy: 0.5811\n",
            "Epoch 704/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2088 - accuracy: 0.9082 - val_loss: 3.9257 - val_accuracy: 0.5541\n",
            "Epoch 705/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1909 - accuracy: 0.9116 - val_loss: 4.1018 - val_accuracy: 0.6216\n",
            "Epoch 706/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2487 - accuracy: 0.9048 - val_loss: 3.9186 - val_accuracy: 0.5541\n",
            "Epoch 707/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2280 - accuracy: 0.9014 - val_loss: 3.7234 - val_accuracy: 0.5946\n",
            "Epoch 708/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3020 - accuracy: 0.8810 - val_loss: 3.1342 - val_accuracy: 0.6216\n",
            "Epoch 709/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3138 - accuracy: 0.8912 - val_loss: 2.8781 - val_accuracy: 0.5811\n",
            "Epoch 710/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3150 - accuracy: 0.8878 - val_loss: 2.9150 - val_accuracy: 0.6216\n",
            "Epoch 711/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4978 - accuracy: 0.8469 - val_loss: 2.7645 - val_accuracy: 0.5270\n",
            "Epoch 712/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3305 - accuracy: 0.8469 - val_loss: 2.1050 - val_accuracy: 0.5811\n",
            "Epoch 713/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3390 - accuracy: 0.8776 - val_loss: 1.8211 - val_accuracy: 0.6216\n",
            "Epoch 714/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2959 - accuracy: 0.8776 - val_loss: 1.5737 - val_accuracy: 0.6622\n",
            "Epoch 715/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2589 - accuracy: 0.8912 - val_loss: 1.5660 - val_accuracy: 0.6216\n",
            "Epoch 716/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2260 - accuracy: 0.9014 - val_loss: 1.7306 - val_accuracy: 0.6216\n",
            "Epoch 717/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2324 - accuracy: 0.8980 - val_loss: 1.9037 - val_accuracy: 0.6216\n",
            "Epoch 718/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2252 - accuracy: 0.8946 - val_loss: 1.9869 - val_accuracy: 0.5811\n",
            "Epoch 719/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2401 - accuracy: 0.8912 - val_loss: 2.1861 - val_accuracy: 0.5676\n",
            "Epoch 720/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2113 - accuracy: 0.9048 - val_loss: 2.2193 - val_accuracy: 0.5676\n",
            "Epoch 721/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2392 - accuracy: 0.8980 - val_loss: 2.4900 - val_accuracy: 0.5676\n",
            "Epoch 722/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2427 - accuracy: 0.8912 - val_loss: 2.8467 - val_accuracy: 0.5946\n",
            "Epoch 723/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2429 - accuracy: 0.8946 - val_loss: 2.9117 - val_accuracy: 0.6216\n",
            "Epoch 724/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2452 - accuracy: 0.9014 - val_loss: 2.6329 - val_accuracy: 0.5811\n",
            "Epoch 725/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2231 - accuracy: 0.8980 - val_loss: 2.4486 - val_accuracy: 0.5811\n",
            "Epoch 726/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1981 - accuracy: 0.9116 - val_loss: 2.3733 - val_accuracy: 0.5811\n",
            "Epoch 727/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2066 - accuracy: 0.9116 - val_loss: 2.4916 - val_accuracy: 0.5946\n",
            "Epoch 728/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2111 - accuracy: 0.9048 - val_loss: 2.6720 - val_accuracy: 0.5811\n",
            "Epoch 729/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1960 - accuracy: 0.9116 - val_loss: 2.7067 - val_accuracy: 0.5811\n",
            "Epoch 730/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1914 - accuracy: 0.9082 - val_loss: 2.7130 - val_accuracy: 0.5811\n",
            "Epoch 731/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1886 - accuracy: 0.9150 - val_loss: 2.6627 - val_accuracy: 0.5946\n",
            "Epoch 732/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2044 - accuracy: 0.8946 - val_loss: 2.6411 - val_accuracy: 0.5946\n",
            "Epoch 733/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1922 - accuracy: 0.9150 - val_loss: 2.5871 - val_accuracy: 0.6081\n",
            "Epoch 734/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1935 - accuracy: 0.9150 - val_loss: 2.6040 - val_accuracy: 0.6081\n",
            "Epoch 735/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1908 - accuracy: 0.9116 - val_loss: 2.5842 - val_accuracy: 0.6216\n",
            "Epoch 736/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1921 - accuracy: 0.9082 - val_loss: 2.5980 - val_accuracy: 0.6081\n",
            "Epoch 737/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1884 - accuracy: 0.9082 - val_loss: 2.6318 - val_accuracy: 0.5946\n",
            "Epoch 738/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1939 - accuracy: 0.9082 - val_loss: 2.6938 - val_accuracy: 0.5946\n",
            "Epoch 739/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1857 - accuracy: 0.9116 - val_loss: 2.7519 - val_accuracy: 0.5811\n",
            "Epoch 740/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1830 - accuracy: 0.9150 - val_loss: 2.7937 - val_accuracy: 0.5811\n",
            "Epoch 741/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1892 - accuracy: 0.9150 - val_loss: 2.8427 - val_accuracy: 0.5811\n",
            "Epoch 742/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1905 - accuracy: 0.9116 - val_loss: 2.8886 - val_accuracy: 0.5811\n",
            "Epoch 743/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1923 - accuracy: 0.9116 - val_loss: 2.8790 - val_accuracy: 0.5676\n",
            "Epoch 744/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1832 - accuracy: 0.9150 - val_loss: 2.7971 - val_accuracy: 0.5676\n",
            "Epoch 745/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1831 - accuracy: 0.9116 - val_loss: 2.7158 - val_accuracy: 0.5811\n",
            "Epoch 746/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1789 - accuracy: 0.9184 - val_loss: 2.7046 - val_accuracy: 0.5811\n",
            "Epoch 747/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1902 - accuracy: 0.9116 - val_loss: 2.6830 - val_accuracy: 0.5946\n",
            "Epoch 748/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1820 - accuracy: 0.9082 - val_loss: 2.6552 - val_accuracy: 0.5946\n",
            "Epoch 749/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1897 - accuracy: 0.9082 - val_loss: 2.6427 - val_accuracy: 0.5946\n",
            "Epoch 750/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1928 - accuracy: 0.9082 - val_loss: 2.6548 - val_accuracy: 0.6081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation with the RNN\n",
        "y_valid_CNN_RNN = CNN_RNN_model.predict(feature_valid)\n",
        "valid_y_CNN_RNN = y_valid_CNN_RNN.copy()\n",
        "for i in range(len(y_valid_CNN_RNN)):\n",
        "    j = np.where(y_valid_CNN_RNN[i] == np.amax(y_valid_CNN_RNN[i]))\n",
        "    valid_y_CNN_RNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN_RNN[i][j] = 1\n",
        "\n",
        "# print acc and report\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN_RNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN_RNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN_RNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv77kyW5kKti",
        "outputId": "5882d104-6dbf-42bb-b141-02b64fd69494"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 7ms/step\n",
            "0.6081081081081081\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.53      0.67        15\n",
            "           1       0.57      0.81      0.67        36\n",
            "           2       0.57      0.35      0.43        23\n",
            "\n",
            "   micro avg       0.61      0.61      0.61        74\n",
            "   macro avg       0.68      0.56      0.59        74\n",
            "weighted avg       0.63      0.61      0.59        74\n",
            " samples avg       0.61      0.61      0.61        74\n",
            "\n",
            "auc score:  0.6621952326764093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_CNN_RNN = CNN_RNN_model.predict(feature_test)\n",
        "# convert the test vector\n",
        "test_y_CNN_RNN = y_test_CNN_RNN.copy()\n",
        "for i in range(len(y_test_CNN_RNN)):\n",
        "    j = np.where(y_test_CNN_RNN[i] == np.amax(y_test_CNN_RNN[i]))\n",
        "    test_y_CNN_RNN[i] = [0, 0, 0]\n",
        "    test_y_CNN_RNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_test_y,test_y_CNN_RNN))\n",
        "print(classification_report(label_test_y,test_y_CNN_RNN))\n",
        "print(\"auc score: \",roc_auc_score(label_test_y,test_y_CNN_RNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKxdxkT1kMhz",
        "outputId": "0fbd85da-4c8a-4106-8511-bf561bb2ae9b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 7ms/step\n",
            "0.5806451612903226\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.35      0.46        17\n",
            "           1       0.57      0.81      0.67        43\n",
            "           2       0.57      0.39      0.46        33\n",
            "\n",
            "   micro avg       0.58      0.58      0.58        93\n",
            "   macro avg       0.60      0.52      0.53        93\n",
            "weighted avg       0.59      0.58      0.56        93\n",
            " samples avg       0.58      0.58      0.58        93\n",
            "\n",
            "auc score:  0.6391156179841472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zFizuxTssBik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 预训练"
      ],
      "metadata": {
        "id": "0VPJnCA57ONW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# 加载预训练的 BERT 模型和分词器\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "# 创建一个字典来存储词及其对应的向量表示\n",
        "BERT = {}\n",
        "\n",
        "# 将 train_text_words 拆分成批次\n",
        "batch_size = 512\n",
        "num_batches = len(train_text_words) // batch_size + 1\n",
        "\n",
        "for i in range(num_batches):\n",
        "    # 获取当前批次的词\n",
        "    start_index = i * batch_size\n",
        "    end_index = min((i + 1) * batch_size, len(train_text_words))\n",
        "    batch_words = train_text_words[start_index:end_index]\n",
        "\n",
        "    # 将词转换为字符串列表\n",
        "    batch_sentences = [' '.join(batch_words)]\n",
        "\n",
        "    # 将字符串列表编码为 BERT 的输入格式\n",
        "    encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    # 使用 BERT 模型获取词的向量表示\n",
        "    with torch.no_grad():\n",
        "        model_outputs = bert_model(**encoded_inputs)\n",
        "        word_embeddings = model_outputs.last_hidden_state\n",
        "\n",
        "    # 获取每个词的向量表示\n",
        "    for sentence in batch_sentences:\n",
        "        # 将句子编码为 BERT 的输入格式\n",
        "        encoded_inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
        "        # 获取句子中的每个词\n",
        "        words = tokenizer.convert_ids_to_tokens(encoded_inputs['input_ids'][0])\n",
        "\n",
        "        for j, word in enumerate(words):\n",
        "            # 如果词是子词(以 ## 开头),则跳过\n",
        "            if word.startswith('##'):\n",
        "                continue\n",
        "\n",
        "            # 如果词还没有在字典中,则将其添加到字典中\n",
        "            if word not in BERT:\n",
        "                # 获取词的向量表示\n",
        "                word_vector = word_embeddings[0][j].cpu().numpy()\n",
        "                # 将词及其对应的向量表示添加到字典中\n",
        "                BERT[word] = word_vector\n",
        "\n",
        "# 打印词向量字典的大小\n",
        "print(\"词向量字典的大小:\", len(BERT))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDu50tTeay-q",
        "outputId": "73d3ffb5-9dbd-4e63-9ead-7516638b8a4e"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "词向量字典的大小: 4336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key_words = ['asset', 'diversification', 'risk', 'allocation', 'investing', 'income', 'passive', 'active', 'fundamental', 'technical', 'equities', 'bonds', 'funds', 'ETFs', 'tolerance', 'portfolio', 'returns']"
      ],
      "metadata": {
        "id": "3iDvmG0TDAvA"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_n_closer(word, n, word2vec):\n",
        "    vect = word2vec[word]\n",
        "    dist_dict = {k: cosine(v, vect) for k, v in word2vec.items()}\n",
        "    closer_words = []\n",
        "    for _ in range(n):\n",
        "        min_key = min(dist_dict.keys(), key=lambda k: dist_dict[k])\n",
        "        closer_words.append(min_key)\n",
        "        del dist_dict[min_key]\n",
        "    return closer_words\n",
        "\n",
        "##knowledge base\n",
        "def create_knowledge_base(num_neighbors, word2vec, key_words):\n",
        "    knowledge_base = set()\n",
        "    out = display(progress(0, len(key_words)-1), display_id=True)\n",
        "    for ii, key_word in enumerate(key_words) :\n",
        "        knowledge_base.add(key_word)\n",
        "        neighbors = []\n",
        "        try :\n",
        "            neighbors = get_n_closer(key_word, num_neighbors, word2vec)\n",
        "        except :\n",
        "            print(key_word + ' not in BERT')\n",
        "\n",
        "        knowledge_base.update(neighbors)\n",
        "\n",
        "        out.update(progress(ii, len(key_words)-1))\n",
        "    return knowledge_base\n",
        "\n",
        "knowledge_base = create_knowledge_base(10, BERT, key_words)\n",
        "print(knowledge_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "KONLKfO9hqWh",
        "outputId": "d2db8a79-52d0-451c-adeb-0f71bd776016"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='16'\n",
              "            max='16',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            16\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diversification not in BERT\n",
            "equities not in BERT\n",
            "ETFs not in BERT\n",
            "{'concentrate', 'appropriate', 'percentage', 'engaging', 'maintenance', 'returns', 'certificates', 'offset', 'requirements', 'investor', 'reaction', 'directional', 'marketing', 'invest', 'expense', 'ETFs', 'assist', 'trusts', 'tolerance', 'interest', 'neutral', 'detailed', 'loans', 'share', 'practical', 'equities', 'transactions', 'bond', 'statutory', 'quantitative', 'savings', 'return', 'portfolio', 'wealth', 'investors', 'investments', 'heading', 'speculative', 'income', 'passive', 'legal', 'technical', 'diversification', 'risks', 'aggressive', 'asset', 'funding', 'accounts', 'yields', 'account', 'accepts', 'contractual', 'premium', 'prediction', 'complex', 'renewal', 'transition', 'fund', 'value', 'shares', 'capital', 'funds', 'investment', 'withdrawing', 'bills', 'dependency', 'exclusive', 'earnings', 'annual', 'invested', 'concentration', 'risk', 'guarantees', 'open', 'researching', 'calculations', 'norm', 'investing', 'allocation', 'harder', 'bonds', 'debts', 'negotiation', 'none', 'securities', 'proficiency', 'organized', 'encounters', 'authorized', 'current', 'incurred', 'sensitivity', 'weakness', 'extension', 'explaining', 'reports', 'free', 'tax', 'skill', 'estate', 'rigorous', 'curve', 'fundamental', 'revenue', 'appreciation', 'updates', 'basic', 'policies', 'active', 'combine', 'focus'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(knowledge_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADobXfjxYYow",
        "outputId": "ef3cb44f-0286-4bc9-bd29-97a324139983"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes a summary, the knowledge base and some hyper parameters and returns the \"num_sent\" sentences\n",
        "# of the summary that are closer to the the knowledge base in term of spacial distances.\n",
        "def extract_sentence_distance(summary, knowledge, n_closer, n_reject, num_sent):\n",
        "    # Split the summary into sentences.\n",
        "    sentences = sent_tokenize(summary)\n",
        "    sentence_scores = []\n",
        "    # Loop over the sentences.\n",
        "    for j, sentence in enumerate(sentences):\n",
        "        # we tokenize and clean the sentence\n",
        "        tokens = tokenizer(sentence)\n",
        "\n",
        "        sentence_barycentre = np.zeros(embedding_size)\n",
        "        effective_len = 0\n",
        "        # Compute the barycentre of the sentence\n",
        "        for token in tokens :\n",
        "            try :\n",
        "                sentence_barycentre += np.array(word2vec[token])\n",
        "                effective_len += 1\n",
        "            except KeyError :\n",
        "                pass\n",
        "            except :\n",
        "                raise\n",
        "\n",
        "        # Reject sentences with less than n_reject words in our word2vec map\n",
        "        if effective_len <= n_reject :\n",
        "            sentence_scores.append(1)\n",
        "\n",
        "        else :\n",
        "            sentence_barycentre = sentence_barycentre/effective_len\n",
        "            # Compute the distance sentece_barycentre -> words in our knowledge base\n",
        "            barycentre_distance = [cosine(sentence_barycentre, word2vec[key_word]) for key_word in knowledge]\n",
        "            barycentre_distance.sort()\n",
        "            # Create the score of the sentence by averaging the \"n_closer\" smallest distances\n",
        "            score = np.mean(barycentre_distance[:n_closer])\n",
        "            sentence_scores.append(score)\n",
        "    # Select the \"num_sent\" sentences that have the smallest score (smallest distance score with the knowledge base)\n",
        "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
        "    top_sentences = sentences[:num_sent]\n",
        "    return ' '.join(top_sentences)\n",
        "\n",
        "#prepare the train,validation and test dataframe\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "X_valid_df = pd.DataFrame(X_valid)\n",
        "X_test_df = pd.DataFrame(X_test)\n",
        "\n",
        "X_train_df['sentences_distance'] = X_train_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
        "X_valid_df['sentences_distance'] = X_valid_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
        "X_test_df['sentences_distance'] = X_test_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)"
      ],
      "metadata": {
        "id": "9lXShspr7UIu"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY0LAcQxmYWs",
        "outputId": "1e3d2ef4-2c87-410f-b864-1c607664b7d0"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149    MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...\n",
              "436    INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...\n",
              "394    Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...\n",
              "440    INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...\n",
              "330    Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...\n",
              "                             ...                        \n",
              "152    MainStay VP MacKay International Equity Portfo...\n",
              "158    MainStay VP MacKay Small Cap Core Portfolio\\n\\...\n",
              "374    Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...\n",
              "308    PIMCO Gurtin California Municipal Intermediate...\n",
              "287    Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...\n",
              "Name: summary, Length: 294, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "nYK_gIIZmmDq",
        "outputId": "d50b63be-97cb-4834-c032-7eb3c45e489f"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               summary  \\\n",
              "149  MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...   \n",
              "436  INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...   \n",
              "394  Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...   \n",
              "440  INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...   \n",
              "330  Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...   \n",
              "..                                                 ...   \n",
              "152  MainStay VP MacKay International Equity Portfo...   \n",
              "158  MainStay VP MacKay Small Cap Core Portfolio\\n\\...   \n",
              "374  Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...   \n",
              "308  PIMCO Gurtin California Municipal Intermediate...   \n",
              "287  Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...   \n",
              "\n",
              "                                    sentences_distance  \n",
              "149  19\\n\\nMainStay VP Epoch U.S. Small Cap Portfol...  \n",
              "436  1 Year\\t3 Years\\t5 Years\\t10 Years\\n$93\\t$290\\...  \n",
              "394  2. 3. A higher portfolio turnover rate may ind...  \n",
              "440  1 Year\\t3 Years\\t5 Years\\t10 Years\\nFund Share...  \n",
              "330  \"Growth\" Investing. \"Growth\" stocks can perfor...  \n",
              "..                                                 ...  \n",
              "152  35\\n\\nMainStay VP MacKay International Equity ...  \n",
              "158  47\\n\\nMainStay VP MacKay Small Cap Core Portfo...  \n",
              "374  A figure of 1.00 would indicate perfect correl...  \n",
              "308  A higher portfolio turnover rate may indicate ...  \n",
              "287  2 \\tWith limited exceptions, for Class A share...  \n",
              "\n",
              "[294 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e3c7097-d70e-428e-ad37-71b9d894e19e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>sentences_distance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...</td>\n",
              "      <td>19\\n\\nMainStay VP Epoch U.S. Small Cap Portfol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...</td>\n",
              "      <td>1 Year\\t3 Years\\t5 Years\\t10 Years\\n$93\\t$290\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...</td>\n",
              "      <td>2. 3. A higher portfolio turnover rate may ind...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...</td>\n",
              "      <td>1 Year\\t3 Years\\t5 Years\\t10 Years\\nFund Share...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...</td>\n",
              "      <td>\"Growth\" Investing. \"Growth\" stocks can perfor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>MainStay VP MacKay International Equity Portfo...</td>\n",
              "      <td>35\\n\\nMainStay VP MacKay International Equity ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>MainStay VP MacKay Small Cap Core Portfolio\\n\\...</td>\n",
              "      <td>47\\n\\nMainStay VP MacKay Small Cap Core Portfo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...</td>\n",
              "      <td>A figure of 1.00 would indicate perfect correl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>PIMCO Gurtin California Municipal Intermediate...</td>\n",
              "      <td>A higher portfolio turnover rate may indicate ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...</td>\n",
              "      <td>2 \\tWith limited exceptions, for Class A share...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e3c7097-d70e-428e-ad37-71b9d894e19e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e3c7097-d70e-428e-ad37-71b9d894e19e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e3c7097-d70e-428e-ad37-71b9d894e19e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0f99e8dd-7d06-4051-ba71-6c4060511492\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0f99e8dd-7d06-4051-ba71-6c4060511492')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0f99e8dd-7d06-4051-ba71-6c4060511492 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df",
              "summary": "{\n  \"name\": \"X_train_df\",\n  \"rows\": 294,\n  \"fields\": [\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 292,\n        \"samples\": [\n          \"Vanguard Short-Term Treasury Index Fund\\n\\nInvestment Objective\\n\\nThe Fund seeks to track the performance of a market-weighted Treasury index with a short-term dollar-weighted average maturity.\\n\\nFees and Expenses\\n\\nThe following table describes the fees and expenses you may pay if you buy and hold Admiral Shares of the Fund.\\n\\nShareholder Fees\\t \\n(Fees paid directly from your investment)\\t \\n \\nSales Charge (Load) Imposed on Purchases\\tNone\\nPurchase Fee\\tNone\\nSales Charge (Load) Imposed on Reinvested Dividends\\tNone\\nRedemption Fee\\tNone\\nAccount Service Fee (for certain fund account balances below $10,000)\\t$20/year\\n \\nAnnual Fund Operating Expenses\\t \\n(Expenses that you pay each year as a percentage of the value of your investment)\\t \\n \\nManagement Fees\\t0.06%\\n12b-1 Distribution Fee\\tNone\\nOther Expenses\\t0.01%\\nTotal Annual Fund Operating Expenses\\t0.07%\\n \\n\\nExample\\n\\nThe following example is intended to help you compare the cost of investing in the Fund\\u2019s Admiral Shares with the cost of investing in other mutual funds. It illustrates the hypothetical expenses that you would incur over various periods if you were to invest $10,000 in the Fund\\u2019s shares. This example assumes that the shares provide a return of 5% each year and that total annual fund operating expenses remain as stated in the preceding table. You would incur these hypothetical expenses whether or not you redeem your investment at the end of the given period. Although your actual costs may be higher or lower, based on these assumptions your costs would be:\\n\\n1 Year\\t3 Years\\t5 Years\\t10 Years\\n$7\\t$23\\t$40\\t$90\\n \\n\\n1\\n\\n \\n\\nPortfolio Turnover\\n\\nThe Fund pays transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d its portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in more taxes when Fund shares are held in a taxable account. These costs, which are not reflected in annual fund operating expenses or in the previous expense example, reduce the Fund\\u2019s performance. During the most recent fiscal year, the Fund\\u2019s portfolio turnover rate was 67% of the average value of its portfolio.\\n\\nPrincipal Investment Strategies\\n\\nThe Fund employs an indexing investment approach designed to track the performance of the Bloomberg Barclays US Treasury 1\\u20133 Year Bond Index. This Index includes fixed income securities issued by the U.S. Treasury (not including inflation-protected securities), all with maturities between 1 and 3 years.\\n\\nThe Fund invests by sampling the Index, meaning that it holds a range of securities that, in the aggregate, approximates the full Index in terms of key risk factors and other characteristics. All of the Fund\\u2019s investments will be selected through the sampling process, and under normal circumstances, at least 80% of the Fund\\u2019s assets will be invested in bonds included in the Index. The Fund maintains a dollar-weighted average maturity consistent with that of the Index. As of August 31, 2018, the dollar-weighted average maturity of the Index was 2.0 years.\\n\\nPrincipal Risks\\n\\nThe Fund is designed for investors with a low tolerance for risk, but you could still lose money by investing in it. The Fund is subject to the following risks, which could affect the Fund\\u2019s performance:\\n\\n\\u2022 Interest rate risk, which is the chance that bond prices will decline because of rising interest rates. Interest rate risk should be low for the Fund because it invests primarily in short-term bonds, whose prices are less sensitive to interest rate changes than are the prices of longer-term bonds.\\n\\n\\u2022 Income risk, which is the chance that the Fund\\u2019s income will decline because of falling interest rates. Income risk is generally high for short-term bond funds, so investors should expect the Fund\\u2019s monthly income to fluctuate.\\n\\n\\u2022 Index sampling risk, which is the chance that the securities selected for the Fund, in the aggregate, will not provide investment performance matching that of the Fund\\u2018s target index. Index sampling risk for the Fund is expected to be low.\\n\\nAn investment in the Fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency.\\n\\n2\\n\\n \\n\\nAnnual Total Returns\\n\\nThe following bar chart and table are intended to help you understand the risks of investing in the Fund. The bar chart shows how the performance of the Fund\\u2019s Admiral Shares has varied from one calendar year to another over the periods shown. The table shows how the average annual total returns of the Admiral Shares compare with those of the Fund\\u2019s target index, which has investment characteristics similar to those of the Fund. The Fund\\u2019s Signal\\u00ae Shares were renamed Admiral Shares on October 16, 2013. Keep in mind that the Fund\\u2019s past performance (before and after taxes) does not indicate how the Fund will perform in the future. Updated performance information is available on our website at vanguard.com/performance or by calling Vanguard toll-free at 800-662-7447.\\n\\nAnnual Total Returns \\u2014 Vanguard Short-Term Treasury Index Fund Admiral Shares1\\n\\n\\n1 The year-to-date return as of the most recent calendar quarter, which ended on September 30, 2018, was 0.17%.\\n\\nDuring the periods shown in the bar chart, the highest return for a calendar quarter was 1.16% (quarter ended June 30, 2010), and the lowest return for a quarter was \\u20130.45% (quarter ended December 31, 2016).\\n\\n3\\n\\n \\n\\nAverage Annual Total Returns for Periods Ended December 31, 2017\\t \\t \\n \\t \\t \\tSince\\n \\t \\t \\tInception\\n \\t \\t \\t(Dec. 28,\\n \\t1 Year\\t5 Years\\t2009)\\nVanguard Short-Term Treasury Index Fund Admiral Shares\\t \\t \\t \\nReturn Before Taxes\\t0.40%\\t0.50%\\t0.81%\\nReturn After Taxes on Distributions\\t\\u20130.08\\t0.20\\t0.54\\nReturn After Taxes on Distributions and Sale of Fund Shares\\t0.22\\t0.25\\t0.51\\nBloomberg Barclays US Treasury 1-3 Year Bond Index\\t \\t \\t \\n(reflects no deduction for fees, expenses, or taxes)\\t0.42%\\t0.57%\\t0.89%\\n \\n\\nActual after-tax returns depend on your tax situation and may differ from those shown in the preceding table. When after-tax returns are calculated, it is assumed that the shareholder was in the highest individual federal marginal income tax bracket at the time of each distribution of income or capital gains or upon redemption. State and local income taxes are not reflected in the calculations. Please note that after-tax returns are not relevant for a shareholder who holds fund shares in a tax-deferred account, such as an individual retirement account or a 401(k) plan. Also, figures captioned Return After Taxes on Distributions and Sale of Fund Shares may be higher than other figures for the same period if a capital loss occurs upon redemption and results in an assumed tax deduction for the shareholder.\\n\\nInvestment Advisor\\nThe Vanguard Group, Inc. (Vanguard)\\n\\nPortfolio Manager\\n\\nJoshua C. Barrickman, CFA, Principal of Vanguard and head of Vanguard\\u2019s Fixed Income Indexing Americas. He has managed the Fund since 2013.\\n\\n4\\n\\n \\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase or redeem shares online through our website (vanguard.com), by mail (The Vanguard Group, P.O. Box 1110, Valley Forge, PA 19482-1110), or by telephone (800-662-2739). The minimum investment amount required to open and maintain a Fund account for Admiral Shares is $3,000. The minimum investment amount required to add to an existing Fund account is generally $1. Financial intermediaries, institutional, and Vanguard-advised clients should contact Vanguard for information on special eligibility rules that may apply to them regarding Admiral Shares. If you are investing through an intermediary, please contact that firm directly for more information regarding your eligibility. If you are investing through an employer-sponsored retirement or savings plan, your plan administrator or your benefits office can provide you with detailed information on how you can invest through your plan.\\n\\nTax Information\\n\\nThe Fund\\u2019s distributions may be taxable as ordinary income or capital gain. If you are investing through a tax-advantaged account, such as an IRA or an employer-sponsored retirement or savings plan, special tax rules apply.\\n\\nPayments to Financial Intermediaries\\n\\nThe Fund and its investment advisor do not pay financial intermediaries for sales of Fund shares.\",\n          \"Eaton Vance TABS Intermediate-Term Municipal Bond Fund\\n\\nInvestment Objective\\n\\nThe Fund\\u2019s investment objective is to seek after-tax total return.\\n\\nFees and Expenses of the Fund\\n\\nThis table describes the fees and expenses that you may pay if you buy and hold shares of the Fund. Investors may also pay commissions or other fees to their financial intermediary when they buy and hold shares of the Fund, which are not reflected below. You may qualify for a reduced sales charge on purchases of Class A shares if you invest, or agree to invest over a 13-month period, at least $100,000 in Eaton Vance funds. Certain financial intermediaries also may offer variations in Fund sales charges to their customers as described in Appendix A \\u2013 Financial Intermediary Sales Charge Variations in this Prospectus. More information about these and other discounts is available from your financial intermediary and in Sales Charges beginning on page 20 of this Prospectus and page 21 of the Fund\\u2019s Statement of Additional Information.\\n\\nShareholder Fees (fees paid directly from your investment)\\tClass A\\tClass C\\tClass I\\nMaximum Sales Charge (Load) Imposed on Purchases (as a percentage of offering price)\\t2.25%\\tNone\\tNone\\nMaximum Deferred Sales Charge (Load) (as a percentage of the lower of net asset value at purchase or redemption)\\tNone\\t1.00%\\tNone\\n \\n\\nAnnual Fund Operating Expenses (expenses you pay each year as a percentage of the value of your investment)\\tClass A\\tClass C\\tClass I\\nManagement Fees\\t0.60%\\t0.60%\\t0.60%\\nDistribution and Service (12b-1) Fees\\t0.25%\\t1.00%\\tNone\\nOther Expenses\\t0.12%\\t0.12%\\t0.12%\\nTotal Annual Fund Operating Expenses\\t0.97%\\t1.72%\\t0.72%\\nExpense Reimbursement (1)\\t(0.07)%\\t(0.07)%\\t(0.07)%\\nTotal Annual Fund Operating Expenses After Expense Reimbursement\\t0.90%\\t1.65%\\t0.65%\\n(1)\\tThe investment adviser and administrator has agreed to reimburse the Fund\\u2019s expenses to the extent that Total Annual Fund Operating Expenses exceed 0.90% for Class A shares, 1.65% for Class C shares and 0.65% for Class I shares. This expense reimbursement will continue through May 31, 2019. Any amendment to or termination of this reimbursement would require approval of the Board of Trustees. The expense reimbursement relates to ordinary operating expenses only and does not include expenses such as: brokerage commissions, acquired fund fees and expenses of unaffiliated funds, interest expense, taxes or litigation expenses. Amounts reimbursed may be recouped by the investment adviser and administrator during the same fiscal year to the extent actual expenses are less than the contractual expense cap during such year.\\nExample. This Example is intended to help you compare the cost of investing in the Fund with the cost of investing in other mutual funds. The Example assumes that you invest $10,000 in the Fund for the time periods indicated and then redeem all of your shares at the end of those periods. The Example also assumes that your investment has a 5% return each year, that the operating expenses remain the same and that any expense reimbursement arrangement remains in place for the contractual period. Although your actual costs may be higher or lower, based on these assumptions your costs would be:\\n\\n \\tExpenses with Redemption\\tExpenses without Redemption\\n \\t1 Year\\t3 Years\\t5 Years\\t10 Years\\t1 Year\\t3 Years\\t5 Years\\t10 Years\\nClass A shares\\t$315\\t$520\\t$743\\t$1,382\\t$315\\t$520\\t$743\\t$1,382\\nClass C shares\\t$268\\t$535\\t$927\\t$2,024\\t$168\\t$535\\t$927\\t$2,024\\nClass I shares\\t$66\\t$223\\t$394\\t$888\\t$66\\t$223\\t$394\\t$888\\nPortfolio Turnover\\n\\nThe Fund pays transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d the portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when Fund shares are held in a taxable account. These costs, which are not reflected in Annual Fund Operating Expenses or in the Example, affect the Fund\\u2019s performance. During the most recent fiscal year, the Fund's portfolio turnover rate was 62% of the average value of its portfolio.\\n\\nEaton Vance TABS Municipal Bond Funds\\t7\\tProspectus dated June 1, 2018\\n \\n \\n\\nPrincipal Investment Strategies\\n\\nUnder normal market conditions, the Fund invests at least 80% of its net assets (plus any borrowings for investment purposes) in a diversified portfolio of municipal obligations the interest on which is exempt from regular federal income tax (the \\u201c80% Policy\\u201d). In seeking the Fund\\u2019s investment objective, the portfolio managers emphasize tax-exempt income. The Fund normally invests in municipal obligations rated in the three highest rating categories (those rated A or higher by S&P Global Ratings (\\u201cS&P\\u201d), Fitch Ratings (\\u201cFitch\\u201d) or Moody\\u2019s Investors Service, Inc. (\\u201cMoody\\u2019s\\u201d)) or, if unrated, determined by the investment adviser to be of comparable quality at the time of purchase. The Fund will not invest more than 50% of its net assets in municipal obligations rated A at the time of purchase by S&P, Fitch or Moody\\u2019s or, if unrated determined by the investment adviser to be of comparable quality. For purposes of rating restrictions, if securities are rated differently by two or more rating agencies, the highest rating is used. The Fund may continue to hold securities that are downgraded (including bonds downgraded to below investment grade credit quality (\\u201cjunk bonds\\u201d)) if the investment adviser believes it would be advantageous to do so. The Fund will not invest in a municipal obligation the interest on which the Fund\\u2019s investment adviser believes is subject to the federal alternative minimum tax.\\n\\nFor its investment in municipal obligations, the Fund invests primarily in general obligation or revenue bonds. The Fund currently targets an average portfolio duration of approximately 5 - 7 years and an average weighted portfolio maturity of approximately 5 - 13 years, but may invest in securities of any maturity or duration, and may in the future alter its maturity or duration target range. The Fund may use various techniques to shorten or lengthen its dollar weighted average portfolio duration, including the acquisition of municipal obligations at a premium or discount. The portfolio managers generally will seek to enhance after-tax total return by actively engaging in relative value trading within the portfolio to take advantage of price opportunities in the markets for municipal obligations. With respect to 20% of its net assets, the Fund may invest in municipal obligations that are not exempt from regular federal income tax, direct obligations of the U.S. Treasury and/or obligations of U.S. Government agencies, instrumentalities and government-sponsored enterprises. The Fund may also invest in cash and money market instruments.\\n\\nThe investment adviser\\u2019s process for selecting municipal obligations for purchase and sale generally includes consideration of the creditworthiness of the issuer or person obligated to repay the obligation. In evaluating creditworthiness, the investment adviser considers ratings assigned by rating agencies and generally performs additional credit and investment analysis.\\n\\nPrincipal Risks\\n\\nMarket Risk. The value of investments held by the Fund may increase or decrease in response to economic, political and financial events (whether real, expected or perceived) in the U.S. and global markets. The frequency and magnitude of such changes in value cannot be predicted. Certain securities and other investments held by the Fund may experience increased volatility, illiquidity, or other potentially adverse effects in reaction to changing market conditions. Actions taken by the U.S. Federal Reserve or foreign central banks to stimulate or stabilize economic growth, such as decreases or increases in short-term interest rates, could cause high volatility in markets. No active trading market may exist for certain investments, which may impair the ability of the Fund to sell or to realize the current valuation of such investments in the event of the need to liquidate such assets. Fixed-income markets may experience periods of relatively high volatility in an environment where U.S. treasury yields are rising.\\n\\nMunicipal Obligation Risk. The amount of public information available about municipal obligations is generally less than for corporate equities or bonds, meaning that the investment performance of municipal obligations may be more dependent on the analytical abilities of the investment adviser than stock or corporate bond investments. The secondary market for municipal obligations also tends to be less well-developed and less liquid than many other securities markets, which may limit the Fund\\u2019s ability to sell its municipal obligations at attractive prices. The differences between the price at which an obligation can be purchased and the price at which it can be sold may widen during periods of market distress. Less liquid obligations can become more difficult to value and be subject to erratic price movements. The increased presence of nontraditional participants (such as proprietary trading desks of investment banks and hedge funds) or the absence of traditional participants (such as individuals, insurance companies, banks and life insurance companies) in the municipal markets may lead to greater volatility in the markets because non-traditional participants may trade more frequently or in greater volume. \\n\\nInterest Rate Risk. In general, the value of income securities will fluctuate based on changes in interest rates. The value of these securities is likely to increase when interest rates fall and decline when interest rates rise. Generally, securities with longer durations are more sensitive to changes in interest rates than shorter duration securities, causing them to be more volatile. Conversely, fixed income securities with shorter durations will be less volatile but may provide lower returns than fixed income securities with longer durations. In a rising interest rate environment, the durations of income securities that have the ability to be prepaid or called by the issuer may be extended. In a declining interest rate environment, the proceeds from prepaid or maturing instruments may have to be reinvested at a lower interest rate.\\n\\nEaton Vance TABS Municipal Bond Funds\\t8\\tProspectus dated June 1, 2018\\n \\nCredit Risk. Investments in municipal obligations and other debt obligations (referred to below as \\u201cdebt instruments\\u201d) are subject to the risk of non-payment of scheduled principal and interest. Changes in economic conditions or other circumstances may reduce the capacity of the party obligated to make principal and interest payments on such instruments and may lead to defaults. Such non-payments and defaults may reduce the value of Fund shares and income distributions. The value of debt instruments also may decline because of concerns about the issuer\\u2019s ability to make principal and interest payments. In addition, the credit ratings of debt instruments may be lowered if the financial condition of the party obligated to make payments with respect to such instruments deteriorates. In order to enforce its rights in the event of a default, bankruptcy or similar situation, the Fund may be required to retain legal or similar counsel, which may increase the Fund\\u2019s operating expenses and adversely affect net asset value. Municipal obligations may be insured as to principal and interest payments. If the claims-paying ability or other rating of the insurer is downgraded by a rating agency, the value of such obligations may be negatively affected.\\n\\nRisks of Principal Only Investments. Principal only investments entitle the Fund to receive the stated value of such investment when held to maturity. The values of principal only investments are subject to greater fluctuation in response to changes in market interest rates than obligations that pay interest currently. The Fund will accrue income on these investments and distribute that income each year. The Fund may be required to sell other investments to obtain cash needed for such income distributions.\\n\\nU.S. Government Securities Risk. Although certain U.S. Government-sponsored agencies (such as the Federal Home Loan Mortgage Corporation and the Federal National Mortgage Association) may be chartered or sponsored by acts of Congress, their securities are neither issued nor guaranteed by the U.S. Treasury. U.S. Treasury securities generally have a lower return than other obligations because of their higher credit quality and market liquidity.\\n\\nTax Risk. Income from tax-exempt municipal obligations could be declared taxable because of changes in tax laws, adverse interpretations by the relevant taxing authority or the non-compliant conduct of the issuer of an obligation.\\n\\nMoney Market Instrument Risk. Money market instruments may be adversely affected by market and economic events, such as a sharp rise in prevailing short-term interest rates; adverse developments in the banking industry, which issues or guarantees many money market instruments; adverse economic, political or other developments affecting issuers of money market instruments; changes in the credit quality of issuers; and default by a counterparty.\\n\\nGeneral Fund Investing Risks. The Fund is not a complete investment program and there is no guarantee that the Fund will achieve its investment objective. It is possible to lose money by investing in the Fund. The Fund is designed to be a long-term investment vehicle and is not suited for short-term trading. Investors in the Fund should have a long-term investment perspective and be able to tolerate potentially sharp declines in value. Purchase and redemption activities by Fund shareholders may impact the management of the Fund and its ability to achieve its investment objective(s). In addition, the redemption by one or more large shareholders or groups of shareholders of their holdings in the Fund could have an adverse impact on the remaining shareholders in the Fund. An investment in the Fund is not a deposit in a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency.\\n\\nPerformance\\n\\nThe following bar chart and table provide some indication of the risks of investing in the Fund by showing changes in the Fund\\u2019s performance from year to year and how the Fund\\u2019s average annual returns over time compare with those of two broad-based securities market indices. The returns in the bar chart are for Class A shares and do not reflect a sales charge. If the sales charge was reflected, the returns would be lower. Past performance (both before and after taxes) is not necessarily an indication of how the Fund will perform in the future. The Fund\\u2019s performance reflects the effects of expense reductions. Absent these reductions, performance would have been lower. Updated Fund performance information can be obtained by visiting www.eatonvance.com.\\n\\n\\n\\nDuring the period from December 31, 2010 to December 31, 2017, the highest quarterly total return for Class A was 3.75% for the quarter ended June 30, 2011, and the lowest quarterly return was \\u20134.07% for the quarter ended December 31, 2016. The year-to-date total return through the end of the most recent calendar quarter (December 31, 2017 to March 31, 2018) was -1.71%.\\n\\nEaton Vance TABS Municipal Bond Funds\\t9\\tProspectus dated June 1, 2018\\n \\n \\n\\nAverage Annual Total Return as of December 31, 2017\\tOne Year\\tFive Years\\tLife of Fund\\nClass A Return Before Taxes\\t2.08%\\t2.12%\\t4.21%\\nClass A Return After Taxes on Distributions\\t2.05%\\t2.08%\\t4.09%\\nClass A Return After Taxes on Distributions and the Sale of Class A Shares\\t1.94%\\t1.97%\\t3.59%\\nClass C Return Before Taxes\\t2.67%\\t1.82%\\t3.74%\\nClass I Return Before Taxes\\t4.71%\\t2.82%\\t4.78%\\nBloomberg Barclays Municipal Managed Money Intermediate (1-17 Year) Bond Index (reflects no deduction for fees, expenses or taxes)\\t4.88%\\t2.55%\\t4.26%\\nBloomberg Barclays 7 Year Municipal Bond Index (reflects no deduction for fees, expenses or taxes)\\t4.49%\\t2.44%\\t4.30%\\nThese returns reflect the maximum sales charge for Class A (2.25%) and any applicable contingent deferred sales charge (\\u201cCDSC\\u201d) for Class C. Class A, Class C and Class I commenced operations on February 1, 2010. Effective February 17, 2015, the Fund changed its name, objective and investment strategy to invest at least 80% of its net assets in a diversified portfolio of municipal obligations, the interest on which is exempt from regular federal income tax. Investors cannot invest directly in an Index.\\n\\nAfter-tax returns are calculated using the highest historical individual federal income tax rates and does not reflect the impact of state and local taxes. Actual after-tax returns depend on a shareholder\\u2019s tax situation and the actual characterization of distributions, and may differ from those shown. After-tax returns are not relevant to shareholders who hold shares in tax-deferred accounts or to shares held by non-taxable entities. After-tax returns for other Classes of shares will vary from the after-tax returns presented for Class A shares. Return After Taxes on Distributions for a period may be the same as Return Before Taxes for that period because no taxable distributions were made during that period. Also, Return After Taxes on distributions and Sale of Fund Shares for a period may be greater than or equal to Return After Taxes on Distributions for the same period because of losses realized on the sale of Fund shares.\\n\\nManagement\\n\\nInvestment Adviser. Eaton Vance Management (\\u201cEaton Vance\\u201d).\\n\\nPortfolio Managers\\n\\nThe portfolio managers of the Fund are part of Eaton Vance\\u2019s Tax-Advantaged Bond Strategies (\\u201cTABS\\u201d) division.\\n\\nJames H. Evans, (lead portfolio manager), Vice President of Eaton Vance, has managed the Fund since it commenced operations in February 2010.\\n\\nBrian C. Barney, Vice President of Eaton Vance, has managed the Fund since June 2010.\\n\\nChristopher J. Harshman, Vice President of Eaton Vance, has managed the Fund since June 2010.\\n\\nFor important information about purchase and sale of shares, taxes and financial intermediary compensation, please turn to \\u201cImportant Information Regarding Fund Shares\\u201d on page 11 of this Prospectus.\\n\\nEaton Vance TABS Municipal Bond Funds\\t10\\tProspectus dated June 1, 2018\\n \\n \\n\\nImportant Information Regarding Fund Shares\\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase, redeem or exchange Fund shares on any business day, which is any day the New York Stock Exchange is open for business. You may purchase, redeem or exchange Fund shares either through your financial intermediary or (except for purchases of Class C shares by accounts with no specified financial intermediary) directly from a Fund either by writing to the Fund, P.O. Box 9653, Providence, RI 02940-9653, or by calling 1-800-262-1122. The minimum initial purchase or exchange into a Fund is $1,000 for each Class (with the exception of Class I) and $250,000 for Class I (waived in certain circumstances). There is no minimum for subsequent investments.\\n\\nTax Information\\n\\nEach Fund\\u2019s distributions are expected to primarily be exempt from regular federal income tax. However, the Fund may also distribute taxable income to the extent that it invests in taxable municipal obligations or other obligations which generate taxable income. Distributions of any net realized gains are expected to be taxable.\\n\\nPayments to Broker-Dealers and Other Financial Intermediaries\\n\\nIf you purchase a Fund\\u2019s shares through a broker-dealer or other financial intermediary (such as a bank) (collectively, \\u201cfinancial intermediaries\\u201d), the Fund, its principal underwriter and its affiliates may pay the financial intermediary for the sale of Fund shares and related services. These payments may create a conflict of interest by influencing the financial intermediary and your salesperson to recommend a Fund over another investment. Ask your salesperson or visit your financial intermediary\\u2019s website for more information.\",\n          \"Fund Summary\\n\\nFund/Class:\\nFidelity Freedom\\u00ae 2050 Fund/Fidelity Freedom\\u00ae 2050 Fund\\n\\nInvestment Objective\\n\\nThe fund seeks high total return until its target retirement date. Thereafter the fund's objective will be to seek high current income and, as a secondary objective, capital appreciation.\\n\\nFee Table\\n\\nThe following table describes the fees and expenses that may be incurred when you buy and hold shares of the fund.\\n\\nShareholder fees\\n\\n(fees paid directly from your investment) \\tNone \\nAnnual Operating Expenses\\n\\n(expenses that you pay each year as a % of the value of your investment)\\n\\nManagement fee(a) \\t \\t0.75% \\nDistribution and/or Service (12b-1) fees \\t \\tNone \\nOther expenses(a) \\t \\t0.00% \\nTotal annual operating expenses(a) \\t \\t0.75% \\n(a)   Adjusted to reflect current fees.\\n\\nThis example helps compare the cost of investing in the fund with the cost of investing in other funds.\\n\\nLet's say, hypothetically, that the annual return for shares of the fund is 5% and that your shareholder fees and the annual operating expenses for shares of the fund are exactly as described in the fee table. This example illustrates the effect of fees and expenses, but is not meant to suggest actual or expected fees and expenses or returns, all of which may vary. For every $10,000 you invested, here's how much you would pay in total expenses if you sell all of your shares at the end of each time period indicated:\\n\\n1 year \\t$76 \\n3 years \\t$238 \\n5 years \\t$415 \\n10 years \\t$926 \\nPortfolio Turnover\\n\\nThe fund will not incur transaction costs, such as commissions, when it buys and sells shares of underlying Fidelity\\u00ae funds (or \\\"turns over\\\" its portfolio), but it could incur transaction costs if it were to buy and sell other types of securities directly. If the fund were to buy and sell other types of securities directly, a higher portfolio turnover rate could indicate higher transaction costs and could result in higher taxes when fund shares are held in a taxable account. Such costs, if incurred, would not be reflected in annual operating expenses or in the example and would affect the fund's performance. During the most recent fiscal year, the fund's portfolio turnover rate was 16% of the average value of its portfolio.\\n\\nPrincipal Investment Strategies\\n\\nInvesting in a combination of Fidelity\\u00ae domestic equity funds, international equity funds (developed and emerging markets), bond funds, and short-term funds (underlying Fidelity\\u00ae funds).\\nAllocating assets according to a neutral asset allocation strategy shown in the glide path below that becomes increasingly conservative until it reaches an allocation similar to that of the Fidelity Freedom\\u00ae Income Fund, approximately 10 to 19 years after the year 2050 (approximately 17% in domestic equity funds, 7% in international equity funds, 46% in bond funds, and 30% in short-term funds).\\nBuying and selling futures contracts (both long and short positions) in an effort to manage cash flows efficiently, remain fully invested, or facilitate asset allocation.\\nFMR Co., Inc. (FMRC) may continue to seek high total return for several years beyond the fund's target retirement date in an effort to achieve the fund's overall investment objective.\\n\\nAs of March 31, 2018, the fund's neutral asset allocation to underlying Fidelity\\u00ae funds and futures was approximately:\\n   \\tDomestic Equity Funds* \\t63% \\n   \\tInternational Equity Funds* \\t27% \\n   \\tBond Funds* \\t10% \\n \\tShort-Term Funds* \\t0% \\n\\n* FMRC may change these percentages over time. As a result of the active asset allocation strategy (discussed below), actual allocations may differ from the neutral allocations above. The allocation percentages may not add to 100% due to rounding.\\n\\nFMRC may use an active asset allocation strategy to increase or decrease neutral asset class exposures reflected above by up to 10% for equity funds (includes domestic equity and international equity funds), bond funds and short-term funds to reflect FMRC's market outlook, which is primarily focused on the intermediate term. The asset allocations in the glide path and pie chart above are referred to as neutral because they do not reflect any decisions made by FMRC to overweight or underweight an asset class.\\nFMRC may also make active asset allocations within other asset classes (including commodities, high yield debt, floating rate debt, real estate debt, inflation-protected debt, and emerging markets debt) from 0% to 10% individually but no more than 25% in aggregate within those other asset classes. Such asset classes are not reflected in the neutral asset allocations reflected in the glide path and pie chart above.\\nDesigned for investors who anticipate retiring in or within a few years of 2050 (target retirement date) at or around age 65 and plan to gradually withdraw the value of their account in the fund over time.\\nPrincipal Investment Risks\\n\\nShareholders should consider that no target date fund is intended as a complete retirement program and there is no guarantee that any single fund will provide sufficient retirement income at or through your retirement. The fund's share price fluctuates, which means you could lose money by investing in the fund, including losses near, at or after the target retirement date.\\n\\nAsset Allocation Risk.  The fund is subject to risks resulting from the Adviser's asset allocation decisions. The selection of underlying funds and the allocation of the fund's assets among various asset classes could cause the fund to lose value or its results to lag relevant benchmarks or other funds with similar objectives. In addition, the fund's active asset allocation strategy may cause the fund to have a risk profile different than that portrayed above from time to time and may increase losses.\\nInvesting in Other Funds.  The fund bears all risks of investment strategies employed by the underlying funds, including the risk that the underlying funds will not meet their investment objectives.\\nStock Market Volatility.  Stock markets are volatile and can decline significantly in response to adverse issuer, political, regulatory, market, or economic developments. Different parts of the market, including different market sectors, and different types of securities can react differently to these developments.\\nInterest Rate Changes.  Interest rate increases can cause the price of a debt or money market security to decrease.\\nForeign Exposure.  Foreign markets, particularly emerging markets, can be more volatile than the U.S. market due to increased risks of adverse issuer, political, regulatory, market, or economic developments and can perform differently from the U.S. market. Emerging markets can be subject to greater social, economic, regulatory, and political uncertainties and can be extremely volatile. Foreign exchange rates also can be extremely volatile.\\nIndustry Exposure.  Market conditions, interest rates, and economic, regulatory, or financial developments could significantly affect a single industry or group of related industries.\\nIssuer-Specific Changes.  The value of an individual security or particular type of security can be more volatile than, and can perform differently from, the market as a whole. Lower-quality debt securities (those of less than investment-grade quality, also referred to as high yield debt securities or junk bonds) and certain types of other securities involve greater risk of default or price changes due to changes in the credit quality of the issuer. The value of lower-quality debt securities and certain types of other securities can be more volatile due to increased sensitivity to adverse issuer, political, regulatory, market, or economic developments.\\nLeverage Risk.  Leverage can increase market exposure, magnify investment risks, and cause losses to be realized more quickly.\\n\\\"Growth\\\" Investing.  \\\"Growth\\\" stocks can perform differently from the market as a whole and other types of stocks and can be more volatile than other types of stocks.\\n\\\"Value\\\" Investing.  \\\"Value\\\" stocks can perform differently from the market as a whole and other types of stocks and can continue to be undervalued by the market for long periods of time.\\nCommodity-Linked Investing.  The value of commodities and commodity-linked investments may be affected by the performance of the overall commodities markets as well as weather, political, tax, and other regulatory and market developments. Commodity-linked investments may be more volatile and less liquid than the underlying commodity, instruments, or measures.\\nAn investment in the fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency. You could lose money by investing in the fund.\\n\\nPerformance\\n\\nThe following information is intended to help you understand the risks of investing in the fund. The information illustrates the changes in the performance of the fund's shares from year to year and compares the performance of the fund's shares to the performance of a securities market index and a hypothetical composite of market indexes over various periods of time. The indexes have characteristics relevant to the fund's investment strategies. Index descriptions appear in the \\\"Additional Index Information\\\" section of the prospectus. Prior to June 1, 2017, the fund operated under a different pricing structure. The fund\\u2019s historical performance prior to June 1, 2017 does not reflect the fund\\u2019s current pricing structure. Past performance (before and after taxes) is not an indication of future performance.\\n\\nVisit www.fidelity.com for more recent performance information.\\n\\nYear-by-Year Returns\\n\\n\\n\\nDuring the periods shown in the chart: \\tReturns \\tQuarter ended \\nHighest Quarter Return \\t19.11% \\tJune 30, 2009 \\nLowest Quarter Return \\t(23.40)% \\tDecember 31, 2008 \\nYear-to-Date Return \\t(0.25)% \\tMarch 31, 2018 \\nAverage Annual Returns\\n\\nAfter-tax returns are calculated using the historical highest individual federal marginal income tax rates, but do not reflect the impact of state or local taxes. Actual after-tax returns may differ depending on your individual circumstances. The after-tax returns shown are not relevant if you hold your shares in a retirement account or in another tax-deferred arrangement, such as an employee benefit plan (profit sharing, 401(k), or 403(b) plan).\\n\\nFor the periods ended December 31, 2017 \\tPast 1 year \\tPast 5 years \\tPast 10 years \\nFidelity Freedom\\u00ae 2050 Fund \\nReturn Before Taxes \\t22.28% \\t11.30% \\t5.31% \\nReturn After Taxes on Distributions \\t21.05% \\t9.54% \\t4.09% \\nReturn After Taxes on Distributions and Sale of Fund Shares \\t13.44% \\t8.52% \\t3.85% \\nS&P 500\\u00ae Index\\n(reflects no deduction for fees, expenses, or taxes) \\t21.83% \\t15.79% \\t8.50% \\nFidelity Freedom 2050 Composite Index\\u2120\\n(reflects no deduction for fees or expenses) \\t20.95% \\t12.05% \\t6.60% \\nInvestment Adviser\\n\\nFMRC (the Adviser), an affiliate of Fidelity Management & Research Company (FMR), is the fund's manager.\\n\\nPortfolio Manager(s)\\n\\nAndrew Dierdorf (co-manager) has managed the fund since June 2011.\\n\\nBrett Sumsion (co-manager) has managed the fund since January 2014.\\n\\nPurchase and Sale of Shares\\n\\nYou may buy or sell shares through a Fidelity\\u00ae brokerage or mutual fund account, through a retirement account, or through an investment professional. You may buy or sell shares in various ways:\\n\\nInternet\\n\\nwww.fidelity.com\\n\\nPhone\\n\\nFidelity Automated Service Telephone (FAST\\u00ae) 1-800-544-5555\\n\\nTo reach a Fidelity representative 1-800-544-6666\\n\\nMail\\n\\nAdditional purchases:\\n\\nFidelity Investments\\nP.O. Box 770001\\nCincinnati, OH 45277-0003\\nRedemptions:\\n\\nFidelity Investments\\nP.O. Box 770001\\nCincinnati, OH 45277-0035\\nTDD- Service for the Deaf and Hearing Impaired\\n\\n1-800-544-0118\\n\\nThe price to buy one share is its net asset value per share (NAV). Shares will be bought at the NAV next calculated after an order is received in proper form.\\n\\nThe price to sell one share is its NAV. Shares will be sold at the NAV next calculated after an order is received in proper form.\\n\\nThe fund is open for business each day the New York Stock Exchange (NYSE) is open.\\n\\nInitial Purchase Minimum \\t$2,500 \\nFor Fidelity\\u00ae Simplified Employee Pension-IRA, Keogh, and Investment Only Retirement accounts \\t$500 \\nThrough regular investment plans in Fidelity\\u00ae Traditional IRAs, Roth IRAs, and Rollover IRAs (requires monthly purchases of $200 until fund balance is $2,500) \\t$200 \\nThe fund may waive or lower purchase minimums in other circumstances.\\n\\nTax Information\\n\\nDistributions you receive from the fund are subject to federal income tax and generally will be taxed as ordinary income or capital gains, and may also be subject to state or local taxes, unless you are investing through a tax-advantaged retirement account (in which case you may be taxed later, upon withdrawal of your investment from such account).\\n\\nPayments to Broker-Dealers and Other Financial Intermediaries\\n\\nThe fund, the Adviser, Fidelity Distributors Corporation (FDC), and/or their affiliates may pay intermediaries, which may include banks, broker-dealers, retirement plan sponsors, administrators, or service-providers (who may be affiliated with the Adviser or FDC), for the sale of fund shares and related services. These payments may create a conflict of interest by influencing your intermediary and your investment professional to recommend the fund over another investment. Ask your investment professional or visit your intermediary's web site for more information.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_distance\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 253,\n        \"samples\": [\n          \"2 \\tWith limited exceptions, for Class A shares, if your Fund account balance is below $650 at the start of business on the Friday prior to the last full week of September of each year, the account will be assessed an account fee of $20. 3 \\tThrough July 31, 2019, Ivy Investment Management Company (IICO), the Fund\\u2019s investment manager, Ivy Distributors, Inc. (IDI), the Fund\\u2019s distributor, and/or Waddell & Reed Services Company, doing business as WI Services Company (WISC), the Fund\\u2019s transfer agent, have contractually agreed to reimburse sufficient management fees, 12b-1 fees and/or shareholder servicing fees to cap the total annual ordinary fund operating expenses (which would exclude interest, taxes, brokerage commissions, acquired fund fees and expenses and extraordinary expenses, if any) as follows: Class E shares at 1.10%. 4\\t \\tProspectus\\t \\tDomestic Equity Funds\\nTable of Contents\\nExample\\n\\nThis example is intended to help you compare the cost of investing in the shares of the Fund with the cost of investing in other mutual funds. 4 \\tThrough July 31, 2020, IICO, IDI and/or WISC have contractually agreed to reimburse sufficient management fees, 12b-1 fees and/or shareholder servicing fees to cap the total annual ordinary fund operating expenses (which would exclude interest, taxes, brokerage commissions, acquired fund fees and expenses and extraordinary expenses, if any) as follows: Class A shares at 1.04%; Class B shares at 2.13%; Class E shares at 1.13%; and Class I shares and Class Y shares at 0.84%. 5 \\tThrough July 31, 2020, IDI and/or WISC have contractually agreed to reimburse sufficient 12b-1 and/or shareholder servicing fees to ensure that the total annual ordinary fund operating expenses of the Class Y shares do not exceed the total annual ordinary fund operating expenses of the Class A shares, as calculated at the end of each month.\",\n          \"A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when fund shares are held in a taxable account. An investment in the fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency. Ask your investment professional or visit your intermediary's web site for more information. Box 770001\\nCincinnati, OH 45277-0035\\nOvernight Express:\\nFidelity Investments\\n100 Crosby Parkway\\nCovington, KY 41015\\nTDD- Service for the Deaf and Hearing Impaired\\n\\n1-800-544-0118\\n\\nThe price to buy one share is its net asset value per share (NAV). Currently, the Board of Trustees of the fund has not authorized such payments for shares of the fund.\",\n          \"2\\n\\n \\n\\nAnnual Total Returns\\n\\nThe following bar chart and table are intended to help you understand the risks of investing in the Fund. 3\\n\\n \\n\\nAverage Annual Total Returns for Periods Ended December 31, 2017\\t \\t \\n \\t \\t \\tSince\\n \\t \\t \\tInception\\n \\t \\t \\t(Dec. 28,\\n \\t1 Year\\t5 Years\\t2009)\\nVanguard Short-Term Treasury Index Fund Admiral Shares\\t \\t \\t \\nReturn Before Taxes\\t0.40%\\t0.50%\\t0.81%\\nReturn After Taxes on Distributions\\t\\u20130.08\\t0.20\\t0.54\\nReturn After Taxes on Distributions and Sale of Fund Shares\\t0.22\\t0.25\\t0.51\\nBloomberg Barclays US Treasury 1-3 Year Bond Index\\t \\t \\t \\n(reflects no deduction for fees, expenses, or taxes)\\t0.42%\\t0.57%\\t0.89%\\n \\n\\nActual after-tax returns depend on your tax situation and may differ from those shown in the preceding table. 4\\n\\n \\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase or redeem shares online through our website (vanguard.com), by mail (The Vanguard Group, P.O. A higher portfolio turnover rate may indicate higher transaction costs and may result in more taxes when Fund shares are held in a taxable account. All of the Fund\\u2019s investments will be selected through the sampling process, and under normal circumstances, at least 80% of the Fund\\u2019s assets will be invested in bonds included in the Index.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df['sentences_distance'][149]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "dweQxPa3mqi1",
        "outputId": "c9307df9-f625-4a6c-b0f7-7d7789842fc2"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'19\\n\\nMainStay VP Epoch U.S. Small Cap Portfolio\\n\\nPrincipal Risks\\nYou can lose money by investing in the Portfolio. 20\\n\\nMainStay VP Epoch U.S. Small Cap Portfolio\\n\\nPerformance data for the classes varies based on differences in their fee and expense structures. 21\\n\\nMainStay VP Epoch U.S. Small Cap Portfolio\\n\\nCompensation to Broker/Dealers and Other Financial Intermediaries\\nThe Portfolio and/or its related companies may pay NYLIAC or other participating insurance companies, broker/dealers, or other financial intermediaries for the sale of Portfolio shares and related services. A higher portfolio turnover rate may indicate higher transaction costs. Also, issuers of convertible securities are often not as strong financially as those issuing securities with higher credit ratings, are more likely to encounter financial difficulties and typically are more vulnerable to changes in the economy, such as a recession or a sustained period of rising interest rates, which could affect their ability to make interest and principal payments.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentence_match(summary, knowledge, num_sent):\n",
        "    sentences = sent_tokenize(summary)\n",
        "    sentence_scores = []\n",
        "    for j, sentence in enumerate(sentences):\n",
        "        set_tokens = set(tokenizer(sentence))\n",
        "\n",
        "        # Find the number of common words between the knowledge base and the sentence\n",
        "        inter_knwoledge = set_tokens.intersection(knowledge)\n",
        "\n",
        "        sentence_scores.append(len(inter_knwoledge))\n",
        "\n",
        "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
        "    top_sentences = sentences[len(sentences)-num_sent-1:]\n",
        "    return ' '.join(top_sentences)\n",
        "\n",
        "X_train_df['sentences_match'] = X_train_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "X_valid_df['sentences_match'] = X_valid_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "X_test_df['sentences_match'] = X_test_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "\n",
        "# produce train_X and test_X\n",
        "train_X = X_train_df['sentences_match'].values\n",
        "train_X = [' '.join(tokenizer(txt)) for txt in train_X]\n",
        "\n",
        "valid_X = X_valid_df['sentences_match'].values\n",
        "valid_X = [' '.join(tokenizer(txt)) for txt in valid_X]\n",
        "\n",
        "test_X = X_test_df['sentences_match'].values\n",
        "test_X = [' '.join(tokenizer(txt)) for txt in test_X]\n",
        "\n",
        "# produce train_y and valid_y\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "encoded_train_y = encoder.fit_transform(y_train)\n",
        "label_train_y = to_categorical(encoded_train_y, num_classes=3)\n",
        "\n",
        "encoded_valid_y = encoder.transform(y_valid)\n",
        "label_valid_y = to_categorical(encoded_valid_y, num_classes=3)\n",
        "\n",
        "encoded_test_y = encoder.fit_transform(y_test)\n",
        "label_test_y = to_categorical(encoded_test_y, num_classes=3)\n",
        "\n",
        "num_words = 2500 # Size of the vocabulary used. we only consider the 2500 most common words. The other words are removed from the texts.\n",
        "maxlen = 150 # Number of word considered for each document. we cut or lengthen the texts to have texts of 150 words.\n",
        "word_dimension = 50 # dimension of our word vectors.\n",
        "\n",
        "keras_tokenizer = Tokenizer(num_words=num_words)\n",
        "\n",
        "keras_tokenizer.fit_on_texts(train_X)\n",
        "\n",
        "word_index = keras_tokenizer.word_index\n",
        "\n",
        "sequences_train = keras_tokenizer.texts_to_sequences(train_X)\n",
        "sequences_valid = keras_tokenizer.texts_to_sequences(valid_X)\n",
        "sequences_test = keras_tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "# truncate or lenthen each text so they have the same length.\n",
        "feature_train = pad_sequences(sequences_train, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "feature_valid = pad_sequences(sequences_valid, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "feature_test = pad_sequences(sequences_test, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "\n",
        "# create our embedding matrix\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, word_dimension))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "uspfbfShk4L_"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "WhMwZ72PmIva",
        "outputId": "5b429548-5860-43f0-d66d-172e4920103b"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               summary  \\\n",
              "149  MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...   \n",
              "436  INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...   \n",
              "394  Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...   \n",
              "440  INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...   \n",
              "330  Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...   \n",
              "..                                                 ...   \n",
              "152  MainStay VP MacKay International Equity Portfo...   \n",
              "158  MainStay VP MacKay Small Cap Core Portfolio\\n\\...   \n",
              "374  Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...   \n",
              "308  PIMCO Gurtin California Municipal Intermediate...   \n",
              "287  Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...   \n",
              "\n",
              "                                    sentences_distance  \\\n",
              "149  19\\n\\nMainStay VP Epoch U.S. Small Cap Portfol...   \n",
              "436  1 Year\\t3 Years\\t5 Years\\t10 Years\\n$93\\t$290\\...   \n",
              "394  2. 3. A higher portfolio turnover rate may ind...   \n",
              "440  1 Year\\t3 Years\\t5 Years\\t10 Years\\nFund Share...   \n",
              "330  \"Growth\" Investing. \"Growth\" stocks can perfor...   \n",
              "..                                                 ...   \n",
              "152  35\\n\\nMainStay VP MacKay International Equity ...   \n",
              "158  47\\n\\nMainStay VP MacKay Small Cap Core Portfo...   \n",
              "374  A figure of 1.00 would indicate perfect correl...   \n",
              "308  A higher portfolio turnover rate may indicate ...   \n",
              "287  2 \\tWith limited exceptions, for Class A share...   \n",
              "\n",
              "                                       sentences_match  \n",
              "149  The security selection process focuses on free...  \n",
              "436  There are no minimum initial or subsequent pur...  \n",
              "394  Total annual Fund operating expenses differ fr...  \n",
              "440  These payments may create a conflict of intere...  \n",
              "330  This example helps compare the cost of investi...  \n",
              "..                                                 ...  \n",
              "152  The table does not include any separate accoun...  \n",
              "158  The table does not include any separate accoun...  \n",
              "374  Tracking error may occur because of difference...  \n",
              "308  This Fee Waiver Agreement renews annually unle...  \n",
              "287  ∎\\t \\tInitial Public Offering (IPO) Risk. ∎\\t ...  \n",
              "\n",
              "[294 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-53f2fab6-e3bd-49d4-aaab-b3394393486f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>sentences_distance</th>\n",
              "      <th>sentences_match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...</td>\n",
              "      <td>19\\n\\nMainStay VP Epoch U.S. Small Cap Portfol...</td>\n",
              "      <td>The security selection process focuses on free...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...</td>\n",
              "      <td>1 Year\\t3 Years\\t5 Years\\t10 Years\\n$93\\t$290\\...</td>\n",
              "      <td>There are no minimum initial or subsequent pur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...</td>\n",
              "      <td>2. 3. A higher portfolio turnover rate may ind...</td>\n",
              "      <td>Total annual Fund operating expenses differ fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...</td>\n",
              "      <td>1 Year\\t3 Years\\t5 Years\\t10 Years\\nFund Share...</td>\n",
              "      <td>These payments may create a conflict of intere...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...</td>\n",
              "      <td>\"Growth\" Investing. \"Growth\" stocks can perfor...</td>\n",
              "      <td>This example helps compare the cost of investi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>MainStay VP MacKay International Equity Portfo...</td>\n",
              "      <td>35\\n\\nMainStay VP MacKay International Equity ...</td>\n",
              "      <td>The table does not include any separate accoun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>MainStay VP MacKay Small Cap Core Portfolio\\n\\...</td>\n",
              "      <td>47\\n\\nMainStay VP MacKay Small Cap Core Portfo...</td>\n",
              "      <td>The table does not include any separate accoun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...</td>\n",
              "      <td>A figure of 1.00 would indicate perfect correl...</td>\n",
              "      <td>Tracking error may occur because of difference...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>PIMCO Gurtin California Municipal Intermediate...</td>\n",
              "      <td>A higher portfolio turnover rate may indicate ...</td>\n",
              "      <td>This Fee Waiver Agreement renews annually unle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...</td>\n",
              "      <td>2 \\tWith limited exceptions, for Class A share...</td>\n",
              "      <td>∎\\t \\tInitial Public Offering (IPO) Risk. ∎\\t ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53f2fab6-e3bd-49d4-aaab-b3394393486f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-53f2fab6-e3bd-49d4-aaab-b3394393486f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-53f2fab6-e3bd-49d4-aaab-b3394393486f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c1fa9612-82f0-4191-82d9-854d7508e470\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c1fa9612-82f0-4191-82d9-854d7508e470')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c1fa9612-82f0-4191-82d9-854d7508e470 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df",
              "summary": "{\n  \"name\": \"X_train_df\",\n  \"rows\": 294,\n  \"fields\": [\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 292,\n        \"samples\": [\n          \"Vanguard Short-Term Treasury Index Fund\\n\\nInvestment Objective\\n\\nThe Fund seeks to track the performance of a market-weighted Treasury index with a short-term dollar-weighted average maturity.\\n\\nFees and Expenses\\n\\nThe following table describes the fees and expenses you may pay if you buy and hold Admiral Shares of the Fund.\\n\\nShareholder Fees\\t \\n(Fees paid directly from your investment)\\t \\n \\nSales Charge (Load) Imposed on Purchases\\tNone\\nPurchase Fee\\tNone\\nSales Charge (Load) Imposed on Reinvested Dividends\\tNone\\nRedemption Fee\\tNone\\nAccount Service Fee (for certain fund account balances below $10,000)\\t$20/year\\n \\nAnnual Fund Operating Expenses\\t \\n(Expenses that you pay each year as a percentage of the value of your investment)\\t \\n \\nManagement Fees\\t0.06%\\n12b-1 Distribution Fee\\tNone\\nOther Expenses\\t0.01%\\nTotal Annual Fund Operating Expenses\\t0.07%\\n \\n\\nExample\\n\\nThe following example is intended to help you compare the cost of investing in the Fund\\u2019s Admiral Shares with the cost of investing in other mutual funds. It illustrates the hypothetical expenses that you would incur over various periods if you were to invest $10,000 in the Fund\\u2019s shares. This example assumes that the shares provide a return of 5% each year and that total annual fund operating expenses remain as stated in the preceding table. You would incur these hypothetical expenses whether or not you redeem your investment at the end of the given period. Although your actual costs may be higher or lower, based on these assumptions your costs would be:\\n\\n1 Year\\t3 Years\\t5 Years\\t10 Years\\n$7\\t$23\\t$40\\t$90\\n \\n\\n1\\n\\n \\n\\nPortfolio Turnover\\n\\nThe Fund pays transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d its portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in more taxes when Fund shares are held in a taxable account. These costs, which are not reflected in annual fund operating expenses or in the previous expense example, reduce the Fund\\u2019s performance. During the most recent fiscal year, the Fund\\u2019s portfolio turnover rate was 67% of the average value of its portfolio.\\n\\nPrincipal Investment Strategies\\n\\nThe Fund employs an indexing investment approach designed to track the performance of the Bloomberg Barclays US Treasury 1\\u20133 Year Bond Index. This Index includes fixed income securities issued by the U.S. Treasury (not including inflation-protected securities), all with maturities between 1 and 3 years.\\n\\nThe Fund invests by sampling the Index, meaning that it holds a range of securities that, in the aggregate, approximates the full Index in terms of key risk factors and other characteristics. All of the Fund\\u2019s investments will be selected through the sampling process, and under normal circumstances, at least 80% of the Fund\\u2019s assets will be invested in bonds included in the Index. The Fund maintains a dollar-weighted average maturity consistent with that of the Index. As of August 31, 2018, the dollar-weighted average maturity of the Index was 2.0 years.\\n\\nPrincipal Risks\\n\\nThe Fund is designed for investors with a low tolerance for risk, but you could still lose money by investing in it. The Fund is subject to the following risks, which could affect the Fund\\u2019s performance:\\n\\n\\u2022 Interest rate risk, which is the chance that bond prices will decline because of rising interest rates. Interest rate risk should be low for the Fund because it invests primarily in short-term bonds, whose prices are less sensitive to interest rate changes than are the prices of longer-term bonds.\\n\\n\\u2022 Income risk, which is the chance that the Fund\\u2019s income will decline because of falling interest rates. Income risk is generally high for short-term bond funds, so investors should expect the Fund\\u2019s monthly income to fluctuate.\\n\\n\\u2022 Index sampling risk, which is the chance that the securities selected for the Fund, in the aggregate, will not provide investment performance matching that of the Fund\\u2018s target index. Index sampling risk for the Fund is expected to be low.\\n\\nAn investment in the Fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency.\\n\\n2\\n\\n \\n\\nAnnual Total Returns\\n\\nThe following bar chart and table are intended to help you understand the risks of investing in the Fund. The bar chart shows how the performance of the Fund\\u2019s Admiral Shares has varied from one calendar year to another over the periods shown. The table shows how the average annual total returns of the Admiral Shares compare with those of the Fund\\u2019s target index, which has investment characteristics similar to those of the Fund. The Fund\\u2019s Signal\\u00ae Shares were renamed Admiral Shares on October 16, 2013. Keep in mind that the Fund\\u2019s past performance (before and after taxes) does not indicate how the Fund will perform in the future. Updated performance information is available on our website at vanguard.com/performance or by calling Vanguard toll-free at 800-662-7447.\\n\\nAnnual Total Returns \\u2014 Vanguard Short-Term Treasury Index Fund Admiral Shares1\\n\\n\\n1 The year-to-date return as of the most recent calendar quarter, which ended on September 30, 2018, was 0.17%.\\n\\nDuring the periods shown in the bar chart, the highest return for a calendar quarter was 1.16% (quarter ended June 30, 2010), and the lowest return for a quarter was \\u20130.45% (quarter ended December 31, 2016).\\n\\n3\\n\\n \\n\\nAverage Annual Total Returns for Periods Ended December 31, 2017\\t \\t \\n \\t \\t \\tSince\\n \\t \\t \\tInception\\n \\t \\t \\t(Dec. 28,\\n \\t1 Year\\t5 Years\\t2009)\\nVanguard Short-Term Treasury Index Fund Admiral Shares\\t \\t \\t \\nReturn Before Taxes\\t0.40%\\t0.50%\\t0.81%\\nReturn After Taxes on Distributions\\t\\u20130.08\\t0.20\\t0.54\\nReturn After Taxes on Distributions and Sale of Fund Shares\\t0.22\\t0.25\\t0.51\\nBloomberg Barclays US Treasury 1-3 Year Bond Index\\t \\t \\t \\n(reflects no deduction for fees, expenses, or taxes)\\t0.42%\\t0.57%\\t0.89%\\n \\n\\nActual after-tax returns depend on your tax situation and may differ from those shown in the preceding table. When after-tax returns are calculated, it is assumed that the shareholder was in the highest individual federal marginal income tax bracket at the time of each distribution of income or capital gains or upon redemption. State and local income taxes are not reflected in the calculations. Please note that after-tax returns are not relevant for a shareholder who holds fund shares in a tax-deferred account, such as an individual retirement account or a 401(k) plan. Also, figures captioned Return After Taxes on Distributions and Sale of Fund Shares may be higher than other figures for the same period if a capital loss occurs upon redemption and results in an assumed tax deduction for the shareholder.\\n\\nInvestment Advisor\\nThe Vanguard Group, Inc. (Vanguard)\\n\\nPortfolio Manager\\n\\nJoshua C. Barrickman, CFA, Principal of Vanguard and head of Vanguard\\u2019s Fixed Income Indexing Americas. He has managed the Fund since 2013.\\n\\n4\\n\\n \\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase or redeem shares online through our website (vanguard.com), by mail (The Vanguard Group, P.O. Box 1110, Valley Forge, PA 19482-1110), or by telephone (800-662-2739). The minimum investment amount required to open and maintain a Fund account for Admiral Shares is $3,000. The minimum investment amount required to add to an existing Fund account is generally $1. Financial intermediaries, institutional, and Vanguard-advised clients should contact Vanguard for information on special eligibility rules that may apply to them regarding Admiral Shares. If you are investing through an intermediary, please contact that firm directly for more information regarding your eligibility. If you are investing through an employer-sponsored retirement or savings plan, your plan administrator or your benefits office can provide you with detailed information on how you can invest through your plan.\\n\\nTax Information\\n\\nThe Fund\\u2019s distributions may be taxable as ordinary income or capital gain. If you are investing through a tax-advantaged account, such as an IRA or an employer-sponsored retirement or savings plan, special tax rules apply.\\n\\nPayments to Financial Intermediaries\\n\\nThe Fund and its investment advisor do not pay financial intermediaries for sales of Fund shares.\",\n          \"Eaton Vance TABS Intermediate-Term Municipal Bond Fund\\n\\nInvestment Objective\\n\\nThe Fund\\u2019s investment objective is to seek after-tax total return.\\n\\nFees and Expenses of the Fund\\n\\nThis table describes the fees and expenses that you may pay if you buy and hold shares of the Fund. Investors may also pay commissions or other fees to their financial intermediary when they buy and hold shares of the Fund, which are not reflected below. You may qualify for a reduced sales charge on purchases of Class A shares if you invest, or agree to invest over a 13-month period, at least $100,000 in Eaton Vance funds. Certain financial intermediaries also may offer variations in Fund sales charges to their customers as described in Appendix A \\u2013 Financial Intermediary Sales Charge Variations in this Prospectus. More information about these and other discounts is available from your financial intermediary and in Sales Charges beginning on page 20 of this Prospectus and page 21 of the Fund\\u2019s Statement of Additional Information.\\n\\nShareholder Fees (fees paid directly from your investment)\\tClass A\\tClass C\\tClass I\\nMaximum Sales Charge (Load) Imposed on Purchases (as a percentage of offering price)\\t2.25%\\tNone\\tNone\\nMaximum Deferred Sales Charge (Load) (as a percentage of the lower of net asset value at purchase or redemption)\\tNone\\t1.00%\\tNone\\n \\n\\nAnnual Fund Operating Expenses (expenses you pay each year as a percentage of the value of your investment)\\tClass A\\tClass C\\tClass I\\nManagement Fees\\t0.60%\\t0.60%\\t0.60%\\nDistribution and Service (12b-1) Fees\\t0.25%\\t1.00%\\tNone\\nOther Expenses\\t0.12%\\t0.12%\\t0.12%\\nTotal Annual Fund Operating Expenses\\t0.97%\\t1.72%\\t0.72%\\nExpense Reimbursement (1)\\t(0.07)%\\t(0.07)%\\t(0.07)%\\nTotal Annual Fund Operating Expenses After Expense Reimbursement\\t0.90%\\t1.65%\\t0.65%\\n(1)\\tThe investment adviser and administrator has agreed to reimburse the Fund\\u2019s expenses to the extent that Total Annual Fund Operating Expenses exceed 0.90% for Class A shares, 1.65% for Class C shares and 0.65% for Class I shares. This expense reimbursement will continue through May 31, 2019. Any amendment to or termination of this reimbursement would require approval of the Board of Trustees. The expense reimbursement relates to ordinary operating expenses only and does not include expenses such as: brokerage commissions, acquired fund fees and expenses of unaffiliated funds, interest expense, taxes or litigation expenses. Amounts reimbursed may be recouped by the investment adviser and administrator during the same fiscal year to the extent actual expenses are less than the contractual expense cap during such year.\\nExample. This Example is intended to help you compare the cost of investing in the Fund with the cost of investing in other mutual funds. The Example assumes that you invest $10,000 in the Fund for the time periods indicated and then redeem all of your shares at the end of those periods. The Example also assumes that your investment has a 5% return each year, that the operating expenses remain the same and that any expense reimbursement arrangement remains in place for the contractual period. Although your actual costs may be higher or lower, based on these assumptions your costs would be:\\n\\n \\tExpenses with Redemption\\tExpenses without Redemption\\n \\t1 Year\\t3 Years\\t5 Years\\t10 Years\\t1 Year\\t3 Years\\t5 Years\\t10 Years\\nClass A shares\\t$315\\t$520\\t$743\\t$1,382\\t$315\\t$520\\t$743\\t$1,382\\nClass C shares\\t$268\\t$535\\t$927\\t$2,024\\t$168\\t$535\\t$927\\t$2,024\\nClass I shares\\t$66\\t$223\\t$394\\t$888\\t$66\\t$223\\t$394\\t$888\\nPortfolio Turnover\\n\\nThe Fund pays transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d the portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when Fund shares are held in a taxable account. These costs, which are not reflected in Annual Fund Operating Expenses or in the Example, affect the Fund\\u2019s performance. During the most recent fiscal year, the Fund's portfolio turnover rate was 62% of the average value of its portfolio.\\n\\nEaton Vance TABS Municipal Bond Funds\\t7\\tProspectus dated June 1, 2018\\n \\n \\n\\nPrincipal Investment Strategies\\n\\nUnder normal market conditions, the Fund invests at least 80% of its net assets (plus any borrowings for investment purposes) in a diversified portfolio of municipal obligations the interest on which is exempt from regular federal income tax (the \\u201c80% Policy\\u201d). In seeking the Fund\\u2019s investment objective, the portfolio managers emphasize tax-exempt income. The Fund normally invests in municipal obligations rated in the three highest rating categories (those rated A or higher by S&P Global Ratings (\\u201cS&P\\u201d), Fitch Ratings (\\u201cFitch\\u201d) or Moody\\u2019s Investors Service, Inc. (\\u201cMoody\\u2019s\\u201d)) or, if unrated, determined by the investment adviser to be of comparable quality at the time of purchase. The Fund will not invest more than 50% of its net assets in municipal obligations rated A at the time of purchase by S&P, Fitch or Moody\\u2019s or, if unrated determined by the investment adviser to be of comparable quality. For purposes of rating restrictions, if securities are rated differently by two or more rating agencies, the highest rating is used. The Fund may continue to hold securities that are downgraded (including bonds downgraded to below investment grade credit quality (\\u201cjunk bonds\\u201d)) if the investment adviser believes it would be advantageous to do so. The Fund will not invest in a municipal obligation the interest on which the Fund\\u2019s investment adviser believes is subject to the federal alternative minimum tax.\\n\\nFor its investment in municipal obligations, the Fund invests primarily in general obligation or revenue bonds. The Fund currently targets an average portfolio duration of approximately 5 - 7 years and an average weighted portfolio maturity of approximately 5 - 13 years, but may invest in securities of any maturity or duration, and may in the future alter its maturity or duration target range. The Fund may use various techniques to shorten or lengthen its dollar weighted average portfolio duration, including the acquisition of municipal obligations at a premium or discount. The portfolio managers generally will seek to enhance after-tax total return by actively engaging in relative value trading within the portfolio to take advantage of price opportunities in the markets for municipal obligations. With respect to 20% of its net assets, the Fund may invest in municipal obligations that are not exempt from regular federal income tax, direct obligations of the U.S. Treasury and/or obligations of U.S. Government agencies, instrumentalities and government-sponsored enterprises. The Fund may also invest in cash and money market instruments.\\n\\nThe investment adviser\\u2019s process for selecting municipal obligations for purchase and sale generally includes consideration of the creditworthiness of the issuer or person obligated to repay the obligation. In evaluating creditworthiness, the investment adviser considers ratings assigned by rating agencies and generally performs additional credit and investment analysis.\\n\\nPrincipal Risks\\n\\nMarket Risk. The value of investments held by the Fund may increase or decrease in response to economic, political and financial events (whether real, expected or perceived) in the U.S. and global markets. The frequency and magnitude of such changes in value cannot be predicted. Certain securities and other investments held by the Fund may experience increased volatility, illiquidity, or other potentially adverse effects in reaction to changing market conditions. Actions taken by the U.S. Federal Reserve or foreign central banks to stimulate or stabilize economic growth, such as decreases or increases in short-term interest rates, could cause high volatility in markets. No active trading market may exist for certain investments, which may impair the ability of the Fund to sell or to realize the current valuation of such investments in the event of the need to liquidate such assets. Fixed-income markets may experience periods of relatively high volatility in an environment where U.S. treasury yields are rising.\\n\\nMunicipal Obligation Risk. The amount of public information available about municipal obligations is generally less than for corporate equities or bonds, meaning that the investment performance of municipal obligations may be more dependent on the analytical abilities of the investment adviser than stock or corporate bond investments. The secondary market for municipal obligations also tends to be less well-developed and less liquid than many other securities markets, which may limit the Fund\\u2019s ability to sell its municipal obligations at attractive prices. The differences between the price at which an obligation can be purchased and the price at which it can be sold may widen during periods of market distress. Less liquid obligations can become more difficult to value and be subject to erratic price movements. The increased presence of nontraditional participants (such as proprietary trading desks of investment banks and hedge funds) or the absence of traditional participants (such as individuals, insurance companies, banks and life insurance companies) in the municipal markets may lead to greater volatility in the markets because non-traditional participants may trade more frequently or in greater volume. \\n\\nInterest Rate Risk. In general, the value of income securities will fluctuate based on changes in interest rates. The value of these securities is likely to increase when interest rates fall and decline when interest rates rise. Generally, securities with longer durations are more sensitive to changes in interest rates than shorter duration securities, causing them to be more volatile. Conversely, fixed income securities with shorter durations will be less volatile but may provide lower returns than fixed income securities with longer durations. In a rising interest rate environment, the durations of income securities that have the ability to be prepaid or called by the issuer may be extended. In a declining interest rate environment, the proceeds from prepaid or maturing instruments may have to be reinvested at a lower interest rate.\\n\\nEaton Vance TABS Municipal Bond Funds\\t8\\tProspectus dated June 1, 2018\\n \\nCredit Risk. Investments in municipal obligations and other debt obligations (referred to below as \\u201cdebt instruments\\u201d) are subject to the risk of non-payment of scheduled principal and interest. Changes in economic conditions or other circumstances may reduce the capacity of the party obligated to make principal and interest payments on such instruments and may lead to defaults. Such non-payments and defaults may reduce the value of Fund shares and income distributions. The value of debt instruments also may decline because of concerns about the issuer\\u2019s ability to make principal and interest payments. In addition, the credit ratings of debt instruments may be lowered if the financial condition of the party obligated to make payments with respect to such instruments deteriorates. In order to enforce its rights in the event of a default, bankruptcy or similar situation, the Fund may be required to retain legal or similar counsel, which may increase the Fund\\u2019s operating expenses and adversely affect net asset value. Municipal obligations may be insured as to principal and interest payments. If the claims-paying ability or other rating of the insurer is downgraded by a rating agency, the value of such obligations may be negatively affected.\\n\\nRisks of Principal Only Investments. Principal only investments entitle the Fund to receive the stated value of such investment when held to maturity. The values of principal only investments are subject to greater fluctuation in response to changes in market interest rates than obligations that pay interest currently. The Fund will accrue income on these investments and distribute that income each year. The Fund may be required to sell other investments to obtain cash needed for such income distributions.\\n\\nU.S. Government Securities Risk. Although certain U.S. Government-sponsored agencies (such as the Federal Home Loan Mortgage Corporation and the Federal National Mortgage Association) may be chartered or sponsored by acts of Congress, their securities are neither issued nor guaranteed by the U.S. Treasury. U.S. Treasury securities generally have a lower return than other obligations because of their higher credit quality and market liquidity.\\n\\nTax Risk. Income from tax-exempt municipal obligations could be declared taxable because of changes in tax laws, adverse interpretations by the relevant taxing authority or the non-compliant conduct of the issuer of an obligation.\\n\\nMoney Market Instrument Risk. Money market instruments may be adversely affected by market and economic events, such as a sharp rise in prevailing short-term interest rates; adverse developments in the banking industry, which issues or guarantees many money market instruments; adverse economic, political or other developments affecting issuers of money market instruments; changes in the credit quality of issuers; and default by a counterparty.\\n\\nGeneral Fund Investing Risks. The Fund is not a complete investment program and there is no guarantee that the Fund will achieve its investment objective. It is possible to lose money by investing in the Fund. The Fund is designed to be a long-term investment vehicle and is not suited for short-term trading. Investors in the Fund should have a long-term investment perspective and be able to tolerate potentially sharp declines in value. Purchase and redemption activities by Fund shareholders may impact the management of the Fund and its ability to achieve its investment objective(s). In addition, the redemption by one or more large shareholders or groups of shareholders of their holdings in the Fund could have an adverse impact on the remaining shareholders in the Fund. An investment in the Fund is not a deposit in a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency.\\n\\nPerformance\\n\\nThe following bar chart and table provide some indication of the risks of investing in the Fund by showing changes in the Fund\\u2019s performance from year to year and how the Fund\\u2019s average annual returns over time compare with those of two broad-based securities market indices. The returns in the bar chart are for Class A shares and do not reflect a sales charge. If the sales charge was reflected, the returns would be lower. Past performance (both before and after taxes) is not necessarily an indication of how the Fund will perform in the future. The Fund\\u2019s performance reflects the effects of expense reductions. Absent these reductions, performance would have been lower. Updated Fund performance information can be obtained by visiting www.eatonvance.com.\\n\\n\\n\\nDuring the period from December 31, 2010 to December 31, 2017, the highest quarterly total return for Class A was 3.75% for the quarter ended June 30, 2011, and the lowest quarterly return was \\u20134.07% for the quarter ended December 31, 2016. The year-to-date total return through the end of the most recent calendar quarter (December 31, 2017 to March 31, 2018) was -1.71%.\\n\\nEaton Vance TABS Municipal Bond Funds\\t9\\tProspectus dated June 1, 2018\\n \\n \\n\\nAverage Annual Total Return as of December 31, 2017\\tOne Year\\tFive Years\\tLife of Fund\\nClass A Return Before Taxes\\t2.08%\\t2.12%\\t4.21%\\nClass A Return After Taxes on Distributions\\t2.05%\\t2.08%\\t4.09%\\nClass A Return After Taxes on Distributions and the Sale of Class A Shares\\t1.94%\\t1.97%\\t3.59%\\nClass C Return Before Taxes\\t2.67%\\t1.82%\\t3.74%\\nClass I Return Before Taxes\\t4.71%\\t2.82%\\t4.78%\\nBloomberg Barclays Municipal Managed Money Intermediate (1-17 Year) Bond Index (reflects no deduction for fees, expenses or taxes)\\t4.88%\\t2.55%\\t4.26%\\nBloomberg Barclays 7 Year Municipal Bond Index (reflects no deduction for fees, expenses or taxes)\\t4.49%\\t2.44%\\t4.30%\\nThese returns reflect the maximum sales charge for Class A (2.25%) and any applicable contingent deferred sales charge (\\u201cCDSC\\u201d) for Class C. Class A, Class C and Class I commenced operations on February 1, 2010. Effective February 17, 2015, the Fund changed its name, objective and investment strategy to invest at least 80% of its net assets in a diversified portfolio of municipal obligations, the interest on which is exempt from regular federal income tax. Investors cannot invest directly in an Index.\\n\\nAfter-tax returns are calculated using the highest historical individual federal income tax rates and does not reflect the impact of state and local taxes. Actual after-tax returns depend on a shareholder\\u2019s tax situation and the actual characterization of distributions, and may differ from those shown. After-tax returns are not relevant to shareholders who hold shares in tax-deferred accounts or to shares held by non-taxable entities. After-tax returns for other Classes of shares will vary from the after-tax returns presented for Class A shares. Return After Taxes on Distributions for a period may be the same as Return Before Taxes for that period because no taxable distributions were made during that period. Also, Return After Taxes on distributions and Sale of Fund Shares for a period may be greater than or equal to Return After Taxes on Distributions for the same period because of losses realized on the sale of Fund shares.\\n\\nManagement\\n\\nInvestment Adviser. Eaton Vance Management (\\u201cEaton Vance\\u201d).\\n\\nPortfolio Managers\\n\\nThe portfolio managers of the Fund are part of Eaton Vance\\u2019s Tax-Advantaged Bond Strategies (\\u201cTABS\\u201d) division.\\n\\nJames H. Evans, (lead portfolio manager), Vice President of Eaton Vance, has managed the Fund since it commenced operations in February 2010.\\n\\nBrian C. Barney, Vice President of Eaton Vance, has managed the Fund since June 2010.\\n\\nChristopher J. Harshman, Vice President of Eaton Vance, has managed the Fund since June 2010.\\n\\nFor important information about purchase and sale of shares, taxes and financial intermediary compensation, please turn to \\u201cImportant Information Regarding Fund Shares\\u201d on page 11 of this Prospectus.\\n\\nEaton Vance TABS Municipal Bond Funds\\t10\\tProspectus dated June 1, 2018\\n \\n \\n\\nImportant Information Regarding Fund Shares\\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase, redeem or exchange Fund shares on any business day, which is any day the New York Stock Exchange is open for business. You may purchase, redeem or exchange Fund shares either through your financial intermediary or (except for purchases of Class C shares by accounts with no specified financial intermediary) directly from a Fund either by writing to the Fund, P.O. Box 9653, Providence, RI 02940-9653, or by calling 1-800-262-1122. The minimum initial purchase or exchange into a Fund is $1,000 for each Class (with the exception of Class I) and $250,000 for Class I (waived in certain circumstances). There is no minimum for subsequent investments.\\n\\nTax Information\\n\\nEach Fund\\u2019s distributions are expected to primarily be exempt from regular federal income tax. However, the Fund may also distribute taxable income to the extent that it invests in taxable municipal obligations or other obligations which generate taxable income. Distributions of any net realized gains are expected to be taxable.\\n\\nPayments to Broker-Dealers and Other Financial Intermediaries\\n\\nIf you purchase a Fund\\u2019s shares through a broker-dealer or other financial intermediary (such as a bank) (collectively, \\u201cfinancial intermediaries\\u201d), the Fund, its principal underwriter and its affiliates may pay the financial intermediary for the sale of Fund shares and related services. These payments may create a conflict of interest by influencing the financial intermediary and your salesperson to recommend a Fund over another investment. Ask your salesperson or visit your financial intermediary\\u2019s website for more information.\",\n          \"Fund Summary\\n\\nFund/Class:\\nFidelity Freedom\\u00ae 2050 Fund/Fidelity Freedom\\u00ae 2050 Fund\\n\\nInvestment Objective\\n\\nThe fund seeks high total return until its target retirement date. Thereafter the fund's objective will be to seek high current income and, as a secondary objective, capital appreciation.\\n\\nFee Table\\n\\nThe following table describes the fees and expenses that may be incurred when you buy and hold shares of the fund.\\n\\nShareholder fees\\n\\n(fees paid directly from your investment) \\tNone \\nAnnual Operating Expenses\\n\\n(expenses that you pay each year as a % of the value of your investment)\\n\\nManagement fee(a) \\t \\t0.75% \\nDistribution and/or Service (12b-1) fees \\t \\tNone \\nOther expenses(a) \\t \\t0.00% \\nTotal annual operating expenses(a) \\t \\t0.75% \\n(a)   Adjusted to reflect current fees.\\n\\nThis example helps compare the cost of investing in the fund with the cost of investing in other funds.\\n\\nLet's say, hypothetically, that the annual return for shares of the fund is 5% and that your shareholder fees and the annual operating expenses for shares of the fund are exactly as described in the fee table. This example illustrates the effect of fees and expenses, but is not meant to suggest actual or expected fees and expenses or returns, all of which may vary. For every $10,000 you invested, here's how much you would pay in total expenses if you sell all of your shares at the end of each time period indicated:\\n\\n1 year \\t$76 \\n3 years \\t$238 \\n5 years \\t$415 \\n10 years \\t$926 \\nPortfolio Turnover\\n\\nThe fund will not incur transaction costs, such as commissions, when it buys and sells shares of underlying Fidelity\\u00ae funds (or \\\"turns over\\\" its portfolio), but it could incur transaction costs if it were to buy and sell other types of securities directly. If the fund were to buy and sell other types of securities directly, a higher portfolio turnover rate could indicate higher transaction costs and could result in higher taxes when fund shares are held in a taxable account. Such costs, if incurred, would not be reflected in annual operating expenses or in the example and would affect the fund's performance. During the most recent fiscal year, the fund's portfolio turnover rate was 16% of the average value of its portfolio.\\n\\nPrincipal Investment Strategies\\n\\nInvesting in a combination of Fidelity\\u00ae domestic equity funds, international equity funds (developed and emerging markets), bond funds, and short-term funds (underlying Fidelity\\u00ae funds).\\nAllocating assets according to a neutral asset allocation strategy shown in the glide path below that becomes increasingly conservative until it reaches an allocation similar to that of the Fidelity Freedom\\u00ae Income Fund, approximately 10 to 19 years after the year 2050 (approximately 17% in domestic equity funds, 7% in international equity funds, 46% in bond funds, and 30% in short-term funds).\\nBuying and selling futures contracts (both long and short positions) in an effort to manage cash flows efficiently, remain fully invested, or facilitate asset allocation.\\nFMR Co., Inc. (FMRC) may continue to seek high total return for several years beyond the fund's target retirement date in an effort to achieve the fund's overall investment objective.\\n\\nAs of March 31, 2018, the fund's neutral asset allocation to underlying Fidelity\\u00ae funds and futures was approximately:\\n   \\tDomestic Equity Funds* \\t63% \\n   \\tInternational Equity Funds* \\t27% \\n   \\tBond Funds* \\t10% \\n \\tShort-Term Funds* \\t0% \\n\\n* FMRC may change these percentages over time. As a result of the active asset allocation strategy (discussed below), actual allocations may differ from the neutral allocations above. The allocation percentages may not add to 100% due to rounding.\\n\\nFMRC may use an active asset allocation strategy to increase or decrease neutral asset class exposures reflected above by up to 10% for equity funds (includes domestic equity and international equity funds), bond funds and short-term funds to reflect FMRC's market outlook, which is primarily focused on the intermediate term. The asset allocations in the glide path and pie chart above are referred to as neutral because they do not reflect any decisions made by FMRC to overweight or underweight an asset class.\\nFMRC may also make active asset allocations within other asset classes (including commodities, high yield debt, floating rate debt, real estate debt, inflation-protected debt, and emerging markets debt) from 0% to 10% individually but no more than 25% in aggregate within those other asset classes. Such asset classes are not reflected in the neutral asset allocations reflected in the glide path and pie chart above.\\nDesigned for investors who anticipate retiring in or within a few years of 2050 (target retirement date) at or around age 65 and plan to gradually withdraw the value of their account in the fund over time.\\nPrincipal Investment Risks\\n\\nShareholders should consider that no target date fund is intended as a complete retirement program and there is no guarantee that any single fund will provide sufficient retirement income at or through your retirement. The fund's share price fluctuates, which means you could lose money by investing in the fund, including losses near, at or after the target retirement date.\\n\\nAsset Allocation Risk.  The fund is subject to risks resulting from the Adviser's asset allocation decisions. The selection of underlying funds and the allocation of the fund's assets among various asset classes could cause the fund to lose value or its results to lag relevant benchmarks or other funds with similar objectives. In addition, the fund's active asset allocation strategy may cause the fund to have a risk profile different than that portrayed above from time to time and may increase losses.\\nInvesting in Other Funds.  The fund bears all risks of investment strategies employed by the underlying funds, including the risk that the underlying funds will not meet their investment objectives.\\nStock Market Volatility.  Stock markets are volatile and can decline significantly in response to adverse issuer, political, regulatory, market, or economic developments. Different parts of the market, including different market sectors, and different types of securities can react differently to these developments.\\nInterest Rate Changes.  Interest rate increases can cause the price of a debt or money market security to decrease.\\nForeign Exposure.  Foreign markets, particularly emerging markets, can be more volatile than the U.S. market due to increased risks of adverse issuer, political, regulatory, market, or economic developments and can perform differently from the U.S. market. Emerging markets can be subject to greater social, economic, regulatory, and political uncertainties and can be extremely volatile. Foreign exchange rates also can be extremely volatile.\\nIndustry Exposure.  Market conditions, interest rates, and economic, regulatory, or financial developments could significantly affect a single industry or group of related industries.\\nIssuer-Specific Changes.  The value of an individual security or particular type of security can be more volatile than, and can perform differently from, the market as a whole. Lower-quality debt securities (those of less than investment-grade quality, also referred to as high yield debt securities or junk bonds) and certain types of other securities involve greater risk of default or price changes due to changes in the credit quality of the issuer. The value of lower-quality debt securities and certain types of other securities can be more volatile due to increased sensitivity to adverse issuer, political, regulatory, market, or economic developments.\\nLeverage Risk.  Leverage can increase market exposure, magnify investment risks, and cause losses to be realized more quickly.\\n\\\"Growth\\\" Investing.  \\\"Growth\\\" stocks can perform differently from the market as a whole and other types of stocks and can be more volatile than other types of stocks.\\n\\\"Value\\\" Investing.  \\\"Value\\\" stocks can perform differently from the market as a whole and other types of stocks and can continue to be undervalued by the market for long periods of time.\\nCommodity-Linked Investing.  The value of commodities and commodity-linked investments may be affected by the performance of the overall commodities markets as well as weather, political, tax, and other regulatory and market developments. Commodity-linked investments may be more volatile and less liquid than the underlying commodity, instruments, or measures.\\nAn investment in the fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency. You could lose money by investing in the fund.\\n\\nPerformance\\n\\nThe following information is intended to help you understand the risks of investing in the fund. The information illustrates the changes in the performance of the fund's shares from year to year and compares the performance of the fund's shares to the performance of a securities market index and a hypothetical composite of market indexes over various periods of time. The indexes have characteristics relevant to the fund's investment strategies. Index descriptions appear in the \\\"Additional Index Information\\\" section of the prospectus. Prior to June 1, 2017, the fund operated under a different pricing structure. The fund\\u2019s historical performance prior to June 1, 2017 does not reflect the fund\\u2019s current pricing structure. Past performance (before and after taxes) is not an indication of future performance.\\n\\nVisit www.fidelity.com for more recent performance information.\\n\\nYear-by-Year Returns\\n\\n\\n\\nDuring the periods shown in the chart: \\tReturns \\tQuarter ended \\nHighest Quarter Return \\t19.11% \\tJune 30, 2009 \\nLowest Quarter Return \\t(23.40)% \\tDecember 31, 2008 \\nYear-to-Date Return \\t(0.25)% \\tMarch 31, 2018 \\nAverage Annual Returns\\n\\nAfter-tax returns are calculated using the historical highest individual federal marginal income tax rates, but do not reflect the impact of state or local taxes. Actual after-tax returns may differ depending on your individual circumstances. The after-tax returns shown are not relevant if you hold your shares in a retirement account or in another tax-deferred arrangement, such as an employee benefit plan (profit sharing, 401(k), or 403(b) plan).\\n\\nFor the periods ended December 31, 2017 \\tPast 1 year \\tPast 5 years \\tPast 10 years \\nFidelity Freedom\\u00ae 2050 Fund \\nReturn Before Taxes \\t22.28% \\t11.30% \\t5.31% \\nReturn After Taxes on Distributions \\t21.05% \\t9.54% \\t4.09% \\nReturn After Taxes on Distributions and Sale of Fund Shares \\t13.44% \\t8.52% \\t3.85% \\nS&P 500\\u00ae Index\\n(reflects no deduction for fees, expenses, or taxes) \\t21.83% \\t15.79% \\t8.50% \\nFidelity Freedom 2050 Composite Index\\u2120\\n(reflects no deduction for fees or expenses) \\t20.95% \\t12.05% \\t6.60% \\nInvestment Adviser\\n\\nFMRC (the Adviser), an affiliate of Fidelity Management & Research Company (FMR), is the fund's manager.\\n\\nPortfolio Manager(s)\\n\\nAndrew Dierdorf (co-manager) has managed the fund since June 2011.\\n\\nBrett Sumsion (co-manager) has managed the fund since January 2014.\\n\\nPurchase and Sale of Shares\\n\\nYou may buy or sell shares through a Fidelity\\u00ae brokerage or mutual fund account, through a retirement account, or through an investment professional. You may buy or sell shares in various ways:\\n\\nInternet\\n\\nwww.fidelity.com\\n\\nPhone\\n\\nFidelity Automated Service Telephone (FAST\\u00ae) 1-800-544-5555\\n\\nTo reach a Fidelity representative 1-800-544-6666\\n\\nMail\\n\\nAdditional purchases:\\n\\nFidelity Investments\\nP.O. Box 770001\\nCincinnati, OH 45277-0003\\nRedemptions:\\n\\nFidelity Investments\\nP.O. Box 770001\\nCincinnati, OH 45277-0035\\nTDD- Service for the Deaf and Hearing Impaired\\n\\n1-800-544-0118\\n\\nThe price to buy one share is its net asset value per share (NAV). Shares will be bought at the NAV next calculated after an order is received in proper form.\\n\\nThe price to sell one share is its NAV. Shares will be sold at the NAV next calculated after an order is received in proper form.\\n\\nThe fund is open for business each day the New York Stock Exchange (NYSE) is open.\\n\\nInitial Purchase Minimum \\t$2,500 \\nFor Fidelity\\u00ae Simplified Employee Pension-IRA, Keogh, and Investment Only Retirement accounts \\t$500 \\nThrough regular investment plans in Fidelity\\u00ae Traditional IRAs, Roth IRAs, and Rollover IRAs (requires monthly purchases of $200 until fund balance is $2,500) \\t$200 \\nThe fund may waive or lower purchase minimums in other circumstances.\\n\\nTax Information\\n\\nDistributions you receive from the fund are subject to federal income tax and generally will be taxed as ordinary income or capital gains, and may also be subject to state or local taxes, unless you are investing through a tax-advantaged retirement account (in which case you may be taxed later, upon withdrawal of your investment from such account).\\n\\nPayments to Broker-Dealers and Other Financial Intermediaries\\n\\nThe fund, the Adviser, Fidelity Distributors Corporation (FDC), and/or their affiliates may pay intermediaries, which may include banks, broker-dealers, retirement plan sponsors, administrators, or service-providers (who may be affiliated with the Adviser or FDC), for the sale of fund shares and related services. These payments may create a conflict of interest by influencing your intermediary and your investment professional to recommend the fund over another investment. Ask your investment professional or visit your intermediary's web site for more information.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_distance\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 253,\n        \"samples\": [\n          \"2 \\tWith limited exceptions, for Class A shares, if your Fund account balance is below $650 at the start of business on the Friday prior to the last full week of September of each year, the account will be assessed an account fee of $20. 3 \\tThrough July 31, 2019, Ivy Investment Management Company (IICO), the Fund\\u2019s investment manager, Ivy Distributors, Inc. (IDI), the Fund\\u2019s distributor, and/or Waddell & Reed Services Company, doing business as WI Services Company (WISC), the Fund\\u2019s transfer agent, have contractually agreed to reimburse sufficient management fees, 12b-1 fees and/or shareholder servicing fees to cap the total annual ordinary fund operating expenses (which would exclude interest, taxes, brokerage commissions, acquired fund fees and expenses and extraordinary expenses, if any) as follows: Class E shares at 1.10%. 4\\t \\tProspectus\\t \\tDomestic Equity Funds\\nTable of Contents\\nExample\\n\\nThis example is intended to help you compare the cost of investing in the shares of the Fund with the cost of investing in other mutual funds. 4 \\tThrough July 31, 2020, IICO, IDI and/or WISC have contractually agreed to reimburse sufficient management fees, 12b-1 fees and/or shareholder servicing fees to cap the total annual ordinary fund operating expenses (which would exclude interest, taxes, brokerage commissions, acquired fund fees and expenses and extraordinary expenses, if any) as follows: Class A shares at 1.04%; Class B shares at 2.13%; Class E shares at 1.13%; and Class I shares and Class Y shares at 0.84%. 5 \\tThrough July 31, 2020, IDI and/or WISC have contractually agreed to reimburse sufficient 12b-1 and/or shareholder servicing fees to ensure that the total annual ordinary fund operating expenses of the Class Y shares do not exceed the total annual ordinary fund operating expenses of the Class A shares, as calculated at the end of each month.\",\n          \"A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when fund shares are held in a taxable account. An investment in the fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency. Ask your investment professional or visit your intermediary's web site for more information. Box 770001\\nCincinnati, OH 45277-0035\\nOvernight Express:\\nFidelity Investments\\n100 Crosby Parkway\\nCovington, KY 41015\\nTDD- Service for the Deaf and Hearing Impaired\\n\\n1-800-544-0118\\n\\nThe price to buy one share is its net asset value per share (NAV). Currently, the Board of Trustees of the fund has not authorized such payments for shares of the fund.\",\n          \"2\\n\\n \\n\\nAnnual Total Returns\\n\\nThe following bar chart and table are intended to help you understand the risks of investing in the Fund. 3\\n\\n \\n\\nAverage Annual Total Returns for Periods Ended December 31, 2017\\t \\t \\n \\t \\t \\tSince\\n \\t \\t \\tInception\\n \\t \\t \\t(Dec. 28,\\n \\t1 Year\\t5 Years\\t2009)\\nVanguard Short-Term Treasury Index Fund Admiral Shares\\t \\t \\t \\nReturn Before Taxes\\t0.40%\\t0.50%\\t0.81%\\nReturn After Taxes on Distributions\\t\\u20130.08\\t0.20\\t0.54\\nReturn After Taxes on Distributions and Sale of Fund Shares\\t0.22\\t0.25\\t0.51\\nBloomberg Barclays US Treasury 1-3 Year Bond Index\\t \\t \\t \\n(reflects no deduction for fees, expenses, or taxes)\\t0.42%\\t0.57%\\t0.89%\\n \\n\\nActual after-tax returns depend on your tax situation and may differ from those shown in the preceding table. 4\\n\\n \\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase or redeem shares online through our website (vanguard.com), by mail (The Vanguard Group, P.O. A higher portfolio turnover rate may indicate higher transaction costs and may result in more taxes when Fund shares are held in a taxable account. All of the Fund\\u2019s investments will be selected through the sampling process, and under normal circumstances, at least 80% of the Fund\\u2019s assets will be invested in bonds included in the Index.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_match\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 255,\n        \"samples\": [\n          \"Tracking error may occur because of differences between the securities held in the Fund\\u2019s portfolio and those included in the Emerging Markets Underlying Index, pricing differences (including differences between a security\\u2019s price at the local market close and the Fund\\u2019s valuation of a security at the time of calculation of the Fund\\u2019s NAV), transaction costs, the Fund\\u2019s holding of cash, differences in timing of the accrual of dividends or interest, tax gains or losses, changes to the Emerging Markets Underlying Index or the need to meet various new or existing regulatory requirements. Under the representative sampling technique, the investment manager will select securities that collectively have an investment profile similar to that of the Emerging Markets Underlying Index, including securities that resemble those included in the Emerging Markets Underlying Index in terms of risk factors, performance attributes and other characteristics, such as market capitalization and industry weightings. When there are more sellers than buyers, prices tend to fall. While MSCI provides descriptions of what the Emerging Markets Underlying Index is designed to achieve, MSCI does not guarantee the quality, accuracy or completeness of data in respect of its indices, and does not guarantee that the Emerging Markets Underlying Index will be in line with the described index methodology. You can obtain updated performance information at libertyshares.com or by calling (800) DIAL BEN/342-5236. You may also incur usual and customary brokerage commissions when buying or selling shares of the Fund, which are not reflected in the Example that follows.\",\n          \"This example does not include any fees paid at the fee-based account or plan level. This example helps compare the cost of investing in the fund with the cost of investing in other funds. This example illustrates the effect of fees and expenses, but is not meant to suggest actual or expected fees and expenses or returns, all of which may vary. Using fundamental analysis of factors such as each issuer's financial condition and industry position, as well as market and economic conditions, to select investments. You could lose money by investing in the fund. You may buy or sell shares in various ways:\\n\\nInternet\\n\\nPlan Accounts:\\n\\nwww.401k.com\\t\\nAll Other Accounts:\\n\\nwww.fidelity.com\\nPhone\\n\\nPlan Accounts:\\n\\nFor Individual Accounts (investing through a retirement plan sponsor or other institution), refer to your plan materials or contact that institution directly.\",\n          \"\\u220e\\t \\tLow-Rated Securities Risk. \\u220e\\t \\tManagement Risk. \\u220e\\t \\tMarket Risk. \\u220e\\t \\tMortgage-Backed and Asset-Backed Securities Risk. \\u220e\\t \\tU.S. Government Securities Risk. \\u220e\\t \\tValue Stock Risk.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def create_CNN_model():\n",
        "    CNN = Sequential()\n",
        "    CNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=True))\n",
        "\n",
        "    CNN.add(Convolution1D(256, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Convolution1D(128, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Convolution1D(64, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Flatten())\n",
        "    CNN.add(Dense(units = 512 , activation = 'relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(Dropout(0.3))\n",
        "    CNN.add(Dense(units = 256 , activation = 'relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(Dropout(0.3))\n",
        "    CNN.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "    opt = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "    CNN.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    return CNN\n",
        "\n",
        "CNN_model = create_CNN_model()\n",
        "\n",
        "# 创建ModelCheckpoint回调函数\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5',\n",
        "                             monitor='val_accuracy',\n",
        "                             save_best_only=True,\n",
        "                             mode='max',\n",
        "                             verbose=1)\n",
        "\n",
        "class_weights = {0: 1.0, 1: 0.5, 2: 3.0}\n",
        "CNN_history = CNN_model.fit(feature_train, label_train_y,\n",
        "                            epochs=700, batch_size=128,\n",
        "                            validation_data=(feature_valid, label_valid_y),\n",
        "                            class_weight=class_weights,\n",
        "                            callbacks=[checkpoint])  # 将回调函数传递给fit函数"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gRNEmwO77Oy",
        "outputId": "9cb042fd-b477-404f-b9ee-68e66ecb9cf5"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/700\n",
            "1/3 [=========>....................] - ETA: 47s - loss: 8.1034 - accuracy: 0.2969\n",
            "Epoch 1: val_accuracy improved from -inf to 0.48649, saving model to best_model.h5\n",
            "3/3 [==============================] - 24s 210ms/step - loss: 8.0705 - accuracy: 0.2755 - val_loss: 7.9020 - val_accuracy: 0.4865\n",
            "Epoch 2/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 8.1411 - accuracy: 0.2656\n",
            "Epoch 2: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 8.0777 - accuracy: 0.2585 - val_loss: 7.8531 - val_accuracy: 0.3108\n",
            "Epoch 3/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.9477 - accuracy: 0.3594\n",
            "Epoch 3: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 7.9210 - accuracy: 0.3231 - val_loss: 7.8073 - val_accuracy: 0.3108\n",
            "Epoch 4/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.8780 - accuracy: 0.2812\n",
            "Epoch 4: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 7.8606 - accuracy: 0.2857 - val_loss: 7.7566 - val_accuracy: 0.3108\n",
            "Epoch 5/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.8727 - accuracy: 0.2812\n",
            "Epoch 5: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 7.8546 - accuracy: 0.2619 - val_loss: 7.6903 - val_accuracy: 0.3108\n",
            "Epoch 6/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.7576 - accuracy: 0.2891\n",
            "Epoch 6: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 7.7395 - accuracy: 0.2687 - val_loss: 7.6199 - val_accuracy: 0.3108\n",
            "Epoch 7/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.6624 - accuracy: 0.2422\n",
            "Epoch 7: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 7.6702 - accuracy: 0.2585 - val_loss: 7.5475 - val_accuracy: 0.3108\n",
            "Epoch 8/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.5246 - accuracy: 0.2734\n",
            "Epoch 8: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 7.6152 - accuracy: 0.2551 - val_loss: 7.4805 - val_accuracy: 0.3108\n",
            "Epoch 9/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.5029 - accuracy: 0.1953\n",
            "Epoch 9: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 7.5342 - accuracy: 0.2551 - val_loss: 7.4090 - val_accuracy: 0.3108\n",
            "Epoch 10/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.4794 - accuracy: 0.2344\n",
            "Epoch 10: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 7.4660 - accuracy: 0.2585 - val_loss: 7.3341 - val_accuracy: 0.3108\n",
            "Epoch 11/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.3493 - accuracy: 0.2031\n",
            "Epoch 11: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 7.3656 - accuracy: 0.2551 - val_loss: 7.2591 - val_accuracy: 0.3108\n",
            "Epoch 12/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.2871 - accuracy: 0.2656\n",
            "Epoch 12: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 7.3142 - accuracy: 0.2551 - val_loss: 7.1811 - val_accuracy: 0.3108\n",
            "Epoch 13/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.2244 - accuracy: 0.2266\n",
            "Epoch 13: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 7.2264 - accuracy: 0.2483 - val_loss: 7.1102 - val_accuracy: 0.3108\n",
            "Epoch 14/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.0937 - accuracy: 0.2734\n",
            "Epoch 14: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 7.1107 - accuracy: 0.2551 - val_loss: 7.0386 - val_accuracy: 0.3108\n",
            "Epoch 15/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.0855 - accuracy: 0.2578\n",
            "Epoch 15: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 7.0618 - accuracy: 0.2517 - val_loss: 6.9639 - val_accuracy: 0.3108\n",
            "Epoch 16/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.0156 - accuracy: 0.2812\n",
            "Epoch 16: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 7.0020 - accuracy: 0.2517 - val_loss: 6.8867 - val_accuracy: 0.3108\n",
            "Epoch 17/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.9511 - accuracy: 0.2344\n",
            "Epoch 17: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.9220 - accuracy: 0.2483 - val_loss: 6.8127 - val_accuracy: 0.3108\n",
            "Epoch 18/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.9084 - accuracy: 0.2891\n",
            "Epoch 18: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.8585 - accuracy: 0.2517 - val_loss: 6.7421 - val_accuracy: 0.3108\n",
            "Epoch 19/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.7421 - accuracy: 0.2188\n",
            "Epoch 19: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 6.7767 - accuracy: 0.2517 - val_loss: 6.6743 - val_accuracy: 0.3108\n",
            "Epoch 20/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.7086 - accuracy: 0.2031\n",
            "Epoch 20: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.7055 - accuracy: 0.2517 - val_loss: 6.6069 - val_accuracy: 0.3108\n",
            "Epoch 21/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.6694 - accuracy: 0.2578\n",
            "Epoch 21: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 6.6588 - accuracy: 0.2517 - val_loss: 6.5362 - val_accuracy: 0.3108\n",
            "Epoch 22/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.6358 - accuracy: 0.2422\n",
            "Epoch 22: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 6.5866 - accuracy: 0.2517 - val_loss: 6.4672 - val_accuracy: 0.3108\n",
            "Epoch 23/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.5074 - accuracy: 0.2969\n",
            "Epoch 23: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 6.5100 - accuracy: 0.2517 - val_loss: 6.3993 - val_accuracy: 0.3108\n",
            "Epoch 24/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.4970 - accuracy: 0.2109\n",
            "Epoch 24: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 6.4583 - accuracy: 0.2551 - val_loss: 6.3330 - val_accuracy: 0.3108\n",
            "Epoch 25/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.4483 - accuracy: 0.2578\n",
            "Epoch 25: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.3798 - accuracy: 0.2517 - val_loss: 6.2696 - val_accuracy: 0.3108\n",
            "Epoch 26/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.3365 - accuracy: 0.2891\n",
            "Epoch 26: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.3238 - accuracy: 0.2517 - val_loss: 6.2081 - val_accuracy: 0.3108\n",
            "Epoch 27/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.2964 - accuracy: 0.2109\n",
            "Epoch 27: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 6.2668 - accuracy: 0.2517 - val_loss: 6.1465 - val_accuracy: 0.3108\n",
            "Epoch 28/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.2707 - accuracy: 0.2734\n",
            "Epoch 28: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.1910 - accuracy: 0.2517 - val_loss: 6.0840 - val_accuracy: 0.3108\n",
            "Epoch 29/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.1358 - accuracy: 0.2812\n",
            "Epoch 29: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 6.1396 - accuracy: 0.2517 - val_loss: 6.0232 - val_accuracy: 0.3108\n",
            "Epoch 30/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.0595 - accuracy: 0.2188\n",
            "Epoch 30: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 6.0742 - accuracy: 0.2517 - val_loss: 5.9656 - val_accuracy: 0.3108\n",
            "Epoch 31/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.0151 - accuracy: 0.2500\n",
            "Epoch 31: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 5.9908 - accuracy: 0.2517 - val_loss: 5.9071 - val_accuracy: 0.3108\n",
            "Epoch 32/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.0385 - accuracy: 0.2578\n",
            "Epoch 32: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.9592 - accuracy: 0.2517 - val_loss: 5.8481 - val_accuracy: 0.3108\n",
            "Epoch 33/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.8632 - accuracy: 0.2812\n",
            "Epoch 33: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.8782 - accuracy: 0.2517 - val_loss: 5.7914 - val_accuracy: 0.3108\n",
            "Epoch 34/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.7871 - accuracy: 0.2422\n",
            "Epoch 34: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.8320 - accuracy: 0.2517 - val_loss: 5.7372 - val_accuracy: 0.3108\n",
            "Epoch 35/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.8200 - accuracy: 0.2656\n",
            "Epoch 35: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 5.7718 - accuracy: 0.2517 - val_loss: 5.6806 - val_accuracy: 0.3108\n",
            "Epoch 36/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.7417 - accuracy: 0.2344\n",
            "Epoch 36: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 5.7244 - accuracy: 0.2517 - val_loss: 5.6244 - val_accuracy: 0.3108\n",
            "Epoch 37/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.6324 - accuracy: 0.2656\n",
            "Epoch 37: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.6759 - accuracy: 0.2517 - val_loss: 5.5701 - val_accuracy: 0.3108\n",
            "Epoch 38/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.6506 - accuracy: 0.2578\n",
            "Epoch 38: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.5885 - accuracy: 0.2517 - val_loss: 5.5185 - val_accuracy: 0.3108\n",
            "Epoch 39/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.5733 - accuracy: 0.2578\n",
            "Epoch 39: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 5.5509 - accuracy: 0.2517 - val_loss: 5.4673 - val_accuracy: 0.3108\n",
            "Epoch 40/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.5303 - accuracy: 0.2812\n",
            "Epoch 40: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.5072 - accuracy: 0.2517 - val_loss: 5.4145 - val_accuracy: 0.3108\n",
            "Epoch 41/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.4628 - accuracy: 0.2500\n",
            "Epoch 41: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.4341 - accuracy: 0.2517 - val_loss: 5.3617 - val_accuracy: 0.3108\n",
            "Epoch 42/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.4174 - accuracy: 0.2344\n",
            "Epoch 42: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 5.3964 - accuracy: 0.2517 - val_loss: 5.3086 - val_accuracy: 0.3108\n",
            "Epoch 43/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.3688 - accuracy: 0.2344\n",
            "Epoch 43: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 5.3387 - accuracy: 0.2517 - val_loss: 5.2557 - val_accuracy: 0.3108\n",
            "Epoch 44/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.3224 - accuracy: 0.2578\n",
            "Epoch 44: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 5.2898 - accuracy: 0.2517 - val_loss: 5.2027 - val_accuracy: 0.3108\n",
            "Epoch 45/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.2606 - accuracy: 0.2891\n",
            "Epoch 45: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.2468 - accuracy: 0.2517 - val_loss: 5.1521 - val_accuracy: 0.3108\n",
            "Epoch 46/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.2145 - accuracy: 0.2891\n",
            "Epoch 46: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.1853 - accuracy: 0.2517 - val_loss: 5.1026 - val_accuracy: 0.3108\n",
            "Epoch 47/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.1443 - accuracy: 0.2500\n",
            "Epoch 47: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 5.1308 - accuracy: 0.2517 - val_loss: 5.0547 - val_accuracy: 0.3108\n",
            "Epoch 48/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.0575 - accuracy: 0.2266\n",
            "Epoch 48: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 5.0886 - accuracy: 0.2517 - val_loss: 5.0061 - val_accuracy: 0.3108\n",
            "Epoch 49/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.0106 - accuracy: 0.1797\n",
            "Epoch 49: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 5.0430 - accuracy: 0.2517 - val_loss: 4.9592 - val_accuracy: 0.3108\n",
            "Epoch 50/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.9799 - accuracy: 0.2031\n",
            "Epoch 50: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.9932 - accuracy: 0.2517 - val_loss: 4.9138 - val_accuracy: 0.3108\n",
            "Epoch 51/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.9778 - accuracy: 0.2344\n",
            "Epoch 51: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.9519 - accuracy: 0.2517 - val_loss: 4.8678 - val_accuracy: 0.3108\n",
            "Epoch 52/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.9225 - accuracy: 0.2578\n",
            "Epoch 52: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.9280 - accuracy: 0.2517 - val_loss: 4.8196 - val_accuracy: 0.3108\n",
            "Epoch 53/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.8786 - accuracy: 0.2500\n",
            "Epoch 53: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.8626 - accuracy: 0.2517 - val_loss: 4.7725 - val_accuracy: 0.3108\n",
            "Epoch 54/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.8303 - accuracy: 0.2891\n",
            "Epoch 54: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.8174 - accuracy: 0.2517 - val_loss: 4.7271 - val_accuracy: 0.3108\n",
            "Epoch 55/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.8376 - accuracy: 0.2812\n",
            "Epoch 55: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.7684 - accuracy: 0.2517 - val_loss: 4.6824 - val_accuracy: 0.3108\n",
            "Epoch 56/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.6872 - accuracy: 0.2656\n",
            "Epoch 56: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.7143 - accuracy: 0.2517 - val_loss: 4.6374 - val_accuracy: 0.3108\n",
            "Epoch 57/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.6866 - accuracy: 0.2422\n",
            "Epoch 57: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.6667 - accuracy: 0.2517 - val_loss: 4.5953 - val_accuracy: 0.3108\n",
            "Epoch 58/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.6199 - accuracy: 0.2344\n",
            "Epoch 58: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.6319 - accuracy: 0.2517 - val_loss: 4.5548 - val_accuracy: 0.3108\n",
            "Epoch 59/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.5863 - accuracy: 0.1953\n",
            "Epoch 59: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.5888 - accuracy: 0.2517 - val_loss: 4.5150 - val_accuracy: 0.3108\n",
            "Epoch 60/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.6569 - accuracy: 0.2500\n",
            "Epoch 60: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 4.5534 - accuracy: 0.2517 - val_loss: 4.4736 - val_accuracy: 0.3108\n",
            "Epoch 61/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.5068 - accuracy: 0.2422\n",
            "Epoch 61: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.5015 - accuracy: 0.2517 - val_loss: 4.4298 - val_accuracy: 0.3108\n",
            "Epoch 62/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.4759 - accuracy: 0.2344\n",
            "Epoch 62: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.4747 - accuracy: 0.2517 - val_loss: 4.3874 - val_accuracy: 0.3108\n",
            "Epoch 63/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.4814 - accuracy: 0.2734\n",
            "Epoch 63: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.4360 - accuracy: 0.2517 - val_loss: 4.3460 - val_accuracy: 0.3108\n",
            "Epoch 64/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.4661 - accuracy: 0.2969\n",
            "Epoch 64: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.3931 - accuracy: 0.2517 - val_loss: 4.3048 - val_accuracy: 0.3108\n",
            "Epoch 65/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.3418 - accuracy: 0.2266\n",
            "Epoch 65: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 4.3470 - accuracy: 0.2517 - val_loss: 4.2654 - val_accuracy: 0.3108\n",
            "Epoch 66/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.3283 - accuracy: 0.2188\n",
            "Epoch 66: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 4.3063 - accuracy: 0.2517 - val_loss: 4.2288 - val_accuracy: 0.3108\n",
            "Epoch 67/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.2683 - accuracy: 0.2500\n",
            "Epoch 67: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 4.2646 - accuracy: 0.2517 - val_loss: 4.1928 - val_accuracy: 0.3108\n",
            "Epoch 68/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.2422 - accuracy: 0.2344\n",
            "Epoch 68: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.2359 - accuracy: 0.2517 - val_loss: 4.1551 - val_accuracy: 0.3108\n",
            "Epoch 69/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.2458 - accuracy: 0.2734\n",
            "Epoch 69: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.1913 - accuracy: 0.2517 - val_loss: 4.1186 - val_accuracy: 0.3108\n",
            "Epoch 70/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.1691 - accuracy: 0.2656\n",
            "Epoch 70: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.1657 - accuracy: 0.2517 - val_loss: 4.0824 - val_accuracy: 0.3108\n",
            "Epoch 71/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.0982 - accuracy: 0.2500\n",
            "Epoch 71: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.1174 - accuracy: 0.2517 - val_loss: 4.0446 - val_accuracy: 0.3108\n",
            "Epoch 72/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.0753 - accuracy: 0.2188\n",
            "Epoch 72: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.0908 - accuracy: 0.2517 - val_loss: 4.0072 - val_accuracy: 0.3108\n",
            "Epoch 73/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.0656 - accuracy: 0.2891\n",
            "Epoch 73: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 4.0471 - accuracy: 0.2517 - val_loss: 3.9718 - val_accuracy: 0.3108\n",
            "Epoch 74/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.0344 - accuracy: 0.2344\n",
            "Epoch 74: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 4.0123 - accuracy: 0.2517 - val_loss: 3.9371 - val_accuracy: 0.3108\n",
            "Epoch 75/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.9442 - accuracy: 0.2656\n",
            "Epoch 75: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 3.9802 - accuracy: 0.2517 - val_loss: 3.9036 - val_accuracy: 0.3108\n",
            "Epoch 76/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.9606 - accuracy: 0.2188\n",
            "Epoch 76: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 3.9361 - accuracy: 0.2517 - val_loss: 3.8706 - val_accuracy: 0.3108\n",
            "Epoch 77/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.9663 - accuracy: 0.2812\n",
            "Epoch 77: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 3.9184 - accuracy: 0.2517 - val_loss: 3.8395 - val_accuracy: 0.3108\n",
            "Epoch 78/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.8858 - accuracy: 0.2734\n",
            "Epoch 78: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.8644 - accuracy: 0.2517 - val_loss: 3.8100 - val_accuracy: 0.3108\n",
            "Epoch 79/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.8751 - accuracy: 0.2344\n",
            "Epoch 79: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 3.8313 - accuracy: 0.2517 - val_loss: 3.7804 - val_accuracy: 0.3108\n",
            "Epoch 80/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.7632 - accuracy: 0.2188\n",
            "Epoch 80: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 3.7965 - accuracy: 0.2517 - val_loss: 3.7488 - val_accuracy: 0.3108\n",
            "Epoch 81/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.7704 - accuracy: 0.2500\n",
            "Epoch 81: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.7610 - accuracy: 0.2517 - val_loss: 3.7147 - val_accuracy: 0.3108\n",
            "Epoch 82/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.7503 - accuracy: 0.2656\n",
            "Epoch 82: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.7377 - accuracy: 0.2517 - val_loss: 3.6796 - val_accuracy: 0.3108\n",
            "Epoch 83/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.7600 - accuracy: 0.2891\n",
            "Epoch 83: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.7142 - accuracy: 0.2517 - val_loss: 3.6479 - val_accuracy: 0.3108\n",
            "Epoch 84/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.6958 - accuracy: 0.2500\n",
            "Epoch 84: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.6719 - accuracy: 0.2517 - val_loss: 3.6192 - val_accuracy: 0.3108\n",
            "Epoch 85/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.6833 - accuracy: 0.3047\n",
            "Epoch 85: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.6493 - accuracy: 0.2517 - val_loss: 3.5899 - val_accuracy: 0.3108\n",
            "Epoch 86/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.6196 - accuracy: 0.3359\n",
            "Epoch 86: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.6146 - accuracy: 0.2517 - val_loss: 3.5571 - val_accuracy: 0.3108\n",
            "Epoch 87/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.5726 - accuracy: 0.2344\n",
            "Epoch 87: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.5929 - accuracy: 0.2517 - val_loss: 3.5248 - val_accuracy: 0.3108\n",
            "Epoch 88/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.5835 - accuracy: 0.2344\n",
            "Epoch 88: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.5619 - accuracy: 0.2517 - val_loss: 3.4957 - val_accuracy: 0.3108\n",
            "Epoch 89/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4827 - accuracy: 0.2500\n",
            "Epoch 89: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 3.5268 - accuracy: 0.2517 - val_loss: 3.4709 - val_accuracy: 0.3108\n",
            "Epoch 90/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.5020 - accuracy: 0.2422\n",
            "Epoch 90: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 3.4997 - accuracy: 0.2517 - val_loss: 3.4476 - val_accuracy: 0.3108\n",
            "Epoch 91/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4431 - accuracy: 0.2578\n",
            "Epoch 91: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.4570 - accuracy: 0.2517 - val_loss: 3.4229 - val_accuracy: 0.3108\n",
            "Epoch 92/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4616 - accuracy: 0.2734\n",
            "Epoch 92: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.4404 - accuracy: 0.2517 - val_loss: 3.3980 - val_accuracy: 0.3108\n",
            "Epoch 93/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4255 - accuracy: 0.2656\n",
            "Epoch 93: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.4124 - accuracy: 0.2517 - val_loss: 3.3760 - val_accuracy: 0.3108\n",
            "Epoch 94/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.3963 - accuracy: 0.2500\n",
            "Epoch 94: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.3810 - accuracy: 0.2517 - val_loss: 3.3513 - val_accuracy: 0.3108\n",
            "Epoch 95/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.3562 - accuracy: 0.2266\n",
            "Epoch 95: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 3.3576 - accuracy: 0.2517 - val_loss: 3.3248 - val_accuracy: 0.3108\n",
            "Epoch 96/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.3577 - accuracy: 0.2422\n",
            "Epoch 96: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.3398 - accuracy: 0.2517 - val_loss: 3.2988 - val_accuracy: 0.3108\n",
            "Epoch 97/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.2956 - accuracy: 0.1953\n",
            "Epoch 97: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.2992 - accuracy: 0.2517 - val_loss: 3.2728 - val_accuracy: 0.3108\n",
            "Epoch 98/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.2514 - accuracy: 0.2422\n",
            "Epoch 98: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.2675 - accuracy: 0.2517 - val_loss: 3.2489 - val_accuracy: 0.3108\n",
            "Epoch 99/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1972 - accuracy: 0.2500\n",
            "Epoch 99: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.2488 - accuracy: 0.2517 - val_loss: 3.2243 - val_accuracy: 0.3108\n",
            "Epoch 100/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.2067 - accuracy: 0.2812\n",
            "Epoch 100: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.2193 - accuracy: 0.2517 - val_loss: 3.1982 - val_accuracy: 0.3108\n",
            "Epoch 101/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.2307 - accuracy: 0.2891\n",
            "Epoch 101: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 3.2012 - accuracy: 0.2517 - val_loss: 3.1736 - val_accuracy: 0.3108\n",
            "Epoch 102/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1789 - accuracy: 0.2656\n",
            "Epoch 102: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.1741 - accuracy: 0.2517 - val_loss: 3.1482 - val_accuracy: 0.3108\n",
            "Epoch 103/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1444 - accuracy: 0.2422\n",
            "Epoch 103: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.1393 - accuracy: 0.2517 - val_loss: 3.1239 - val_accuracy: 0.3108\n",
            "Epoch 104/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1163 - accuracy: 0.2031\n",
            "Epoch 104: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.1211 - accuracy: 0.2517 - val_loss: 3.1016 - val_accuracy: 0.3108\n",
            "Epoch 105/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1357 - accuracy: 0.2812\n",
            "Epoch 105: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.0987 - accuracy: 0.2517 - val_loss: 3.0816 - val_accuracy: 0.3108\n",
            "Epoch 106/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.0351 - accuracy: 0.2266\n",
            "Epoch 106: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 3.0695 - accuracy: 0.2517 - val_loss: 3.0577 - val_accuracy: 0.3108\n",
            "Epoch 107/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.0906 - accuracy: 0.2812\n",
            "Epoch 107: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.0498 - accuracy: 0.2517 - val_loss: 3.0327 - val_accuracy: 0.3108\n",
            "Epoch 108/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.0537 - accuracy: 0.2266\n",
            "Epoch 108: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.0286 - accuracy: 0.2517 - val_loss: 3.0079 - val_accuracy: 0.3108\n",
            "Epoch 109/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9902 - accuracy: 0.2188\n",
            "Epoch 109: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.0044 - accuracy: 0.2517 - val_loss: 2.9821 - val_accuracy: 0.3108\n",
            "Epoch 110/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9929 - accuracy: 0.2578\n",
            "Epoch 110: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.9773 - accuracy: 0.2517 - val_loss: 2.9596 - val_accuracy: 0.3108\n",
            "Epoch 111/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9893 - accuracy: 0.2656\n",
            "Epoch 111: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.9564 - accuracy: 0.2517 - val_loss: 2.9402 - val_accuracy: 0.3108\n",
            "Epoch 112/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9379 - accuracy: 0.2109\n",
            "Epoch 112: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.9348 - accuracy: 0.2517 - val_loss: 2.9258 - val_accuracy: 0.3108\n",
            "Epoch 113/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9951 - accuracy: 0.3125\n",
            "Epoch 113: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.9130 - accuracy: 0.2517 - val_loss: 2.9107 - val_accuracy: 0.3108\n",
            "Epoch 114/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9591 - accuracy: 0.3047\n",
            "Epoch 114: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.8878 - accuracy: 0.2517 - val_loss: 2.8871 - val_accuracy: 0.3108\n",
            "Epoch 115/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9142 - accuracy: 0.2500\n",
            "Epoch 115: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 2.8710 - accuracy: 0.2517 - val_loss: 2.8598 - val_accuracy: 0.3108\n",
            "Epoch 116/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.8016 - accuracy: 0.2188\n",
            "Epoch 116: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.8448 - accuracy: 0.2517 - val_loss: 2.8340 - val_accuracy: 0.3108\n",
            "Epoch 117/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.8414 - accuracy: 0.2344\n",
            "Epoch 117: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.8310 - accuracy: 0.2517 - val_loss: 2.8119 - val_accuracy: 0.3108\n",
            "Epoch 118/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7479 - accuracy: 0.2500\n",
            "Epoch 118: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.8071 - accuracy: 0.2517 - val_loss: 2.7923 - val_accuracy: 0.3108\n",
            "Epoch 119/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7307 - accuracy: 0.2656\n",
            "Epoch 119: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.7907 - accuracy: 0.2517 - val_loss: 2.7745 - val_accuracy: 0.3108\n",
            "Epoch 120/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7133 - accuracy: 0.2188\n",
            "Epoch 120: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.7613 - accuracy: 0.2517 - val_loss: 2.7578 - val_accuracy: 0.3108\n",
            "Epoch 121/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7770 - accuracy: 0.2578\n",
            "Epoch 121: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.7450 - accuracy: 0.2517 - val_loss: 2.7477 - val_accuracy: 0.3108\n",
            "Epoch 122/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7391 - accuracy: 0.2734\n",
            "Epoch 122: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.7290 - accuracy: 0.2517 - val_loss: 2.7385 - val_accuracy: 0.3108\n",
            "Epoch 123/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7102 - accuracy: 0.2266\n",
            "Epoch 123: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.7148 - accuracy: 0.2517 - val_loss: 2.7222 - val_accuracy: 0.3108\n",
            "Epoch 124/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6592 - accuracy: 0.2422\n",
            "Epoch 124: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.6832 - accuracy: 0.2517 - val_loss: 2.7015 - val_accuracy: 0.3108\n",
            "Epoch 125/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6215 - accuracy: 0.2500\n",
            "Epoch 125: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.6553 - accuracy: 0.2517 - val_loss: 2.6796 - val_accuracy: 0.3108\n",
            "Epoch 126/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6557 - accuracy: 0.2734\n",
            "Epoch 126: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.6451 - accuracy: 0.2517 - val_loss: 2.6575 - val_accuracy: 0.3108\n",
            "Epoch 127/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6248 - accuracy: 0.2344\n",
            "Epoch 127: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.6311 - accuracy: 0.2517 - val_loss: 2.6359 - val_accuracy: 0.3108\n",
            "Epoch 128/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6242 - accuracy: 0.2812\n",
            "Epoch 128: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.6129 - accuracy: 0.2517 - val_loss: 2.6183 - val_accuracy: 0.3108\n",
            "Epoch 129/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6507 - accuracy: 0.2656\n",
            "Epoch 129: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.5917 - accuracy: 0.2517 - val_loss: 2.6011 - val_accuracy: 0.3108\n",
            "Epoch 130/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6217 - accuracy: 0.2734\n",
            "Epoch 130: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 2.5736 - accuracy: 0.2517 - val_loss: 2.5812 - val_accuracy: 0.3108\n",
            "Epoch 131/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.5164 - accuracy: 0.2500\n",
            "Epoch 131: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 2.5561 - accuracy: 0.2517 - val_loss: 2.5584 - val_accuracy: 0.3108\n",
            "Epoch 132/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.5990 - accuracy: 0.3047\n",
            "Epoch 132: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 2.5521 - accuracy: 0.2517 - val_loss: 2.5376 - val_accuracy: 0.3108\n",
            "Epoch 133/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.5134 - accuracy: 0.2031\n",
            "Epoch 133: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.5239 - accuracy: 0.2517 - val_loss: 2.5175 - val_accuracy: 0.3108\n",
            "Epoch 134/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4866 - accuracy: 0.2266\n",
            "Epoch 134: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.5048 - accuracy: 0.2517 - val_loss: 2.5050 - val_accuracy: 0.3108\n",
            "Epoch 135/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.5337 - accuracy: 0.2656\n",
            "Epoch 135: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.4774 - accuracy: 0.2517 - val_loss: 2.4965 - val_accuracy: 0.3108\n",
            "Epoch 136/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.5030 - accuracy: 0.2500\n",
            "Epoch 136: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.4814 - accuracy: 0.2517 - val_loss: 2.4773 - val_accuracy: 0.3108\n",
            "Epoch 137/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4464 - accuracy: 0.2812\n",
            "Epoch 137: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.4638 - accuracy: 0.2517 - val_loss: 2.4550 - val_accuracy: 0.3108\n",
            "Epoch 138/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4023 - accuracy: 0.2422\n",
            "Epoch 138: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.4382 - accuracy: 0.2517 - val_loss: 2.4382 - val_accuracy: 0.3108\n",
            "Epoch 139/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3892 - accuracy: 0.2266\n",
            "Epoch 139: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.4291 - accuracy: 0.2517 - val_loss: 2.4227 - val_accuracy: 0.3108\n",
            "Epoch 140/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4460 - accuracy: 0.2500\n",
            "Epoch 140: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.4139 - accuracy: 0.2517 - val_loss: 2.4109 - val_accuracy: 0.3108\n",
            "Epoch 141/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3581 - accuracy: 0.2188\n",
            "Epoch 141: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3879 - accuracy: 0.2517 - val_loss: 2.4020 - val_accuracy: 0.3108\n",
            "Epoch 142/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4078 - accuracy: 0.2891\n",
            "Epoch 142: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3862 - accuracy: 0.2517 - val_loss: 2.3888 - val_accuracy: 0.3108\n",
            "Epoch 143/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3660 - accuracy: 0.2969\n",
            "Epoch 143: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3563 - accuracy: 0.2517 - val_loss: 2.3737 - val_accuracy: 0.3108\n",
            "Epoch 144/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3609 - accuracy: 0.2500\n",
            "Epoch 144: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3574 - accuracy: 0.2517 - val_loss: 2.3536 - val_accuracy: 0.3108\n",
            "Epoch 145/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3095 - accuracy: 0.2188\n",
            "Epoch 145: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3363 - accuracy: 0.2517 - val_loss: 2.3356 - val_accuracy: 0.3108\n",
            "Epoch 146/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3845 - accuracy: 0.2734\n",
            "Epoch 146: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3207 - accuracy: 0.2517 - val_loss: 2.3191 - val_accuracy: 0.3108\n",
            "Epoch 147/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2837 - accuracy: 0.2734\n",
            "Epoch 147: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 2.3057 - accuracy: 0.2517 - val_loss: 2.3073 - val_accuracy: 0.3108\n",
            "Epoch 148/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2334 - accuracy: 0.2188\n",
            "Epoch 148: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.2764 - accuracy: 0.2517 - val_loss: 2.3000 - val_accuracy: 0.3108\n",
            "Epoch 149/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1967 - accuracy: 0.1797\n",
            "Epoch 149: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.2670 - accuracy: 0.2517 - val_loss: 2.2924 - val_accuracy: 0.3108\n",
            "Epoch 150/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2538 - accuracy: 0.2891\n",
            "Epoch 150: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.2622 - accuracy: 0.2517 - val_loss: 2.2810 - val_accuracy: 0.3108\n",
            "Epoch 151/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2247 - accuracy: 0.2344\n",
            "Epoch 151: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.2349 - accuracy: 0.2517 - val_loss: 2.2681 - val_accuracy: 0.3108\n",
            "Epoch 152/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2155 - accuracy: 0.2656\n",
            "Epoch 152: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.2332 - accuracy: 0.2517 - val_loss: 2.2591 - val_accuracy: 0.3108\n",
            "Epoch 153/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1924 - accuracy: 0.2422\n",
            "Epoch 153: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.2155 - accuracy: 0.2517 - val_loss: 2.2473 - val_accuracy: 0.3108\n",
            "Epoch 154/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2282 - accuracy: 0.2969\n",
            "Epoch 154: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.2004 - accuracy: 0.2517 - val_loss: 2.2366 - val_accuracy: 0.3108\n",
            "Epoch 155/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1770 - accuracy: 0.2266\n",
            "Epoch 155: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.2025 - accuracy: 0.2517 - val_loss: 2.2198 - val_accuracy: 0.3108\n",
            "Epoch 156/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2100 - accuracy: 0.2812\n",
            "Epoch 156: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.1878 - accuracy: 0.2517 - val_loss: 2.2021 - val_accuracy: 0.3108\n",
            "Epoch 157/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1727 - accuracy: 0.2578\n",
            "Epoch 157: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.1746 - accuracy: 0.2517 - val_loss: 2.1923 - val_accuracy: 0.3108\n",
            "Epoch 158/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1274 - accuracy: 0.2188\n",
            "Epoch 158: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.1562 - accuracy: 0.2517 - val_loss: 2.1843 - val_accuracy: 0.3108\n",
            "Epoch 159/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1503 - accuracy: 0.2891\n",
            "Epoch 159: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.1400 - accuracy: 0.2517 - val_loss: 2.1750 - val_accuracy: 0.3108\n",
            "Epoch 160/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0967 - accuracy: 0.2891\n",
            "Epoch 160: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.1344 - accuracy: 0.2517 - val_loss: 2.1622 - val_accuracy: 0.3108\n",
            "Epoch 161/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1148 - accuracy: 0.2891\n",
            "Epoch 161: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.1130 - accuracy: 0.2517 - val_loss: 2.1528 - val_accuracy: 0.3108\n",
            "Epoch 162/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1204 - accuracy: 0.2266\n",
            "Epoch 162: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.1015 - accuracy: 0.2517 - val_loss: 2.1488 - val_accuracy: 0.3108\n",
            "Epoch 163/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1149 - accuracy: 0.2812\n",
            "Epoch 163: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.0962 - accuracy: 0.2517 - val_loss: 2.1455 - val_accuracy: 0.3108\n",
            "Epoch 164/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0737 - accuracy: 0.2500\n",
            "Epoch 164: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.0767 - accuracy: 0.2517 - val_loss: 2.1372 - val_accuracy: 0.3108\n",
            "Epoch 165/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0773 - accuracy: 0.2500\n",
            "Epoch 165: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0638 - accuracy: 0.2517 - val_loss: 2.1293 - val_accuracy: 0.3108\n",
            "Epoch 166/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0052 - accuracy: 0.2344\n",
            "Epoch 166: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0552 - accuracy: 0.2517 - val_loss: 2.1254 - val_accuracy: 0.3108\n",
            "Epoch 167/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0503 - accuracy: 0.2734\n",
            "Epoch 167: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0529 - accuracy: 0.2517 - val_loss: 2.1178 - val_accuracy: 0.3108\n",
            "Epoch 168/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0417 - accuracy: 0.2422\n",
            "Epoch 168: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0304 - accuracy: 0.2517 - val_loss: 2.1008 - val_accuracy: 0.3108\n",
            "Epoch 169/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1064 - accuracy: 0.2422\n",
            "Epoch 169: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0213 - accuracy: 0.2517 - val_loss: 2.0914 - val_accuracy: 0.3108\n",
            "Epoch 170/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0257 - accuracy: 0.2109\n",
            "Epoch 170: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0203 - accuracy: 0.2517 - val_loss: 2.0868 - val_accuracy: 0.3108\n",
            "Epoch 171/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0023 - accuracy: 0.2422\n",
            "Epoch 171: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.9988 - accuracy: 0.2517 - val_loss: 2.0811 - val_accuracy: 0.3108\n",
            "Epoch 172/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9834 - accuracy: 0.2109\n",
            "Epoch 172: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.9968 - accuracy: 0.2517 - val_loss: 2.0610 - val_accuracy: 0.3108\n",
            "Epoch 173/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9315 - accuracy: 0.2422\n",
            "Epoch 173: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9798 - accuracy: 0.2517 - val_loss: 2.0401 - val_accuracy: 0.3108\n",
            "Epoch 174/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9953 - accuracy: 0.2266\n",
            "Epoch 174: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9744 - accuracy: 0.2517 - val_loss: 2.0231 - val_accuracy: 0.3108\n",
            "Epoch 175/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9335 - accuracy: 0.2422\n",
            "Epoch 175: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9636 - accuracy: 0.2517 - val_loss: 2.0054 - val_accuracy: 0.3108\n",
            "Epoch 176/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9469 - accuracy: 0.2500\n",
            "Epoch 176: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9568 - accuracy: 0.2517 - val_loss: 1.9947 - val_accuracy: 0.3108\n",
            "Epoch 177/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9803 - accuracy: 0.2656\n",
            "Epoch 177: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.9437 - accuracy: 0.2517 - val_loss: 1.9888 - val_accuracy: 0.3108\n",
            "Epoch 178/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9955 - accuracy: 0.2969\n",
            "Epoch 178: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.9321 - accuracy: 0.2517 - val_loss: 1.9881 - val_accuracy: 0.3108\n",
            "Epoch 179/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9380 - accuracy: 0.2969\n",
            "Epoch 179: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9301 - accuracy: 0.2517 - val_loss: 1.9895 - val_accuracy: 0.3108\n",
            "Epoch 180/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9137 - accuracy: 0.2266\n",
            "Epoch 180: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9184 - accuracy: 0.2517 - val_loss: 1.9908 - val_accuracy: 0.3108\n",
            "Epoch 181/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8499 - accuracy: 0.1953\n",
            "Epoch 181: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8999 - accuracy: 0.2517 - val_loss: 1.9958 - val_accuracy: 0.3108\n",
            "Epoch 182/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8934 - accuracy: 0.2734\n",
            "Epoch 182: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.8851 - accuracy: 0.2517 - val_loss: 2.0027 - val_accuracy: 0.3108\n",
            "Epoch 183/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9087 - accuracy: 0.2266\n",
            "Epoch 183: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8842 - accuracy: 0.2517 - val_loss: 1.9988 - val_accuracy: 0.3108\n",
            "Epoch 184/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9135 - accuracy: 0.2266\n",
            "Epoch 184: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.8721 - accuracy: 0.2517 - val_loss: 1.9855 - val_accuracy: 0.3108\n",
            "Epoch 185/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8930 - accuracy: 0.2891\n",
            "Epoch 185: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.8595 - accuracy: 0.2517 - val_loss: 1.9640 - val_accuracy: 0.3108\n",
            "Epoch 186/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8713 - accuracy: 0.2891\n",
            "Epoch 186: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.8562 - accuracy: 0.2517 - val_loss: 1.9418 - val_accuracy: 0.3108\n",
            "Epoch 187/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8636 - accuracy: 0.2656\n",
            "Epoch 187: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8441 - accuracy: 0.2517 - val_loss: 1.9242 - val_accuracy: 0.3108\n",
            "Epoch 188/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8348 - accuracy: 0.2344\n",
            "Epoch 188: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8400 - accuracy: 0.2517 - val_loss: 1.9099 - val_accuracy: 0.3108\n",
            "Epoch 189/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7902 - accuracy: 0.2109\n",
            "Epoch 189: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8278 - accuracy: 0.2517 - val_loss: 1.8986 - val_accuracy: 0.3108\n",
            "Epoch 190/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8306 - accuracy: 0.2812\n",
            "Epoch 190: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.8179 - accuracy: 0.2517 - val_loss: 1.8952 - val_accuracy: 0.3108\n",
            "Epoch 191/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8331 - accuracy: 0.2344\n",
            "Epoch 191: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.8103 - accuracy: 0.2517 - val_loss: 1.8955 - val_accuracy: 0.3108\n",
            "Epoch 192/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8422 - accuracy: 0.2344\n",
            "Epoch 192: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.7987 - accuracy: 0.2517 - val_loss: 1.8975 - val_accuracy: 0.3108\n",
            "Epoch 193/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7550 - accuracy: 0.2578\n",
            "Epoch 193: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.7885 - accuracy: 0.2517 - val_loss: 1.9006 - val_accuracy: 0.3108\n",
            "Epoch 194/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7787 - accuracy: 0.2188\n",
            "Epoch 194: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.7872 - accuracy: 0.2517 - val_loss: 1.8989 - val_accuracy: 0.3108\n",
            "Epoch 195/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8002 - accuracy: 0.2422\n",
            "Epoch 195: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.7771 - accuracy: 0.2517 - val_loss: 1.8928 - val_accuracy: 0.3108\n",
            "Epoch 196/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7722 - accuracy: 0.2344\n",
            "Epoch 196: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.7826 - accuracy: 0.2517 - val_loss: 1.8782 - val_accuracy: 0.3108\n",
            "Epoch 197/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8130 - accuracy: 0.2031\n",
            "Epoch 197: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.7608 - accuracy: 0.2517 - val_loss: 1.8635 - val_accuracy: 0.3108\n",
            "Epoch 198/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7518 - accuracy: 0.2656\n",
            "Epoch 198: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.7646 - accuracy: 0.2517 - val_loss: 1.8573 - val_accuracy: 0.3108\n",
            "Epoch 199/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8118 - accuracy: 0.2578\n",
            "Epoch 199: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.7425 - accuracy: 0.2517 - val_loss: 1.8547 - val_accuracy: 0.3108\n",
            "Epoch 200/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7545 - accuracy: 0.2422\n",
            "Epoch 200: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.7401 - accuracy: 0.2517 - val_loss: 1.8451 - val_accuracy: 0.3108\n",
            "Epoch 201/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7159 - accuracy: 0.2344\n",
            "Epoch 201: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.7327 - accuracy: 0.2517 - val_loss: 1.8310 - val_accuracy: 0.3108\n",
            "Epoch 202/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7288 - accuracy: 0.2266\n",
            "Epoch 202: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.7210 - accuracy: 0.2517 - val_loss: 1.8233 - val_accuracy: 0.3108\n",
            "Epoch 203/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6864 - accuracy: 0.2656\n",
            "Epoch 203: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.7223 - accuracy: 0.2517 - val_loss: 1.8257 - val_accuracy: 0.3108\n",
            "Epoch 204/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7433 - accuracy: 0.2891\n",
            "Epoch 204: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.7110 - accuracy: 0.2517 - val_loss: 1.8242 - val_accuracy: 0.3108\n",
            "Epoch 205/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7267 - accuracy: 0.2500\n",
            "Epoch 205: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.7120 - accuracy: 0.2517 - val_loss: 1.8157 - val_accuracy: 0.3108\n",
            "Epoch 206/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7344 - accuracy: 0.2734\n",
            "Epoch 206: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.7031 - accuracy: 0.2517 - val_loss: 1.8070 - val_accuracy: 0.3108\n",
            "Epoch 207/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6451 - accuracy: 0.2031\n",
            "Epoch 207: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.6978 - accuracy: 0.2517 - val_loss: 1.7921 - val_accuracy: 0.3108\n",
            "Epoch 208/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6858 - accuracy: 0.2422\n",
            "Epoch 208: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6759 - accuracy: 0.2517 - val_loss: 1.7877 - val_accuracy: 0.3108\n",
            "Epoch 209/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6671 - accuracy: 0.2344\n",
            "Epoch 209: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6675 - accuracy: 0.2517 - val_loss: 1.7868 - val_accuracy: 0.3108\n",
            "Epoch 210/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6640 - accuracy: 0.2266\n",
            "Epoch 210: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.6684 - accuracy: 0.2517 - val_loss: 1.7861 - val_accuracy: 0.3108\n",
            "Epoch 211/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6995 - accuracy: 0.2578\n",
            "Epoch 211: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.6580 - accuracy: 0.2517 - val_loss: 1.7875 - val_accuracy: 0.3108\n",
            "Epoch 212/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6437 - accuracy: 0.2734\n",
            "Epoch 212: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.6581 - accuracy: 0.2517 - val_loss: 1.7785 - val_accuracy: 0.3108\n",
            "Epoch 213/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6349 - accuracy: 0.2734\n",
            "Epoch 213: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6411 - accuracy: 0.2517 - val_loss: 1.7614 - val_accuracy: 0.3108\n",
            "Epoch 214/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6276 - accuracy: 0.2266\n",
            "Epoch 214: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6432 - accuracy: 0.2517 - val_loss: 1.7485 - val_accuracy: 0.3108\n",
            "Epoch 215/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6683 - accuracy: 0.2578\n",
            "Epoch 215: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.6379 - accuracy: 0.2517 - val_loss: 1.7385 - val_accuracy: 0.3108\n",
            "Epoch 216/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6058 - accuracy: 0.2344\n",
            "Epoch 216: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6340 - accuracy: 0.2517 - val_loss: 1.7284 - val_accuracy: 0.3108\n",
            "Epoch 217/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6363 - accuracy: 0.2812\n",
            "Epoch 217: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6258 - accuracy: 0.2517 - val_loss: 1.7204 - val_accuracy: 0.3108\n",
            "Epoch 218/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6038 - accuracy: 0.2422\n",
            "Epoch 218: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6093 - accuracy: 0.2517 - val_loss: 1.7138 - val_accuracy: 0.3108\n",
            "Epoch 219/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6058 - accuracy: 0.2578\n",
            "Epoch 219: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.6119 - accuracy: 0.2517 - val_loss: 1.7070 - val_accuracy: 0.3108\n",
            "Epoch 220/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6540 - accuracy: 0.2812\n",
            "Epoch 220: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6123 - accuracy: 0.2517 - val_loss: 1.6987 - val_accuracy: 0.3108\n",
            "Epoch 221/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6099 - accuracy: 0.2422\n",
            "Epoch 221: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.5948 - accuracy: 0.2517 - val_loss: 1.6917 - val_accuracy: 0.3108\n",
            "Epoch 222/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5995 - accuracy: 0.2266\n",
            "Epoch 222: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.5941 - accuracy: 0.2517 - val_loss: 1.6863 - val_accuracy: 0.3108\n",
            "Epoch 223/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6218 - accuracy: 0.2656\n",
            "Epoch 223: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.5848 - accuracy: 0.2517 - val_loss: 1.6848 - val_accuracy: 0.3108\n",
            "Epoch 224/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6330 - accuracy: 0.3125\n",
            "Epoch 224: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.5831 - accuracy: 0.2517 - val_loss: 1.6890 - val_accuracy: 0.3108\n",
            "Epoch 225/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5886 - accuracy: 0.2891\n",
            "Epoch 225: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.5746 - accuracy: 0.2517 - val_loss: 1.6937 - val_accuracy: 0.3108\n",
            "Epoch 226/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5658 - accuracy: 0.1797\n",
            "Epoch 226: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.5816 - accuracy: 0.2517 - val_loss: 1.6922 - val_accuracy: 0.3108\n",
            "Epoch 227/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5533 - accuracy: 0.3125\n",
            "Epoch 227: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.5748 - accuracy: 0.2517 - val_loss: 1.6848 - val_accuracy: 0.3108\n",
            "Epoch 228/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5751 - accuracy: 0.2891\n",
            "Epoch 228: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.5617 - accuracy: 0.2517 - val_loss: 1.6656 - val_accuracy: 0.3108\n",
            "Epoch 229/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5560 - accuracy: 0.2969\n",
            "Epoch 229: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.5574 - accuracy: 0.2517 - val_loss: 1.6457 - val_accuracy: 0.3108\n",
            "Epoch 230/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5957 - accuracy: 0.2734\n",
            "Epoch 230: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.5538 - accuracy: 0.2517 - val_loss: 1.6347 - val_accuracy: 0.3108\n",
            "Epoch 231/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5393 - accuracy: 0.2109\n",
            "Epoch 231: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.5474 - accuracy: 0.2517 - val_loss: 1.6319 - val_accuracy: 0.3108\n",
            "Epoch 232/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5611 - accuracy: 0.2656\n",
            "Epoch 232: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.5393 - accuracy: 0.2517 - val_loss: 1.6308 - val_accuracy: 0.3108\n",
            "Epoch 233/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5325 - accuracy: 0.2578\n",
            "Epoch 233: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5308 - accuracy: 0.2517 - val_loss: 1.6287 - val_accuracy: 0.3108\n",
            "Epoch 234/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5163 - accuracy: 0.2500\n",
            "Epoch 234: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5343 - accuracy: 0.2517 - val_loss: 1.6263 - val_accuracy: 0.3108\n",
            "Epoch 235/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5433 - accuracy: 0.2812\n",
            "Epoch 235: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.5187 - accuracy: 0.2517 - val_loss: 1.6250 - val_accuracy: 0.3108\n",
            "Epoch 236/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5249 - accuracy: 0.2188\n",
            "Epoch 236: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5114 - accuracy: 0.2517 - val_loss: 1.6239 - val_accuracy: 0.3108\n",
            "Epoch 237/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4801 - accuracy: 0.2422\n",
            "Epoch 237: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5093 - accuracy: 0.2517 - val_loss: 1.6226 - val_accuracy: 0.3108\n",
            "Epoch 238/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4806 - accuracy: 0.2422\n",
            "Epoch 238: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5069 - accuracy: 0.2517 - val_loss: 1.6195 - val_accuracy: 0.3108\n",
            "Epoch 239/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5206 - accuracy: 0.2734\n",
            "Epoch 239: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5020 - accuracy: 0.2517 - val_loss: 1.6135 - val_accuracy: 0.3108\n",
            "Epoch 240/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5481 - accuracy: 0.2266\n",
            "Epoch 240: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4999 - accuracy: 0.2517 - val_loss: 1.6049 - val_accuracy: 0.3108\n",
            "Epoch 241/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4915 - accuracy: 0.2500\n",
            "Epoch 241: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4945 - accuracy: 0.2517 - val_loss: 1.5957 - val_accuracy: 0.3108\n",
            "Epoch 242/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4644 - accuracy: 0.2578\n",
            "Epoch 242: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4934 - accuracy: 0.2517 - val_loss: 1.5872 - val_accuracy: 0.3108\n",
            "Epoch 243/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4378 - accuracy: 0.1719\n",
            "Epoch 243: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4888 - accuracy: 0.2517 - val_loss: 1.5828 - val_accuracy: 0.3108\n",
            "Epoch 244/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4710 - accuracy: 0.2969\n",
            "Epoch 244: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4891 - accuracy: 0.2517 - val_loss: 1.5824 - val_accuracy: 0.3108\n",
            "Epoch 245/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4143 - accuracy: 0.2891\n",
            "Epoch 245: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4672 - accuracy: 0.2517 - val_loss: 1.5856 - val_accuracy: 0.3108\n",
            "Epoch 246/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4654 - accuracy: 0.2812\n",
            "Epoch 246: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.4702 - accuracy: 0.2517 - val_loss: 1.5891 - val_accuracy: 0.3108\n",
            "Epoch 247/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4016 - accuracy: 0.2031\n",
            "Epoch 247: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4674 - accuracy: 0.2517 - val_loss: 1.5938 - val_accuracy: 0.3108\n",
            "Epoch 248/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4769 - accuracy: 0.2266\n",
            "Epoch 248: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4675 - accuracy: 0.2517 - val_loss: 1.5981 - val_accuracy: 0.3108\n",
            "Epoch 249/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4307 - accuracy: 0.2266\n",
            "Epoch 249: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4616 - accuracy: 0.2517 - val_loss: 1.5960 - val_accuracy: 0.3108\n",
            "Epoch 250/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4601 - accuracy: 0.2891\n",
            "Epoch 250: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4548 - accuracy: 0.2517 - val_loss: 1.5904 - val_accuracy: 0.3108\n",
            "Epoch 251/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4851 - accuracy: 0.2578\n",
            "Epoch 251: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.4545 - accuracy: 0.2517 - val_loss: 1.5814 - val_accuracy: 0.3108\n",
            "Epoch 252/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4128 - accuracy: 0.2109\n",
            "Epoch 252: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4459 - accuracy: 0.2517 - val_loss: 1.5732 - val_accuracy: 0.3108\n",
            "Epoch 253/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4434 - accuracy: 0.2109\n",
            "Epoch 253: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.4401 - accuracy: 0.2517 - val_loss: 1.5698 - val_accuracy: 0.3108\n",
            "Epoch 254/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4010 - accuracy: 0.2422\n",
            "Epoch 254: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.4416 - accuracy: 0.2517 - val_loss: 1.5661 - val_accuracy: 0.3108\n",
            "Epoch 255/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4673 - accuracy: 0.2344\n",
            "Epoch 255: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4306 - accuracy: 0.2517 - val_loss: 1.5599 - val_accuracy: 0.3108\n",
            "Epoch 256/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4301 - accuracy: 0.2656\n",
            "Epoch 256: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4282 - accuracy: 0.2517 - val_loss: 1.5494 - val_accuracy: 0.3108\n",
            "Epoch 257/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4097 - accuracy: 0.2266\n",
            "Epoch 257: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4278 - accuracy: 0.2517 - val_loss: 1.5350 - val_accuracy: 0.3108\n",
            "Epoch 258/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4057 - accuracy: 0.2422\n",
            "Epoch 258: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4151 - accuracy: 0.2517 - val_loss: 1.5292 - val_accuracy: 0.3108\n",
            "Epoch 259/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4276 - accuracy: 0.3047\n",
            "Epoch 259: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4190 - accuracy: 0.2517 - val_loss: 1.5294 - val_accuracy: 0.3108\n",
            "Epoch 260/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3972 - accuracy: 0.2109\n",
            "Epoch 260: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4125 - accuracy: 0.2517 - val_loss: 1.5308 - val_accuracy: 0.3108\n",
            "Epoch 261/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4291 - accuracy: 0.2734\n",
            "Epoch 261: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4186 - accuracy: 0.2517 - val_loss: 1.5345 - val_accuracy: 0.3108\n",
            "Epoch 262/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4312 - accuracy: 0.2266\n",
            "Epoch 262: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4080 - accuracy: 0.2517 - val_loss: 1.5327 - val_accuracy: 0.3108\n",
            "Epoch 263/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4685 - accuracy: 0.2344\n",
            "Epoch 263: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4074 - accuracy: 0.2517 - val_loss: 1.5289 - val_accuracy: 0.3108\n",
            "Epoch 264/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4086 - accuracy: 0.2734\n",
            "Epoch 264: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4039 - accuracy: 0.2517 - val_loss: 1.5253 - val_accuracy: 0.3108\n",
            "Epoch 265/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4032 - accuracy: 0.2812\n",
            "Epoch 265: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3977 - accuracy: 0.2517 - val_loss: 1.5151 - val_accuracy: 0.3108\n",
            "Epoch 266/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4265 - accuracy: 0.2500\n",
            "Epoch 266: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3954 - accuracy: 0.2517 - val_loss: 1.4994 - val_accuracy: 0.3108\n",
            "Epoch 267/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3882 - accuracy: 0.2500\n",
            "Epoch 267: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3931 - accuracy: 0.2517 - val_loss: 1.4874 - val_accuracy: 0.3108\n",
            "Epoch 268/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3723 - accuracy: 0.2578\n",
            "Epoch 268: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3888 - accuracy: 0.2517 - val_loss: 1.4865 - val_accuracy: 0.3108\n",
            "Epoch 269/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4167 - accuracy: 0.2344\n",
            "Epoch 269: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3818 - accuracy: 0.2517 - val_loss: 1.4889 - val_accuracy: 0.3108\n",
            "Epoch 270/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4014 - accuracy: 0.2969\n",
            "Epoch 270: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3795 - accuracy: 0.2517 - val_loss: 1.4946 - val_accuracy: 0.3108\n",
            "Epoch 271/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4031 - accuracy: 0.2500\n",
            "Epoch 271: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3773 - accuracy: 0.2517 - val_loss: 1.4999 - val_accuracy: 0.3108\n",
            "Epoch 272/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3696 - accuracy: 0.2891\n",
            "Epoch 272: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3792 - accuracy: 0.2517 - val_loss: 1.5026 - val_accuracy: 0.3108\n",
            "Epoch 273/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3404 - accuracy: 0.2578\n",
            "Epoch 273: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3775 - accuracy: 0.2517 - val_loss: 1.4966 - val_accuracy: 0.3108\n",
            "Epoch 274/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3461 - accuracy: 0.2031\n",
            "Epoch 274: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3679 - accuracy: 0.2517 - val_loss: 1.4879 - val_accuracy: 0.3108\n",
            "Epoch 275/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3521 - accuracy: 0.2500\n",
            "Epoch 275: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3668 - accuracy: 0.2517 - val_loss: 1.4817 - val_accuracy: 0.3108\n",
            "Epoch 276/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3146 - accuracy: 0.2188\n",
            "Epoch 276: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3647 - accuracy: 0.2517 - val_loss: 1.4719 - val_accuracy: 0.3108\n",
            "Epoch 277/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3594 - accuracy: 0.2344\n",
            "Epoch 277: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3622 - accuracy: 0.2517 - val_loss: 1.4636 - val_accuracy: 0.3108\n",
            "Epoch 278/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3259 - accuracy: 0.2500\n",
            "Epoch 278: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3549 - accuracy: 0.2517 - val_loss: 1.4623 - val_accuracy: 0.3108\n",
            "Epoch 279/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3991 - accuracy: 0.2891\n",
            "Epoch 279: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3561 - accuracy: 0.2517 - val_loss: 1.4630 - val_accuracy: 0.3108\n",
            "Epoch 280/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3562 - accuracy: 0.2734\n",
            "Epoch 280: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3539 - accuracy: 0.2517 - val_loss: 1.4598 - val_accuracy: 0.3108\n",
            "Epoch 281/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3673 - accuracy: 0.2031\n",
            "Epoch 281: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3496 - accuracy: 0.2517 - val_loss: 1.4555 - val_accuracy: 0.3108\n",
            "Epoch 282/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3447 - accuracy: 0.2734\n",
            "Epoch 282: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.3453 - accuracy: 0.2517 - val_loss: 1.4570 - val_accuracy: 0.3108\n",
            "Epoch 283/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3980 - accuracy: 0.2734\n",
            "Epoch 283: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3452 - accuracy: 0.2517 - val_loss: 1.4598 - val_accuracy: 0.3108\n",
            "Epoch 284/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2674 - accuracy: 0.2500\n",
            "Epoch 284: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3438 - accuracy: 0.2517 - val_loss: 1.4600 - val_accuracy: 0.3108\n",
            "Epoch 285/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3847 - accuracy: 0.2422\n",
            "Epoch 285: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3397 - accuracy: 0.2517 - val_loss: 1.4569 - val_accuracy: 0.3108\n",
            "Epoch 286/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3078 - accuracy: 0.2344\n",
            "Epoch 286: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3368 - accuracy: 0.2517 - val_loss: 1.4578 - val_accuracy: 0.3108\n",
            "Epoch 287/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3038 - accuracy: 0.2500\n",
            "Epoch 287: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3333 - accuracy: 0.2517 - val_loss: 1.4587 - val_accuracy: 0.3108\n",
            "Epoch 288/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3321 - accuracy: 0.2578\n",
            "Epoch 288: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3313 - accuracy: 0.2517 - val_loss: 1.4544 - val_accuracy: 0.3108\n",
            "Epoch 289/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3465 - accuracy: 0.2812\n",
            "Epoch 289: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.3274 - accuracy: 0.2517 - val_loss: 1.4536 - val_accuracy: 0.3108\n",
            "Epoch 290/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3390 - accuracy: 0.2656\n",
            "Epoch 290: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.3248 - accuracy: 0.2517 - val_loss: 1.4528 - val_accuracy: 0.3108\n",
            "Epoch 291/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3456 - accuracy: 0.2656\n",
            "Epoch 291: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.3232 - accuracy: 0.2517 - val_loss: 1.4475 - val_accuracy: 0.3108\n",
            "Epoch 292/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3384 - accuracy: 0.2500\n",
            "Epoch 292: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3195 - accuracy: 0.2517 - val_loss: 1.4416 - val_accuracy: 0.3108\n",
            "Epoch 293/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3232 - accuracy: 0.2109\n",
            "Epoch 293: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3156 - accuracy: 0.2517 - val_loss: 1.4346 - val_accuracy: 0.3108\n",
            "Epoch 294/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3132 - accuracy: 0.3047\n",
            "Epoch 294: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.3151 - accuracy: 0.2517 - val_loss: 1.4276 - val_accuracy: 0.3108\n",
            "Epoch 295/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3212 - accuracy: 0.2344\n",
            "Epoch 295: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3102 - accuracy: 0.2517 - val_loss: 1.4225 - val_accuracy: 0.3108\n",
            "Epoch 296/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3257 - accuracy: 0.2188\n",
            "Epoch 296: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3113 - accuracy: 0.2517 - val_loss: 1.4232 - val_accuracy: 0.3108\n",
            "Epoch 297/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3200 - accuracy: 0.2734\n",
            "Epoch 297: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3060 - accuracy: 0.2517 - val_loss: 1.4300 - val_accuracy: 0.3108\n",
            "Epoch 298/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3278 - accuracy: 0.2656\n",
            "Epoch 298: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3049 - accuracy: 0.2517 - val_loss: 1.4355 - val_accuracy: 0.3108\n",
            "Epoch 299/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3175 - accuracy: 0.2734\n",
            "Epoch 299: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3019 - accuracy: 0.2517 - val_loss: 1.4356 - val_accuracy: 0.3108\n",
            "Epoch 300/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3294 - accuracy: 0.2891\n",
            "Epoch 300: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3011 - accuracy: 0.2517 - val_loss: 1.4297 - val_accuracy: 0.3108\n",
            "Epoch 301/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3432 - accuracy: 0.2812\n",
            "Epoch 301: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2979 - accuracy: 0.2517 - val_loss: 1.4238 - val_accuracy: 0.3108\n",
            "Epoch 302/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2388 - accuracy: 0.2500\n",
            "Epoch 302: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2942 - accuracy: 0.2517 - val_loss: 1.4213 - val_accuracy: 0.3108\n",
            "Epoch 303/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3058 - accuracy: 0.2812\n",
            "Epoch 303: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2935 - accuracy: 0.2517 - val_loss: 1.4202 - val_accuracy: 0.3108\n",
            "Epoch 304/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3516 - accuracy: 0.2656\n",
            "Epoch 304: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2892 - accuracy: 0.2517 - val_loss: 1.4194 - val_accuracy: 0.3108\n",
            "Epoch 305/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2827 - accuracy: 0.2812\n",
            "Epoch 305: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2876 - accuracy: 0.2517 - val_loss: 1.4183 - val_accuracy: 0.3108\n",
            "Epoch 306/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3219 - accuracy: 0.1953\n",
            "Epoch 306: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.2898 - accuracy: 0.2517 - val_loss: 1.4121 - val_accuracy: 0.3108\n",
            "Epoch 307/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3355 - accuracy: 0.2969\n",
            "Epoch 307: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2885 - accuracy: 0.2517 - val_loss: 1.4072 - val_accuracy: 0.3108\n",
            "Epoch 308/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2371 - accuracy: 0.2031\n",
            "Epoch 308: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.2829 - accuracy: 0.2517 - val_loss: 1.4053 - val_accuracy: 0.3108\n",
            "Epoch 309/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2386 - accuracy: 0.2500\n",
            "Epoch 309: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2825 - accuracy: 0.2517 - val_loss: 1.4053 - val_accuracy: 0.3108\n",
            "Epoch 310/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2625 - accuracy: 0.2344\n",
            "Epoch 310: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2787 - accuracy: 0.2517 - val_loss: 1.4069 - val_accuracy: 0.3108\n",
            "Epoch 311/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3014 - accuracy: 0.2734\n",
            "Epoch 311: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2794 - accuracy: 0.2517 - val_loss: 1.4078 - val_accuracy: 0.3108\n",
            "Epoch 312/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2557 - accuracy: 0.2969\n",
            "Epoch 312: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2731 - accuracy: 0.2517 - val_loss: 1.4047 - val_accuracy: 0.3108\n",
            "Epoch 313/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2241 - accuracy: 0.2578\n",
            "Epoch 313: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2744 - accuracy: 0.2517 - val_loss: 1.3972 - val_accuracy: 0.3108\n",
            "Epoch 314/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3257 - accuracy: 0.2656\n",
            "Epoch 314: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2680 - accuracy: 0.2517 - val_loss: 1.3923 - val_accuracy: 0.3108\n",
            "Epoch 315/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2281 - accuracy: 0.2656\n",
            "Epoch 315: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2686 - accuracy: 0.2517 - val_loss: 1.3901 - val_accuracy: 0.3108\n",
            "Epoch 316/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2797 - accuracy: 0.2656\n",
            "Epoch 316: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2700 - accuracy: 0.2517 - val_loss: 1.3871 - val_accuracy: 0.3108\n",
            "Epoch 317/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2711 - accuracy: 0.2031\n",
            "Epoch 317: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2651 - accuracy: 0.2517 - val_loss: 1.3852 - val_accuracy: 0.3108\n",
            "Epoch 318/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3002 - accuracy: 0.2578\n",
            "Epoch 318: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2643 - accuracy: 0.2517 - val_loss: 1.3866 - val_accuracy: 0.3108\n",
            "Epoch 319/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2281 - accuracy: 0.2891\n",
            "Epoch 319: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2646 - accuracy: 0.2517 - val_loss: 1.3850 - val_accuracy: 0.3108\n",
            "Epoch 320/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2276 - accuracy: 0.2266\n",
            "Epoch 320: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2585 - accuracy: 0.2517 - val_loss: 1.3802 - val_accuracy: 0.3108\n",
            "Epoch 321/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2420 - accuracy: 0.3047\n",
            "Epoch 321: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2560 - accuracy: 0.2517 - val_loss: 1.3759 - val_accuracy: 0.3108\n",
            "Epoch 322/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2402 - accuracy: 0.2656\n",
            "Epoch 322: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2544 - accuracy: 0.2517 - val_loss: 1.3739 - val_accuracy: 0.3108\n",
            "Epoch 323/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2410 - accuracy: 0.2344\n",
            "Epoch 323: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2569 - accuracy: 0.2517 - val_loss: 1.3733 - val_accuracy: 0.3108\n",
            "Epoch 324/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2314 - accuracy: 0.2578\n",
            "Epoch 324: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2527 - accuracy: 0.2517 - val_loss: 1.3713 - val_accuracy: 0.3108\n",
            "Epoch 325/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2128 - accuracy: 0.2109\n",
            "Epoch 325: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2517 - accuracy: 0.2517 - val_loss: 1.3662 - val_accuracy: 0.3108\n",
            "Epoch 326/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2134 - accuracy: 0.2344\n",
            "Epoch 326: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2499 - accuracy: 0.2517 - val_loss: 1.3628 - val_accuracy: 0.3108\n",
            "Epoch 327/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2837 - accuracy: 0.3047\n",
            "Epoch 327: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2469 - accuracy: 0.2517 - val_loss: 1.3608 - val_accuracy: 0.3108\n",
            "Epoch 328/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1838 - accuracy: 0.2188\n",
            "Epoch 328: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2509 - accuracy: 0.2517 - val_loss: 1.3579 - val_accuracy: 0.3108\n",
            "Epoch 329/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2668 - accuracy: 0.2734\n",
            "Epoch 329: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2475 - accuracy: 0.2517 - val_loss: 1.3564 - val_accuracy: 0.3108\n",
            "Epoch 330/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1950 - accuracy: 0.2422\n",
            "Epoch 330: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2424 - accuracy: 0.2517 - val_loss: 1.3543 - val_accuracy: 0.3108\n",
            "Epoch 331/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2410 - accuracy: 0.2344\n",
            "Epoch 331: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2437 - accuracy: 0.2517 - val_loss: 1.3506 - val_accuracy: 0.3108\n",
            "Epoch 332/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2304 - accuracy: 0.2422\n",
            "Epoch 332: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2424 - accuracy: 0.2517 - val_loss: 1.3485 - val_accuracy: 0.3108\n",
            "Epoch 333/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2565 - accuracy: 0.2344\n",
            "Epoch 333: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2393 - accuracy: 0.2517 - val_loss: 1.3484 - val_accuracy: 0.3108\n",
            "Epoch 334/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2418 - accuracy: 0.1953\n",
            "Epoch 334: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2396 - accuracy: 0.2517 - val_loss: 1.3501 - val_accuracy: 0.3108\n",
            "Epoch 335/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1636 - accuracy: 0.2031\n",
            "Epoch 335: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2338 - accuracy: 0.2517 - val_loss: 1.3527 - val_accuracy: 0.3108\n",
            "Epoch 336/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1930 - accuracy: 0.2812\n",
            "Epoch 336: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2309 - accuracy: 0.2517 - val_loss: 1.3551 - val_accuracy: 0.3108\n",
            "Epoch 337/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2025 - accuracy: 0.2578\n",
            "Epoch 337: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2337 - accuracy: 0.2517 - val_loss: 1.3536 - val_accuracy: 0.3108\n",
            "Epoch 338/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2238 - accuracy: 0.2188\n",
            "Epoch 338: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2288 - accuracy: 0.2517 - val_loss: 1.3510 - val_accuracy: 0.3108\n",
            "Epoch 339/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2850 - accuracy: 0.2734\n",
            "Epoch 339: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2306 - accuracy: 0.2517 - val_loss: 1.3523 - val_accuracy: 0.3108\n",
            "Epoch 340/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1993 - accuracy: 0.2188\n",
            "Epoch 340: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2290 - accuracy: 0.2517 - val_loss: 1.3574 - val_accuracy: 0.3108\n",
            "Epoch 341/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2360 - accuracy: 0.2891\n",
            "Epoch 341: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2258 - accuracy: 0.2517 - val_loss: 1.3658 - val_accuracy: 0.3108\n",
            "Epoch 342/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2134 - accuracy: 0.2344\n",
            "Epoch 342: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2216 - accuracy: 0.2517 - val_loss: 1.3734 - val_accuracy: 0.3108\n",
            "Epoch 343/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1923 - accuracy: 0.2812\n",
            "Epoch 343: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2275 - accuracy: 0.2517 - val_loss: 1.3758 - val_accuracy: 0.3108\n",
            "Epoch 344/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2271 - accuracy: 0.2891\n",
            "Epoch 344: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2215 - accuracy: 0.2517 - val_loss: 1.3739 - val_accuracy: 0.3108\n",
            "Epoch 345/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1127 - accuracy: 0.1562\n",
            "Epoch 345: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2232 - accuracy: 0.2517 - val_loss: 1.3678 - val_accuracy: 0.3108\n",
            "Epoch 346/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2076 - accuracy: 0.2578\n",
            "Epoch 346: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2204 - accuracy: 0.2517 - val_loss: 1.3628 - val_accuracy: 0.3108\n",
            "Epoch 347/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2087 - accuracy: 0.2500\n",
            "Epoch 347: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2238 - accuracy: 0.2517 - val_loss: 1.3563 - val_accuracy: 0.3108\n",
            "Epoch 348/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1771 - accuracy: 0.2344\n",
            "Epoch 348: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2203 - accuracy: 0.2517 - val_loss: 1.3515 - val_accuracy: 0.3108\n",
            "Epoch 349/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2296 - accuracy: 0.2500\n",
            "Epoch 349: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2167 - accuracy: 0.2517 - val_loss: 1.3489 - val_accuracy: 0.3108\n",
            "Epoch 350/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2495 - accuracy: 0.2812\n",
            "Epoch 350: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2157 - accuracy: 0.2517 - val_loss: 1.3484 - val_accuracy: 0.3108\n",
            "Epoch 351/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2303 - accuracy: 0.2422\n",
            "Epoch 351: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2138 - accuracy: 0.2517 - val_loss: 1.3504 - val_accuracy: 0.3108\n",
            "Epoch 352/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2608 - accuracy: 0.2891\n",
            "Epoch 352: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2139 - accuracy: 0.2517 - val_loss: 1.3539 - val_accuracy: 0.3108\n",
            "Epoch 353/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1984 - accuracy: 0.2656\n",
            "Epoch 353: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2118 - accuracy: 0.2517 - val_loss: 1.3566 - val_accuracy: 0.3108\n",
            "Epoch 354/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2193 - accuracy: 0.2266\n",
            "Epoch 354: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2123 - accuracy: 0.2517 - val_loss: 1.3573 - val_accuracy: 0.3108\n",
            "Epoch 355/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1766 - accuracy: 0.2578\n",
            "Epoch 355: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2101 - accuracy: 0.2517 - val_loss: 1.3565 - val_accuracy: 0.3108\n",
            "Epoch 356/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1443 - accuracy: 0.2188\n",
            "Epoch 356: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2107 - accuracy: 0.2517 - val_loss: 1.3512 - val_accuracy: 0.3108\n",
            "Epoch 357/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2424 - accuracy: 0.2422\n",
            "Epoch 357: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2078 - accuracy: 0.2517 - val_loss: 1.3466 - val_accuracy: 0.3108\n",
            "Epoch 358/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2918 - accuracy: 0.2656\n",
            "Epoch 358: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2068 - accuracy: 0.2517 - val_loss: 1.3425 - val_accuracy: 0.3108\n",
            "Epoch 359/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2291 - accuracy: 0.2734\n",
            "Epoch 359: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2088 - accuracy: 0.2517 - val_loss: 1.3381 - val_accuracy: 0.3108\n",
            "Epoch 360/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2368 - accuracy: 0.3047\n",
            "Epoch 360: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2022 - accuracy: 0.2517 - val_loss: 1.3358 - val_accuracy: 0.3108\n",
            "Epoch 361/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2309 - accuracy: 0.2969\n",
            "Epoch 361: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2021 - accuracy: 0.2517 - val_loss: 1.3326 - val_accuracy: 0.3108\n",
            "Epoch 362/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2820 - accuracy: 0.2500\n",
            "Epoch 362: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2026 - accuracy: 0.2517 - val_loss: 1.3283 - val_accuracy: 0.3108\n",
            "Epoch 363/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2156 - accuracy: 0.2812\n",
            "Epoch 363: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2016 - accuracy: 0.2517 - val_loss: 1.3253 - val_accuracy: 0.3108\n",
            "Epoch 364/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2323 - accuracy: 0.2344\n",
            "Epoch 364: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2002 - accuracy: 0.2517 - val_loss: 1.3229 - val_accuracy: 0.3108\n",
            "Epoch 365/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2058 - accuracy: 0.2500\n",
            "Epoch 365: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2002 - accuracy: 0.2517 - val_loss: 1.3203 - val_accuracy: 0.3108\n",
            "Epoch 366/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1983 - accuracy: 0.2500\n",
            "Epoch 366: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1977 - accuracy: 0.2517 - val_loss: 1.3170 - val_accuracy: 0.3108\n",
            "Epoch 367/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2001 - accuracy: 0.2656\n",
            "Epoch 367: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1977 - accuracy: 0.2517 - val_loss: 1.3134 - val_accuracy: 0.3108\n",
            "Epoch 368/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1476 - accuracy: 0.1875\n",
            "Epoch 368: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1969 - accuracy: 0.2517 - val_loss: 1.3137 - val_accuracy: 0.3108\n",
            "Epoch 369/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2201 - accuracy: 0.2734\n",
            "Epoch 369: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1982 - accuracy: 0.2517 - val_loss: 1.3161 - val_accuracy: 0.3108\n",
            "Epoch 370/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1504 - accuracy: 0.2891\n",
            "Epoch 370: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1927 - accuracy: 0.2517 - val_loss: 1.3176 - val_accuracy: 0.3108\n",
            "Epoch 371/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2004 - accuracy: 0.2188\n",
            "Epoch 371: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1956 - accuracy: 0.2517 - val_loss: 1.3184 - val_accuracy: 0.3108\n",
            "Epoch 372/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2125 - accuracy: 0.2812\n",
            "Epoch 372: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1916 - accuracy: 0.2517 - val_loss: 1.3194 - val_accuracy: 0.3108\n",
            "Epoch 373/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2104 - accuracy: 0.2344\n",
            "Epoch 373: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1917 - accuracy: 0.2517 - val_loss: 1.3165 - val_accuracy: 0.3108\n",
            "Epoch 374/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1979 - accuracy: 0.2344\n",
            "Epoch 374: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1893 - accuracy: 0.2517 - val_loss: 1.3149 - val_accuracy: 0.3108\n",
            "Epoch 375/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1743 - accuracy: 0.2266\n",
            "Epoch 375: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1899 - accuracy: 0.2517 - val_loss: 1.3159 - val_accuracy: 0.3108\n",
            "Epoch 376/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1757 - accuracy: 0.2422\n",
            "Epoch 376: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1878 - accuracy: 0.2517 - val_loss: 1.3164 - val_accuracy: 0.3108\n",
            "Epoch 377/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1652 - accuracy: 0.2344\n",
            "Epoch 377: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1887 - accuracy: 0.2517 - val_loss: 1.3136 - val_accuracy: 0.3108\n",
            "Epoch 378/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2333 - accuracy: 0.2578\n",
            "Epoch 378: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1881 - accuracy: 0.2517 - val_loss: 1.3099 - val_accuracy: 0.3108\n",
            "Epoch 379/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2518 - accuracy: 0.2734\n",
            "Epoch 379: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1874 - accuracy: 0.2517 - val_loss: 1.3074 - val_accuracy: 0.3108\n",
            "Epoch 380/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1617 - accuracy: 0.2188\n",
            "Epoch 380: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1860 - accuracy: 0.2517 - val_loss: 1.3058 - val_accuracy: 0.3108\n",
            "Epoch 381/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1572 - accuracy: 0.2500\n",
            "Epoch 381: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1859 - accuracy: 0.2517 - val_loss: 1.3045 - val_accuracy: 0.3108\n",
            "Epoch 382/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1569 - accuracy: 0.2422\n",
            "Epoch 382: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1857 - accuracy: 0.2517 - val_loss: 1.3038 - val_accuracy: 0.3108\n",
            "Epoch 383/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1583 - accuracy: 0.2188\n",
            "Epoch 383: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1829 - accuracy: 0.2517 - val_loss: 1.3035 - val_accuracy: 0.3108\n",
            "Epoch 384/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1922 - accuracy: 0.2344\n",
            "Epoch 384: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1835 - accuracy: 0.2517 - val_loss: 1.3053 - val_accuracy: 0.3108\n",
            "Epoch 385/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1889 - accuracy: 0.2812\n",
            "Epoch 385: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1830 - accuracy: 0.2517 - val_loss: 1.3082 - val_accuracy: 0.3108\n",
            "Epoch 386/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1259 - accuracy: 0.2578\n",
            "Epoch 386: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1833 - accuracy: 0.2517 - val_loss: 1.3098 - val_accuracy: 0.3108\n",
            "Epoch 387/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2116 - accuracy: 0.2500\n",
            "Epoch 387: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1804 - accuracy: 0.2517 - val_loss: 1.3103 - val_accuracy: 0.3108\n",
            "Epoch 388/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1498 - accuracy: 0.2422\n",
            "Epoch 388: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1797 - accuracy: 0.2517 - val_loss: 1.3096 - val_accuracy: 0.3108\n",
            "Epoch 389/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1891 - accuracy: 0.2344\n",
            "Epoch 389: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.1770 - accuracy: 0.2517 - val_loss: 1.3112 - val_accuracy: 0.3108\n",
            "Epoch 390/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1910 - accuracy: 0.2266\n",
            "Epoch 390: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1777 - accuracy: 0.2517 - val_loss: 1.3158 - val_accuracy: 0.3108\n",
            "Epoch 391/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2010 - accuracy: 0.2656\n",
            "Epoch 391: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1780 - accuracy: 0.2517 - val_loss: 1.3218 - val_accuracy: 0.3108\n",
            "Epoch 392/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1193 - accuracy: 0.2109\n",
            "Epoch 392: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1785 - accuracy: 0.2517 - val_loss: 1.3243 - val_accuracy: 0.3108\n",
            "Epoch 393/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1496 - accuracy: 0.2188\n",
            "Epoch 393: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1788 - accuracy: 0.2517 - val_loss: 1.3244 - val_accuracy: 0.3108\n",
            "Epoch 394/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1801 - accuracy: 0.2344\n",
            "Epoch 394: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1704 - accuracy: 0.2517 - val_loss: 1.3248 - val_accuracy: 0.3108\n",
            "Epoch 395/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2210 - accuracy: 0.2578\n",
            "Epoch 395: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.1749 - accuracy: 0.2517 - val_loss: 1.3222 - val_accuracy: 0.3108\n",
            "Epoch 396/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1947 - accuracy: 0.2031\n",
            "Epoch 396: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1758 - accuracy: 0.2517 - val_loss: 1.3171 - val_accuracy: 0.3108\n",
            "Epoch 397/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1617 - accuracy: 0.2422\n",
            "Epoch 397: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.1730 - accuracy: 0.2517 - val_loss: 1.3134 - val_accuracy: 0.3108\n",
            "Epoch 398/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1808 - accuracy: 0.2188\n",
            "Epoch 398: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 1.1712 - accuracy: 0.2517 - val_loss: 1.3101 - val_accuracy: 0.3108\n",
            "Epoch 399/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1352 - accuracy: 0.2734\n",
            "Epoch 399: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1730 - accuracy: 0.2517 - val_loss: 1.3080 - val_accuracy: 0.3108\n",
            "Epoch 400/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1107 - accuracy: 0.2578\n",
            "Epoch 400: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1700 - accuracy: 0.2517 - val_loss: 1.3075 - val_accuracy: 0.3108\n",
            "Epoch 401/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1790 - accuracy: 0.2266\n",
            "Epoch 401: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1709 - accuracy: 0.2517 - val_loss: 1.3095 - val_accuracy: 0.3108\n",
            "Epoch 402/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1526 - accuracy: 0.2812\n",
            "Epoch 402: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1704 - accuracy: 0.2517 - val_loss: 1.3122 - val_accuracy: 0.3108\n",
            "Epoch 403/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1723 - accuracy: 0.2656\n",
            "Epoch 403: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1672 - accuracy: 0.2517 - val_loss: 1.3109 - val_accuracy: 0.3108\n",
            "Epoch 404/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1635 - accuracy: 0.2578\n",
            "Epoch 404: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1662 - accuracy: 0.2517 - val_loss: 1.3068 - val_accuracy: 0.3108\n",
            "Epoch 405/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1803 - accuracy: 0.2578\n",
            "Epoch 405: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1680 - accuracy: 0.2517 - val_loss: 1.3014 - val_accuracy: 0.3108\n",
            "Epoch 406/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1759 - accuracy: 0.2734\n",
            "Epoch 406: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1678 - accuracy: 0.2517 - val_loss: 1.2948 - val_accuracy: 0.3108\n",
            "Epoch 407/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2016 - accuracy: 0.2578\n",
            "Epoch 407: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.1656 - accuracy: 0.2517 - val_loss: 1.2892 - val_accuracy: 0.3108\n",
            "Epoch 408/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2281 - accuracy: 0.2969\n",
            "Epoch 408: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1637 - accuracy: 0.2517 - val_loss: 1.2869 - val_accuracy: 0.3108\n",
            "Epoch 409/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1581 - accuracy: 0.2734\n",
            "Epoch 409: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1661 - accuracy: 0.2517 - val_loss: 1.2856 - val_accuracy: 0.3108\n",
            "Epoch 410/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1958 - accuracy: 0.2656\n",
            "Epoch 410: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1650 - accuracy: 0.2517 - val_loss: 1.2842 - val_accuracy: 0.3108\n",
            "Epoch 411/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1361 - accuracy: 0.2344\n",
            "Epoch 411: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1647 - accuracy: 0.2517 - val_loss: 1.2829 - val_accuracy: 0.3108\n",
            "Epoch 412/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2027 - accuracy: 0.2734\n",
            "Epoch 412: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1647 - accuracy: 0.2517 - val_loss: 1.2821 - val_accuracy: 0.3108\n",
            "Epoch 413/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1143 - accuracy: 0.2188\n",
            "Epoch 413: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1622 - accuracy: 0.2517 - val_loss: 1.2834 - val_accuracy: 0.3108\n",
            "Epoch 414/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2017 - accuracy: 0.2500\n",
            "Epoch 414: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1631 - accuracy: 0.2517 - val_loss: 1.2859 - val_accuracy: 0.3108\n",
            "Epoch 415/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1553 - accuracy: 0.2266\n",
            "Epoch 415: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1615 - accuracy: 0.2517 - val_loss: 1.2880 - val_accuracy: 0.3108\n",
            "Epoch 416/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1685 - accuracy: 0.2188\n",
            "Epoch 416: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1613 - accuracy: 0.2517 - val_loss: 1.2913 - val_accuracy: 0.3108\n",
            "Epoch 417/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1509 - accuracy: 0.2188\n",
            "Epoch 417: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1615 - accuracy: 0.2517 - val_loss: 1.2954 - val_accuracy: 0.3108\n",
            "Epoch 418/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1354 - accuracy: 0.2656\n",
            "Epoch 418: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1603 - accuracy: 0.2517 - val_loss: 1.2979 - val_accuracy: 0.3108\n",
            "Epoch 419/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1133 - accuracy: 0.2500\n",
            "Epoch 419: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1606 - accuracy: 0.2517 - val_loss: 1.2980 - val_accuracy: 0.3108\n",
            "Epoch 420/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1475 - accuracy: 0.2422\n",
            "Epoch 420: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1588 - accuracy: 0.2517 - val_loss: 1.2989 - val_accuracy: 0.3108\n",
            "Epoch 421/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1539 - accuracy: 0.2500\n",
            "Epoch 421: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1592 - accuracy: 0.2517 - val_loss: 1.2999 - val_accuracy: 0.3108\n",
            "Epoch 422/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1952 - accuracy: 0.2734\n",
            "Epoch 422: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1575 - accuracy: 0.2517 - val_loss: 1.2999 - val_accuracy: 0.3108\n",
            "Epoch 423/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2173 - accuracy: 0.2656\n",
            "Epoch 423: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1584 - accuracy: 0.2517 - val_loss: 1.2991 - val_accuracy: 0.3108\n",
            "Epoch 424/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1810 - accuracy: 0.2578\n",
            "Epoch 424: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1556 - accuracy: 0.2517 - val_loss: 1.2983 - val_accuracy: 0.3108\n",
            "Epoch 425/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1963 - accuracy: 0.2344\n",
            "Epoch 425: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1555 - accuracy: 0.2517 - val_loss: 1.2969 - val_accuracy: 0.3108\n",
            "Epoch 426/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1384 - accuracy: 0.2500\n",
            "Epoch 426: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1548 - accuracy: 0.2517 - val_loss: 1.2947 - val_accuracy: 0.3108\n",
            "Epoch 427/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1797 - accuracy: 0.2812\n",
            "Epoch 427: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1569 - accuracy: 0.2517 - val_loss: 1.2907 - val_accuracy: 0.3108\n",
            "Epoch 428/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1693 - accuracy: 0.2656\n",
            "Epoch 428: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1548 - accuracy: 0.2517 - val_loss: 1.2855 - val_accuracy: 0.3108\n",
            "Epoch 429/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1671 - accuracy: 0.2812\n",
            "Epoch 429: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1537 - accuracy: 0.2517 - val_loss: 1.2810 - val_accuracy: 0.3108\n",
            "Epoch 430/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1225 - accuracy: 0.2422\n",
            "Epoch 430: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1538 - accuracy: 0.2517 - val_loss: 1.2788 - val_accuracy: 0.3108\n",
            "Epoch 431/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2071 - accuracy: 0.3203\n",
            "Epoch 431: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1543 - accuracy: 0.2517 - val_loss: 1.2780 - val_accuracy: 0.3108\n",
            "Epoch 432/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1250 - accuracy: 0.2344\n",
            "Epoch 432: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1518 - accuracy: 0.2517 - val_loss: 1.2760 - val_accuracy: 0.3108\n",
            "Epoch 433/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1462 - accuracy: 0.2656\n",
            "Epoch 433: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1522 - accuracy: 0.2517 - val_loss: 1.2745 - val_accuracy: 0.3108\n",
            "Epoch 434/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1152 - accuracy: 0.2344\n",
            "Epoch 434: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1520 - accuracy: 0.2517 - val_loss: 1.2739 - val_accuracy: 0.3108\n",
            "Epoch 435/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2117 - accuracy: 0.2656\n",
            "Epoch 435: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1527 - accuracy: 0.2517 - val_loss: 1.2750 - val_accuracy: 0.3108\n",
            "Epoch 436/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1368 - accuracy: 0.2969\n",
            "Epoch 436: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1512 - accuracy: 0.2517 - val_loss: 1.2768 - val_accuracy: 0.3108\n",
            "Epoch 437/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1496 - accuracy: 0.2578\n",
            "Epoch 437: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1521 - accuracy: 0.2517 - val_loss: 1.2760 - val_accuracy: 0.3108\n",
            "Epoch 438/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1094 - accuracy: 0.2578\n",
            "Epoch 438: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1493 - accuracy: 0.2517 - val_loss: 1.2749 - val_accuracy: 0.3108\n",
            "Epoch 439/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1900 - accuracy: 0.2500\n",
            "Epoch 439: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1496 - accuracy: 0.2517 - val_loss: 1.2735 - val_accuracy: 0.3108\n",
            "Epoch 440/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1576 - accuracy: 0.2188\n",
            "Epoch 440: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1501 - accuracy: 0.2517 - val_loss: 1.2730 - val_accuracy: 0.3108\n",
            "Epoch 441/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1403 - accuracy: 0.2578\n",
            "Epoch 441: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1503 - accuracy: 0.2517 - val_loss: 1.2730 - val_accuracy: 0.3108\n",
            "Epoch 442/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1485 - accuracy: 0.3359\n",
            "Epoch 442: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1503 - accuracy: 0.2517 - val_loss: 1.2728 - val_accuracy: 0.3108\n",
            "Epoch 443/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1469 - accuracy: 0.2578\n",
            "Epoch 443: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1481 - accuracy: 0.2517 - val_loss: 1.2717 - val_accuracy: 0.3108\n",
            "Epoch 444/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1859 - accuracy: 0.2734\n",
            "Epoch 444: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1495 - accuracy: 0.2517 - val_loss: 1.2712 - val_accuracy: 0.3108\n",
            "Epoch 445/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1753 - accuracy: 0.2422\n",
            "Epoch 445: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1503 - accuracy: 0.2517 - val_loss: 1.2701 - val_accuracy: 0.3108\n",
            "Epoch 446/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1097 - accuracy: 0.2109\n",
            "Epoch 446: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1461 - accuracy: 0.2517 - val_loss: 1.2700 - val_accuracy: 0.3108\n",
            "Epoch 447/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1159 - accuracy: 0.2812\n",
            "Epoch 447: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1467 - accuracy: 0.2517 - val_loss: 1.2703 - val_accuracy: 0.3108\n",
            "Epoch 448/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1931 - accuracy: 0.2734\n",
            "Epoch 448: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1462 - accuracy: 0.2517 - val_loss: 1.2696 - val_accuracy: 0.3108\n",
            "Epoch 449/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1415 - accuracy: 0.2656\n",
            "Epoch 449: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1462 - accuracy: 0.2517 - val_loss: 1.2712 - val_accuracy: 0.3108\n",
            "Epoch 450/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1881 - accuracy: 0.3281\n",
            "Epoch 450: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1466 - accuracy: 0.2517 - val_loss: 1.2742 - val_accuracy: 0.3108\n",
            "Epoch 451/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1486 - accuracy: 0.2266\n",
            "Epoch 451: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1447 - accuracy: 0.2517 - val_loss: 1.2749 - val_accuracy: 0.3108\n",
            "Epoch 452/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0956 - accuracy: 0.2422\n",
            "Epoch 452: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1449 - accuracy: 0.2517 - val_loss: 1.2752 - val_accuracy: 0.3108\n",
            "Epoch 453/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1022 - accuracy: 0.2266\n",
            "Epoch 453: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1446 - accuracy: 0.2517 - val_loss: 1.2761 - val_accuracy: 0.3108\n",
            "Epoch 454/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0981 - accuracy: 0.2188\n",
            "Epoch 454: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1460 - accuracy: 0.2517 - val_loss: 1.2766 - val_accuracy: 0.3108\n",
            "Epoch 455/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1534 - accuracy: 0.2344\n",
            "Epoch 455: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1431 - accuracy: 0.2517 - val_loss: 1.2759 - val_accuracy: 0.3108\n",
            "Epoch 456/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1214 - accuracy: 0.2422\n",
            "Epoch 456: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1446 - accuracy: 0.2517 - val_loss: 1.2741 - val_accuracy: 0.3108\n",
            "Epoch 457/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1261 - accuracy: 0.2500\n",
            "Epoch 457: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1432 - accuracy: 0.2517 - val_loss: 1.2712 - val_accuracy: 0.3108\n",
            "Epoch 458/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1719 - accuracy: 0.2578\n",
            "Epoch 458: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1447 - accuracy: 0.2517 - val_loss: 1.2670 - val_accuracy: 0.3108\n",
            "Epoch 459/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1236 - accuracy: 0.2578\n",
            "Epoch 459: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1439 - accuracy: 0.2517 - val_loss: 1.2632 - val_accuracy: 0.3108\n",
            "Epoch 460/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0971 - accuracy: 0.2500\n",
            "Epoch 460: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1421 - accuracy: 0.2517 - val_loss: 1.2596 - val_accuracy: 0.3108\n",
            "Epoch 461/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2103 - accuracy: 0.2656\n",
            "Epoch 461: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1410 - accuracy: 0.2517 - val_loss: 1.2581 - val_accuracy: 0.3108\n",
            "Epoch 462/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1560 - accuracy: 0.2656\n",
            "Epoch 462: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1429 - accuracy: 0.2517 - val_loss: 1.2577 - val_accuracy: 0.3108\n",
            "Epoch 463/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1621 - accuracy: 0.2734\n",
            "Epoch 463: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1415 - accuracy: 0.2517 - val_loss: 1.2565 - val_accuracy: 0.3108\n",
            "Epoch 464/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1202 - accuracy: 0.3047\n",
            "Epoch 464: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1418 - accuracy: 0.2517 - val_loss: 1.2560 - val_accuracy: 0.3108\n",
            "Epoch 465/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1144 - accuracy: 0.2266\n",
            "Epoch 465: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1412 - accuracy: 0.2517 - val_loss: 1.2557 - val_accuracy: 0.3108\n",
            "Epoch 466/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1337 - accuracy: 0.2266\n",
            "Epoch 466: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1402 - accuracy: 0.2517 - val_loss: 1.2573 - val_accuracy: 0.3108\n",
            "Epoch 467/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1816 - accuracy: 0.2734\n",
            "Epoch 467: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1413 - accuracy: 0.2517 - val_loss: 1.2601 - val_accuracy: 0.3108\n",
            "Epoch 468/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1128 - accuracy: 0.2734\n",
            "Epoch 468: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1397 - accuracy: 0.2517 - val_loss: 1.2625 - val_accuracy: 0.3108\n",
            "Epoch 469/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0993 - accuracy: 0.2578\n",
            "Epoch 469: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1403 - accuracy: 0.2517 - val_loss: 1.2631 - val_accuracy: 0.3108\n",
            "Epoch 470/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1480 - accuracy: 0.2188\n",
            "Epoch 470: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1379 - accuracy: 0.2517 - val_loss: 1.2638 - val_accuracy: 0.3108\n",
            "Epoch 471/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1626 - accuracy: 0.2656\n",
            "Epoch 471: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1391 - accuracy: 0.2517 - val_loss: 1.2631 - val_accuracy: 0.3108\n",
            "Epoch 472/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1752 - accuracy: 0.2969\n",
            "Epoch 472: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1394 - accuracy: 0.2517 - val_loss: 1.2619 - val_accuracy: 0.3108\n",
            "Epoch 473/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1723 - accuracy: 0.2812\n",
            "Epoch 473: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1384 - accuracy: 0.2517 - val_loss: 1.2628 - val_accuracy: 0.3108\n",
            "Epoch 474/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1230 - accuracy: 0.2656\n",
            "Epoch 474: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1390 - accuracy: 0.2517 - val_loss: 1.2649 - val_accuracy: 0.3108\n",
            "Epoch 475/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1490 - accuracy: 0.2500\n",
            "Epoch 475: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1375 - accuracy: 0.2517 - val_loss: 1.2670 - val_accuracy: 0.3108\n",
            "Epoch 476/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1369 - accuracy: 0.2734\n",
            "Epoch 476: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1365 - accuracy: 0.2517 - val_loss: 1.2691 - val_accuracy: 0.3108\n",
            "Epoch 477/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1309 - accuracy: 0.2109\n",
            "Epoch 477: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1380 - accuracy: 0.2517 - val_loss: 1.2701 - val_accuracy: 0.3108\n",
            "Epoch 478/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1484 - accuracy: 0.2891\n",
            "Epoch 478: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1376 - accuracy: 0.2517 - val_loss: 1.2707 - val_accuracy: 0.3108\n",
            "Epoch 479/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1102 - accuracy: 0.2500\n",
            "Epoch 479: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1370 - accuracy: 0.2517 - val_loss: 1.2715 - val_accuracy: 0.3108\n",
            "Epoch 480/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1280 - accuracy: 0.2344\n",
            "Epoch 480: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1365 - accuracy: 0.2517 - val_loss: 1.2721 - val_accuracy: 0.3108\n",
            "Epoch 481/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1553 - accuracy: 0.2734\n",
            "Epoch 481: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1363 - accuracy: 0.2517 - val_loss: 1.2723 - val_accuracy: 0.3108\n",
            "Epoch 482/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1257 - accuracy: 0.2031\n",
            "Epoch 482: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1362 - accuracy: 0.2517 - val_loss: 1.2736 - val_accuracy: 0.3108\n",
            "Epoch 483/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1328 - accuracy: 0.2969\n",
            "Epoch 483: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1360 - accuracy: 0.2517 - val_loss: 1.2754 - val_accuracy: 0.3108\n",
            "Epoch 484/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1191 - accuracy: 0.2344\n",
            "Epoch 484: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1366 - accuracy: 0.2517 - val_loss: 1.2763 - val_accuracy: 0.3108\n",
            "Epoch 485/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1146 - accuracy: 0.1875\n",
            "Epoch 485: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1360 - accuracy: 0.2517 - val_loss: 1.2772 - val_accuracy: 0.3108\n",
            "Epoch 486/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1950 - accuracy: 0.2266\n",
            "Epoch 486: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1356 - accuracy: 0.2517 - val_loss: 1.2779 - val_accuracy: 0.3108\n",
            "Epoch 487/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0978 - accuracy: 0.2188\n",
            "Epoch 487: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1362 - accuracy: 0.2517 - val_loss: 1.2790 - val_accuracy: 0.3108\n",
            "Epoch 488/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1283 - accuracy: 0.2656\n",
            "Epoch 488: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1346 - accuracy: 0.2517 - val_loss: 1.2792 - val_accuracy: 0.3108\n",
            "Epoch 489/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1448 - accuracy: 0.2344\n",
            "Epoch 489: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1347 - accuracy: 0.2517 - val_loss: 1.2772 - val_accuracy: 0.3108\n",
            "Epoch 490/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2154 - accuracy: 0.2422\n",
            "Epoch 490: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1331 - accuracy: 0.2517 - val_loss: 1.2760 - val_accuracy: 0.3108\n",
            "Epoch 491/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1403 - accuracy: 0.2266\n",
            "Epoch 491: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1340 - accuracy: 0.2517 - val_loss: 1.2747 - val_accuracy: 0.3108\n",
            "Epoch 492/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0947 - accuracy: 0.2578\n",
            "Epoch 492: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1339 - accuracy: 0.2517 - val_loss: 1.2728 - val_accuracy: 0.3108\n",
            "Epoch 493/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1204 - accuracy: 0.2266\n",
            "Epoch 493: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1342 - accuracy: 0.2517 - val_loss: 1.2709 - val_accuracy: 0.3108\n",
            "Epoch 494/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1118 - accuracy: 0.2266\n",
            "Epoch 494: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1344 - accuracy: 0.2517 - val_loss: 1.2698 - val_accuracy: 0.3108\n",
            "Epoch 495/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1697 - accuracy: 0.2734\n",
            "Epoch 495: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1335 - accuracy: 0.2517 - val_loss: 1.2692 - val_accuracy: 0.3108\n",
            "Epoch 496/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1241 - accuracy: 0.2344\n",
            "Epoch 496: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1342 - accuracy: 0.2517 - val_loss: 1.2687 - val_accuracy: 0.3108\n",
            "Epoch 497/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1444 - accuracy: 0.2891\n",
            "Epoch 497: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1341 - accuracy: 0.2517 - val_loss: 1.2688 - val_accuracy: 0.3108\n",
            "Epoch 498/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1656 - accuracy: 0.2266\n",
            "Epoch 498: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1329 - accuracy: 0.2517 - val_loss: 1.2694 - val_accuracy: 0.3108\n",
            "Epoch 499/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0892 - accuracy: 0.2500\n",
            "Epoch 499: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1337 - accuracy: 0.2517 - val_loss: 1.2705 - val_accuracy: 0.3108\n",
            "Epoch 500/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1388 - accuracy: 0.2266\n",
            "Epoch 500: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1341 - accuracy: 0.2517 - val_loss: 1.2692 - val_accuracy: 0.3108\n",
            "Epoch 501/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1457 - accuracy: 0.2266\n",
            "Epoch 501: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1317 - accuracy: 0.2517 - val_loss: 1.2665 - val_accuracy: 0.3108\n",
            "Epoch 502/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1320 - accuracy: 0.2344\n",
            "Epoch 502: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1321 - accuracy: 0.2517 - val_loss: 1.2643 - val_accuracy: 0.3108\n",
            "Epoch 503/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1553 - accuracy: 0.2422\n",
            "Epoch 503: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1305 - accuracy: 0.2517 - val_loss: 1.2635 - val_accuracy: 0.3108\n",
            "Epoch 504/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1546 - accuracy: 0.2266\n",
            "Epoch 504: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1318 - accuracy: 0.2517 - val_loss: 1.2635 - val_accuracy: 0.3108\n",
            "Epoch 505/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0425 - accuracy: 0.1875\n",
            "Epoch 505: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1310 - accuracy: 0.2517 - val_loss: 1.2644 - val_accuracy: 0.3108\n",
            "Epoch 506/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1410 - accuracy: 0.2891\n",
            "Epoch 506: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1316 - accuracy: 0.2517 - val_loss: 1.2665 - val_accuracy: 0.3108\n",
            "Epoch 507/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1025 - accuracy: 0.2891\n",
            "Epoch 507: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1318 - accuracy: 0.2517 - val_loss: 1.2671 - val_accuracy: 0.3108\n",
            "Epoch 508/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0858 - accuracy: 0.2500\n",
            "Epoch 508: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1311 - accuracy: 0.2517 - val_loss: 1.2650 - val_accuracy: 0.3108\n",
            "Epoch 509/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1313 - accuracy: 0.2500\n",
            "Epoch 509: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1309 - accuracy: 0.2517 - val_loss: 1.2612 - val_accuracy: 0.3108\n",
            "Epoch 510/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0906 - accuracy: 0.2578\n",
            "Epoch 510: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1287 - accuracy: 0.2517 - val_loss: 1.2569 - val_accuracy: 0.3108\n",
            "Epoch 511/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1456 - accuracy: 0.2422\n",
            "Epoch 511: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1300 - accuracy: 0.2517 - val_loss: 1.2538 - val_accuracy: 0.3108\n",
            "Epoch 512/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1689 - accuracy: 0.2500\n",
            "Epoch 512: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1302 - accuracy: 0.2517 - val_loss: 1.2511 - val_accuracy: 0.3108\n",
            "Epoch 513/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1505 - accuracy: 0.2578\n",
            "Epoch 513: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1315 - accuracy: 0.2517 - val_loss: 1.2480 - val_accuracy: 0.3108\n",
            "Epoch 514/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0887 - accuracy: 0.2656\n",
            "Epoch 514: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1303 - accuracy: 0.2517 - val_loss: 1.2449 - val_accuracy: 0.3108\n",
            "Epoch 515/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1476 - accuracy: 0.2812\n",
            "Epoch 515: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1302 - accuracy: 0.2517 - val_loss: 1.2427 - val_accuracy: 0.3108\n",
            "Epoch 516/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1758 - accuracy: 0.2734\n",
            "Epoch 516: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1294 - accuracy: 0.2517 - val_loss: 1.2431 - val_accuracy: 0.3108\n",
            "Epoch 517/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1131 - accuracy: 0.2266\n",
            "Epoch 517: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1304 - accuracy: 0.2517 - val_loss: 1.2461 - val_accuracy: 0.3108\n",
            "Epoch 518/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1116 - accuracy: 0.2344\n",
            "Epoch 518: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1305 - accuracy: 0.2517 - val_loss: 1.2491 - val_accuracy: 0.3108\n",
            "Epoch 519/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1834 - accuracy: 0.2812\n",
            "Epoch 519: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1299 - accuracy: 0.2517 - val_loss: 1.2498 - val_accuracy: 0.3108\n",
            "Epoch 520/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1014 - accuracy: 0.2344\n",
            "Epoch 520: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1289 - accuracy: 0.2517 - val_loss: 1.2494 - val_accuracy: 0.3108\n",
            "Epoch 521/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1556 - accuracy: 0.2812\n",
            "Epoch 521: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1289 - accuracy: 0.2517 - val_loss: 1.2494 - val_accuracy: 0.3108\n",
            "Epoch 522/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0839 - accuracy: 0.2266\n",
            "Epoch 522: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1280 - accuracy: 0.2517 - val_loss: 1.2485 - val_accuracy: 0.3108\n",
            "Epoch 523/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1710 - accuracy: 0.2422\n",
            "Epoch 523: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1288 - accuracy: 0.2517 - val_loss: 1.2482 - val_accuracy: 0.3108\n",
            "Epoch 524/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1281 - accuracy: 0.2109\n",
            "Epoch 524: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1283 - accuracy: 0.2517 - val_loss: 1.2483 - val_accuracy: 0.3108\n",
            "Epoch 525/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1386 - accuracy: 0.2500\n",
            "Epoch 525: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1280 - accuracy: 0.2517 - val_loss: 1.2486 - val_accuracy: 0.3108\n",
            "Epoch 526/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1571 - accuracy: 0.2969\n",
            "Epoch 526: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1287 - accuracy: 0.2517 - val_loss: 1.2484 - val_accuracy: 0.3108\n",
            "Epoch 527/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1545 - accuracy: 0.2344\n",
            "Epoch 527: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1278 - accuracy: 0.2517 - val_loss: 1.2479 - val_accuracy: 0.3108\n",
            "Epoch 528/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0891 - accuracy: 0.2578\n",
            "Epoch 528: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1274 - accuracy: 0.2517 - val_loss: 1.2484 - val_accuracy: 0.3108\n",
            "Epoch 529/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0926 - accuracy: 0.2109\n",
            "Epoch 529: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1277 - accuracy: 0.2517 - val_loss: 1.2499 - val_accuracy: 0.3108\n",
            "Epoch 530/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1607 - accuracy: 0.2656\n",
            "Epoch 530: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1276 - accuracy: 0.2517 - val_loss: 1.2527 - val_accuracy: 0.3108\n",
            "Epoch 531/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1532 - accuracy: 0.2578\n",
            "Epoch 531: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1268 - accuracy: 0.2517 - val_loss: 1.2553 - val_accuracy: 0.3108\n",
            "Epoch 532/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1161 - accuracy: 0.2188\n",
            "Epoch 532: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1271 - accuracy: 0.2517 - val_loss: 1.2580 - val_accuracy: 0.3108\n",
            "Epoch 533/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1455 - accuracy: 0.2734\n",
            "Epoch 533: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1275 - accuracy: 0.2517 - val_loss: 1.2610 - val_accuracy: 0.3108\n",
            "Epoch 534/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1261 - accuracy: 0.2734\n",
            "Epoch 534: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1261 - accuracy: 0.2517 - val_loss: 1.2624 - val_accuracy: 0.3108\n",
            "Epoch 535/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0928 - accuracy: 0.2422\n",
            "Epoch 535: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1261 - accuracy: 0.2517 - val_loss: 1.2625 - val_accuracy: 0.3108\n",
            "Epoch 536/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1409 - accuracy: 0.2266\n",
            "Epoch 536: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1259 - accuracy: 0.2517 - val_loss: 1.2621 - val_accuracy: 0.3108\n",
            "Epoch 537/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1030 - accuracy: 0.2656\n",
            "Epoch 537: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1272 - accuracy: 0.2517 - val_loss: 1.2608 - val_accuracy: 0.3108\n",
            "Epoch 538/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1251 - accuracy: 0.2734\n",
            "Epoch 538: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1263 - accuracy: 0.2517 - val_loss: 1.2586 - val_accuracy: 0.3108\n",
            "Epoch 539/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1881 - accuracy: 0.2656\n",
            "Epoch 539: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1264 - accuracy: 0.2517 - val_loss: 1.2563 - val_accuracy: 0.3108\n",
            "Epoch 540/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1549 - accuracy: 0.2344\n",
            "Epoch 540: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1258 - accuracy: 0.2517 - val_loss: 1.2543 - val_accuracy: 0.3108\n",
            "Epoch 541/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0670 - accuracy: 0.2578\n",
            "Epoch 541: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1261 - accuracy: 0.2517 - val_loss: 1.2533 - val_accuracy: 0.3108\n",
            "Epoch 542/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0837 - accuracy: 0.2656\n",
            "Epoch 542: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1261 - accuracy: 0.2517 - val_loss: 1.2528 - val_accuracy: 0.3108\n",
            "Epoch 543/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2173 - accuracy: 0.2422\n",
            "Epoch 543: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1274 - accuracy: 0.2517 - val_loss: 1.2528 - val_accuracy: 0.3108\n",
            "Epoch 544/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1060 - accuracy: 0.2188\n",
            "Epoch 544: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1269 - accuracy: 0.2517 - val_loss: 1.2533 - val_accuracy: 0.3108\n",
            "Epoch 545/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0725 - accuracy: 0.2109\n",
            "Epoch 545: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1259 - accuracy: 0.2517 - val_loss: 1.2536 - val_accuracy: 0.3108\n",
            "Epoch 546/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1238 - accuracy: 0.2969\n",
            "Epoch 546: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1253 - accuracy: 0.2517 - val_loss: 1.2525 - val_accuracy: 0.3108\n",
            "Epoch 547/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1858 - accuracy: 0.2891\n",
            "Epoch 547: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1258 - accuracy: 0.2517 - val_loss: 1.2507 - val_accuracy: 0.3108\n",
            "Epoch 548/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0798 - accuracy: 0.2109\n",
            "Epoch 548: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1250 - accuracy: 0.2517 - val_loss: 1.2493 - val_accuracy: 0.3108\n",
            "Epoch 549/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1914 - accuracy: 0.2344\n",
            "Epoch 549: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1270 - accuracy: 0.2517 - val_loss: 1.2485 - val_accuracy: 0.3108\n",
            "Epoch 550/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1124 - accuracy: 0.2422\n",
            "Epoch 550: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1256 - accuracy: 0.2517 - val_loss: 1.2482 - val_accuracy: 0.3108\n",
            "Epoch 551/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1240 - accuracy: 0.2578\n",
            "Epoch 551: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1260 - accuracy: 0.2517 - val_loss: 1.2483 - val_accuracy: 0.3108\n",
            "Epoch 552/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1357 - accuracy: 0.2734\n",
            "Epoch 552: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1253 - accuracy: 0.2517 - val_loss: 1.2492 - val_accuracy: 0.3108\n",
            "Epoch 553/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0968 - accuracy: 0.2734\n",
            "Epoch 553: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1244 - accuracy: 0.2517 - val_loss: 1.2493 - val_accuracy: 0.3108\n",
            "Epoch 554/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1424 - accuracy: 0.2734\n",
            "Epoch 554: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1243 - accuracy: 0.2517 - val_loss: 1.2490 - val_accuracy: 0.3108\n",
            "Epoch 555/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1914 - accuracy: 0.2500\n",
            "Epoch 555: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1243 - accuracy: 0.2517 - val_loss: 1.2488 - val_accuracy: 0.3108\n",
            "Epoch 556/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1754 - accuracy: 0.2656\n",
            "Epoch 556: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1243 - accuracy: 0.2517 - val_loss: 1.2478 - val_accuracy: 0.3108\n",
            "Epoch 557/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1151 - accuracy: 0.2109\n",
            "Epoch 557: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1235 - accuracy: 0.2517 - val_loss: 1.2461 - val_accuracy: 0.3108\n",
            "Epoch 558/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1528 - accuracy: 0.2344\n",
            "Epoch 558: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1246 - accuracy: 0.2517 - val_loss: 1.2457 - val_accuracy: 0.3108\n",
            "Epoch 559/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1261 - accuracy: 0.2500\n",
            "Epoch 559: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1244 - accuracy: 0.2517 - val_loss: 1.2464 - val_accuracy: 0.3108\n",
            "Epoch 560/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1362 - accuracy: 0.2891\n",
            "Epoch 560: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1242 - accuracy: 0.2517 - val_loss: 1.2480 - val_accuracy: 0.3108\n",
            "Epoch 561/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1591 - accuracy: 0.2422\n",
            "Epoch 561: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1247 - accuracy: 0.2517 - val_loss: 1.2503 - val_accuracy: 0.3108\n",
            "Epoch 562/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1267 - accuracy: 0.2109\n",
            "Epoch 562: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1234 - accuracy: 0.2517 - val_loss: 1.2529 - val_accuracy: 0.3108\n",
            "Epoch 563/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0984 - accuracy: 0.2344\n",
            "Epoch 563: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1248 - accuracy: 0.2517 - val_loss: 1.2557 - val_accuracy: 0.3108\n",
            "Epoch 564/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1663 - accuracy: 0.2656\n",
            "Epoch 564: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1235 - accuracy: 0.2517 - val_loss: 1.2579 - val_accuracy: 0.3108\n",
            "Epoch 565/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1240 - accuracy: 0.2734\n",
            "Epoch 565: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1240 - accuracy: 0.2517 - val_loss: 1.2590 - val_accuracy: 0.3108\n",
            "Epoch 566/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1315 - accuracy: 0.2734\n",
            "Epoch 566: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1236 - accuracy: 0.2517 - val_loss: 1.2588 - val_accuracy: 0.3108\n",
            "Epoch 567/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1301 - accuracy: 0.2812\n",
            "Epoch 567: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.1236 - accuracy: 0.2517 - val_loss: 1.2568 - val_accuracy: 0.3108\n",
            "Epoch 568/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1494 - accuracy: 0.2031\n",
            "Epoch 568: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.1239 - accuracy: 0.2517 - val_loss: 1.2535 - val_accuracy: 0.3108\n",
            "Epoch 569/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1070 - accuracy: 0.2656\n",
            "Epoch 569: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1232 - accuracy: 0.2517 - val_loss: 1.2529 - val_accuracy: 0.3108\n",
            "Epoch 570/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1549 - accuracy: 0.2266\n",
            "Epoch 570: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1228 - accuracy: 0.2517 - val_loss: 1.2540 - val_accuracy: 0.3108\n",
            "Epoch 571/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1407 - accuracy: 0.2734\n",
            "Epoch 571: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1223 - accuracy: 0.2517 - val_loss: 1.2551 - val_accuracy: 0.3108\n",
            "Epoch 572/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1060 - accuracy: 0.2500\n",
            "Epoch 572: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1228 - accuracy: 0.2517 - val_loss: 1.2554 - val_accuracy: 0.3108\n",
            "Epoch 573/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1448 - accuracy: 0.2109\n",
            "Epoch 573: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1226 - accuracy: 0.2517 - val_loss: 1.2559 - val_accuracy: 0.3108\n",
            "Epoch 574/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1051 - accuracy: 0.2500\n",
            "Epoch 574: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1228 - accuracy: 0.2517 - val_loss: 1.2581 - val_accuracy: 0.3108\n",
            "Epoch 575/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1533 - accuracy: 0.2656\n",
            "Epoch 575: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1220 - accuracy: 0.2517 - val_loss: 1.2609 - val_accuracy: 0.3108\n",
            "Epoch 576/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1068 - accuracy: 0.2500\n",
            "Epoch 576: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1235 - accuracy: 0.2517 - val_loss: 1.2627 - val_accuracy: 0.3108\n",
            "Epoch 577/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1115 - accuracy: 0.2578\n",
            "Epoch 577: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1233 - accuracy: 0.2517 - val_loss: 1.2631 - val_accuracy: 0.3108\n",
            "Epoch 578/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1289 - accuracy: 0.2422\n",
            "Epoch 578: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1224 - accuracy: 0.2517 - val_loss: 1.2627 - val_accuracy: 0.3108\n",
            "Epoch 579/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0895 - accuracy: 0.2969\n",
            "Epoch 579: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1235 - accuracy: 0.2517 - val_loss: 1.2618 - val_accuracy: 0.3108\n",
            "Epoch 580/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1133 - accuracy: 0.2734\n",
            "Epoch 580: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.1224 - accuracy: 0.2517 - val_loss: 1.2592 - val_accuracy: 0.3108\n",
            "Epoch 581/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1950 - accuracy: 0.3125\n",
            "Epoch 581: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1227 - accuracy: 0.2517 - val_loss: 1.2559 - val_accuracy: 0.3108\n",
            "Epoch 582/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0581 - accuracy: 0.2891\n",
            "Epoch 582: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1230 - accuracy: 0.2517 - val_loss: 1.2518 - val_accuracy: 0.3108\n",
            "Epoch 583/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1267 - accuracy: 0.2656\n",
            "Epoch 583: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1227 - accuracy: 0.2517 - val_loss: 1.2479 - val_accuracy: 0.3108\n",
            "Epoch 584/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0618 - accuracy: 0.2031\n",
            "Epoch 584: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1230 - accuracy: 0.2517 - val_loss: 1.2442 - val_accuracy: 0.3108\n",
            "Epoch 585/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1114 - accuracy: 0.2188\n",
            "Epoch 585: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1225 - accuracy: 0.2517 - val_loss: 1.2430 - val_accuracy: 0.3108\n",
            "Epoch 586/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0658 - accuracy: 0.2109\n",
            "Epoch 586: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1213 - accuracy: 0.2517 - val_loss: 1.2442 - val_accuracy: 0.3108\n",
            "Epoch 587/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1233 - accuracy: 0.2734\n",
            "Epoch 587: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1220 - accuracy: 0.2517 - val_loss: 1.2465 - val_accuracy: 0.3108\n",
            "Epoch 588/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1391 - accuracy: 0.2812\n",
            "Epoch 588: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1226 - accuracy: 0.2517 - val_loss: 1.2502 - val_accuracy: 0.3108\n",
            "Epoch 589/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0655 - accuracy: 0.2188\n",
            "Epoch 589: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1216 - accuracy: 0.2517 - val_loss: 1.2530 - val_accuracy: 0.3108\n",
            "Epoch 590/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1293 - accuracy: 0.2031\n",
            "Epoch 590: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1229 - accuracy: 0.2517 - val_loss: 1.2551 - val_accuracy: 0.3108\n",
            "Epoch 591/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1197 - accuracy: 0.2344\n",
            "Epoch 591: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1204 - accuracy: 0.2517 - val_loss: 1.2584 - val_accuracy: 0.3108\n",
            "Epoch 592/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1091 - accuracy: 0.2578\n",
            "Epoch 592: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1216 - accuracy: 0.2517 - val_loss: 1.2600 - val_accuracy: 0.3108\n",
            "Epoch 593/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1054 - accuracy: 0.2500\n",
            "Epoch 593: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1220 - accuracy: 0.2517 - val_loss: 1.2590 - val_accuracy: 0.3108\n",
            "Epoch 594/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1242 - accuracy: 0.2344\n",
            "Epoch 594: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1227 - accuracy: 0.2517 - val_loss: 1.2570 - val_accuracy: 0.3108\n",
            "Epoch 595/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1018 - accuracy: 0.2422\n",
            "Epoch 595: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1227 - accuracy: 0.2517 - val_loss: 1.2550 - val_accuracy: 0.3108\n",
            "Epoch 596/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1223 - accuracy: 0.2344\n",
            "Epoch 596: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1218 - accuracy: 0.2517 - val_loss: 1.2533 - val_accuracy: 0.3108\n",
            "Epoch 597/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0794 - accuracy: 0.2656\n",
            "Epoch 597: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1219 - accuracy: 0.2517 - val_loss: 1.2509 - val_accuracy: 0.3108\n",
            "Epoch 598/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1451 - accuracy: 0.2422\n",
            "Epoch 598: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1223 - accuracy: 0.2517 - val_loss: 1.2473 - val_accuracy: 0.3108\n",
            "Epoch 599/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0481 - accuracy: 0.1953\n",
            "Epoch 599: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1208 - accuracy: 0.2517 - val_loss: 1.2437 - val_accuracy: 0.3108\n",
            "Epoch 600/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1778 - accuracy: 0.2500\n",
            "Epoch 600: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1214 - accuracy: 0.2517 - val_loss: 1.2414 - val_accuracy: 0.3108\n",
            "Epoch 601/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1732 - accuracy: 0.2812\n",
            "Epoch 601: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1205 - accuracy: 0.2517 - val_loss: 1.2393 - val_accuracy: 0.3108\n",
            "Epoch 602/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1524 - accuracy: 0.2656\n",
            "Epoch 602: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1207 - accuracy: 0.2517 - val_loss: 1.2381 - val_accuracy: 0.3108\n",
            "Epoch 603/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1954 - accuracy: 0.2500\n",
            "Epoch 603: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1208 - accuracy: 0.2517 - val_loss: 1.2383 - val_accuracy: 0.3108\n",
            "Epoch 604/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1279 - accuracy: 0.2109\n",
            "Epoch 604: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1202 - accuracy: 0.2517 - val_loss: 1.2385 - val_accuracy: 0.3108\n",
            "Epoch 605/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1646 - accuracy: 0.2812\n",
            "Epoch 605: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1214 - accuracy: 0.2517 - val_loss: 1.2386 - val_accuracy: 0.3108\n",
            "Epoch 606/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1322 - accuracy: 0.3125\n",
            "Epoch 606: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1212 - accuracy: 0.2517 - val_loss: 1.2393 - val_accuracy: 0.3108\n",
            "Epoch 607/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1203 - accuracy: 0.2578\n",
            "Epoch 607: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1207 - accuracy: 0.2517 - val_loss: 1.2394 - val_accuracy: 0.3108\n",
            "Epoch 608/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0780 - accuracy: 0.2188\n",
            "Epoch 608: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1193 - accuracy: 0.2517 - val_loss: 1.2402 - val_accuracy: 0.3108\n",
            "Epoch 609/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1314 - accuracy: 0.2734\n",
            "Epoch 609: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1205 - accuracy: 0.2517 - val_loss: 1.2415 - val_accuracy: 0.3108\n",
            "Epoch 610/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0575 - accuracy: 0.2109\n",
            "Epoch 610: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1207 - accuracy: 0.2517 - val_loss: 1.2426 - val_accuracy: 0.3108\n",
            "Epoch 611/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2032 - accuracy: 0.3438\n",
            "Epoch 611: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1219 - accuracy: 0.2517 - val_loss: 1.2438 - val_accuracy: 0.3108\n",
            "Epoch 612/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1228 - accuracy: 0.2500\n",
            "Epoch 612: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1204 - accuracy: 0.2517 - val_loss: 1.2426 - val_accuracy: 0.3108\n",
            "Epoch 613/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1304 - accuracy: 0.2109\n",
            "Epoch 613: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1195 - accuracy: 0.2517 - val_loss: 1.2413 - val_accuracy: 0.3108\n",
            "Epoch 614/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1502 - accuracy: 0.2734\n",
            "Epoch 614: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1211 - accuracy: 0.2517 - val_loss: 1.2416 - val_accuracy: 0.3108\n",
            "Epoch 615/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1375 - accuracy: 0.2578\n",
            "Epoch 615: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1201 - accuracy: 0.2517 - val_loss: 1.2430 - val_accuracy: 0.3108\n",
            "Epoch 616/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1306 - accuracy: 0.2734\n",
            "Epoch 616: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1202 - accuracy: 0.2517 - val_loss: 1.2446 - val_accuracy: 0.3108\n",
            "Epoch 617/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1208 - accuracy: 0.2734\n",
            "Epoch 617: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1204 - accuracy: 0.2517 - val_loss: 1.2463 - val_accuracy: 0.3108\n",
            "Epoch 618/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1606 - accuracy: 0.2500\n",
            "Epoch 618: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1204 - accuracy: 0.2517 - val_loss: 1.2477 - val_accuracy: 0.3108\n",
            "Epoch 619/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0770 - accuracy: 0.2266\n",
            "Epoch 619: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1197 - accuracy: 0.2517 - val_loss: 1.2477 - val_accuracy: 0.3108\n",
            "Epoch 620/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1724 - accuracy: 0.2812\n",
            "Epoch 620: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1204 - accuracy: 0.2517 - val_loss: 1.2478 - val_accuracy: 0.3108\n",
            "Epoch 621/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1267 - accuracy: 0.3359\n",
            "Epoch 621: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1203 - accuracy: 0.2517 - val_loss: 1.2487 - val_accuracy: 0.3108\n",
            "Epoch 622/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1413 - accuracy: 0.3047\n",
            "Epoch 622: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1196 - accuracy: 0.2517 - val_loss: 1.2502 - val_accuracy: 0.3108\n",
            "Epoch 623/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1221 - accuracy: 0.2109\n",
            "Epoch 623: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1203 - accuracy: 0.2517 - val_loss: 1.2505 - val_accuracy: 0.3108\n",
            "Epoch 624/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1238 - accuracy: 0.2500\n",
            "Epoch 624: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1202 - accuracy: 0.2517 - val_loss: 1.2514 - val_accuracy: 0.3108\n",
            "Epoch 625/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1515 - accuracy: 0.2656\n",
            "Epoch 625: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1193 - accuracy: 0.2517 - val_loss: 1.2529 - val_accuracy: 0.3108\n",
            "Epoch 626/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0999 - accuracy: 0.2734\n",
            "Epoch 626: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1190 - accuracy: 0.2517 - val_loss: 1.2529 - val_accuracy: 0.3108\n",
            "Epoch 627/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1246 - accuracy: 0.2266\n",
            "Epoch 627: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1195 - accuracy: 0.2517 - val_loss: 1.2514 - val_accuracy: 0.3108\n",
            "Epoch 628/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1782 - accuracy: 0.2734\n",
            "Epoch 628: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1206 - accuracy: 0.2517 - val_loss: 1.2499 - val_accuracy: 0.3108\n",
            "Epoch 629/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1034 - accuracy: 0.2500\n",
            "Epoch 629: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1206 - accuracy: 0.2517 - val_loss: 1.2477 - val_accuracy: 0.3108\n",
            "Epoch 630/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1590 - accuracy: 0.2344\n",
            "Epoch 630: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1200 - accuracy: 0.2517 - val_loss: 1.2469 - val_accuracy: 0.3108\n",
            "Epoch 631/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1501 - accuracy: 0.2344\n",
            "Epoch 631: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1205 - accuracy: 0.2517 - val_loss: 1.2478 - val_accuracy: 0.3108\n",
            "Epoch 632/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1119 - accuracy: 0.2344\n",
            "Epoch 632: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1199 - accuracy: 0.2517 - val_loss: 1.2486 - val_accuracy: 0.3108\n",
            "Epoch 633/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1209 - accuracy: 0.2500\n",
            "Epoch 633: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1187 - accuracy: 0.2517 - val_loss: 1.2491 - val_accuracy: 0.3108\n",
            "Epoch 634/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1279 - accuracy: 0.2734\n",
            "Epoch 634: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1183 - accuracy: 0.2517 - val_loss: 1.2494 - val_accuracy: 0.3108\n",
            "Epoch 635/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0628 - accuracy: 0.2188\n",
            "Epoch 635: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1198 - accuracy: 0.2517 - val_loss: 1.2500 - val_accuracy: 0.3108\n",
            "Epoch 636/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0975 - accuracy: 0.2734\n",
            "Epoch 636: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1186 - accuracy: 0.2517 - val_loss: 1.2508 - val_accuracy: 0.3108\n",
            "Epoch 637/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1480 - accuracy: 0.2734\n",
            "Epoch 637: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1195 - accuracy: 0.2517 - val_loss: 1.2498 - val_accuracy: 0.3108\n",
            "Epoch 638/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1151 - accuracy: 0.2656\n",
            "Epoch 638: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1197 - accuracy: 0.2517 - val_loss: 1.2480 - val_accuracy: 0.3108\n",
            "Epoch 639/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0820 - accuracy: 0.2734\n",
            "Epoch 639: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1195 - accuracy: 0.2517 - val_loss: 1.2467 - val_accuracy: 0.3108\n",
            "Epoch 640/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1660 - accuracy: 0.2578\n",
            "Epoch 640: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1202 - accuracy: 0.2517 - val_loss: 1.2451 - val_accuracy: 0.3108\n",
            "Epoch 641/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0742 - accuracy: 0.2266\n",
            "Epoch 641: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1191 - accuracy: 0.2517 - val_loss: 1.2442 - val_accuracy: 0.3108\n",
            "Epoch 642/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1267 - accuracy: 0.2578\n",
            "Epoch 642: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1177 - accuracy: 0.2517 - val_loss: 1.2451 - val_accuracy: 0.3108\n",
            "Epoch 643/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1640 - accuracy: 0.2734\n",
            "Epoch 643: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1177 - accuracy: 0.2517 - val_loss: 1.2474 - val_accuracy: 0.3108\n",
            "Epoch 644/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1281 - accuracy: 0.2734\n",
            "Epoch 644: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1193 - accuracy: 0.2517 - val_loss: 1.2509 - val_accuracy: 0.3108\n",
            "Epoch 645/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0784 - accuracy: 0.2422\n",
            "Epoch 645: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1190 - accuracy: 0.2517 - val_loss: 1.2521 - val_accuracy: 0.3108\n",
            "Epoch 646/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1633 - accuracy: 0.2969\n",
            "Epoch 646: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1201 - accuracy: 0.2517 - val_loss: 1.2522 - val_accuracy: 0.3108\n",
            "Epoch 647/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1388 - accuracy: 0.2578\n",
            "Epoch 647: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1199 - accuracy: 0.2517 - val_loss: 1.2513 - val_accuracy: 0.3108\n",
            "Epoch 648/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0904 - accuracy: 0.2578\n",
            "Epoch 648: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1193 - accuracy: 0.2517 - val_loss: 1.2490 - val_accuracy: 0.3108\n",
            "Epoch 649/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1316 - accuracy: 0.2500\n",
            "Epoch 649: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1201 - accuracy: 0.2517 - val_loss: 1.2469 - val_accuracy: 0.3108\n",
            "Epoch 650/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1134 - accuracy: 0.2812\n",
            "Epoch 650: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1184 - accuracy: 0.2517 - val_loss: 1.2458 - val_accuracy: 0.3108\n",
            "Epoch 651/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1018 - accuracy: 0.2500\n",
            "Epoch 651: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1190 - accuracy: 0.2517 - val_loss: 1.2462 - val_accuracy: 0.3108\n",
            "Epoch 652/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1596 - accuracy: 0.3047\n",
            "Epoch 652: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1184 - accuracy: 0.2517 - val_loss: 1.2473 - val_accuracy: 0.3108\n",
            "Epoch 653/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1201 - accuracy: 0.2500\n",
            "Epoch 653: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1190 - accuracy: 0.2517 - val_loss: 1.2470 - val_accuracy: 0.3108\n",
            "Epoch 654/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0992 - accuracy: 0.2734\n",
            "Epoch 654: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1181 - accuracy: 0.2517 - val_loss: 1.2467 - val_accuracy: 0.3108\n",
            "Epoch 655/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0796 - accuracy: 0.1797\n",
            "Epoch 655: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1189 - accuracy: 0.2517 - val_loss: 1.2461 - val_accuracy: 0.3108\n",
            "Epoch 656/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1096 - accuracy: 0.2734\n",
            "Epoch 656: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1194 - accuracy: 0.2517 - val_loss: 1.2472 - val_accuracy: 0.3108\n",
            "Epoch 657/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1169 - accuracy: 0.2031\n",
            "Epoch 657: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1202 - accuracy: 0.2517 - val_loss: 1.2488 - val_accuracy: 0.3108\n",
            "Epoch 658/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1633 - accuracy: 0.2031\n",
            "Epoch 658: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1181 - accuracy: 0.2517 - val_loss: 1.2508 - val_accuracy: 0.3108\n",
            "Epoch 659/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1314 - accuracy: 0.2500\n",
            "Epoch 659: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1199 - accuracy: 0.2517 - val_loss: 1.2523 - val_accuracy: 0.3108\n",
            "Epoch 660/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1147 - accuracy: 0.2812\n",
            "Epoch 660: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1199 - accuracy: 0.2517 - val_loss: 1.2519 - val_accuracy: 0.3108\n",
            "Epoch 661/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0952 - accuracy: 0.2109\n",
            "Epoch 661: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1188 - accuracy: 0.2517 - val_loss: 1.2513 - val_accuracy: 0.3108\n",
            "Epoch 662/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1845 - accuracy: 0.2734\n",
            "Epoch 662: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1187 - accuracy: 0.2517 - val_loss: 1.2515 - val_accuracy: 0.3108\n",
            "Epoch 663/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1114 - accuracy: 0.2344\n",
            "Epoch 663: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1182 - accuracy: 0.2517 - val_loss: 1.2504 - val_accuracy: 0.3108\n",
            "Epoch 664/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1368 - accuracy: 0.2578\n",
            "Epoch 664: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1193 - accuracy: 0.2517 - val_loss: 1.2494 - val_accuracy: 0.3108\n",
            "Epoch 665/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1298 - accuracy: 0.2344\n",
            "Epoch 665: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1185 - accuracy: 0.2517 - val_loss: 1.2486 - val_accuracy: 0.3108\n",
            "Epoch 666/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1223 - accuracy: 0.2656\n",
            "Epoch 666: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1187 - accuracy: 0.2517 - val_loss: 1.2474 - val_accuracy: 0.3108\n",
            "Epoch 667/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0578 - accuracy: 0.2578\n",
            "Epoch 667: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1172 - accuracy: 0.2517 - val_loss: 1.2456 - val_accuracy: 0.3108\n",
            "Epoch 668/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1402 - accuracy: 0.2500\n",
            "Epoch 668: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1189 - accuracy: 0.2517 - val_loss: 1.2452 - val_accuracy: 0.3108\n",
            "Epoch 669/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1323 - accuracy: 0.2109\n",
            "Epoch 669: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1191 - accuracy: 0.2517 - val_loss: 1.2454 - val_accuracy: 0.3108\n",
            "Epoch 670/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1458 - accuracy: 0.2578\n",
            "Epoch 670: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1183 - accuracy: 0.2517 - val_loss: 1.2465 - val_accuracy: 0.3108\n",
            "Epoch 671/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1530 - accuracy: 0.2266\n",
            "Epoch 671: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1187 - accuracy: 0.2517 - val_loss: 1.2475 - val_accuracy: 0.3108\n",
            "Epoch 672/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0853 - accuracy: 0.2656\n",
            "Epoch 672: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1189 - accuracy: 0.2517 - val_loss: 1.2483 - val_accuracy: 0.3108\n",
            "Epoch 673/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1753 - accuracy: 0.2578\n",
            "Epoch 673: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1192 - accuracy: 0.2517 - val_loss: 1.2492 - val_accuracy: 0.3108\n",
            "Epoch 674/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0770 - accuracy: 0.2578\n",
            "Epoch 674: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1177 - accuracy: 0.2517 - val_loss: 1.2500 - val_accuracy: 0.3108\n",
            "Epoch 675/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1191 - accuracy: 0.2891\n",
            "Epoch 675: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1191 - accuracy: 0.2517 - val_loss: 1.2494 - val_accuracy: 0.3108\n",
            "Epoch 676/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1000 - accuracy: 0.2500\n",
            "Epoch 676: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1178 - accuracy: 0.2517 - val_loss: 1.2468 - val_accuracy: 0.3108\n",
            "Epoch 677/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1224 - accuracy: 0.2266\n",
            "Epoch 677: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1184 - accuracy: 0.2517 - val_loss: 1.2455 - val_accuracy: 0.3108\n",
            "Epoch 678/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0681 - accuracy: 0.1797\n",
            "Epoch 678: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1168 - accuracy: 0.2517 - val_loss: 1.2459 - val_accuracy: 0.3108\n",
            "Epoch 679/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0877 - accuracy: 0.2812\n",
            "Epoch 679: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1179 - accuracy: 0.2517 - val_loss: 1.2465 - val_accuracy: 0.3108\n",
            "Epoch 680/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1179 - accuracy: 0.2188\n",
            "Epoch 680: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1197 - accuracy: 0.2517 - val_loss: 1.2458 - val_accuracy: 0.3108\n",
            "Epoch 681/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0443 - accuracy: 0.2734\n",
            "Epoch 681: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1186 - accuracy: 0.2517 - val_loss: 1.2449 - val_accuracy: 0.3108\n",
            "Epoch 682/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1012 - accuracy: 0.2734\n",
            "Epoch 682: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1178 - accuracy: 0.2517 - val_loss: 1.2449 - val_accuracy: 0.3108\n",
            "Epoch 683/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1677 - accuracy: 0.3047\n",
            "Epoch 683: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1162 - accuracy: 0.2517 - val_loss: 1.2467 - val_accuracy: 0.3108\n",
            "Epoch 684/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1269 - accuracy: 0.2344\n",
            "Epoch 684: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1179 - accuracy: 0.2517 - val_loss: 1.2480 - val_accuracy: 0.3108\n",
            "Epoch 685/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0898 - accuracy: 0.2344\n",
            "Epoch 685: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1172 - accuracy: 0.2517 - val_loss: 1.2490 - val_accuracy: 0.3108\n",
            "Epoch 686/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0853 - accuracy: 0.2109\n",
            "Epoch 686: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1185 - accuracy: 0.2517 - val_loss: 1.2507 - val_accuracy: 0.3108\n",
            "Epoch 687/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1238 - accuracy: 0.2812\n",
            "Epoch 687: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1182 - accuracy: 0.2517 - val_loss: 1.2535 - val_accuracy: 0.3108\n",
            "Epoch 688/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1066 - accuracy: 0.3125\n",
            "Epoch 688: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1175 - accuracy: 0.2517 - val_loss: 1.2565 - val_accuracy: 0.3108\n",
            "Epoch 689/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0873 - accuracy: 0.2578\n",
            "Epoch 689: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1176 - accuracy: 0.2517 - val_loss: 1.2574 - val_accuracy: 0.3108\n",
            "Epoch 690/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0651 - accuracy: 0.2109\n",
            "Epoch 690: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1182 - accuracy: 0.2517 - val_loss: 1.2566 - val_accuracy: 0.3108\n",
            "Epoch 691/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1336 - accuracy: 0.2109\n",
            "Epoch 691: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1187 - accuracy: 0.2517 - val_loss: 1.2564 - val_accuracy: 0.3108\n",
            "Epoch 692/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1633 - accuracy: 0.2422\n",
            "Epoch 692: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1195 - accuracy: 0.2517 - val_loss: 1.2566 - val_accuracy: 0.3108\n",
            "Epoch 693/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0995 - accuracy: 0.2031\n",
            "Epoch 693: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1186 - accuracy: 0.2517 - val_loss: 1.2560 - val_accuracy: 0.3108\n",
            "Epoch 694/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1314 - accuracy: 0.2266\n",
            "Epoch 694: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1174 - accuracy: 0.2517 - val_loss: 1.2565 - val_accuracy: 0.3108\n",
            "Epoch 695/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1458 - accuracy: 0.2031\n",
            "Epoch 695: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1175 - accuracy: 0.2517 - val_loss: 1.2584 - val_accuracy: 0.3108\n",
            "Epoch 696/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1229 - accuracy: 0.2812\n",
            "Epoch 696: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1175 - accuracy: 0.2517 - val_loss: 1.2602 - val_accuracy: 0.3108\n",
            "Epoch 697/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1059 - accuracy: 0.2734\n",
            "Epoch 697: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1168 - accuracy: 0.2517 - val_loss: 1.2615 - val_accuracy: 0.3108\n",
            "Epoch 698/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2288 - accuracy: 0.3281\n",
            "Epoch 698: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1196 - accuracy: 0.2517 - val_loss: 1.2630 - val_accuracy: 0.3108\n",
            "Epoch 699/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0476 - accuracy: 0.2578\n",
            "Epoch 699: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1182 - accuracy: 0.2517 - val_loss: 1.2630 - val_accuracy: 0.3108\n",
            "Epoch 700/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1091 - accuracy: 0.2500\n",
            "Epoch 700: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1191 - accuracy: 0.2517 - val_loss: 1.2624 - val_accuracy: 0.3108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_CNN = CNN_model.predict(feature_valid)\n",
        "\n",
        "# convert the validation vector\n",
        "valid_y_CNN = y_valid_CNN.copy()\n",
        "for i in range(len(y_valid_CNN)):\n",
        "    j = np.where(y_valid_CNN[i] == np.amax(y_valid_CNN[i]))\n",
        "    valid_y_CNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN))"
      ],
      "metadata": {
        "id": "y89umNf39Ufr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_CNN = CNN_model.predict(feature_test)\n",
        "# convert the test vector\n",
        "test_y_CNN = y_test_CNN.copy()\n",
        "for i in range(len(y_test_CNN)):\n",
        "    j = np.where(y_test_CNN[i] == np.amax(y_test_CNN[i]))\n",
        "    test_y_CNN[i] = [0, 0, 0]\n",
        "    test_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_test_y,test_y_CNN))\n",
        "print(classification_report(label_test_y,test_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_test_y,test_y_CNN))"
      ],
      "metadata": {
        "id": "i6gRsP5r9ggG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}