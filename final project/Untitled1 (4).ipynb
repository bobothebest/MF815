{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rn60xnbcSQDf"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 装包和导入"
      ],
      "metadata": {
        "id": "rn60xnbcSQDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow>=2.15.0\"\n",
        "!pip install --upgrade tensorflow-hub\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/drive/MyDrive\n",
        "# Import the libraries\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import ceil\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import collections\n",
        "import random\n",
        "import time\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, Dropout, LSTM, Bidirectional"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8qyZWnzXPsv",
        "outputId": "d5f3e395-077a-45da-9c8c-9a219f1bbc7f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow>=2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (3.20.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.15.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.2.2)\n",
            "Mounted at /content/gdrive\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive'\n",
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We now set the directory to access the data\n",
        "def find(name, path):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        if name in files:\n",
        "            return os.path.join(root, name)\n",
        "\n",
        "# The folder with the data and this script should be saved in your drive.\n",
        "script_name = 'CourseWork_v2.ipynb'\n",
        "script_path = find(script_name, '/content/gdrive/My Drive')\n",
        "#DIRECTORY = '.'#os.path.dirname(script_path)\n",
        "# If your Drive is too large and the \"find\" function takes to much time, you can set the directory manually :\n",
        "\n",
        "#SUMMARY_PATH = '/content/drive/MyDrive/MutualFundSummary'\n",
        "#SUMMARY_LABELS_PATH = '/content/drive/MyDrive/MutualFundLabels.csv'\n",
        "\n",
        "DIRECTORY = '/content/gdrive/MyDrive/Colab Notebooks/NLP_app'\n",
        "\n",
        "SUMMARY_PATH = '/content/gdrive/MyDrive/Colab Notebooks/NLP_app/MutualFundSummary'\n",
        "SUMMARY_LABELS_PATH = '/content/gdrive/MyDrive/Colab Notebooks/MF815/NLP/NLP_app/MutualFundLabels.csv'\n",
        "\n",
        "glove_word2vec = 'glove.6B.50d.txt'\n",
        "our_word2vec = 'word2vec_perso.txt'\n",
        "\n",
        "# Progress bar\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "# Save a word2vec dictionary.\n",
        "def save_word2vec(filename):\n",
        "    with open(os.path.join('/content/drive/MyDrive', filename),'a' , encoding='utf-8') as f :\n",
        "        for k, v in word2vec.items():\n",
        "            line = k+' '+str(list(v)).strip('[]').replace(',','')+'\\n'\n",
        "            f.write(line)\n",
        "\n",
        "# Load a word2vec dictionary.\n",
        "def load_word2vec(filename):\n",
        "    word2vec = {}\n",
        "    with open(os.path.join('/content/drive/MyDrive', filename), encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            try :\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vec = np.asarray(values[1:], dtype='float32')\n",
        "                word2vec[word] = vec\n",
        "            except :\n",
        "                None\n",
        "    return word2vec\n",
        "\n",
        "# read the repo in PATH and append the texts in a list\n",
        "def get_data(PATH):\n",
        "    list_dir = os.listdir(PATH)\n",
        "    texts = []\n",
        "    fund_names = []\n",
        "    out = display(progress(0, len(list_dir)-1), display_id=True)\n",
        "    for ii, filename in enumerate(list_dir) :\n",
        "        with open(PATH+'/'+filename, 'r', encoding=\"utf8\") as f :\n",
        "            txt = f.read()\n",
        "            try :\n",
        "                txt_split = txt.split('<head_breaker>')\n",
        "                summary = txt_split[1].strip()\n",
        "                fund_name = txt_split[0].strip()\n",
        "            except :\n",
        "                summary = txt\n",
        "                fund_name = ''\n",
        "        texts.append(summary)\n",
        "        fund_names.append(fund_name)\n",
        "        out.update(progress(ii, len(list_dir)-1))\n",
        "    return fund_names, texts"
      ],
      "metadata": {
        "id": "vL7JHDW6HBYX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#导入数据，划分训练集和测试集"
      ],
      "metadata": {
        "id": "lQI2_lFnSZIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "#labels = pd.read_csv('/content/MutualFundLabels.csv')\n",
        "labels = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/MutualFundLabels.csv')\n",
        "labels = labels.drop('Performance fee?', axis=1)\n",
        "\n",
        "removes = labels.loc[(labels['Ivestment Strategy']=='Long Short Funds (High Risk)')|\n",
        "                     (labels['Ivestment Strategy']=='Commodities Fund (Low Risk)')]\n",
        "labels_clean = labels.drop(removes.index)\n",
        "\n",
        "fund_names, summaries = get_data(SUMMARY_PATH)\n",
        "cleaned_fund_names = labels_clean['fund_name'].tolist()\n",
        "\n",
        "# Create a dictionary to store the summaries without labels\n",
        "unlabeled_summaries = {}\n",
        "\n",
        "for name, summary in zip(fund_names, summaries):\n",
        "    if name not in cleaned_fund_names:\n",
        "        unlabeled_summaries[name] = summary\n",
        "\n",
        "# Create a DataFrame with the unlabeled summaries\n",
        "df_unlabeled_summaries = pd.DataFrame(data={'fund_name': list(unlabeled_summaries.keys()),\n",
        "                                            'summary': list(unlabeled_summaries.values())})\n",
        "\n",
        "# Save the unlabeled summaries as a text file\n",
        "with open('unlabeled_summaries.txt', 'w', encoding='utf-8') as f:\n",
        "    for summary in df_unlabeled_summaries['summary']:\n",
        "        f.write(summary + '\\n')\n",
        "\n",
        "print(f\"Number of summaries without labels: {len(df_unlabeled_summaries)}\")\n",
        "\n",
        "# Create a new DataFrame 'test' with only the 'summary' column from df_unlabeled_summaries\n",
        "X_test = pd.DataFrame(df_unlabeled_summaries['summary'], columns=['summary'])\n",
        "\n",
        "print(f\"Number of summaries in the 'test' DataFrame: {len(X_test)}\")\n",
        "\n",
        "\n",
        "# Continue with the rest of the code\n",
        "fund_name_counts = {name: 0 for name in fund_names}\n",
        "for name in fund_names:\n",
        "    if name in cleaned_fund_names:\n",
        "        fund_name_counts[name] += 1\n",
        "\n",
        "single_occurrences = {name: count for name, count in fund_name_counts.items() if count == 1}\n",
        "print(f\"Number of matching fund names that appear exactly once: {len(single_occurrences)}\")\n",
        "\n",
        "multiple_occurrences = {name: count for name, count in fund_name_counts.items() if count > 1}\n",
        "if multiple_occurrences:\n",
        "    print(f\"There are fund names that appear more than once:\")\n",
        "    for name, count in multiple_occurrences.items():\n",
        "        print(f\"{name}: {count} times\")\n",
        "else:\n",
        "    print(\"No fund names appear more than once.\")\n",
        "\n",
        "labels_clean_filtered = labels_clean[labels_clean['fund_name'].isin(fund_names)]\n",
        "df_summaries = pd.DataFrame(data={'fund_name':fund_names, 'summary':summaries})\n",
        "merge = labels_clean_filtered.merge(df_summaries, on=['fund_name'], how='left')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "87p4v-Y2UuCP",
        "outputId": "d3817356-1031-4e82-e574-6e92e1e42623"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='544'\n",
              "            max='544',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            544\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of summaries without labels: 84\n",
            "Number of summaries in the 'test' DataFrame: 84\n",
            "Number of matching fund names that appear exactly once: 461\n",
            "No fund names appear more than once.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 使用 to_string() 方法完整打印第一行，确保宽度足够\n",
        "first_row_string = X_test.head(1).to_string(index=False, max_colwidth=1000)\n",
        "\n",
        "# 打印这一行的完整内容\n",
        "print(first_row_string)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29rB8JBoU0xA",
        "outputId": "40edcec5-e5e2-4510-affb-b35884c15419"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 summary\n",
            "AB Arizona Portfolio\\n--------------------------------------------------------------------------------\\n\\nINVESTMENT OBJECTIVE:\\nThe investment objective of the Portfolio is to earn the highest level of\\ncurrent income exempt from both federal income tax and State of Arizona personal\\nincome tax that is available without assuming what the Adviser considers to be\\nundue risk.\\n\\nFEES AND EXPENSES OF THE PORTFOLIO:\\n\\n\\nThis table describes the fees and expenses that you may pay if you buy and hold\\nshares of the Portfolio. You may qualify for sales charge reductions if you and\\nmembers of your family invest, or agree to invest in the future, at least\\n$100,000 in AB Mutual Funds. More information about these and other discounts is\\navailable from your financial intermediary and in Investing in the\\nPortfolios--Sales Charge Reduction Programs for Class A Shares on page 71 of\\nthis Prospectus, in Appendix B--Financial Intermediary Waivers of this\\nProspectus and in Purchase of Shares--...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "ToVvKikmiKhg",
        "outputId": "df56d2ae-a22b-42ec-dbfb-ee2695608051"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       id                                          fund_name  \\\n",
              "0    0000051931-18-000151                   American Funds College 2018 Fund   \n",
              "1    0000051931-18-000151                   American Funds College 2021 Fund   \n",
              "2    0000051931-18-000151                   American Funds College 2024 Fund   \n",
              "3    0000051931-18-000151                   American Funds College 2027 Fund   \n",
              "4    0000051931-18-000151                   American Funds College 2030 Fund   \n",
              "..                    ...                                                ...   \n",
              "456  0001710607-18-000172    American Century Diversified Corporate Bond ETF   \n",
              "457  0001710607-18-000172    American Century Diversified Municipal Bond ETF   \n",
              "458  0001710607-18-000172  American Century Quality Diversified Internati...   \n",
              "459  0001710607-18-000172     American Century STOXX U.S. Quality Growth ETF   \n",
              "460  0001710607-18-000172      American Century STOXX U.S. Quality Value ETF   \n",
              "\n",
              "                    Ivestment Strategy Leverage?  \\\n",
              "0             Balanced Fund (Low Risk)       Yes   \n",
              "1             Balanced Fund (Low Risk)       Yes   \n",
              "2             Balanced Fund (Low Risk)       Yes   \n",
              "3             Balanced Fund (Low Risk)       Yes   \n",
              "4             Balanced Fund (Low Risk)       Yes   \n",
              "..                                 ...       ...   \n",
              "456  Fixed Income Long Only (Low Risk)       Yes   \n",
              "457  Fixed Income Long Only (Low Risk)        No   \n",
              "458        Equity Long Only (Low Risk)       Yes   \n",
              "459        Equity Long Only (Low Risk)       Yes   \n",
              "460        Equity Long Only (Low Risk)       Yes   \n",
              "\n",
              "                                 Portfolio composition  Concentration  \\\n",
              "0                          Investment grade securities    Diversified   \n",
              "1                          Investment grade securities    Diversified   \n",
              "2                          Investment grade securities    Diversified   \n",
              "3                          Investment grade securities    Diversified   \n",
              "4                          Investment grade securities    Diversified   \n",
              "..                                                 ...            ...   \n",
              "456                        Investment grade securities    Diversified   \n",
              "457                        Investment grade securities    Diversified   \n",
              "458  Sub-investment grade securities or emerging ma...    Diversified   \n",
              "459                                    Listed Equities    Diversified   \n",
              "460                                    Listed Equities    Diversified   \n",
              "\n",
              "                                               summary  \n",
              "0    American Funds College 2018 Fund\\n\\nInvestment...  \n",
              "1    American Funds College 2021 Fund\\n\\nInvestment...  \n",
              "2    American Funds College 2024 Fund\\n\\nInvestment...  \n",
              "3    American Funds College 2027 Fund\\n\\nInvestment...  \n",
              "4    American Funds College 2030 Fund\\n\\nInvestment...  \n",
              "..                                                 ...  \n",
              "456  Fund Summary\\nInvestment Objective\\nThe fund s...  \n",
              "457  Fund Summary\\nInvestment Objective\\nThe fund s...  \n",
              "458  Fund Summary\\nInvestment Objective\\nThe fund s...  \n",
              "459  Fund Summary\\nInvestment Objective\\nThe fund s...  \n",
              "460  Fund Summary\\nInvestment Objective\\nThe fund s...  \n",
              "\n",
              "[461 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc69123a-74be-43b4-802f-63e2b8496a4c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>fund_name</th>\n",
              "      <th>Ivestment Strategy</th>\n",
              "      <th>Leverage?</th>\n",
              "      <th>Portfolio composition</th>\n",
              "      <th>Concentration</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000051931-18-000151</td>\n",
              "      <td>American Funds College 2018 Fund</td>\n",
              "      <td>Balanced Fund (Low Risk)</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Investment grade securities</td>\n",
              "      <td>Diversified</td>\n",
              "      <td>American Funds College 2018 Fund\\n\\nInvestment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000051931-18-000151</td>\n",
              "      <td>American Funds College 2021 Fund</td>\n",
              "      <td>Balanced Fund (Low Risk)</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Investment grade securities</td>\n",
              "      <td>Diversified</td>\n",
              "      <td>American Funds College 2021 Fund\\n\\nInvestment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000051931-18-000151</td>\n",
              "      <td>American Funds College 2024 Fund</td>\n",
              "      <td>Balanced Fund (Low Risk)</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Investment grade securities</td>\n",
              "      <td>Diversified</td>\n",
              "      <td>American Funds College 2024 Fund\\n\\nInvestment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0000051931-18-000151</td>\n",
              "      <td>American Funds College 2027 Fund</td>\n",
              "      <td>Balanced Fund (Low Risk)</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Investment grade securities</td>\n",
              "      <td>Diversified</td>\n",
              "      <td>American Funds College 2027 Fund\\n\\nInvestment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000051931-18-000151</td>\n",
              "      <td>American Funds College 2030 Fund</td>\n",
              "      <td>Balanced Fund (Low Risk)</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Investment grade securities</td>\n",
              "      <td>Diversified</td>\n",
              "      <td>American Funds College 2030 Fund\\n\\nInvestment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>0001710607-18-000172</td>\n",
              "      <td>American Century Diversified Corporate Bond ETF</td>\n",
              "      <td>Fixed Income Long Only (Low Risk)</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Investment grade securities</td>\n",
              "      <td>Diversified</td>\n",
              "      <td>Fund Summary\\nInvestment Objective\\nThe fund s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>457</th>\n",
              "      <td>0001710607-18-000172</td>\n",
              "      <td>American Century Diversified Municipal Bond ETF</td>\n",
              "      <td>Fixed Income Long Only (Low Risk)</td>\n",
              "      <td>No</td>\n",
              "      <td>Investment grade securities</td>\n",
              "      <td>Diversified</td>\n",
              "      <td>Fund Summary\\nInvestment Objective\\nThe fund s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>0001710607-18-000172</td>\n",
              "      <td>American Century Quality Diversified Internati...</td>\n",
              "      <td>Equity Long Only (Low Risk)</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Sub-investment grade securities or emerging ma...</td>\n",
              "      <td>Diversified</td>\n",
              "      <td>Fund Summary\\nInvestment Objective\\nThe fund s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>0001710607-18-000172</td>\n",
              "      <td>American Century STOXX U.S. Quality Growth ETF</td>\n",
              "      <td>Equity Long Only (Low Risk)</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Listed Equities</td>\n",
              "      <td>Diversified</td>\n",
              "      <td>Fund Summary\\nInvestment Objective\\nThe fund s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>0001710607-18-000172</td>\n",
              "      <td>American Century STOXX U.S. Quality Value ETF</td>\n",
              "      <td>Equity Long Only (Low Risk)</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Listed Equities</td>\n",
              "      <td>Diversified</td>\n",
              "      <td>Fund Summary\\nInvestment Objective\\nThe fund s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>461 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc69123a-74be-43b4-802f-63e2b8496a4c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bc69123a-74be-43b4-802f-63e2b8496a4c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bc69123a-74be-43b4-802f-63e2b8496a4c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4f811eb8-27fb-4a7d-82c6-fc3fd80f1a5d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f811eb8-27fb-4a7d-82c6-fc3fd80f1a5d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4f811eb8-27fb-4a7d-82c6-fc3fd80f1a5d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "merge",
              "summary": "{\n  \"name\": \"merge\",\n  \"rows\": 461,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 58,\n        \"samples\": [\n          \"0000051931-18-000151\",\n          \"0000051931-18-001409\",\n          \"0001193125-18-286983\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fund_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 461,\n        \"samples\": [\n          \"Janus Henderson Overseas Portfolio\",\n          \"New World Fund\",\n          \"American Funds Moderate Growth and Income Portfolio\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ivestment Strategy\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Balanced Fund (Low Risk)\",\n          \"Fixed Income Long Only (Low Risk)\",\n          \"Equity Long Only (Low Risk)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Leverage?\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"No\",\n          \"Yes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Portfolio composition\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Investment grade securities\",\n          \"Listed Equities\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" Concentration\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Concentrated by issuer / sector / jurisdiction\",\n          \"Diversified\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 457,\n        \"samples\": [\n          \"JPMorgan Tax Aware Equity Fund\\nClass/Ticker: R6/JPERX\\nWhat is the goal of the Fund?\\nThe Fund\\u2019s goal is to provide high after-tax total return from a portfolio of selected equity securities.\\nFees and Expenses of the Fund\\nThe following table describes the fees and expenses that you may pay if you buy and hold shares of the Fund.\\nANNUAL FUND OPERATING EXPENSES\\n(Expenses that you pay each year as a percentage of the value\\nof your investment)\\n \\tClass R6\\nManagement Fees\\t0.35%\\nDistribution (Rule 12b-1) Fees\\tNONE\\nOther Expenses\\t0.11\\nService Fees\\tNONE\\nRemainder of Other Expenses1\\t0.11\\nTotal Annual Fund Operating Expenses\\t0.46\\nFee Waivers and/or Expense Reimbursements2\\t(0.02)\\nTotal Annual Fund Operating Expenses after Fee Waivers and/or Expense Reimbursements2\\t0.44\\n1\\t\\u201cRemainder of Other Expenses\\u201d are based on estimated amounts for the current fiscal year.\\n2\\tThe Fund\\u2019s adviser and/or its affiliates have contractually agreed to waive fees and/or reimburse expenses to the extent Total Annual Fund Operating Expenses (excluding Acquired Fund Fees and Expenses other than certain money market fund fees as described below, dividend and interest expenses related to short sales, interest, taxes, expenses related to litigation and potential litigation, and extraordinary expenses) exceed 0.44% of the average daily net assets of Class R6 Shares. The Fund may invest in one or more money market funds advised by the adviser or its affiliates (affiliated money market funds). The Fund\\u2019s adviser, shareholder servicing agent and/or administrator have contractually agreed to waive fees and/or reimburse expenses in an amount sufficient to offset the respective net fees each collects from the affiliated money market funds on the Fund\\u2019s investment in such money market funds. These waivers are in effect through 9/30/19, at which time the adviser and/or its affiliates will determine whether to renew or revise them.\\nExample\\nThis Example is intended to help you compare the cost of investing in the Fund with the cost of investing in other mutual funds. The Example assumes that you invest $10,000 in the Fund for the time periods indicated. The Example also assumes that your investment has a 5% return each year and that the Fund\\u2019s operating expenses are equal to the total annual fund operating expenses after fee waivers and expense reimbursements shown in the fee table through 9/30/19 and total annual fund operating expenses thereafter. Your actual costs may be higher or lower.\\nWHETHER OR NOT YOU SELL YOUR SHARES, YOUR COST WOULD BE:\\n \\t1 Year\\t \\t3 Years\\t \\t5 Years\\t \\t10 Years\\nCLASS R6 SHARES ($)\\t45\\t \\t146\\t \\t256\\t \\t577\\nPortfolio Turnover\\nThe Fund pays transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d its portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when Fund shares are held in a taxable account. These costs, which are not reflected in annual fund operating expenses or in the Example, affect the Fund\\u2019s performance. During the Fund\\u2019s most recent fiscal year, the Fund\\u2019s portfolio turnover rate was 28% of the average value of its portfolio.\\nWhat are the Fund\\u2019s main investment strategies?\\nUnder normal circumstances, the Fund invests at least 80% of its Assets in equity securities. \\u201cAssets\\u201d means net assets, plus the amount of borrowings for investment purposes. In implementing its main strategy, the Fund primarily invests in common stocks of large and medium capitalization U.S. companies, but it may also invest up to 20% of its Assets in common stocks of foreign companies, including depositary receipts. Large and medium capitalization companies are companies with market capitalizations equal to those within the universe of the S&P 500 Index at the time of purchase. As of December 31, 2017, the market capitalizations of the companies in the S& P 500 Index ranged from $3.0 billion to $868.9 billion.\\nSector by sector, the Fund\\u2019s weightings are similar to those of the S&P 500 Index. Within each sector, the Fund focuses on those equity securities that it considers undervalued and seeks to outperform the S&P 500 through superior stock selection. By emphasizing undervalued securities, the Fund seeks to produce returns that exceed those of the S&P 500 Index. At the same time, by controlling the sector weightings of the Fund so that they can differ only moderately from the sector weightings of the S&P 500 Index, the Fund seeks to limits its volatility to that of the overall market, as represented by this index.\\nDerivatives, which are instruments that have a value based on another instrument, exchange rate or index, may be used as substitutes for securities in which the Fund can invest. To the extent the Fund uses derivatives, the Fund will primarily use futures contracts to more effectively gain targeted equity exposure from its cash positions.\\nThe Fund seeks to minimize shareholders\\u2019 tax liability in connection with the Fund\\u2019s distribution of realized capital gain by minimizing the net gains available for distribution. As part of its tax aware strategy, the Fund typically sells securities when the anticipated performance benefit justifies the resulting gain. This strategy often includes minimizing the sale of securities with large unrealized gain, holding securities long enough to avoid\\nSEPTEMBER 29, 2018  |  1\\n \\n\\nTable of Contents\\nJPMorgan Tax Aware Equity Fund (continued)\\nshort-term capital gains taxes, selling securities with a higher cost basis first and offsetting capital gains realized in one security by selling another security at a capital loss. In addition, the Fund seeks to minimize distributions that are taxed as ordinary income and not qualified dividend income.\\nInvestment Process: In managing the Fund, the adviser employs a three-step process that combines research, valuation and stock selection. The adviser takes an in-depth look at company prospects over a period as long as five years, which is designed to provide insight into a company\\u2019s real growth potential. The research findings allow the adviser to rank the companies in each sector group according to what it believes to be their relative value. As a part of its investment process, the adviser seeks to assess the impact of environmental, social and governance factors (including accounting and tax policies, disclosure and investor communication, shareholder rights and remuneration policies) on the cash flows of many companies in which it may invest to identify issuers that the adviser believes will be negatively impacted by such factors relative to other issuers. These determinations may not be conclusive and securities of such issuers may be purchased and retained by the Fund.\\nOn behalf of the Fund, the adviser then buys and sells equity securities, using the research and valuation rankings as a basis. In general, the adviser buys equity securities that are identified as undervalued and considers selling them when they appear overvalued. Along with attractive valuation, the adviser often considers a number of other criteria:\\n\\u2022\\tcatalysts that could trigger a rise in a stock\\u2019s price\\n\\u2022\\thigh perceived potential reward compared to perceived potential risk\\n\\u2022\\tpossible temporary mispricings caused by apparent market overreactions\\nThe frequency with which the Fund buys and sells securities will vary from year to year, depending on market conditions and the implementation of the tax aware strategy.\\nThe Fund\\u2019s Main Investment Risks\\nThe Fund is subject to management risk and may not achieve its objective if the adviser\\u2019s expectations regarding particular instruments or markets are not met.\\n    \\nAn investment in this Fund or any other fund may not provide a complete investment program. The suitability of an investment in the Fund should be considered based on the investment objective, strategies and risks described in this prospectus, considered in light of all of the other investments in your portfolio, as well as your risk tolerance, financial goals and time horizons. You may want to consult with a financial advisor to determine if this Fund is suitable for you.\\nThe Fund is subject to the main risks noted below, any of which may adversely affect the Fund\\u2019s performance and ability to meet its investment objective.\\nEquity Market Risk. The price of equity securities may rise or fall because of changes in the broad market or changes in a company\\u2019s financial condition, sometimes rapidly or unpredictably. These price movements may result from factors affecting individual companies, sectors or industries selected for the Fund\\u2019s portfolio or the securities market as a whole, such as changes in economic or political conditions. When the value of the Fund\\u2019s securities does down, your investment in the Fund decreases in value.\\nGeneral Market Risk. Economies and financial markets throughout the world are becoming increasingly interconnected, which increases the likelihood that events or conditions in one country or region will adversely impact markets or issuers in other countries or regions. Securities in the Fund\\u2019s portfolio may underperform in comparison to securities in general financial markets, a particular financial market or other asset classes, due to a number of factors, including inflation (or expectations for inflation), interest rates, global demand for particular products or resources, natural disasters or events, terrorism, regulatory events and government controls.\\nIndustry and Sector Focus Risk. At times the Fund may increase the relative emphasis of its investments in a particular industry or sector. The prices of securities of issuers in a particular industry or sector may be more susceptible to fluctuations due to changes in economic or business conditions, government regulations, availability of basic resources or supplies, or other events that affect that industry or sector more than securities of issuers in other industries and sectors. To the extent that the Fund increases the relative emphasis of its investments in a particular industry or sector, its shares\\u2019 values may fluctuate in response to events affecting that industry or sector.\\nMid Cap Company Risk. The Fund may invest in large and mid capitalization companies, and the Fund\\u2019s risks increase as it invests more heavily in mid capitalization companies. Investments in mid cap companies may be riskier, more volatile and more vulnerable to economic, market and industry changes than investment in larger, more established companies. The securities of mid-cap companies may trade less frequently and in smaller volumes than securities of larger companies. As a result, share price changes may be more sudden or erratic than the prices of other equity securities, especially over the short term.\\nValue Strategy Risk. An undervalued stock may decrease in price or may not increase in price as anticipated by the adviser if other investors fail to recognize the company\\u2019s value or the factors that the adviser believes will cause the stock price to increase do not occur.\\nTax Aware Investing Risk. The Fund\\u2019s tax aware strategies may reduce your taxable income, but will not eliminate it. These strategies may require trade-offs that reduce pretax income. Managing the Fund to maximize after-tax returns may also potentially have a negative effect on the Fund\\u2019s performance.\\n \\n \\n2  |   J.P. MORGAN TAX AWARE FUNDS\\n \\n\\nTable of Contents\\nBecause tax consequences are considered in managing the Fund, the Fund\\u2019s pre-tax performance may be lower than that of a similar fund that is not tax-managed.\\nSmaller Company Risk. Investments in securities of smaller companies may be riskier, less liquid, more volatile and vulnerable to economic, market and industry changes than securities of larger more established companies. As a result, changes in the price of debt or equity issued by such companies may be more sudden or erratic than the prices of other equity securities, especially over the short term.\\nReal Estate Securities Risk. The value of real estate securities in general, and REITs in particular, are subject to the same risks as direct investments in real estate and mortgages which include, but are not limited to, sensitivity to changes in real estate values and property taxes, interest rate risk, tax and regulatory risk, fluctuations in rent schedules and operating expenses, adverse changes in local, regional or general economic conditions, deterioration of the real estate market and the financial circumstances of tenants and sellers, unfavorable changes in zoning, building, environmental and other laws, the need for unanticipated renovations, unexpected increases in the cost of energy and environmental factors. The underlying mortgage loans may be subject to the risks of default or of prepayments that occur earlier or later than expected, and such loans may also include so-called \\u201csub-prime\\u201d mortgages.\\nDerivatives Risk. Derivatives, including futures, may be riskier than other types of investments and may increase the volatility of the Fund. Derivatives may be sensitive to changes in economic and market conditions and may create leverage, which could result in losses that significantly exceed the Fund\\u2019s original investment. Derivatives expose the Fund to counterparty risk which is the risk that the derivative counterparty will not fulfill its contractual obligations (and includes credit risk associated with the counterparty). Certain derivatives are synthetic instruments that attempt to replicate the performance of certain reference assets. With regard to such derivatives, the Fund does not have a claim on the reference assets and is subject to enhanced counterparty risk.\\nForeign Securities Risk. Investments in foreign issuers and foreign securities (including depositary receipts) are subject to additional risks, including political and economic risks, civil conflicts and war, greater volatility, expropriation and nationalization risks, sanctions or other measures by the United States or other governments, currency fluctuations, higher transaction costs, delayed settlement, possible foreign controls on investment and less stringent investor protection and disclosure standards of foreign markets. In certain markets where securities and other instruments are not traded \\u201cdelivery versus payment,\\u201d the Fund may not receive timely payment for securities or other instruments it has delivered or receive delivery of securities paid for and may be subject to increased risk that the counterparty will fail to make payments or delivery when due or default completely. Events and evolving conditions in certain economies or markets may alter the risks associated\\nwith investments tied to countries or regions that historically were perceived as comparatively stable becoming riskier and more volatile.\\nTransactions Risk. The Fund could experience a loss and its liquidity may be negatively impacted when selling securities to meet redemption requests by shareholders. The risk of loss increases if the redemption requests are unusually large or frequent or occur in times of overall market turmoil or declining prices. Similarly, large purchases of Fund shares may adversely affect the Fund\\u2019s performance to the extent that the Fund is delayed in investing new cash and is required to maintain a larger cash position than it ordinarily would.\\n    \\nInvestments in the Fund are not deposits or obligations of, or guaranteed or endorsed by, any bank and are not insured or guaranteed by the FDIC, the Federal Reserve Board or any other government agency.\\nYou could lose money investing in the Fund.\\nThe Fund\\u2019s Past Performance\\nThis section provides some indication of the risks of investing in the Fund. The Class R6 Shares have not commenced operations as of the date of this prospectus. Therefore, the bar chart shows how the performance of the Fund\\u2019s Class I Shares (which are not offered in this prospectus) has varied from year to year for the past ten calendar years. The table shows the average annual total returns for the past one year, five years and ten years. The table compares that performance to the S&P 500 Index and the Lipper Large-Cap Core Funds Index, an index based on the total returns of certain mutual funds within the Fund\\u2019s designated category as determined by Lipper. Unlike the other index, the Lipper index includes the fees and expenses of the mutual funds included in the index. The actual returns of Class I Shares would have been lower because the Class R6 Shares have different fees and expenses than Class I Shares. Past performance (before and after taxes) is not necessarily an indication of how the Fund will perform in the future. Updated performance information is available by visiting www.jpmorganfunds.com or by calling 1-800-480-4111.\\nSEPTEMBER 29, 2018  |  3\\n \\n\\nTable of Contents\\nJPMorgan Tax Aware Equity Fund (continued)\\nYEAR-BY-YEAR RETURNS \\u2014 CLASS I SHARES\\n\\nBest Quarter\\t2nd quarter, 2009\\t16.01%\\nWorst Quarter\\t4th quarter, 2008\\t-21.04%\\nThe Fund\\u2019s year-to-date total return through 6/30/18 was 2.80%.\\nAVERAGE ANNUAL TOTAL RETURNS\\n(For periods ended December 31, 2017)\\n \\tPast\\n1 Year\\t \\tPast\\n5 Years\\t \\tPast\\n10 Years\\nCLASS I SHARES\\t \\t \\t \\t \\t \\nReturn Before Taxes\\t23.37%\\t \\t16.37%\\t \\t8.89%\\nReturn After Taxes on Distributions\\t21.16\\t \\t15.09\\t \\t8.16\\nReturn After Taxes on Distributions and Sale of Fund Shares\\t15.00\\t \\t13.07\\t \\t7.17\\nS&P 500 INDEX\\t \\t \\t \\t \\t \\n(Reflects No Deduction for Fees, Expenses, or Taxes)\\t21.83\\t \\t15.79\\t \\t8.50\\nLIPPER LARGE-CAP CORE FUNDS INDEX\\t \\t \\t \\t \\t \\n(Reflects No Deduction for Taxes)\\t20.90\\t \\t14.63\\t \\t7.58\\nAfter-tax returns are calculated using the historical highest individual federal marginal income tax rates and do not reflect the impact of state and local taxes. Actual after-tax returns depend on your tax situations and may differ from those shown. The after-tax returns shown are not relevant to investors who hold their shares through tax-deferred arrangements such as 401(k) plans or individual retirement accounts.\\nManagement\\nJ.P. Morgan Investment Management Inc.\\nPortfolio Manager\\tManaged\\nFund Since\\tPrimary Title with\\nInvestment Adviser\\nSusan Bao\\t2008\\tManaging Director\\nPurchase and Sale of Fund Shares\\nFor Class R6 Shares\\t \\nTo establish an account\\t$15,000,000 for Direct Investors\\n \\t$5,000,000 for Discretionary Accounts\\nTo add to an account\\tNo minimum levels\\nThere is no investment minimum for other Class R6 eligible investors.\\nIn general, you may purchase or redeem shares on any business day:\\n\\u2022\\tThrough your Financial Intermediary for the eligible retirement plan or college savings plan through which you invest in the Fund\\n\\u2022\\tBy writing to J.P. Morgan Funds Services, P.O. Box 8528, Boston, MA 02266-8528\\n\\u2022\\tAfter you open an account, by calling J.P. Morgan Funds Services at 1-800-480-4111\\nTax Information\\nThe Fund intends to make distributions that may be taxed as ordinary income or capital gains, except when your investment is in an 401(k) plan or other tax-advantaged investment plan. In which case you may be subject to federal income tax upon withdrawal from the tax-advantaged investment plan.\\nPayments to Broker-Dealers and Other Financial Intermediaries\\nIf you purchase shares of the Fund through a broker-dealer or other financial intermediary (such as a bank), the Fund and its related companies may pay the financial intermediary for the sale of Fund shares and related services. These payments may create a conflict of interest by influencing the broker-dealer or financial intermediary and your salesperson to recommend the Fund over another investment. Ask your salesperson or visit your financial intermediary\\u2019s website for more information.\",\n          \"American Funds Moderate Growth and Income Portfolio\\n\\nInvestment objectives The fund\\u2019s investment objectives are to provide current income and long-term growth of capital and income.\\n\\nFees and expenses of the fund This table describes the fees and expenses that you may pay if you buy and hold shares of the fund. In addition to the fees and expenses described below, you may also be required to pay brokerage commissions on purchases and sales of Class F-2 or F-3 shares of the fund. You may qualify for a Class A sales charge discount if you and your family invest, or agree to invest in the future, at least $25,000 in American Funds. More information about these and other discounts is available from your financial professional, in the \\u201cSales charge reductions and waivers\\u201d sections on page 76 of the prospectus and on page 96 of the fund\\u2019s statement of additional information, and in the sales charge waiver appendix to this prospectus.\\n\\n \\t \\t \\t \\t \\t \\t \\nShareholder fees (fees paid directly from your investment)\\nShare class:\\tA, 529-A\\nand ABLE-A\\tC and\\n529-C\\t529-E\\tT and\\n529-T\\tAll F and 529-F share classes\\tAll R\\nshare\\nclasses\\nMaximum sales charge (load) imposed on purchases (as a percentage of offering price)\\t5.75%\\tnone\\tnone\\t2.50%\\tnone\\tnone\\nMaximum deferred sales charge (load) (as a percentage of the amount redeemed)\\t1.001\\t1.00%\\tnone\\tnone\\tnone\\tnone\\nMaximum sales charge (load) imposed on reinvested dividends\\tnone\\tnone\\tnone\\tnone\\tnone\\tnone\\nRedemption or exchange fees\\tnone\\tnone\\tnone\\tnone\\tnone\\tnone\\n \\t \\t \\t \\t \\t \\t \\t \\nAnnual fund operating expenses (expenses that you pay each year as a percentage of the value of your investment)\\nShare class:\\tA\\tC\\tT\\tF-1\\tF-2\\tF-3\\t529-A\\nManagement fees\\tnone\\tnone\\tnone\\tnone\\tnone\\tnone\\tnone\\nDistribution and/or service (12b-1) fees\\t0.26%2\\t1.00%\\t0.25%\\t0.25%\\tnone\\tnone\\t0.26%2\\nOther expenses\\t0.11\\t0.11\\t0.113\\t0.14\\t0.13%\\t0.02%3\\t0.18\\nAcquired (underlying) fund fees and expenses\\t0.38\\t0.38\\t0.38\\t0.38\\t0.38\\t0.38\\t0.38\\nTotal annual fund operating expenses\\t0.75\\t1.49\\t0.74\\t0.77\\t0.51\\t0.40\\t0.82\\n \\t \\t \\t \\t \\t \\t \\t \\nShare class:\\t529-C\\t529-E\\t529-T\\t529-F-1\\tABLE-A\\tR-1\\tR-2\\nManagement fees\\tnone\\tnone\\tnone\\tnone\\tnone\\tnone\\tnone\\nDistribution and/or service (12b-1) fees\\t1.00%\\t0.50%\\t0.25%\\t0.00%\\t0.30%\\t1.00%\\t0.75%\\nOther expenses\\t0.18\\t0.14\\t0.183\\t0.18\\t0.143\\t0.11\\t0.35\\nAcquired (underlying) fund fees and expenses\\t0.38\\t0.38\\t0.38\\t0.38\\t0.38\\t0.38\\t0.38\\nTotal annual fund operating expenses\\t1.56\\t1.02\\t0.81\\t0.56\\t0.82\\t1.49\\t1.48\\nFee waiver\\t \\u2014\\t \\u2014\\t \\u2014\\t \\u2014\\t0.074\\t \\u2014\\t \\u2014\\nTotal annual fund operating expenses after fee waiver\\t1.56\\t1.02\\t0.81\\t0.56\\t0.75\\t1.49\\t1.48\\n \\t \\t \\t \\t \\t \\t \\t \\nShare class:\\tR-2E\\tR-3\\tR-4\\tR-5E\\tR-5\\tR-6\\t \\nManagement fees\\tnone\\tnone\\tnone\\tnone\\tnone\\tnone\\t \\nDistribution and/or service (12b-1) fees\\t0.60%\\t0.50%\\t0.25%\\tnone\\tnone\\tnone\\t \\nOther expenses\\t0.20\\t0.15\\t0.10\\t0.16%\\t0.07%\\t0.02%\\t \\nAcquired (underlying) fund fees and expenses\\t0.38\\t0.38\\t0.38\\t0.38\\t0.38\\t0.38\\t \\nTotal annual fund operating expenses\\t1.18\\t1.03\\t0.73\\t0.54\\t0.45\\t0.40\\t \\n1 A contingent deferred sales charge of 1.00% applies on certain redemptions made within 18 months following purchases of $1 million or more made without an initial sales charge. Contingent deferred sales charge is calculated based on the lesser of the offering price and market value of shares being sold.\\n\\n2 Restated to reflect current fees.\\n\\n3 Based on estimated amounts for the current fiscal year.\\n\\n4 Virginia529, as program administrator of ABLEAmerica, is currently waiving the fee owed to it as compensation for its oversight and administration of ABLEAmerica. This waiver will be in effect through at least July 1, 2019. Subject to the terms of its contractual arrangement with the investment adviser, Virginia529 may elect to extend, modify or terminate the waiver at that time.\\n\\nAmerican Funds Portfolio Series / Prospectus     20\\n\\n\\n \\n \\n \\n\\nExample This example is intended to help you compare the cost of investing in the fund with the cost of investing in other mutual funds.\\n\\nThe example assumes that you invest $10,000 in the fund for the time periods indicated and then redeem all of your shares at the end of those periods. The example also assumes that your investment has a 5% return each year and that the fund\\u2019s operating expenses remain the same. The example reflects the fee waiver described above through the expiration date of such waiver and total annual operating expenses thereafter. You may be required to pay brokerage commissions on your purchases and sales of Class F-2 or F-3 shares of the fund, which are not reflected in the example. Although your actual costs may be higher or lower, based on these assumptions your costs would be:\\n\\n \\t \\t \\t \\t \\t \\t \\t \\t \\t \\t \\t \\t \\t \\nShare class:\\tA\\tC\\tT\\tF-1\\tF-2\\tF-3\\t529-A\\t529-C\\t529-E\\t529-T\\t529-F-1\\tABLE-A\\tR-1\\n1 year\\t$ 647\\t$ 252\\t$ 324\\t$ 79\\t$ 52\\t$ 41\\t$ 654\\t$ 259\\t$ 104\\t$ 331\\t$ 57\\t$ 647\\t$ 152\\n3 years\\t801\\t471\\t481\\t246\\t164\\t128\\t822\\t493\\t325\\t502\\t179\\t815\\t471\\n5 years\\t968\\t813\\t651\\t428\\t285\\t224\\t1,004\\t850\\t563\\t688\\t313\\t997\\t813\\n10 years\\t1,452\\t1,779\\t1,145\\t954\\t640\\t505\\t1,530\\t1,856\\t1,248\\t1,227\\t701\\t1,524\\t1,779\\n \\t \\t \\t \\t \\t \\t \\t \\t \\t \\t \\t \\nShare class:\\tR-2\\tR-2E\\tR-3\\tR-4\\tR-5E\\tR-5\\tR-6\\tFor the share classes listed to the right, you would pay the following if you did not redeem your shares:\\tShare class:\\tC\\t529-C\\n1 year\\t$ 151\\t$ 120\\t$ 105\\t$ 75\\t$ 55\\t$ 46\\t$ 41\\t1 year\\t$ 152\\t$ 159\\n3 years\\t468\\t375\\t328\\t233\\t173\\t144\\t128\\t3 years\\t471\\t493\\n5 years\\t808\\t649\\t569\\t406\\t302\\t252\\t224\\t5 years\\t813\\t850\\n10 years\\t1,768\\t1,432\\t1,259\\t906\\t677\\t567\\t505\\t10 years\\t1,779\\t1,856\\nPortfolio turnover The fund may pay transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d its portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when fund shares are held in a taxable account. These costs, which are not reflected in annual fund operating expenses or in the example, affect the fund\\u2019s investment results. During the most recent fiscal year, the fund\\u2019s portfolio turnover rate was 15% of the average value of its portfolio.\\n\\nPrincipal investment strategies The fund will attempt to achieve its investment objectives by investing in a mix of American Funds in different combinations and weightings. The underlying American Funds may include growth, growth-and-income, equity-income, balanced and bond funds.\\n\\nNormally through its investments in the underlying funds, the fund will maintain at least 45% of the value of its net assets in common stocks and other equity investments and at least 25% of the value of its net assets in bonds and other debt securities (including money market instruments).\\n\\nThe fund will have exposure to growth-oriented common stocks. The fund will seek to generate some of its income from exposure to dividend paying stocks. The fund will seek exposure to issuers domiciled outside the United States, including those domiciled in emerging markets. The investment adviser believes that exposure to issuers domiciled outside the United States can help provide diversification when seeking current income and long-term growth of capital.\\n\\nWith respect to its fixed-income investments, the underlying funds in which the fund invests may hold debt securities with a wide range of maturities. The fund may invest in underlying funds with significant exposure to bonds rated BB+ or below and Ba1 or below by Nationally Recognized Statistical Rating Organizations designated by the fund\\u2019s investment adviser, or unrated but determined by the fund\\u2019s investment adviser to be of equivalent quality. Securities rated BB+ or below and Ba1 or below are sometimes referred to as \\u201cjunk bonds.\\u201d Exposure to lower rated securities may help the fund achieve its objective of providing current income.\\n\\nThe underlying funds may hold securities issued and guaranteed by the U.S. government, securities issued by federal agencies and instrumentalities and securities backed by mortgages or other assets. The underlying funds may also invest in the debt securities of governments, agencies, corporations and other entities domiciled outside the United States.\\n\\nThe fund\\u2019s investment adviser seeks to create combinations of underlying funds that complement each other with a goal of achieving the fund\\u2019s investment objective of providing current income and long-term growth of capital and income. In making this determination, the fund\\u2019s investment adviser considers the historical volatility and returns of the underlying funds and how various combinations would have behaved in past market environments. It also considers, among other topics, current market conditions and the investment positions of the underlying funds.\\n\\nThe fund\\u2019s investment adviser periodically reviews the investment strategies and asset mix of the underlying funds. The investment adviser will also consider whether overall market conditions would favor a change in the exposure of the fund to various asset types or geographic regions. Based on these considerations, the investment adviser may make adjustments to underlying fund holdings by adjusting the percentage of individual underlying funds within the fund, or adding or removing underlying funds. The investment adviser may also determine not to change the underlying fund allocations, particularly in response to short-term market movements, if in its opinion the combination of underlying funds is appropriate to meet the fund\\u2019s investment objective.\\n\\n21     American Funds Portfolio Series / Prospectus\\n\\n\\n \\n \\n \\n\\nPrincipal risks This section describes the principal risks associated with the fund\\u2019s and its underlying funds\\u2019 principal investment strategies. You may lose money by investing in the fund. The likelihood of loss may be greater if you invest for a shorter period of time. Investors in the fund should have a long-term perspective and be able to tolerate potentially sharp declines in value.\\n\\nThe following are principal risks associated with the fund\\u2019s investment strategies.\\n\\nAllocation risk \\u2014 Investments in the fund are subject to risks related to the investment adviser\\u2019s allocation choices. The selection of the underlying funds and the allocation of the fund\\u2019s assets could cause the fund to lose value or its results to lag relevant benchmarks or other funds with similar objectives.\\n\\nFund structure \\u2014 The fund invests in underlying funds and incurs expenses related to the underlying funds. In addition, investors in the fund will incur fees to pay for certain expenses related to the operations of the fund. An investor holding the underlying funds directly and in the same proportions as the fund would incur lower overall expenses but would not receive the benefit of the portfolio management and other services provided by the fund.\\n\\nUnderlying fund risks \\u2014 Because the fund\\u2019s investments consist of underlying funds, the fund\\u2019s risks are directly related to the risks of the underlying funds. For this reason, it is important to understand the risks associated with investing in the underlying funds, as described below.\\n\\nThe following are principal risks associated with the underlying funds\\u2019 investment strategies.\\n\\nMarket conditions \\u2014 The prices of, and the income generated by, the common stocks, bonds and other securities held by the underlying funds may decline \\u2013 sometimes rapidly or unpredictably \\u2013 due to various factors, including events or conditions affecting the general economy or particular industries; overall market changes; local, regional or global political, social or economic instability; governmental or governmental agency responses to economic conditions; and currency exchange rate, interest rate and commodity price fluctuations.\\n\\nIssuer risks \\u2014 The prices of, and the income generated by, securities held by the underlying funds may decline in response to various factors directly related to the issuers of such securities, including reduced demand for an issuer\\u2019s goods or services, poor management performance and strategic initiatives such as mergers, acquisitions or dispositions and the market response to any such initiatives.\\n\\nInvesting in stocks \\u2014 Investing in stocks may involve larger price swings and greater potential for loss than other types of investments. As a result, the value of the underlying funds may be subject to sharp, short-term declines in value. Income provided by an underlying fund may be reduced by changes in the dividend policies of, and the capital resources available at, the companies in which the underlying fund invests.\\n\\nInvesting in growth-oriented stocks \\u2014 Growth-oriented common stocks and other equity-type securities (such as preferred stocks, convertible preferred stocks and convertible bonds) may involve larger price swings and greater potential for loss than other types of investments.\\n\\nInvesting in income-oriented stocks \\u2014 Income provided by an underlying fund may be reduced by changes in the dividend policies of, and the capital resources available for dividend payments at, the companies in which the underlying fund invests.\\n\\nInvesting in debt instruments \\u2014 The prices of, and the income generated by, bonds and other debt securities held by an underlying fund may be affected by changing interest rates and by changes in the effective maturities and credit ratings of these securities.\\n\\nRising interest rates will generally cause the prices of bonds and other debt securities to fall. Falling interest rates may cause an issuer to redeem, call or refinance a debt security before its stated maturity, which may result in the fund having to reinvest the proceeds in lower yielding securities. Longer maturity debt securities generally have greater sensitivity to changes in interest rates and may be subject to greater price fluctuations than shorter maturity debt securities.\\n\\nBonds and other debt securities are also subject to credit risk, which is the possibility that the credit strength of an issuer will weaken and/or an issuer of a debt security will fail to make timely payments of principal or interest and the security will go into default. Credit risk is gauged, in part, by the credit ratings of the debt securities in which the underlying fund invests. However, ratings are only the opinions of the rating agencies issuing them and are not guarantees as to credit quality or an evaluation of market risk. The underlying funds\\u2019 investment adviser relies on its own credit analysts to research issuers and issues in seeking to mitigate various credit and default risks.\\n\\nAmerican Funds Portfolio Series / Prospectus     22\\n\\n\\n \\n \\n \\n\\nInvesting in lower rated debt instruments \\u2014 Lower rated bonds and other lower rated debt securities generally have higher rates of interest and involve greater risk of default or price declines due to changes in the issuer\\u2019s creditworthiness than those of higher quality debt securities. The market prices of these securities may fluctuate more than the prices of higher quality debt securities and may decline significantly in periods of general economic difficulty. These risks may be increased with respect to investments in junk bonds.\\n\\nInvesting outside the United States \\u2014 Securities of issuers domiciled outside the United States, or with significant operations or revenues outside the United States, may lose value because of adverse political, social, economic or market developments (including social instability, regional conflicts, terrorism and war) in the countries or regions in which the issuers operate or generate revenue. These securities may also lose value due to changes in foreign currency exchange rates against the U.S. dollar and/or currencies of other countries. Issuers of these securities may be more susceptible to actions of foreign governments, such as the imposition of price controls or punitive taxes, that could adversely impact the value of these securities. Securities markets in certain countries may be more volatile and/or less liquid than those in the United States. Investments outside the United States may also be subject to different accounting practices and different regulatory, legal and reporting standards and practices, and may be more difficult to value, than those in the United States. In addition, the value of investments outside the United States may be reduced by foreign taxes, including foreign withholding taxes on interest and dividends. Further, there may be increased risks of delayed settlement of securities purchased or sold by an underlying fund. The risks of investing outside the United States may be heightened in connection with investments in emerging markets.\\n\\nInvesting in emerging markets \\u2014 Investing in emerging markets may involve risks in addition to and greater than those generally associated with investing in the securities markets of developed countries. For instance, developing countries may have less developed legal and accounting systems than those in developed countries. The governments of these countries may be less stable and more likely to impose capital controls, nationalize a company or industry, place restrictions on foreign ownership and on withdrawing sale proceeds of securities from the country, and/or impose punitive taxes that could adversely affect the prices of securities. In addition, the economies of these countries may be dependent on relatively few industries that are more susceptible to local and global changes. Securities markets in these countries can also be relatively small and have substantially lower trading volumes. As a result, securities issued in these countries may be more volatile and less liquid, and may be more difficult to value, than securities issued in countries with more developed economies and/or markets. Less certainty with respect to security valuations may lead to additional challenges and risks in calculating the underlying fund\\u2019s net asset value. Additionally, there may be increased settlement risks for transactions in local securities.\\n\\nInvesting in securities backed by the U.S. government \\u2014 Securities backed by the U.S. Treasury or the full faith and credit of the U.S. government are guaranteed only as to the timely payment of interest and principal when held to maturity. Accordingly, the current market values for these securities will fluctuate with changes in interest rates. Securities issued by government-sponsored entities and federal agencies and instrumentalities that are not backed by the full faith and credit of the U.S. government are neither issued nor guaranteed by the U.S. government.\\n\\nInvesting in mortgage-related and other asset-backed securities \\u2014 Mortgage-related securities, such as mortgage-backed securities, and other asset-backed securities, include debt obligations that represent interests in pools of mortgages or other income-bearing assets, such as consumer loans or receivables. Such securities often involve risks that are different from or more acute than the risks associated with investing in other types of debt securities. Mortgage-backed and other asset-backed securities are subject to changes in the payment patterns of borrowers of the underlying debt. When interest rates fall, borrowers are more likely to refinance or prepay their debt before its stated maturity. This may result in an underlying fund having to reinvest the proceeds in lower yielding securities, effectively reducing the underlying fund\\u2019s income. Conversely, if interest rates rise and borrowers repay their debt more slowly than expected, the time in which the mortgage-backed and other asset-backed securities are paid off could be extended, reducing an underlying fund\\u2019s cash available for reinvestment in higher yielding securities.\\n\\nLiquidity risk \\u2014 Certain underlying fund holdings may be deemed to be less liquid or illiquid because they cannot be readily sold without significantly impacting the value of the holdings. Liquidity risk may result from the lack of an active market for a holding, legal or contractual restrictions on resale, or the reduced number and capacity of market participants to make a market in such holding. Market prices for less liquid or illiquid holdings may be volatile, and reduced liquidity may have an adverse impact on the market price of such holdings. Additionally, the sale of less liquid or illiquid holdings may involve substantial delays (including delays in settlement) and additional costs and the underlying fund may be unable to sell such holdings when necessary to meet its liquidity needs.\\n\\n23     American Funds Portfolio Series / Prospectus\\n\\n\\n \\n \\n \\n\\nInvesting in future delivery contracts \\u2014 An underlying fund may enter into contracts, such as to-be-announced contracts and mortgage dollar rolls, that involve an underlying fund selling mortgage-related securities and simultaneously contracting to repurchase similar securities for delivery at a future date at a predetermined price. This can increase the underlying fund\\u2019s market exposure, and the market price of the securities that the underlying fund contracts to repurchase could drop below their purchase price. While an underlying fund can preserve and generate capital through the use of such contracts by, for example, realizing the difference between the sale price and the future purchase price, the income generated by the underlying fund may be reduced by engaging in such transactions. In addition, these transactions may increase the turnover rate of the underlying fund.\\n\\nInvesting in inflation-linked bonds \\u2014 The values of inflation-linked bonds generally fluctuate in response to changes in real interest rates \\u2014 i.e., rates of interest after factoring in inflation. A rise in real interest rates may cause the prices of inflation-linked securities to fall, while a decline in real interest rates may cause the prices to increase. Inflation-linked bonds may experience greater losses than other debt securities with similar durations when real interest rates rise faster than nominal interest rates. There can be no assurance that the value of an inflation-linked security will be directly correlated to changes in interest rates; for example, if interest rates rise for reasons other than inflation, the increase may not be reflected in the security\\u2019s inflation measure.\\n\\nInvesting in inflation-linked bonds may also reduce an underlying fund\\u2019s distributable income during periods of extreme deflation. If prices for goods and services decline throughout the economy, the principal and income on inflation-linked securities may decline and result in losses to the underlying fund.\\n\\nInvesting in derivatives \\u2014 The use of derivatives involves a variety of risks, which may be different from, or greater than, the risks associated with investing in traditional cash securities, such as stocks and bonds. Changes in the value of a derivative may not correlate perfectly with, and may be more sensitive to market events than, the underlying asset, rate or index, and a derivative instrument may expose the underlying fund to losses in excess of its initial investment. Derivatives may be difficult for the underlying fund to buy or sell at an opportune time or price and may be difficult to terminate or otherwise offset. The underlying fund\\u2019s use of derivatives may result in losses to the underlying fund, and investing in derivatives may reduce the underlying fund\\u2019s returns and increase the underlying fund\\u2019s price volatility. The underlying fund\\u2019s counterparty to a derivative transaction (including, if applicable, the underlying fund\\u2019s clearing broker, the derivatives exchange or the clearinghouse) may be unable or unwilling to honor its financial obligations in respect of the transaction.\\n\\nManagement \\u2014 The investment adviser to the fund and to the underlying funds actively manages each underlying fund\\u2019s investments. Consequently, the underlying funds are subject to the risk that the methods and analyses employed by the investment adviser in this process may not produce the desired results. This could cause an underlying fund to lose value or its investment results to lag relevant benchmarks or other funds with similar objectives.\\n\\nYour investment in the fund is not a bank deposit and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other governmental agency, entity or person. You should consider how this fund fits into your overall investment program.\\n\\nAmerican Funds Portfolio Series / Prospectus     24\\n\\n\\n \\n \\n \\n\\nInvestment results The following bar chart shows how the fund\\u2019s investment results have varied from year to year, and the following table shows how the fund\\u2019s average annual total returns for various periods compare with a broad measure of securities market results and other applicable measures of market results. This information provides some indication of the risks of investing in the fund. The fund has selected to add the MSCI All Country World ex USA Index and the Bloomberg Barclays U.S. Aggregate Index as dual primary indexes with the S&P 500 Index. These additions will provide a better range of comparison benchmarks to measure the series' objective. The Lipper Mixed-Asset Target Allocation Growth Funds Index includes funds that disclose investment objectives and/or strategies reasonably comparable to those of the fund. Past investment results (before and after taxes) are not predictive of future investment results. Updated information on the fund\\u2019s investment results can be obtained by visiting americanfunds.com.\\n\\n\\n\\n \\t \\t \\t \\t \\t \\nAverage annual total returns For the periods ended December 31, 2017 (with maximum sales charge):\\nShare class\\tInception date\\t1 year\\t5 years\\tLifetime\\nA \\u2014 Before taxes\\t5/18/2012\\t10.00%\\t8.10%\\t9.18%\\n\\u2014 After taxes on distributions\\t \\t9.14\\t7.38\\t8.51\\n\\u2014 After taxes on distributions and sale of fund shares\\t \\t6.42\\t6.32\\t7.26\\n \\t \\t \\t \\t \\nShare classes (before taxes)\\tInception date\\t1 year\\t5 years\\tLifetime\\nC\\t5/18/2012\\t14.77%\\t8.55%\\t9.49%\\nF-1\\t5/18/2012\\t16.66\\t9.34\\t10.30\\nF-2\\t5/18/2012\\t16.94\\t9.64\\t10.58\\n529-A\\t5/18/2012\\t9.94\\t8.03\\t9.10\\n529-C\\t5/18/2012\\t14.72\\t8.45\\t9.39\\n529-E\\t5/18/2012\\t16.36\\t9.04\\t9.98\\n529-F-1\\t5/18/2012\\t16.87\\t9.55\\t10.49\\nR-1\\t5/18/2012\\t15.77\\t8.54\\t9.49\\nR-2\\t5/18/2012\\t15.79\\t8.52\\t9.51\\nR-2E\\t8/29/2014\\t16.13\\tN/A\\t6.66\\nR-3\\t5/18/2012\\t16.31\\t9.03\\t9.98\\nR-4\\t5/18/2012\\t16.70\\t9.38\\t10.32\\nR-5E\\t11/20/2015\\t16.91\\tN/A\\t9.90\\nR-5\\t5/18/2012\\t16.99\\t9.69\\t10.63\\nR-6\\t5/18/2012\\t17.05\\t9.74\\t10.66\\n \\t \\t \\t \\nIndexes\\t1 year\\t5 years\\tLifetime\\n(from Class A inception)\\nS&P 500 Index (reflects no deductions for sales charges, account fees, expenses or U.S. federal income taxes)\\t21.83%\\t15.79%\\t16.19%\\nMSCI\\u00ae All Country World ex USA Index (reflects no deductions for sales charges, account fees, expenses or U.S. federal income taxes)\\t27.19\\t6.80\\t9.39\\nBloomberg Barclays U.S. Aggregate Index (reflects no deductions for sales charges, account fees, expenses or U.S. federal income taxes)\\t3.54\\t2.10\\t2.27\\nLipper Mixed-Asset Target Allocation Growth Funds Index (reflects no deductions for sales charges, account fees or U.S. federal income taxes)\\t16.67\\t9.96\\t10.82\\nAfter-tax returns are shown only for Class A shares; after-tax returns for other share classes will vary. After-tax returns are calculated using the highest individual federal income tax rates in effect during each year of the periods shown and do not reflect the impact of state and local taxes. Your actual after-tax returns depend on your individual tax situation and likely will differ from the results shown above. In addition, after-tax returns are not relevant if you hold your fund shares through a tax-deferred arrangement, such as a 401(k) plan, individual retirement account (IRA) or 529 college savings plan.\\n\\n25     American Funds Portfolio Series / Prospectus\\n\\n\\n \\n \\n \\n\\nManagement\\n\\nInvestment adviser Capital Research and Management CompanySM\\n\\nPortfolio oversight committee The investment adviser\\u2019s Portfolio Oversight Committee develops the allocation approach and selects the underlying funds in which the fund invests. The members of the Portfolio Oversight Committee are:\\n\\n \\t \\t \\nInvestment professional/\\nSeries title (if applicable)\\tInvestment professional\\nexperience in this fund\\tPrimary title with investment adviser\\nBradley J. Vogt Vice Chairman of the Board\\t6 years\\tPartner \\u2013 Capital Research Global Investors\\nAlan N. Berro Senior Vice President\\t6 years\\tPartner \\u2013 Capital World Investors\\nJoanna F. Jonsson Senior Vice President\\t4 years\\tPartner \\u2013 Capital World Investors\\nJames B. Lovelace Senior Vice President\\t6 years\\tPartner \\u2013 Capital Research Global Investors\\nWesley Phoa Senior Vice President\\t6 years\\tPartner \\u2013 Capital Fixed Income Investors\\nJohn H. Smet Senior Vice President\\t6 years\\tPartner \\u2013 Capital Fixed Income Investors\\nAndrew B. Suzman Senior Vice President\\t6 years\\tPartner \\u2013 Capital World Investors\\nPurchase and sale of fund shares The minimum amount to establish an account for all share classes is normally $250 and the minimum to add to an account is $50. For a payroll deduction retirement plan account, payroll deduction savings plan account or employer-sponsored 529 account, the minimum is $25 to establish or add to an account. For accounts with Class F-3 shares held and serviced by the fund\\u2019s transfer agent, the minimum investment amount is $1,000,000.\\n\\nIf you are a retail investor, you may sell (redeem) shares on any business day through your dealer or financial advisor or by writing to American Funds Service Company\\u00ae at P.O. Box 6007, Indianapolis, Indiana 46206-6007; telephoning American Funds Service Company at (800) 421-4225; faxing American Funds Service Company at (888) 421-4351; or accessing our website at americanfunds.com. Please contact your plan administrator or recordkeeper to sell (redeem) shares from your retirement plan.\\n\\nTax information Dividends and capital gain distributions you receive from the fund are subject to federal income taxes and may also be subject to state and local taxes, unless you are tax-exempt or your account is tax-favored.\\n\\nPayments to broker-dealers and other financial intermediaries If you purchase shares of the fund through a broker-dealer or other financial intermediary (such as a bank), the fund and the fund\\u2019s distributor or its affiliates may pay the intermediary for the sale of fund shares and related services. These payments may create a conflict of interest by influencing the broker-dealer or other intermediary and your individual financial advisor to recommend the fund over another investment. Ask your individual financial advisor or visit your financial intermediary\\u2019s website for more information.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(merge['summary'], merge['Ivestment Strategy'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "PgD_Rzzlil0P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk8JHPBPi1-C",
        "outputId": "69e7dff7-dfc8-4001-9d0d-de302f2ca64e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "410    Fund Summary\\n\\nFund/Class:\\nFidelity® Emergin...\n",
              "265    Ivy Core Equity Fund\\n\\nObjective\\n\\nTo seek t...\n",
              "57     Fund Summary\\n\\nInvestment Objective\\nThe fund...\n",
              "199    SUMMARY OF COLUMBIA VP – LARGE CAP GROWTH FUND...\n",
              "175    Janus Henderson International Opportunities Fu...\n",
              "                             ...                        \n",
              "106    Eaton Vance Global Small-Cap Fund\\n\\nIn connec...\n",
              "270    Ivy Global Growth Fund\\n\\nObjective\\n\\nTo seek...\n",
              "348    Franklin FTSE China ETF\\n\\nInvestment Goal\\n\\n...\n",
              "435    INVESTMENT OBJECTIVE\\nThe USAA Growth and Tax ...\n",
              "102    Fund Summary\\n\\nInvestment Objective\\n\\nThe Fu...\n",
              "Name: summary, Length: 368, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBcFtl9pjFis",
        "outputId": "76330411-13c9-470a-cfa1-66cd991dcf78"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "225    Janus Henderson Overseas Portfolio\\nTicker:\\tN...\n",
              "30     New World Fund\\n\\nInvestment objective The fun...\n",
              "39     American Funds Moderate Growth and Income Port...\n",
              "222    Janus Henderson Global Technology Portfolio\\nT...\n",
              "124    Summary of Key Information\\n\\n \\n\\nInvestment ...\n",
              "                             ...                        \n",
              "22     International Fund\\n\\nInvestment objective The...\n",
              "358    Franklin FTSE Mexico ETF\\n\\nInvestment Goal\\n\\...\n",
              "46     American Funds 2010 Target Date Retirement Fun...\n",
              "168    Janus Henderson Global Life Sciences Fund\\nTic...\n",
              "347    Franklin FTSE Canada ETF\\n\\nInvestment Goal\\n\\...\n",
              "Name: summary, Length: 93, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMwpYnjdjHEM",
        "outputId": "3ee18e51-b942-4755-f6d7-9bcc5e679c48"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "410          Equity Long Only (Low Risk)\n",
              "265          Equity Long Only (Low Risk)\n",
              "57           Equity Long Only (Low Risk)\n",
              "199          Equity Long Only (Low Risk)\n",
              "175          Equity Long Only (Low Risk)\n",
              "                     ...                \n",
              "106          Equity Long Only (Low Risk)\n",
              "270          Equity Long Only (Low Risk)\n",
              "348          Equity Long Only (Low Risk)\n",
              "435             Balanced Fund (Low Risk)\n",
              "102    Fixed Income Long Only (Low Risk)\n",
              "Name: Ivestment Strategy, Length: 368, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeOaJsgIjmD5",
        "outputId": "c3c872f4-48bf-4b2d-cf8a-1b362fd0701f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "225             Balanced Fund (Low Risk)\n",
              "30              Balanced Fund (Low Risk)\n",
              "39              Balanced Fund (Low Risk)\n",
              "222          Equity Long Only (Low Risk)\n",
              "124    Fixed Income Long Only (Low Risk)\n",
              "                     ...                \n",
              "22           Equity Long Only (Low Risk)\n",
              "358          Equity Long Only (Low Risk)\n",
              "46              Balanced Fund (Low Risk)\n",
              "168          Equity Long Only (Low Risk)\n",
              "347          Equity Long Only (Low Risk)\n",
              "Name: Ivestment Strategy, Length: 93, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "对数据进行清洗和分词"
      ],
      "metadata": {
        "id": "EAXyYdchj89s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\")+list(string.punctuation)+['``',\"''\"]+[\"]\",\"[\",\"*\"]+['doe', 'ha', 'wa'])\n",
        "\n",
        "# clean and tokenize without lemmatizing\n",
        "def tokenizer(txt):\n",
        "    txt = txt.replace('\\n', ' ').replace('\\t', ' ').lower()\n",
        "    word_tokens = word_tokenize(txt)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_sentence = [w for w in filtered_sentence if re.sub(\"[^A-Za-z ]+\",'',w) != '']\n",
        "    return filtered_sentence\n",
        "\n",
        "train_text_words = np.concatenate([tokenizer(summary) for summary in X_train])\n",
        "\n",
        "train_text_words[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-vSeldjjnHR",
        "outputId": "df90eb0d-cacf-49f5-b4fb-3ec7d7560559"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['fund', 'summary', 'fund/class', 'fidelity®', 'emerging',\n",
              "       'markets', 'fund/fidelity®', 'emerging', 'markets', 'fund',\n",
              "       'investment', 'objective', 'fund', 'seeks', 'capital',\n",
              "       'appreciation', 'fee', 'table', 'following', 'table'], dtype='<U44')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-gram Model"
      ],
      "metadata": {
        "id": "K0qMwgxrkXBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "batch_size = 128\n",
        "num_epochs = 3\n",
        "\n",
        "# Word2vec parameters\n",
        "embedding_size = 50\n",
        "max_vocabulary_size = 5000\n",
        "min_occurrence = 10\n",
        "skip_window = 3\n",
        "num_skips = 4\n",
        "\n",
        "count = [('UNK', -1)]\n",
        "count.extend(collections.Counter(train_text_words).most_common(max_vocabulary_size - 1))\n",
        "# Remove samples with less than 'min_occurrence' occurrences\n",
        "for i in range(len(count) - 1, -1, -1):\n",
        "    if count[i][1] < min_occurrence:\n",
        "        count.pop(i)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "word2id = dict()\n",
        "for i, (word, _)in enumerate(count):\n",
        "    word2id[word] = i\n",
        "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
        "vocab_size = len(id2word)\n",
        "\n",
        "data = list()\n",
        "unk_count = 0\n",
        "for word in train_text_words:\n",
        "    # Retrieve a word id, or assign it index 0 ('UNK') if not in dictionary\n",
        "    index = word2id.get(word, 0)\n",
        "    if index == 0:\n",
        "        unk_count += 1\n",
        "    data.append(index)\n",
        "count[0] = ('UNK', unk_count)\n",
        "\n",
        "# build OneHot vector from index\n",
        "def to_one_hot(data_point_index, vocab_size):\n",
        "    temp = np.zeros(vocab_size)\n",
        "    temp[data_point_index] = 1\n",
        "    return temp\n",
        "\n",
        "# Generate training batch for the skip-gram model\n",
        "def batch_generator(batch_size, num_skips, skip_window, vocab_size):\n",
        "    data_index = 0\n",
        "    while True :\n",
        "        assert batch_size % num_skips == 0\n",
        "        assert num_skips <= 2 * skip_window\n",
        "        # batch is filled with 128 inputs\n",
        "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "        # labels is filled with 128 outputs\n",
        "        labels = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "        span = 2 * skip_window + 1\n",
        "        # buffer keep track of the visited indexes visited\n",
        "        buffer = collections.deque(maxlen=span)\n",
        "        if data_index + span > len(data):\n",
        "            data_index = 0\n",
        "            # We stop the loop when we went through all the corpus\n",
        "            break\n",
        "        buffer.extend(data[data_index:data_index + span])\n",
        "        data_index += span\n",
        "        for i in range(batch_size // num_skips):\n",
        "            # Take the context current word\n",
        "            context_words = [w for w in range(span) if w != skip_window]\n",
        "            # Randomly select num_skips words in the context\n",
        "            words_to_use = random.sample(context_words, num_skips)\n",
        "            for j, context_word in enumerate(words_to_use):\n",
        "                # Creates one raw data\n",
        "                batch[i * num_skips + j] = buffer[skip_window]\n",
        "                labels[i * num_skips + j] = buffer[context_word]\n",
        "            if data_index == len(data):\n",
        "                buffer.extend(data[0:span])\n",
        "                data_index = span\n",
        "            else:\n",
        "                buffer.append(data[data_index])\n",
        "                data_index += 1\n",
        "        # Backtrack a little bit to avoid skipping words in the end of a batch\n",
        "        data_index = (data_index + len(data) - span) % len(data)\n",
        "\n",
        "        # translate word index to on-hot representation\n",
        "        batch_one_hot = np.array([to_one_hot(b, vocab_size) for b in batch])\n",
        "        labels_one_hot = np.array([to_one_hot(l, vocab_size) for l in labels])\n",
        "\n",
        "        # output one batch\n",
        "        yield batch_one_hot, labels_one_hot"
      ],
      "metadata": {
        "id": "OgK649OBkNMY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create en compile the Autoencoder\n",
        "def creat_word2vec_model():\n",
        "    input_word = Input(shape=(vocab_size,))\n",
        "\n",
        "    encoded = Dense(embedding_size, activation='linear')(input_word)\n",
        "    decoded = Dense(vocab_size, activation='softmax')(encoded)\n",
        "\n",
        "    autoencoder = Model(input_word, decoded)\n",
        "    encoder = Model(input_word, encoded)\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return encoder, autoencoder\n",
        "\n",
        "encoder, autoencoder = creat_word2vec_model()\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN4ONa4Okbpj",
        "outputId": "aaa9bec3-6530-48ae-bd4c-1757db7466ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 2999)]            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 50)                150000    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2999)              152949    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 302949 (1.16 MB)\n",
            "Trainable params: 302949 (1.16 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit_generator(batch_generator(batch_size, num_skips, skip_window, vocab_size), steps_per_epoch=ceil(len(data) / batch_size), epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX1svpvMke9U",
        "outputId": "c27dae4e-db9c-4d8a-f3a2-f573e1a194cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-a9705c45700e>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  autoencoder.fit_generator(batch_generator(batch_size, num_skips, skip_window, vocab_size), steps_per_epoch=ceil(len(data) / batch_size), epochs=num_epochs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5104/5104 [==============================] - 48s 9ms/step - loss: 0.0177\n",
            "Epoch 2/3\n",
            "5104/5104 [==============================] - 34s 7ms/step - loss: 0.0025\n",
            "Epoch 3/3\n",
            "5104/5104 [==============================] - 34s 7ms/step - loss: 0.0025\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7be0344fe230>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vecotrize(word):\n",
        "    word_one_hot = to_one_hot(word2id[word], vocab_size)\n",
        "    return encoder.predict(np.array([word_one_hot]))[0]\n",
        "\n",
        "word2vec = {w : vecotrize(w) for w in word2id.keys()}\n",
        "save_word2vec('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/train_word2vec')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_MpIyS_kiA9",
        "outputId": "cdc5a573-d9c1-4e81-d85a-55bdb7139470"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# 将word2vec存储到文件\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/word2vec.pkl', 'wb') as f:\n",
        "    pickle.dump(word2vec, f)"
      ],
      "metadata": {
        "id": "f_FQrH_bkj5Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# 从文件中加载word2vec\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/word2vec.pkl', 'rb') as f:\n",
        "    loaded_word2vec = pickle.load(f)\n",
        "\n",
        "# 现在可以使用加载后的word2vec进行相关操作\n",
        "print(loaded_word2vec['applies'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoxSeETmkqc2",
        "outputId": "86f1bbf1-d2cf-4d04-a89c-6b34cfaaefc8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.40828422  0.47012526 -0.40947387  0.4886571  -0.5518719  -0.39241928\n",
            " -0.5401243  -0.5611947   0.5496588   0.4871601   0.35493436  0.41499206\n",
            "  0.46812093  0.4647784   0.41069517  0.42259383  0.539541    0.36927092\n",
            " -0.58554935  0.4859196  -0.4302955   0.48177817 -0.32811436  0.3740036\n",
            " -0.51541406 -0.40884832  0.3931866   0.37424377  0.42890206  0.38710773\n",
            " -0.44505247 -0.4220277  -0.5855024  -0.47997475 -0.35378954  0.39862266\n",
            " -0.38268578 -0.4475251   0.3823932  -0.4204011  -0.46706894 -0.46867824\n",
            "  0.4649396   0.3915875  -0.45137233  0.4779493   0.35524365 -0.48428047\n",
            "  0.431617   -0.49263707]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemma_tokenizer(text):\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text.replace(\"'\",\" \"))]\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\")+list(string.punctuation)+['``',\"''\",\"’\"]+[\"]\",\"[\",\"*\"]+['doe', 'ha', 'wa'] +['--']+ [''])"
      ],
      "metadata": {
        "id": "3X_x3dFUkwy2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######原始的找key_words的方式\n",
        "max_features = 18\n",
        "tfidf = TfidfVectorizer(input='content', tokenizer=lemma_tokenizer, stop_words=list(stop_words), max_features=max_features)\n",
        "tfidf_train = tfidf.fit_transform(X_train)\n",
        "key_words = tfidf.get_feature_names_out() # 常用关键词"
      ],
      "metadata": {
        "id": "T4EFGdyDk2ZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd12826-1f75-44f9-9fdb-96caafa14c3c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9Yzbfsxk-48",
        "outputId": "dd8fb4c6-13b9-4bc3-a70e-f82fd81fa309"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['class', 'company', 'expense', 'fee', 'fund', 'investment',\n",
              "       'market', 'may', 'performance', 'portfolio', 'rate', 'return',\n",
              "       'risk', 'security', 'share', 'tax', 'value', 'year'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_n_closer(word, n, word2vec):\n",
        "    vect = word2vec[word]\n",
        "    dist_dict = {k: cosine(v, vect) for k, v in word2vec.items()}\n",
        "    closer_words = []\n",
        "    for _ in range(n):\n",
        "        min_key = min(dist_dict.keys(), key=lambda k: dist_dict[k])\n",
        "        closer_words.append(min_key)\n",
        "        del dist_dict[min_key]\n",
        "    return closer_words\n",
        "\n",
        "##knowledge base\n",
        "def create_knowledge_base(num_neighbors, word2vec, key_words):\n",
        "    knowledge_base = set()\n",
        "    out = display(progress(0, len(key_words)-1), display_id=True)\n",
        "    for ii, key_word in enumerate(key_words) :\n",
        "        knowledge_base.add(key_word)\n",
        "        neighbors = []\n",
        "        try :\n",
        "            neighbors = get_n_closer(key_word, num_neighbors, word2vec)\n",
        "        except :\n",
        "            print(key_word + ' not in word2vec')\n",
        "\n",
        "        knowledge_base.update(neighbors)\n",
        "\n",
        "        out.update(progress(ii, len(key_words)-1))\n",
        "    return knowledge_base\n",
        "\n",
        "knowledge_base = create_knowledge_base(5, loaded_word2vec, key_words)\n",
        "print(knowledge_base)\n",
        "\n",
        "# Takes a summary, the knowledge base and some hyper parameters and returns the \"num_sent\" sentences\n",
        "# of the summary that are closer to the the knowledge base in term of spacial distances.\n",
        "def extract_sentence_distance(summary, knowledge, n_closer, n_reject, num_sent):\n",
        "    # Split the summary into sentences.\n",
        "    sentences = sent_tokenize(summary)\n",
        "    sentence_scores = []\n",
        "    # Loop over the sentences.\n",
        "    for j, sentence in enumerate(sentences):\n",
        "        # we tokenize and clean the sentence\n",
        "        tokens = tokenizer(sentence)\n",
        "\n",
        "        sentence_barycentre = np.zeros(embedding_size)\n",
        "        effective_len = 0\n",
        "        # Compute the barycentre of the sentence\n",
        "        for token in tokens :\n",
        "            try :\n",
        "                sentence_barycentre += np.array(loaded_word2vec[token])\n",
        "                effective_len += 1\n",
        "            except KeyError :\n",
        "                pass\n",
        "            except :\n",
        "                raise\n",
        "\n",
        "        # Reject sentences with less than n_reject words in our word2vec map\n",
        "        if effective_len <= n_reject :\n",
        "            sentence_scores.append(1)\n",
        "\n",
        "        else :\n",
        "            sentence_barycentre = sentence_barycentre/effective_len\n",
        "            # Compute the distance sentece_barycentre -> words in our knowledge base\n",
        "            barycentre_distance = [cosine(sentence_barycentre, loaded_word2vec[key_word]) for key_word in knowledge]\n",
        "            barycentre_distance.sort()\n",
        "            # Create the score of the sentence by averaging the \"n_closer\" smallest distances\n",
        "            score = np.mean(barycentre_distance[:n_closer])\n",
        "            sentence_scores.append(score)\n",
        "    # Select the \"num_sent\" sentences that have the smallest score (smallest distance score with the knowledge base)\n",
        "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
        "    top_sentences = sentences[:num_sent]\n",
        "    return ' '.join(top_sentences)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "_kYpHBTelA-Q",
        "outputId": "bb80a7f9-a139-4dd8-9ebf-b707ea7bb020"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='17'\n",
              "            max='17',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            17\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'indicate', 'security', 'waivers', 'year', 'exempt', 'institutional', 'result', 'performance', 'shareholder', 'tax', 'income', 'return', 'volatile', 'pay', 'involves', 'fund', 'affect', 'reflected', 'rate', 'b', 'intended', 'less', 'sells', 'fiscal', 'r6', 'recent', 'debt', 'seeks', 'marginal', 'turnover', 'c', 'conditions', 'believes', 'market', 'distributions', 'manager', 'visit', 'individual', 'fixed-income', 'holdings', 'liquidity', 'risk', 'past', 'years', 'pays', 'n', 'variety', 'prices', 'shows', 'remain', 'attractive', 'ended', 'achieve', 'waiver', 'could', 'decline', 'may', 'value', 'inception', 'objective', 'end', 'derivatives', 'types', 'share', 'class', 'markets', 'taxes', 'turns', 'credit', 'reimbursement', 'increased', 'portfolio', 'r', 'higher', 'investment', 'issuer', 'december', \"'s\", 'expense', 'fee', 'company', 'common', 'greater', 'reimbursements'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(knowledge_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWyXNuI-lCLb",
        "outputId": "18b49a64-da41-416c-8a11-b7804a6a4e32"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Measure Distance"
      ],
      "metadata": {
        "id": "QPezzHqilLQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the train,validation and test dataframe\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "X_valid_df = pd.DataFrame(X_valid)\n",
        "X_test_df = pd.DataFrame(X_test)\n",
        "\n",
        "embedding_size = 50\n",
        "\n",
        "X_train_df['sentences_distance'] = X_train_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
        "X_valid_df['sentences_distance'] = X_valid_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
        "X_test_df['sentences_distance'] = X_test_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)"
      ],
      "metadata": {
        "id": "UsMecpvblORA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentence_match(summary, knowledge, num_sent):\n",
        "    sentences = sent_tokenize(summary)\n",
        "    sentence_scores = []\n",
        "    for j, sentence in enumerate(sentences):\n",
        "        set_tokens = set(tokenizer(sentence))\n",
        "\n",
        "        # Find the number of common words between the knowledge base and the sentence\n",
        "        inter_knwoledge = set_tokens.intersection(knowledge)\n",
        "\n",
        "        sentence_scores.append(len(inter_knwoledge))\n",
        "\n",
        "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
        "    top_sentences = sentences[len(sentences)-num_sent-1:]\n",
        "    return ' '.join(top_sentences)\n",
        "\n",
        "X_train_df['sentences_match'] = X_train_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "X_valid_df['sentences_match'] = X_valid_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "X_test_df['sentences_match'] = X_test_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "\n",
        "# produce train_X and test_X\n",
        "train_X = X_train_df['sentences_match'].values\n",
        "train_X = [' '.join(tokenizer(txt)) for txt in train_X]\n",
        "\n",
        "valid_X = X_valid_df['sentences_match'].values\n",
        "valid_X = [' '.join(tokenizer(txt)) for txt in valid_X]\n",
        "\n",
        "test_X = X_test_df['sentences_match'].values\n",
        "test_X = [' '.join(tokenizer(txt)) for txt in test_X]\n",
        "\n",
        "# produce train_y and valid_y\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "encoded_train_y = encoder.fit_transform(y_train)\n",
        "label_train_y = to_categorical(encoded_train_y, num_classes=3)\n",
        "\n",
        "encoded_valid_y = encoder.transform(y_valid)\n",
        "label_valid_y = to_categorical(encoded_valid_y, num_classes=3)\n",
        "\n",
        "\n",
        "num_words = 2500 # Size of the vocabulary used. we only consider the 2500 most common words. The other words are removed from the texts.\n",
        "maxlen = 150 # Number of word considered for each document. we cut or lengthen the texts to have texts of 150 words.\n",
        "word_dimension = 50 # dimension of our word vectors.\n",
        "\n",
        "keras_tokenizer = Tokenizer(num_words=num_words)\n",
        "\n",
        "keras_tokenizer.fit_on_texts(train_X)\n",
        "\n",
        "word_index = keras_tokenizer.word_index\n",
        "\n",
        "sequences_train = keras_tokenizer.texts_to_sequences(train_X)\n",
        "sequences_valid = keras_tokenizer.texts_to_sequences(valid_X)\n",
        "sequences_test = keras_tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "# truncate or lenthen each text so they have the same length.\n",
        "feature_train = pad_sequences(sequences_train, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "feature_valid = pad_sequences(sequences_valid, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "feature_test = pad_sequences(sequences_test, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "\n",
        "# create our embedding matrix\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, word_dimension))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = loaded_word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "vGkgRiCIlama"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classifier"
      ],
      "metadata": {
        "id": "OYbiQhYLloHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN1"
      ],
      "metadata": {
        "id": "tVg-7LYUpbjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def create_CNN_model():\n",
        "    CNN = Sequential()\n",
        "    # The Embedding layer takes the embedding matrix as an argument and transform the inputed the sequences of index to sequences of vectors.\n",
        "    CNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=True))\n",
        "    CNN.add(Convolution1D(64, 5, activation = 'relu'))\n",
        "    CNN.add(MaxPooling1D(pool_size = 5))\n",
        "    CNN.add(Convolution1D(32, 5, activation = 'relu'))\n",
        "    CNN.add(MaxPooling1D(pool_size = 5))\n",
        "    CNN.add(Flatten())\n",
        "    CNN.add(Dense(units = 128 , activation = 'relu'))\n",
        "    CNN.add(Dropout(0.5))\n",
        "    CNN.add(Dense(units = 3, activation = 'softmax'))\n",
        "    CNN.compile(optimizer = 'RMSprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    return CNN\n",
        "\n",
        "CNN_model = create_CNN_model()\n",
        "\n",
        "# 创建ModelCheckpoint回调函数\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "# 记录开始时间\n",
        "start_time = time.time()\n",
        "\n",
        "CNN_history = CNN_model.fit(feature_train, label_train_y,\n",
        "                             epochs=800, batch_size=100,\n",
        "                             validation_data=(feature_valid, label_valid_y),\n",
        "                             callbacks=[checkpoint])\n",
        "\n",
        "# 记录结束时间\n",
        "end_time = time.time()\n",
        "\n",
        "# 计算训练时间\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"Training time: {training_time:.2f} seconds\")\n",
        "CNN_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzCvS2jNzc6u",
        "outputId": "b3337b94-8702-4410-8391-f662c9b90381"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.1342 - accuracy: 0.4103\n",
            "Epoch 1: val_accuracy improved from -inf to 0.46237, saving model to best_model.h5\n",
            "4/4 [==============================] - 2s 195ms/step - loss: 1.1342 - accuracy: 0.4103 - val_loss: 1.0997 - val_accuracy: 0.4624\n",
            "Epoch 2/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.9395 - accuracy: 0.6400"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - ETA: 0s - loss: 0.9930 - accuracy: 0.5462\n",
            "Epoch 2: val_accuracy did not improve from 0.46237\n",
            "4/4 [==============================] - 0s 127ms/step - loss: 0.9930 - accuracy: 0.5462 - val_loss: 1.0167 - val_accuracy: 0.4624\n",
            "Epoch 3/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.9790 - accuracy: 0.5652\n",
            "Epoch 3: val_accuracy did not improve from 0.46237\n",
            "4/4 [==============================] - 0s 88ms/step - loss: 0.9790 - accuracy: 0.5652 - val_loss: 1.1105 - val_accuracy: 0.4624\n",
            "Epoch 4/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.9795 - accuracy: 0.5598\n",
            "Epoch 4: val_accuracy did not improve from 0.46237\n",
            "4/4 [==============================] - 0s 122ms/step - loss: 0.9795 - accuracy: 0.5598 - val_loss: 0.9998 - val_accuracy: 0.4624\n",
            "Epoch 5/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.9061 - accuracy: 0.5733\n",
            "Epoch 5: val_accuracy improved from 0.46237 to 0.47312, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 95ms/step - loss: 0.9227 - accuracy: 0.5652 - val_loss: 0.9649 - val_accuracy: 0.4731\n",
            "Epoch 6/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.8799 - accuracy: 0.5924\n",
            "Epoch 6: val_accuracy did not improve from 0.47312\n",
            "4/4 [==============================] - 0s 122ms/step - loss: 0.8799 - accuracy: 0.5924 - val_loss: 0.9516 - val_accuracy: 0.4624\n",
            "Epoch 7/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.8836 - accuracy: 0.6087\n",
            "Epoch 7: val_accuracy improved from 0.47312 to 0.49462, saving model to best_model.h5\n",
            "4/4 [==============================] - 1s 194ms/step - loss: 0.8836 - accuracy: 0.6087 - val_loss: 0.9305 - val_accuracy: 0.4946\n",
            "Epoch 8/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.8431 - accuracy: 0.6005\n",
            "Epoch 8: val_accuracy improved from 0.49462 to 0.61290, saving model to best_model.h5\n",
            "4/4 [==============================] - 1s 192ms/step - loss: 0.8431 - accuracy: 0.6005 - val_loss: 0.9247 - val_accuracy: 0.6129\n",
            "Epoch 9/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.8036 - accuracy: 0.6576\n",
            "Epoch 9: val_accuracy improved from 0.61290 to 0.69892, saving model to best_model.h5\n",
            "4/4 [==============================] - 1s 139ms/step - loss: 0.8036 - accuracy: 0.6576 - val_loss: 0.9125 - val_accuracy: 0.6989\n",
            "Epoch 10/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.7905 - accuracy: 0.6766\n",
            "Epoch 10: val_accuracy did not improve from 0.69892\n",
            "4/4 [==============================] - 1s 186ms/step - loss: 0.7905 - accuracy: 0.6766 - val_loss: 0.8815 - val_accuracy: 0.5054\n",
            "Epoch 11/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.7098 - accuracy: 0.7011\n",
            "Epoch 11: val_accuracy did not improve from 0.69892\n",
            "4/4 [==============================] - 1s 183ms/step - loss: 0.7098 - accuracy: 0.7011 - val_loss: 0.8466 - val_accuracy: 0.5591\n",
            "Epoch 12/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.6705 - accuracy: 0.7283\n",
            "Epoch 12: val_accuracy improved from 0.69892 to 0.74194, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.6705 - accuracy: 0.7283 - val_loss: 0.7911 - val_accuracy: 0.7419\n",
            "Epoch 13/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.6758 - accuracy: 0.7228\n",
            "Epoch 13: val_accuracy did not improve from 0.74194\n",
            "4/4 [==============================] - 0s 85ms/step - loss: 0.6758 - accuracy: 0.7228 - val_loss: 0.9104 - val_accuracy: 0.5699\n",
            "Epoch 14/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.6878 - accuracy: 0.7038\n",
            "Epoch 14: val_accuracy did not improve from 0.74194\n",
            "4/4 [==============================] - 0s 85ms/step - loss: 0.6878 - accuracy: 0.7038 - val_loss: 0.7836 - val_accuracy: 0.6667\n",
            "Epoch 15/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.6370 - accuracy: 0.7609\n",
            "Epoch 15: val_accuracy did not improve from 0.74194\n",
            "4/4 [==============================] - 0s 87ms/step - loss: 0.6370 - accuracy: 0.7609 - val_loss: 0.7295 - val_accuracy: 0.6882\n",
            "Epoch 16/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5350 - accuracy: 0.7880\n",
            "Epoch 16: val_accuracy improved from 0.74194 to 0.75269, saving model to best_model.h5\n",
            "4/4 [==============================] - 1s 132ms/step - loss: 0.5350 - accuracy: 0.7880 - val_loss: 0.6986 - val_accuracy: 0.7527\n",
            "Epoch 17/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5289 - accuracy: 0.8300\n",
            "Epoch 17: val_accuracy did not improve from 0.75269\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.5685 - accuracy: 0.8043 - val_loss: 0.7391 - val_accuracy: 0.6774\n",
            "Epoch 18/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.7826\n",
            "Epoch 18: val_accuracy did not improve from 0.75269\n",
            "4/4 [==============================] - 0s 90ms/step - loss: 0.5313 - accuracy: 0.7826 - val_loss: 0.7148 - val_accuracy: 0.7527\n",
            "Epoch 19/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.8315\n",
            "Epoch 19: val_accuracy did not improve from 0.75269\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.5008 - accuracy: 0.8315 - val_loss: 0.7678 - val_accuracy: 0.6882\n",
            "Epoch 20/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.4484 - accuracy: 0.8261\n",
            "Epoch 20: val_accuracy did not improve from 0.75269\n",
            "4/4 [==============================] - 0s 118ms/step - loss: 0.4484 - accuracy: 0.8261 - val_loss: 0.6906 - val_accuracy: 0.7527\n",
            "Epoch 21/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.4260 - accuracy: 0.8424\n",
            "Epoch 21: val_accuracy did not improve from 0.75269\n",
            "4/4 [==============================] - 0s 89ms/step - loss: 0.4260 - accuracy: 0.8424 - val_loss: 0.7383 - val_accuracy: 0.6989\n",
            "Epoch 22/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.3545 - accuracy: 0.8800\n",
            "Epoch 22: val_accuracy did not improve from 0.75269\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.3983 - accuracy: 0.8641 - val_loss: 0.8829 - val_accuracy: 0.6344\n",
            "Epoch 23/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.8288\n",
            "Epoch 23: val_accuracy did not improve from 0.75269\n",
            "4/4 [==============================] - 0s 127ms/step - loss: 0.4083 - accuracy: 0.8288 - val_loss: 0.7026 - val_accuracy: 0.7204\n",
            "Epoch 24/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3430 - accuracy: 0.8913\n",
            "Epoch 24: val_accuracy improved from 0.75269 to 0.76344, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 94ms/step - loss: 0.3430 - accuracy: 0.8913 - val_loss: 0.6619 - val_accuracy: 0.7634\n",
            "Epoch 25/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.8614\n",
            "Epoch 25: val_accuracy did not improve from 0.76344\n",
            "4/4 [==============================] - 0s 123ms/step - loss: 0.3641 - accuracy: 0.8614 - val_loss: 0.7081 - val_accuracy: 0.7312\n",
            "Epoch 26/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.8967\n",
            "Epoch 26: val_accuracy did not improve from 0.76344\n",
            "4/4 [==============================] - 0s 118ms/step - loss: 0.3471 - accuracy: 0.8967 - val_loss: 0.6812 - val_accuracy: 0.7634\n",
            "Epoch 27/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.8804\n",
            "Epoch 27: val_accuracy did not improve from 0.76344\n",
            "4/4 [==============================] - 0s 122ms/step - loss: 0.3162 - accuracy: 0.8804 - val_loss: 0.6913 - val_accuracy: 0.6774\n",
            "Epoch 28/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3491 - accuracy: 0.8995\n",
            "Epoch 28: val_accuracy improved from 0.76344 to 0.77419, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 124ms/step - loss: 0.3491 - accuracy: 0.8995 - val_loss: 0.7008 - val_accuracy: 0.7742\n",
            "Epoch 29/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2563 - accuracy: 0.9200\n",
            "Epoch 29: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 87ms/step - loss: 0.2474 - accuracy: 0.9266 - val_loss: 0.6962 - val_accuracy: 0.7204\n",
            "Epoch 30/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9239\n",
            "Epoch 30: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.2363 - accuracy: 0.9239 - val_loss: 0.6886 - val_accuracy: 0.7419\n",
            "Epoch 31/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9457\n",
            "Epoch 31: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.2076 - accuracy: 0.9457 - val_loss: 0.8975 - val_accuracy: 0.6989\n",
            "Epoch 32/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.8913\n",
            "Epoch 32: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.2897 - accuracy: 0.8913 - val_loss: 0.7394 - val_accuracy: 0.7312\n",
            "Epoch 33/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.2038 - accuracy: 0.9467\n",
            "Epoch 33: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.2024 - accuracy: 0.9457 - val_loss: 0.6608 - val_accuracy: 0.7527\n",
            "Epoch 34/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9348\n",
            "Epoch 34: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 90ms/step - loss: 0.2244 - accuracy: 0.9348 - val_loss: 0.7042 - val_accuracy: 0.7527\n",
            "Epoch 35/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1715 - accuracy: 0.9538\n",
            "Epoch 35: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 85ms/step - loss: 0.1715 - accuracy: 0.9538 - val_loss: 0.7070 - val_accuracy: 0.7634\n",
            "Epoch 36/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9429\n",
            "Epoch 36: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 86ms/step - loss: 0.1870 - accuracy: 0.9429 - val_loss: 0.7261 - val_accuracy: 0.7419\n",
            "Epoch 37/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9592\n",
            "Epoch 37: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 91ms/step - loss: 0.1714 - accuracy: 0.9592 - val_loss: 0.7106 - val_accuracy: 0.7527\n",
            "Epoch 38/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9538\n",
            "Epoch 38: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 122ms/step - loss: 0.1512 - accuracy: 0.9538 - val_loss: 0.6792 - val_accuracy: 0.7742\n",
            "Epoch 39/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.9538\n",
            "Epoch 39: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 88ms/step - loss: 0.1416 - accuracy: 0.9538 - val_loss: 0.7071 - val_accuracy: 0.7312\n",
            "Epoch 40/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9620\n",
            "Epoch 40: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.1557 - accuracy: 0.9620 - val_loss: 0.7183 - val_accuracy: 0.7419\n",
            "Epoch 41/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9755\n",
            "Epoch 41: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 159ms/step - loss: 0.1155 - accuracy: 0.9755 - val_loss: 0.8221 - val_accuracy: 0.7204\n",
            "Epoch 42/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9592\n",
            "Epoch 42: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 137ms/step - loss: 0.1232 - accuracy: 0.9592 - val_loss: 0.7374 - val_accuracy: 0.7527\n",
            "Epoch 43/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1049 - accuracy: 0.9600\n",
            "Epoch 43: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0953 - accuracy: 0.9783 - val_loss: 0.7050 - val_accuracy: 0.7419\n",
            "Epoch 44/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9429\n",
            "Epoch 44: val_accuracy did not improve from 0.77419\n",
            "4/4 [==============================] - 0s 74ms/step - loss: 0.1606 - accuracy: 0.9429 - val_loss: 0.6995 - val_accuracy: 0.7742\n",
            "Epoch 45/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0904 - accuracy: 0.9800\n",
            "Epoch 45: val_accuracy improved from 0.77419 to 0.78495, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 89ms/step - loss: 0.0910 - accuracy: 0.9810 - val_loss: 0.7650 - val_accuracy: 0.7849\n",
            "Epoch 46/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.1195 - accuracy: 0.9567\n",
            "Epoch 46: val_accuracy did not improve from 0.78495\n",
            "4/4 [==============================] - 0s 78ms/step - loss: 0.1152 - accuracy: 0.9620 - val_loss: 0.7750 - val_accuracy: 0.7527\n",
            "Epoch 47/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9755\n",
            "Epoch 47: val_accuracy did not improve from 0.78495\n",
            "4/4 [==============================] - 0s 133ms/step - loss: 0.0809 - accuracy: 0.9755 - val_loss: 0.7413 - val_accuracy: 0.7849\n",
            "Epoch 48/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9701\n",
            "Epoch 48: val_accuracy did not improve from 0.78495\n",
            "4/4 [==============================] - 0s 81ms/step - loss: 0.1075 - accuracy: 0.9701 - val_loss: 0.8849 - val_accuracy: 0.7634\n",
            "Epoch 49/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9755\n",
            "Epoch 49: val_accuracy did not improve from 0.78495\n",
            "4/4 [==============================] - 0s 129ms/step - loss: 0.0828 - accuracy: 0.9755 - val_loss: 0.7580 - val_accuracy: 0.7849\n",
            "Epoch 50/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0967 - accuracy: 0.9700\n",
            "Epoch 50: val_accuracy improved from 0.78495 to 0.79570, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0667 - accuracy: 0.9837 - val_loss: 0.7291 - val_accuracy: 0.7957\n",
            "Epoch 51/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0852 - accuracy: 0.9800\n",
            "Epoch 51: val_accuracy did not improve from 0.79570\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0895 - accuracy: 0.9810 - val_loss: 1.1807 - val_accuracy: 0.7204\n",
            "Epoch 52/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9674\n",
            "Epoch 52: val_accuracy did not improve from 0.79570\n",
            "4/4 [==============================] - 0s 97ms/step - loss: 0.0811 - accuracy: 0.9674 - val_loss: 0.7676 - val_accuracy: 0.7742\n",
            "Epoch 53/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9647\n",
            "Epoch 53: val_accuracy did not improve from 0.79570\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.0728 - accuracy: 0.9647 - val_loss: 0.7350 - val_accuracy: 0.7849\n",
            "Epoch 54/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9837\n",
            "Epoch 54: val_accuracy did not improve from 0.79570\n",
            "4/4 [==============================] - 0s 119ms/step - loss: 0.0575 - accuracy: 0.9837 - val_loss: 1.0246 - val_accuracy: 0.7204\n",
            "Epoch 55/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0523 - accuracy: 0.9867\n",
            "Epoch 55: val_accuracy did not improve from 0.79570\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.0541 - accuracy: 0.9864 - val_loss: 1.0471 - val_accuracy: 0.7204\n",
            "Epoch 56/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0780 - accuracy: 0.9700\n",
            "Epoch 56: val_accuracy did not improve from 0.79570\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0776 - accuracy: 0.9701 - val_loss: 0.7699 - val_accuracy: 0.7849\n",
            "Epoch 57/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9837\n",
            "Epoch 57: val_accuracy did not improve from 0.79570\n",
            "4/4 [==============================] - 0s 89ms/step - loss: 0.0507 - accuracy: 0.9837 - val_loss: 0.8378 - val_accuracy: 0.7634\n",
            "Epoch 58/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0917 - accuracy: 0.9750\n",
            "Epoch 58: val_accuracy did not improve from 0.79570\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.1191 - accuracy: 0.9674 - val_loss: 0.9162 - val_accuracy: 0.7527\n",
            "Epoch 59/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9783\n",
            "Epoch 59: val_accuracy improved from 0.79570 to 0.80645, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.0931 - accuracy: 0.9783 - val_loss: 0.7450 - val_accuracy: 0.8065\n",
            "Epoch 60/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0544 - accuracy: 0.9850\n",
            "Epoch 60: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0573 - accuracy: 0.9810 - val_loss: 0.9114 - val_accuracy: 0.7742\n",
            "Epoch 61/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9864\n",
            "Epoch 61: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0451 - accuracy: 0.9864 - val_loss: 0.9039 - val_accuracy: 0.7419\n",
            "Epoch 62/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0527 - accuracy: 0.9700\n",
            "Epoch 62: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0676 - accuracy: 0.9783 - val_loss: 0.8563 - val_accuracy: 0.7527\n",
            "Epoch 63/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0454 - accuracy: 0.9900\n",
            "Epoch 63: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0766 - accuracy: 0.9783 - val_loss: 0.8823 - val_accuracy: 0.7419\n",
            "Epoch 64/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9701\n",
            "Epoch 64: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 89ms/step - loss: 0.0871 - accuracy: 0.9701 - val_loss: 0.7813 - val_accuracy: 0.8065\n",
            "Epoch 65/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9755\n",
            "Epoch 65: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.0682 - accuracy: 0.9755 - val_loss: 0.9993 - val_accuracy: 0.7419\n",
            "Epoch 66/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1319 - accuracy: 0.9600\n",
            "Epoch 66: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0777 - accuracy: 0.9783 - val_loss: 1.0824 - val_accuracy: 0.7312\n",
            "Epoch 67/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0726 - accuracy: 0.9800\n",
            "Epoch 67: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0689 - accuracy: 0.9783 - val_loss: 0.7973 - val_accuracy: 0.7742\n",
            "Epoch 68/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0570 - accuracy: 0.9800\n",
            "Epoch 68: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.0612 - accuracy: 0.9783 - val_loss: 0.8801 - val_accuracy: 0.7634\n",
            "Epoch 69/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9810\n",
            "Epoch 69: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0554 - accuracy: 0.9810 - val_loss: 0.8587 - val_accuracy: 0.7527\n",
            "Epoch 70/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9755\n",
            "Epoch 70: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.0599 - accuracy: 0.9755 - val_loss: 0.7670 - val_accuracy: 0.7849\n",
            "Epoch 71/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9864\n",
            "Epoch 71: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.0501 - accuracy: 0.9864 - val_loss: 0.9676 - val_accuracy: 0.7312\n",
            "Epoch 72/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9810\n",
            "Epoch 72: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.0598 - accuracy: 0.9810 - val_loss: 0.9698 - val_accuracy: 0.7419\n",
            "Epoch 73/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9891\n",
            "Epoch 73: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0435 - accuracy: 0.9891 - val_loss: 0.8629 - val_accuracy: 0.7849\n",
            "Epoch 74/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9837\n",
            "Epoch 74: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 92ms/step - loss: 0.0485 - accuracy: 0.9837 - val_loss: 0.9127 - val_accuracy: 0.7957\n",
            "Epoch 75/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9755\n",
            "Epoch 75: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 87ms/step - loss: 0.0810 - accuracy: 0.9755 - val_loss: 0.8752 - val_accuracy: 0.7634\n",
            "Epoch 76/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0507 - accuracy: 0.9800\n",
            "Epoch 76: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0608 - accuracy: 0.9810 - val_loss: 0.8130 - val_accuracy: 0.8065\n",
            "Epoch 77/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9783\n",
            "Epoch 77: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.0549 - accuracy: 0.9783 - val_loss: 0.8432 - val_accuracy: 0.7849\n",
            "Epoch 78/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0312 - accuracy: 0.9900\n",
            "Epoch 78: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0585 - accuracy: 0.9810 - val_loss: 0.8869 - val_accuracy: 0.7527\n",
            "Epoch 79/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9864\n",
            "Epoch 79: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 87ms/step - loss: 0.0542 - accuracy: 0.9864 - val_loss: 1.0832 - val_accuracy: 0.7419\n",
            "Epoch 80/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9810\n",
            "Epoch 80: val_accuracy did not improve from 0.80645\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0406 - accuracy: 0.9810 - val_loss: 0.9458 - val_accuracy: 0.8065\n",
            "Epoch 81/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0416 - accuracy: 0.9850\n",
            "Epoch 81: val_accuracy improved from 0.80645 to 0.82796, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 0.0440 - accuracy: 0.9837 - val_loss: 0.7547 - val_accuracy: 0.8280\n",
            "Epoch 82/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9783\n",
            "Epoch 82: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 0.0551 - accuracy: 0.9783 - val_loss: 0.8742 - val_accuracy: 0.7957\n",
            "Epoch 83/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9810\n",
            "Epoch 83: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0434 - accuracy: 0.9810 - val_loss: 0.8493 - val_accuracy: 0.7957\n",
            "Epoch 84/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9837\n",
            "Epoch 84: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 92ms/step - loss: 0.0562 - accuracy: 0.9837 - val_loss: 0.9000 - val_accuracy: 0.7634\n",
            "Epoch 85/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0275 - accuracy: 0.9900\n",
            "Epoch 85: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0427 - accuracy: 0.9837 - val_loss: 1.0075 - val_accuracy: 0.7634\n",
            "Epoch 86/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9864\n",
            "Epoch 86: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0352 - accuracy: 0.9864 - val_loss: 0.9548 - val_accuracy: 0.7957\n",
            "Epoch 87/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9810\n",
            "Epoch 87: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 0.0533 - accuracy: 0.9810 - val_loss: 0.9786 - val_accuracy: 0.7419\n",
            "Epoch 88/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0428 - accuracy: 0.9850\n",
            "Epoch 88: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0331 - accuracy: 0.9918 - val_loss: 1.0719 - val_accuracy: 0.7634\n",
            "Epoch 89/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9810\n",
            "Epoch 89: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.0492 - accuracy: 0.9810 - val_loss: 0.9884 - val_accuracy: 0.7419\n",
            "Epoch 90/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9864\n",
            "Epoch 90: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.0576 - accuracy: 0.9864 - val_loss: 0.9543 - val_accuracy: 0.7634\n",
            "Epoch 91/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 91: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0413 - accuracy: 0.9810 - val_loss: 0.9897 - val_accuracy: 0.7527\n",
            "Epoch 92/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0505 - accuracy: 0.9700\n",
            "Epoch 92: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0408 - accuracy: 0.9783 - val_loss: 1.0051 - val_accuracy: 0.7527\n",
            "Epoch 93/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0259 - accuracy: 0.9900\n",
            "Epoch 93: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0501 - accuracy: 0.9864 - val_loss: 0.9234 - val_accuracy: 0.7849\n",
            "Epoch 94/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9810\n",
            "Epoch 94: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 88ms/step - loss: 0.0450 - accuracy: 0.9810 - val_loss: 0.8608 - val_accuracy: 0.7849\n",
            "Epoch 95/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0420 - accuracy: 0.9800\n",
            "Epoch 95: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0486 - accuracy: 0.9783 - val_loss: 0.8116 - val_accuracy: 0.7849\n",
            "Epoch 96/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0241 - accuracy: 0.9900\n",
            "Epoch 96: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0502 - accuracy: 0.9810 - val_loss: 0.8661 - val_accuracy: 0.7312\n",
            "Epoch 97/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9755\n",
            "Epoch 97: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.0483 - accuracy: 0.9755 - val_loss: 0.9874 - val_accuracy: 0.7849\n",
            "Epoch 98/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0733 - accuracy: 0.9600\n",
            "Epoch 98: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0482 - accuracy: 0.9755 - val_loss: 0.9242 - val_accuracy: 0.7849\n",
            "Epoch 99/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0205 - accuracy: 1.0000\n",
            "Epoch 99: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0283 - accuracy: 0.9864 - val_loss: 1.0771 - val_accuracy: 0.7742\n",
            "Epoch 100/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0252 - accuracy: 0.9900\n",
            "Epoch 100: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 78ms/step - loss: 0.0350 - accuracy: 0.9810 - val_loss: 0.8927 - val_accuracy: 0.7849\n",
            "Epoch 101/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9864\n",
            "Epoch 101: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 129ms/step - loss: 0.0444 - accuracy: 0.9864 - val_loss: 1.1026 - val_accuracy: 0.7527\n",
            "Epoch 102/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9783\n",
            "Epoch 102: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 79ms/step - loss: 0.0415 - accuracy: 0.9783 - val_loss: 1.0699 - val_accuracy: 0.7849\n",
            "Epoch 103/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0125 - accuracy: 0.9950\n",
            "Epoch 103: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 83ms/step - loss: 0.0324 - accuracy: 0.9864 - val_loss: 1.1530 - val_accuracy: 0.7419\n",
            "Epoch 104/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9755\n",
            "Epoch 104: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 82ms/step - loss: 0.0436 - accuracy: 0.9755 - val_loss: 1.0608 - val_accuracy: 0.7527\n",
            "Epoch 105/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9810\n",
            "Epoch 105: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 133ms/step - loss: 0.0387 - accuracy: 0.9810 - val_loss: 1.1012 - val_accuracy: 0.7742\n",
            "Epoch 106/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9783\n",
            "Epoch 106: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 75ms/step - loss: 0.0437 - accuracy: 0.9783 - val_loss: 1.1095 - val_accuracy: 0.7742\n",
            "Epoch 107/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0171 - accuracy: 0.9900\n",
            "Epoch 107: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0376 - accuracy: 0.9864 - val_loss: 0.9591 - val_accuracy: 0.7527\n",
            "Epoch 108/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0445 - accuracy: 0.9750\n",
            "Epoch 108: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 78ms/step - loss: 0.0322 - accuracy: 0.9837 - val_loss: 1.0104 - val_accuracy: 0.7634\n",
            "Epoch 109/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0426 - accuracy: 0.9800\n",
            "Epoch 109: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0361 - accuracy: 0.9810 - val_loss: 1.0402 - val_accuracy: 0.7634\n",
            "Epoch 110/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0256 - accuracy: 0.9900\n",
            "Epoch 110: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 82ms/step - loss: 0.0418 - accuracy: 0.9837 - val_loss: 1.0397 - val_accuracy: 0.7849\n",
            "Epoch 111/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9864\n",
            "Epoch 111: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 83ms/step - loss: 0.0298 - accuracy: 0.9864 - val_loss: 1.1008 - val_accuracy: 0.7634\n",
            "Epoch 112/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0406 - accuracy: 0.9900\n",
            "Epoch 112: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 77ms/step - loss: 0.0349 - accuracy: 0.9918 - val_loss: 1.1009 - val_accuracy: 0.7957\n",
            "Epoch 113/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9837\n",
            "Epoch 113: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0319 - accuracy: 0.9837 - val_loss: 0.9444 - val_accuracy: 0.7849\n",
            "Epoch 114/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0286 - accuracy: 0.9950\n",
            "Epoch 114: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.0298 - accuracy: 0.9891 - val_loss: 1.0334 - val_accuracy: 0.7204\n",
            "Epoch 115/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0425 - accuracy: 0.9800\n",
            "Epoch 115: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0371 - accuracy: 0.9783 - val_loss: 1.0994 - val_accuracy: 0.7849\n",
            "Epoch 116/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0720 - accuracy: 0.9400\n",
            "Epoch 116: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0342 - accuracy: 0.9783 - val_loss: 1.0589 - val_accuracy: 0.7634\n",
            "Epoch 117/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9891\n",
            "Epoch 117: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0225 - accuracy: 0.9891 - val_loss: 1.2388 - val_accuracy: 0.7527\n",
            "Epoch 118/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9810\n",
            "Epoch 118: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0287 - accuracy: 0.9810 - val_loss: 1.1376 - val_accuracy: 0.7634\n",
            "Epoch 119/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0389 - accuracy: 0.9900\n",
            "Epoch 119: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0468 - accuracy: 0.9864 - val_loss: 0.9788 - val_accuracy: 0.7957\n",
            "Epoch 120/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9864\n",
            "Epoch 120: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.0439 - accuracy: 0.9864 - val_loss: 0.9955 - val_accuracy: 0.7742\n",
            "Epoch 121/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9783\n",
            "Epoch 121: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0316 - accuracy: 0.9783 - val_loss: 1.1181 - val_accuracy: 0.7527\n",
            "Epoch 122/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0583 - accuracy: 0.9700\n",
            "Epoch 122: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0395 - accuracy: 0.9783 - val_loss: 1.1572 - val_accuracy: 0.7634\n",
            "Epoch 123/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9810\n",
            "Epoch 123: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.0359 - accuracy: 0.9810 - val_loss: 0.9613 - val_accuracy: 0.7312\n",
            "Epoch 124/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9918\n",
            "Epoch 124: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0302 - accuracy: 0.9918 - val_loss: 1.1284 - val_accuracy: 0.7849\n",
            "Epoch 125/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9783\n",
            "Epoch 125: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0301 - accuracy: 0.9783 - val_loss: 1.1356 - val_accuracy: 0.7957\n",
            "Epoch 126/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0212 - accuracy: 0.9900\n",
            "Epoch 126: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0301 - accuracy: 0.9837 - val_loss: 0.9147 - val_accuracy: 0.7742\n",
            "Epoch 127/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0661 - accuracy: 0.9800\n",
            "Epoch 127: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0376 - accuracy: 0.9864 - val_loss: 0.9405 - val_accuracy: 0.7849\n",
            "Epoch 128/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
            "Epoch 128: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0269 - accuracy: 0.9864 - val_loss: 1.0745 - val_accuracy: 0.7419\n",
            "Epoch 129/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0263 - accuracy: 0.9800\n",
            "Epoch 129: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0305 - accuracy: 0.9783 - val_loss: 1.1459 - val_accuracy: 0.7527\n",
            "Epoch 130/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0263 - accuracy: 0.9833\n",
            "Epoch 130: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.0324 - accuracy: 0.9810 - val_loss: 0.9541 - val_accuracy: 0.7419\n",
            "Epoch 131/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0446 - accuracy: 0.9900\n",
            "Epoch 131: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0535 - accuracy: 0.9728 - val_loss: 1.0484 - val_accuracy: 0.7527\n",
            "Epoch 132/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9864\n",
            "Epoch 132: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 86ms/step - loss: 0.0304 - accuracy: 0.9864 - val_loss: 1.1974 - val_accuracy: 0.7742\n",
            "Epoch 133/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0867 - accuracy: 0.9700\n",
            "Epoch 133: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0440 - accuracy: 0.9837 - val_loss: 1.1810 - val_accuracy: 0.7527\n",
            "Epoch 134/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9891\n",
            "Epoch 134: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0348 - accuracy: 0.9891 - val_loss: 1.0826 - val_accuracy: 0.7634\n",
            "Epoch 135/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9891\n",
            "Epoch 135: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 91ms/step - loss: 0.0290 - accuracy: 0.9891 - val_loss: 1.1433 - val_accuracy: 0.7742\n",
            "Epoch 136/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 136: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0310 - accuracy: 0.9837 - val_loss: 1.2268 - val_accuracy: 0.7527\n",
            "Epoch 137/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 137: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0271 - accuracy: 0.9837 - val_loss: 1.0688 - val_accuracy: 0.7742\n",
            "Epoch 138/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0403 - accuracy: 0.9800\n",
            "Epoch 138: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0266 - accuracy: 0.9864 - val_loss: 1.0697 - val_accuracy: 0.7957\n",
            "Epoch 139/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9864\n",
            "Epoch 139: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 88ms/step - loss: 0.0298 - accuracy: 0.9864 - val_loss: 1.1632 - val_accuracy: 0.7634\n",
            "Epoch 140/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 140: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0370 - accuracy: 0.9837 - val_loss: 1.0154 - val_accuracy: 0.7742\n",
            "Epoch 141/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0275 - accuracy: 0.9800\n",
            "Epoch 141: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0327 - accuracy: 0.9864 - val_loss: 1.0786 - val_accuracy: 0.7849\n",
            "Epoch 142/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0166 - accuracy: 0.9900\n",
            "Epoch 142: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0304 - accuracy: 0.9837 - val_loss: 1.0416 - val_accuracy: 0.8172\n",
            "Epoch 143/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 143: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.0197 - accuracy: 0.9891 - val_loss: 1.0593 - val_accuracy: 0.8065\n",
            "Epoch 144/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0458 - accuracy: 0.9850\n",
            "Epoch 144: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0304 - accuracy: 0.9891 - val_loss: 1.1616 - val_accuracy: 0.7527\n",
            "Epoch 145/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0357 - accuracy: 0.9767\n",
            "Epoch 145: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0336 - accuracy: 0.9783 - val_loss: 0.9979 - val_accuracy: 0.8065\n",
            "Epoch 146/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0507 - accuracy: 0.9800\n",
            "Epoch 146: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0307 - accuracy: 0.9864 - val_loss: 1.1750 - val_accuracy: 0.7849\n",
            "Epoch 147/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9891\n",
            "Epoch 147: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.0286 - accuracy: 0.9891 - val_loss: 1.0405 - val_accuracy: 0.8065\n",
            "Epoch 148/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0439 - accuracy: 0.9700\n",
            "Epoch 148: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0313 - accuracy: 0.9810 - val_loss: 1.3987 - val_accuracy: 0.7742\n",
            "Epoch 149/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0253 - accuracy: 0.9900\n",
            "Epoch 149: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0458 - accuracy: 0.9864 - val_loss: 0.9299 - val_accuracy: 0.7957\n",
            "Epoch 150/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0520 - accuracy: 0.9800\n",
            "Epoch 150: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0328 - accuracy: 0.9810 - val_loss: 1.0344 - val_accuracy: 0.7957\n",
            "Epoch 151/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9837\n",
            "Epoch 151: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0313 - accuracy: 0.9837 - val_loss: 0.9864 - val_accuracy: 0.8065\n",
            "Epoch 152/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9864\n",
            "Epoch 152: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.0273 - accuracy: 0.9864 - val_loss: 1.2410 - val_accuracy: 0.7419\n",
            "Epoch 153/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0134 - accuracy: 0.9900\n",
            "Epoch 153: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0291 - accuracy: 0.9837 - val_loss: 1.0757 - val_accuracy: 0.7634\n",
            "Epoch 154/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9837\n",
            "Epoch 154: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.0274 - accuracy: 0.9837 - val_loss: 1.2501 - val_accuracy: 0.7634\n",
            "Epoch 155/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0204 - accuracy: 0.9900\n",
            "Epoch 155: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0401 - accuracy: 0.9810 - val_loss: 1.0344 - val_accuracy: 0.7742\n",
            "Epoch 156/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9891\n",
            "Epoch 156: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0270 - accuracy: 0.9891 - val_loss: 0.9901 - val_accuracy: 0.7849\n",
            "Epoch 157/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9837\n",
            "Epoch 157: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.0305 - accuracy: 0.9837 - val_loss: 1.0324 - val_accuracy: 0.7957\n",
            "Epoch 158/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9783\n",
            "Epoch 158: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.0346 - accuracy: 0.9783 - val_loss: 1.1658 - val_accuracy: 0.8065\n",
            "Epoch 159/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9783\n",
            "Epoch 159: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0315 - accuracy: 0.9783 - val_loss: 1.1284 - val_accuracy: 0.7742\n",
            "Epoch 160/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0247 - accuracy: 0.9900\n",
            "Epoch 160: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0288 - accuracy: 0.9864 - val_loss: 1.1393 - val_accuracy: 0.7742\n",
            "Epoch 161/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 161: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0313 - accuracy: 0.9864 - val_loss: 1.1899 - val_accuracy: 0.7527\n",
            "Epoch 162/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0353 - accuracy: 0.9800\n",
            "Epoch 162: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0286 - accuracy: 0.9837 - val_loss: 1.0601 - val_accuracy: 0.7957\n",
            "Epoch 163/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0236 - accuracy: 0.9800\n",
            "Epoch 163: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0272 - accuracy: 0.9837 - val_loss: 1.1337 - val_accuracy: 0.7742\n",
            "Epoch 164/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0204 - accuracy: 0.9900\n",
            "Epoch 164: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0341 - accuracy: 0.9837 - val_loss: 1.0525 - val_accuracy: 0.7742\n",
            "Epoch 165/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9837\n",
            "Epoch 165: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0279 - accuracy: 0.9837 - val_loss: 1.1328 - val_accuracy: 0.7849\n",
            "Epoch 166/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0149 - accuracy: 0.9900\n",
            "Epoch 166: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0326 - accuracy: 0.9837 - val_loss: 1.2057 - val_accuracy: 0.7957\n",
            "Epoch 167/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0338 - accuracy: 0.9800\n",
            "Epoch 167: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0242 - accuracy: 0.9837 - val_loss: 1.1489 - val_accuracy: 0.7742\n",
            "Epoch 168/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0259 - accuracy: 0.9800\n",
            "Epoch 168: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0228 - accuracy: 0.9864 - val_loss: 1.2295 - val_accuracy: 0.7634\n",
            "Epoch 169/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0126 - accuracy: 1.0000\n",
            "Epoch 169: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0233 - accuracy: 0.9864 - val_loss: 1.2325 - val_accuracy: 0.7849\n",
            "Epoch 170/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0436 - accuracy: 0.9900\n",
            "Epoch 170: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0290 - accuracy: 0.9864 - val_loss: 1.1997 - val_accuracy: 0.7957\n",
            "Epoch 171/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0278 - accuracy: 0.9800\n",
            "Epoch 171: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0285 - accuracy: 0.9837 - val_loss: 1.1757 - val_accuracy: 0.7957\n",
            "Epoch 172/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0135 - accuracy: 0.9900\n",
            "Epoch 172: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0222 - accuracy: 0.9891 - val_loss: 1.2663 - val_accuracy: 0.8065\n",
            "Epoch 173/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0272 - accuracy: 0.9900\n",
            "Epoch 173: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0322 - accuracy: 0.9864 - val_loss: 1.1521 - val_accuracy: 0.7742\n",
            "Epoch 174/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9864\n",
            "Epoch 174: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0241 - accuracy: 0.9864 - val_loss: 1.1826 - val_accuracy: 0.7634\n",
            "Epoch 175/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0188 - accuracy: 0.9950\n",
            "Epoch 175: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0324 - accuracy: 0.9864 - val_loss: 1.1115 - val_accuracy: 0.7957\n",
            "Epoch 176/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0266 - accuracy: 0.9800\n",
            "Epoch 176: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0263 - accuracy: 0.9810 - val_loss: 1.2119 - val_accuracy: 0.7849\n",
            "Epoch 177/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0153 - accuracy: 0.9900\n",
            "Epoch 177: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0273 - accuracy: 0.9810 - val_loss: 1.1679 - val_accuracy: 0.8172\n",
            "Epoch 178/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9891\n",
            "Epoch 178: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.0210 - accuracy: 0.9891 - val_loss: 1.1962 - val_accuracy: 0.8172\n",
            "Epoch 179/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0115 - accuracy: 0.9900\n",
            "Epoch 179: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0224 - accuracy: 0.9837 - val_loss: 1.2367 - val_accuracy: 0.7957\n",
            "Epoch 180/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0241 - accuracy: 0.9900\n",
            "Epoch 180: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0307 - accuracy: 0.9864 - val_loss: 1.1643 - val_accuracy: 0.7742\n",
            "Epoch 181/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0170 - accuracy: 0.9800\n",
            "Epoch 181: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0348 - accuracy: 0.9783 - val_loss: 1.1525 - val_accuracy: 0.7957\n",
            "Epoch 182/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9837\n",
            "Epoch 182: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0262 - accuracy: 0.9837 - val_loss: 1.1405 - val_accuracy: 0.7742\n",
            "Epoch 183/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0196 - accuracy: 1.0000\n",
            "Epoch 183: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0244 - accuracy: 0.9891 - val_loss: 1.2174 - val_accuracy: 0.8172\n",
            "Epoch 184/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0263 - accuracy: 0.9833\n",
            "Epoch 184: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 0.0253 - accuracy: 0.9864 - val_loss: 1.2287 - val_accuracy: 0.7634\n",
            "Epoch 185/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0193 - accuracy: 1.0000\n",
            "Epoch 185: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0237 - accuracy: 0.9864 - val_loss: 1.1325 - val_accuracy: 0.7742\n",
            "Epoch 186/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0456 - accuracy: 0.9800\n",
            "Epoch 186: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0223 - accuracy: 0.9946 - val_loss: 1.1631 - val_accuracy: 0.8172\n",
            "Epoch 187/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0155 - accuracy: 0.9900\n",
            "Epoch 187: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0239 - accuracy: 0.9810 - val_loss: 1.2765 - val_accuracy: 0.7634\n",
            "Epoch 188/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0280 - accuracy: 0.9800\n",
            "Epoch 188: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0217 - accuracy: 0.9837 - val_loss: 1.2177 - val_accuracy: 0.7742\n",
            "Epoch 189/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9783\n",
            "Epoch 189: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0256 - accuracy: 0.9783 - val_loss: 1.2073 - val_accuracy: 0.7957\n",
            "Epoch 190/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0233 - accuracy: 0.9800\n",
            "Epoch 190: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0303 - accuracy: 0.9837 - val_loss: 1.3485 - val_accuracy: 0.7849\n",
            "Epoch 191/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0415 - accuracy: 0.9700\n",
            "Epoch 191: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0257 - accuracy: 0.9810 - val_loss: 1.3509 - val_accuracy: 0.7742\n",
            "Epoch 192/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 192: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0295 - accuracy: 0.9837 - val_loss: 1.3993 - val_accuracy: 0.7634\n",
            "Epoch 193/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0254 - accuracy: 0.9800\n",
            "Epoch 193: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0239 - accuracy: 0.9837 - val_loss: 1.1984 - val_accuracy: 0.7634\n",
            "Epoch 194/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0461 - accuracy: 0.9700\n",
            "Epoch 194: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0222 - accuracy: 0.9891 - val_loss: 1.3225 - val_accuracy: 0.7742\n",
            "Epoch 195/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0134 - accuracy: 0.9900\n",
            "Epoch 195: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0229 - accuracy: 0.9864 - val_loss: 1.2810 - val_accuracy: 0.7527\n",
            "Epoch 196/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 196: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0255 - accuracy: 0.9783 - val_loss: 1.1998 - val_accuracy: 0.7849\n",
            "Epoch 197/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0212 - accuracy: 0.9800\n",
            "Epoch 197: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0319 - accuracy: 0.9755 - val_loss: 1.1999 - val_accuracy: 0.7527\n",
            "Epoch 198/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9810\n",
            "Epoch 198: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 81ms/step - loss: 0.0272 - accuracy: 0.9810 - val_loss: 1.1848 - val_accuracy: 0.7634\n",
            "Epoch 199/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9837\n",
            "Epoch 199: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 79ms/step - loss: 0.0288 - accuracy: 0.9837 - val_loss: 1.2968 - val_accuracy: 0.7742\n",
            "Epoch 200/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9810\n",
            "Epoch 200: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 135ms/step - loss: 0.0257 - accuracy: 0.9810 - val_loss: 1.3859 - val_accuracy: 0.7742\n",
            "Epoch 201/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0319 - accuracy: 0.9800\n",
            "Epoch 201: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0271 - accuracy: 0.9837 - val_loss: 1.4751 - val_accuracy: 0.7527\n",
            "Epoch 202/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0442 - accuracy: 0.9700\n",
            "Epoch 202: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0222 - accuracy: 0.9837 - val_loss: 1.5309 - val_accuracy: 0.7742\n",
            "Epoch 203/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0351 - accuracy: 0.9800\n",
            "Epoch 203: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0216 - accuracy: 0.9864 - val_loss: 1.4860 - val_accuracy: 0.7957\n",
            "Epoch 204/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9864\n",
            "Epoch 204: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 92ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 1.3346 - val_accuracy: 0.8065\n",
            "Epoch 205/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9810\n",
            "Epoch 205: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 84ms/step - loss: 0.0307 - accuracy: 0.9810 - val_loss: 1.4151 - val_accuracy: 0.7849\n",
            "Epoch 206/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0449 - accuracy: 0.9800\n",
            "Epoch 206: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0358 - accuracy: 0.9864 - val_loss: 1.1538 - val_accuracy: 0.7634\n",
            "Epoch 207/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0197 - accuracy: 0.9900\n",
            "Epoch 207: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0267 - accuracy: 0.9891 - val_loss: 1.3785 - val_accuracy: 0.7527\n",
            "Epoch 208/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0475 - accuracy: 0.9700\n",
            "Epoch 208: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0254 - accuracy: 0.9837 - val_loss: 1.3026 - val_accuracy: 0.7849\n",
            "Epoch 209/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0220 - accuracy: 0.9900\n",
            "Epoch 209: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0247 - accuracy: 0.9864 - val_loss: 1.4176 - val_accuracy: 0.7742\n",
            "Epoch 210/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0468 - accuracy: 0.9800\n",
            "Epoch 210: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0232 - accuracy: 0.9837 - val_loss: 1.4849 - val_accuracy: 0.7742\n",
            "Epoch 211/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0207 - accuracy: 0.9850\n",
            "Epoch 211: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 82ms/step - loss: 0.0246 - accuracy: 0.9837 - val_loss: 1.4544 - val_accuracy: 0.7742\n",
            "Epoch 212/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0157 - accuracy: 0.9900\n",
            "Epoch 212: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0204 - accuracy: 0.9837 - val_loss: 1.3490 - val_accuracy: 0.7742\n",
            "Epoch 213/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0145 - accuracy: 0.9900\n",
            "Epoch 213: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0214 - accuracy: 0.9864 - val_loss: 1.4997 - val_accuracy: 0.7527\n",
            "Epoch 214/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0230 - accuracy: 0.9800\n",
            "Epoch 214: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0227 - accuracy: 0.9837 - val_loss: 1.3985 - val_accuracy: 0.7742\n",
            "Epoch 215/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9891\n",
            "Epoch 215: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 89ms/step - loss: 0.0295 - accuracy: 0.9891 - val_loss: 1.4538 - val_accuracy: 0.7527\n",
            "Epoch 216/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0368 - accuracy: 0.9900\n",
            "Epoch 216: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0313 - accuracy: 0.9864 - val_loss: 1.3728 - val_accuracy: 0.7419\n",
            "Epoch 217/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9837\n",
            "Epoch 217: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0242 - accuracy: 0.9837 - val_loss: 1.4403 - val_accuracy: 0.7634\n",
            "Epoch 218/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9837\n",
            "Epoch 218: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0289 - accuracy: 0.9837 - val_loss: 1.3118 - val_accuracy: 0.7634\n",
            "Epoch 219/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 219: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0222 - accuracy: 0.9891 - val_loss: 1.4976 - val_accuracy: 0.7742\n",
            "Epoch 220/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0223 - accuracy: 0.9900\n",
            "Epoch 220: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.0239 - accuracy: 0.9891 - val_loss: 1.3184 - val_accuracy: 0.7849\n",
            "Epoch 221/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9864\n",
            "Epoch 221: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0235 - accuracy: 0.9864 - val_loss: 1.3633 - val_accuracy: 0.7849\n",
            "Epoch 222/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9891\n",
            "Epoch 222: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0210 - accuracy: 0.9891 - val_loss: 1.3624 - val_accuracy: 0.7527\n",
            "Epoch 223/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9891\n",
            "Epoch 223: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 63ms/step - loss: 0.0211 - accuracy: 0.9891 - val_loss: 1.6034 - val_accuracy: 0.7634\n",
            "Epoch 224/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0412 - accuracy: 0.9800\n",
            "Epoch 224: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0231 - accuracy: 0.9810 - val_loss: 1.5900 - val_accuracy: 0.7634\n",
            "Epoch 225/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0186 - accuracy: 0.9900\n",
            "Epoch 225: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0245 - accuracy: 0.9837 - val_loss: 1.3516 - val_accuracy: 0.7957\n",
            "Epoch 226/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 226: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0203 - accuracy: 0.9837 - val_loss: 1.5366 - val_accuracy: 0.7634\n",
            "Epoch 227/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9891\n",
            "Epoch 227: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 89ms/step - loss: 0.0216 - accuracy: 0.9891 - val_loss: 1.3402 - val_accuracy: 0.7634\n",
            "Epoch 228/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 3.8197e-04 - accuracy: 1.0000\n",
            "Epoch 228: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0250 - accuracy: 0.9891 - val_loss: 1.3875 - val_accuracy: 0.7527\n",
            "Epoch 229/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0369 - accuracy: 0.9700\n",
            "Epoch 229: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0227 - accuracy: 0.9783 - val_loss: 1.4980 - val_accuracy: 0.7742\n",
            "Epoch 230/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9864\n",
            "Epoch 230: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.0226 - accuracy: 0.9864 - val_loss: 1.3872 - val_accuracy: 0.7957\n",
            "Epoch 231/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0146 - accuracy: 1.0000\n",
            "Epoch 231: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0221 - accuracy: 0.9891 - val_loss: 1.3014 - val_accuracy: 0.7634\n",
            "Epoch 232/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0195 - accuracy: 0.9800\n",
            "Epoch 232: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0278 - accuracy: 0.9810 - val_loss: 1.4910 - val_accuracy: 0.7742\n",
            "Epoch 233/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0253 - accuracy: 0.9800\n",
            "Epoch 233: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0239 - accuracy: 0.9810 - val_loss: 1.4226 - val_accuracy: 0.7742\n",
            "Epoch 234/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 234: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0193 - accuracy: 0.9891 - val_loss: 1.4228 - val_accuracy: 0.7634\n",
            "Epoch 235/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 8.3191e-04 - accuracy: 1.0000\n",
            "Epoch 235: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0229 - accuracy: 0.9864 - val_loss: 1.3234 - val_accuracy: 0.7634\n",
            "Epoch 236/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 236: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0279 - accuracy: 0.9837 - val_loss: 1.4123 - val_accuracy: 0.7204\n",
            "Epoch 237/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0101 - accuracy: 0.9900\n",
            "Epoch 237: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0288 - accuracy: 0.9864 - val_loss: 1.5328 - val_accuracy: 0.7742\n",
            "Epoch 238/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9918\n",
            "Epoch 238: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.0225 - accuracy: 0.9918 - val_loss: 1.4410 - val_accuracy: 0.7849\n",
            "Epoch 239/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0347 - accuracy: 0.9700\n",
            "Epoch 239: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0237 - accuracy: 0.9810 - val_loss: 1.6734 - val_accuracy: 0.7634\n",
            "Epoch 240/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 240: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0235 - accuracy: 0.9891 - val_loss: 1.4119 - val_accuracy: 0.7849\n",
            "Epoch 241/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0298 - accuracy: 0.9800\n",
            "Epoch 241: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0254 - accuracy: 0.9837 - val_loss: 1.6071 - val_accuracy: 0.7204\n",
            "Epoch 242/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9837\n",
            "Epoch 242: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0252 - accuracy: 0.9837 - val_loss: 1.4651 - val_accuracy: 0.7957\n",
            "Epoch 243/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0218 - accuracy: 0.9900\n",
            "Epoch 243: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0252 - accuracy: 0.9891 - val_loss: 1.5311 - val_accuracy: 0.8065\n",
            "Epoch 244/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.9665e-04 - accuracy: 1.0000\n",
            "Epoch 244: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0253 - accuracy: 0.9810 - val_loss: 1.5553 - val_accuracy: 0.7742\n",
            "Epoch 245/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0244 - accuracy: 0.9900\n",
            "Epoch 245: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0270 - accuracy: 0.9837 - val_loss: 1.3715 - val_accuracy: 0.7957\n",
            "Epoch 246/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0268 - accuracy: 0.9900\n",
            "Epoch 246: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0235 - accuracy: 0.9837 - val_loss: 1.5438 - val_accuracy: 0.7957\n",
            "Epoch 247/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9864\n",
            "Epoch 247: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 90ms/step - loss: 0.0200 - accuracy: 0.9864 - val_loss: 1.6436 - val_accuracy: 0.7849\n",
            "Epoch 248/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0201 - accuracy: 0.9800\n",
            "Epoch 248: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0235 - accuracy: 0.9864 - val_loss: 1.4924 - val_accuracy: 0.7742\n",
            "Epoch 249/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9810\n",
            "Epoch 249: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0219 - accuracy: 0.9810 - val_loss: 1.5926 - val_accuracy: 0.7742\n",
            "Epoch 250/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 250: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0218 - accuracy: 0.9918 - val_loss: 1.7159 - val_accuracy: 0.7634\n",
            "Epoch 251/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 251: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0273 - accuracy: 0.9864 - val_loss: 1.4763 - val_accuracy: 0.8280\n",
            "Epoch 252/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0218 - accuracy: 0.9900\n",
            "Epoch 252: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0311 - accuracy: 0.9837 - val_loss: 1.4886 - val_accuracy: 0.7634\n",
            "Epoch 253/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0143 - accuracy: 0.9900\n",
            "Epoch 253: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0260 - accuracy: 0.9810 - val_loss: 1.2904 - val_accuracy: 0.7419\n",
            "Epoch 254/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0373 - accuracy: 0.9700\n",
            "Epoch 254: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0273 - accuracy: 0.9810 - val_loss: 1.3493 - val_accuracy: 0.7742\n",
            "Epoch 255/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0386 - accuracy: 0.9800\n",
            "Epoch 255: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0222 - accuracy: 0.9891 - val_loss: 1.5635 - val_accuracy: 0.7742\n",
            "Epoch 256/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0138 - accuracy: 0.9900\n",
            "Epoch 256: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0197 - accuracy: 0.9918 - val_loss: 1.6292 - val_accuracy: 0.7742\n",
            "Epoch 257/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0201 - accuracy: 0.9800\n",
            "Epoch 257: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0214 - accuracy: 0.9837 - val_loss: 1.5502 - val_accuracy: 0.8065\n",
            "Epoch 258/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9864\n",
            "Epoch 258: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0216 - accuracy: 0.9864 - val_loss: 1.9196 - val_accuracy: 0.7742\n",
            "Epoch 259/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0405 - accuracy: 0.9900\n",
            "Epoch 259: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0283 - accuracy: 0.9918 - val_loss: 1.5873 - val_accuracy: 0.7742\n",
            "Epoch 260/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0359 - accuracy: 0.9800\n",
            "Epoch 260: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0276 - accuracy: 0.9810 - val_loss: 1.6112 - val_accuracy: 0.7849\n",
            "Epoch 261/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9864\n",
            "Epoch 261: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0241 - accuracy: 0.9864 - val_loss: 1.6274 - val_accuracy: 0.7742\n",
            "Epoch 262/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0302 - accuracy: 0.9700\n",
            "Epoch 262: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0229 - accuracy: 0.9810 - val_loss: 1.4325 - val_accuracy: 0.7849\n",
            "Epoch 263/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0583 - accuracy: 0.9600\n",
            "Epoch 263: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0245 - accuracy: 0.9837 - val_loss: 1.4743 - val_accuracy: 0.7849\n",
            "Epoch 264/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 264: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0386 - accuracy: 0.9837 - val_loss: 1.3151 - val_accuracy: 0.8065\n",
            "Epoch 265/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0082 - accuracy: 0.9900\n",
            "Epoch 265: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0273 - accuracy: 0.9783 - val_loss: 1.4462 - val_accuracy: 0.7527\n",
            "Epoch 266/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 266: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0242 - accuracy: 0.9837 - val_loss: 1.4646 - val_accuracy: 0.7527\n",
            "Epoch 267/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9837\n",
            "Epoch 267: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0245 - accuracy: 0.9837 - val_loss: 1.4792 - val_accuracy: 0.7634\n",
            "Epoch 268/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9891\n",
            "Epoch 268: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.0225 - accuracy: 0.9891 - val_loss: 1.5149 - val_accuracy: 0.7527\n",
            "Epoch 269/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9864\n",
            "Epoch 269: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0245 - accuracy: 0.9864 - val_loss: 1.7611 - val_accuracy: 0.7634\n",
            "Epoch 270/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0167 - accuracy: 0.9900\n",
            "Epoch 270: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0191 - accuracy: 0.9864 - val_loss: 1.5927 - val_accuracy: 0.7527\n",
            "Epoch 271/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9918    \n",
            "Epoch 271: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0216 - accuracy: 0.9918 - val_loss: 1.7781 - val_accuracy: 0.7849\n",
            "Epoch 272/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0540 - accuracy: 0.9800\n",
            "Epoch 272: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0219 - accuracy: 0.9918 - val_loss: 1.7158 - val_accuracy: 0.7849\n",
            "Epoch 273/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9837\n",
            "Epoch 273: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 0.0280 - accuracy: 0.9837 - val_loss: 1.7085 - val_accuracy: 0.7742\n",
            "Epoch 274/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9837\n",
            "Epoch 274: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0263 - accuracy: 0.9837 - val_loss: 1.6577 - val_accuracy: 0.7742\n",
            "Epoch 275/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9864\n",
            "Epoch 275: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 0.0262 - accuracy: 0.9864 - val_loss: 1.3735 - val_accuracy: 0.7634\n",
            "Epoch 276/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 276: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0206 - accuracy: 0.9891 - val_loss: 1.6923 - val_accuracy: 0.7742\n",
            "Epoch 277/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0231 - accuracy: 0.9800\n",
            "Epoch 277: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0217 - accuracy: 0.9837 - val_loss: 1.8062 - val_accuracy: 0.7527\n",
            "Epoch 278/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0251 - accuracy: 0.9900\n",
            "Epoch 278: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0265 - accuracy: 0.9891 - val_loss: 1.4753 - val_accuracy: 0.7957\n",
            "Epoch 279/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 279: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0268 - accuracy: 0.9837 - val_loss: 1.4527 - val_accuracy: 0.7957\n",
            "Epoch 280/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9864\n",
            "Epoch 280: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.0242 - accuracy: 0.9864 - val_loss: 1.6175 - val_accuracy: 0.7742\n",
            "Epoch 281/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0294 - accuracy: 0.9900\n",
            "Epoch 281: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0245 - accuracy: 0.9891 - val_loss: 1.5113 - val_accuracy: 0.7742\n",
            "Epoch 282/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0086 - accuracy: 0.9900\n",
            "Epoch 282: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0221 - accuracy: 0.9864 - val_loss: 1.4668 - val_accuracy: 0.7742\n",
            "Epoch 283/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0289 - accuracy: 0.9900\n",
            "Epoch 283: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 1.3410 - val_accuracy: 0.7957\n",
            "Epoch 284/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0466 - accuracy: 0.9700\n",
            "Epoch 284: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0259 - accuracy: 0.9810 - val_loss: 1.4998 - val_accuracy: 0.7849\n",
            "Epoch 285/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 285: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 1.6780 - val_accuracy: 0.7742\n",
            "Epoch 286/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9891\n",
            "Epoch 286: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.0206 - accuracy: 0.9891 - val_loss: 1.8783 - val_accuracy: 0.7419\n",
            "Epoch 287/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0499 - accuracy: 0.9700\n",
            "Epoch 287: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0252 - accuracy: 0.9837 - val_loss: 1.4851 - val_accuracy: 0.7742\n",
            "Epoch 288/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0280 - accuracy: 0.9800\n",
            "Epoch 288: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0189 - accuracy: 0.9864 - val_loss: 1.5820 - val_accuracy: 0.7634\n",
            "Epoch 289/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0300 - accuracy: 0.9800\n",
            "Epoch 289: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0237 - accuracy: 0.9864 - val_loss: 1.6044 - val_accuracy: 0.7634\n",
            "Epoch 290/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0087 - accuracy: 0.9900\n",
            "Epoch 290: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0219 - accuracy: 0.9810 - val_loss: 1.4817 - val_accuracy: 0.7849\n",
            "Epoch 291/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0321 - accuracy: 0.9800\n",
            "Epoch 291: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 0.0280 - accuracy: 0.9837 - val_loss: 1.8298 - val_accuracy: 0.7634\n",
            "Epoch 292/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 292: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0198 - accuracy: 0.9918 - val_loss: 1.7565 - val_accuracy: 0.7527\n",
            "Epoch 293/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 293: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0277 - accuracy: 0.9864 - val_loss: 1.5903 - val_accuracy: 0.7742\n",
            "Epoch 294/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0206 - accuracy: 0.9900\n",
            "Epoch 294: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0214 - accuracy: 0.9891 - val_loss: 1.6273 - val_accuracy: 0.7849\n",
            "Epoch 295/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0221 - accuracy: 0.9900\n",
            "Epoch 295: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0217 - accuracy: 0.9837 - val_loss: 1.7524 - val_accuracy: 0.7634\n",
            "Epoch 296/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0151 - accuracy: 0.9900\n",
            "Epoch 296: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0357 - accuracy: 0.9837 - val_loss: 1.5721 - val_accuracy: 0.7634\n",
            "Epoch 297/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9810\n",
            "Epoch 297: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.0216 - accuracy: 0.9810 - val_loss: 1.6543 - val_accuracy: 0.7742\n",
            "Epoch 298/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0366 - accuracy: 0.9700\n",
            "Epoch 298: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0200 - accuracy: 0.9837 - val_loss: 1.6941 - val_accuracy: 0.7849\n",
            "Epoch 299/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0439 - accuracy: 0.9700\n",
            "Epoch 299: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0233 - accuracy: 0.9837 - val_loss: 1.5509 - val_accuracy: 0.7849\n",
            "Epoch 300/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 3.2348e-04 - accuracy: 1.0000\n",
            "Epoch 300: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0243 - accuracy: 0.9891 - val_loss: 1.6229 - val_accuracy: 0.7527\n",
            "Epoch 301/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0128 - accuracy: 0.9900\n",
            "Epoch 301: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0253 - accuracy: 0.9864 - val_loss: 1.6170 - val_accuracy: 0.7527\n",
            "Epoch 302/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0229 - accuracy: 0.9900\n",
            "Epoch 302: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0201 - accuracy: 0.9891 - val_loss: 1.6724 - val_accuracy: 0.7527\n",
            "Epoch 303/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9800\n",
            "Epoch 303: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0241 - accuracy: 0.9837 - val_loss: 1.7158 - val_accuracy: 0.7742\n",
            "Epoch 304/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0271 - accuracy: 0.9800\n",
            "Epoch 304: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0192 - accuracy: 0.9864 - val_loss: 1.6732 - val_accuracy: 0.7742\n",
            "Epoch 305/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 305: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0225 - accuracy: 0.9837 - val_loss: 1.5757 - val_accuracy: 0.7527\n",
            "Epoch 306/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 306: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0206 - accuracy: 0.9837 - val_loss: 1.6464 - val_accuracy: 0.7419\n",
            "Epoch 307/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0130 - accuracy: 1.0000\n",
            "Epoch 307: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0231 - accuracy: 0.9891 - val_loss: 1.5133 - val_accuracy: 0.7957\n",
            "Epoch 308/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0166 - accuracy: 0.9900\n",
            "Epoch 308: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0216 - accuracy: 0.9891 - val_loss: 1.8012 - val_accuracy: 0.7527\n",
            "Epoch 309/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1046 - accuracy: 0.9700\n",
            "Epoch 309: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0560 - accuracy: 0.9755 - val_loss: 1.5814 - val_accuracy: 0.7849\n",
            "Epoch 310/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9864\n",
            "Epoch 310: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.0185 - accuracy: 0.9864 - val_loss: 1.6018 - val_accuracy: 0.7957\n",
            "Epoch 311/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0205 - accuracy: 0.9800\n",
            "Epoch 311: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0240 - accuracy: 0.9810 - val_loss: 1.5913 - val_accuracy: 0.7634\n",
            "Epoch 312/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0247 - accuracy: 0.9800\n",
            "Epoch 312: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0211 - accuracy: 0.9864 - val_loss: 1.7761 - val_accuracy: 0.7634\n",
            "Epoch 313/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0142 - accuracy: 0.9900\n",
            "Epoch 313: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0211 - accuracy: 0.9837 - val_loss: 1.7663 - val_accuracy: 0.7634\n",
            "Epoch 314/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0143 - accuracy: 0.9900\n",
            "Epoch 314: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0219 - accuracy: 0.9864 - val_loss: 1.7539 - val_accuracy: 0.7742\n",
            "Epoch 315/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 315: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0211 - accuracy: 0.9810 - val_loss: 1.7435 - val_accuracy: 0.7742\n",
            "Epoch 316/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0179 - accuracy: 1.0000\n",
            "Epoch 316: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0242 - accuracy: 0.9864 - val_loss: 1.5657 - val_accuracy: 0.7742\n",
            "Epoch 317/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 317: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0236 - accuracy: 0.9837 - val_loss: 1.6033 - val_accuracy: 0.7634\n",
            "Epoch 318/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0262 - accuracy: 0.9900\n",
            "Epoch 318: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0227 - accuracy: 0.9864 - val_loss: 1.4888 - val_accuracy: 0.7957\n",
            "Epoch 319/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0109 - accuracy: 0.9900\n",
            "Epoch 319: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0199 - accuracy: 0.9864 - val_loss: 1.4501 - val_accuracy: 0.7849\n",
            "Epoch 320/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9810\n",
            "Epoch 320: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 86ms/step - loss: 0.0230 - accuracy: 0.9810 - val_loss: 1.5810 - val_accuracy: 0.7742\n",
            "Epoch 321/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0265 - accuracy: 0.9900\n",
            "Epoch 321: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0213 - accuracy: 0.9864 - val_loss: 1.7261 - val_accuracy: 0.7634\n",
            "Epoch 322/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9783\n",
            "Epoch 322: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 90ms/step - loss: 0.0258 - accuracy: 0.9783 - val_loss: 1.6582 - val_accuracy: 0.7742\n",
            "Epoch 323/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0195 - accuracy: 0.9900\n",
            "Epoch 323: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0214 - accuracy: 0.9864 - val_loss: 1.7918 - val_accuracy: 0.7742\n",
            "Epoch 324/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 324: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0228 - accuracy: 0.9918 - val_loss: 1.6315 - val_accuracy: 0.7742\n",
            "Epoch 325/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0152 - accuracy: 0.9900\n",
            "Epoch 325: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0233 - accuracy: 0.9837 - val_loss: 1.4802 - val_accuracy: 0.7742\n",
            "Epoch 326/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0140 - accuracy: 0.9900\n",
            "Epoch 326: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0194 - accuracy: 0.9864 - val_loss: 1.5731 - val_accuracy: 0.7742\n",
            "Epoch 327/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9864\n",
            "Epoch 327: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 83ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 1.7681 - val_accuracy: 0.7634\n",
            "Epoch 328/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 328: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0205 - accuracy: 0.9891 - val_loss: 1.5377 - val_accuracy: 0.7419\n",
            "Epoch 329/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0158 - accuracy: 0.9900\n",
            "Epoch 329: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0282 - accuracy: 0.9864 - val_loss: 1.7470 - val_accuracy: 0.7742\n",
            "Epoch 330/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0151 - accuracy: 0.9900\n",
            "Epoch 330: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0220 - accuracy: 0.9918 - val_loss: 1.6477 - val_accuracy: 0.7849\n",
            "Epoch 331/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0225 - accuracy: 0.9800\n",
            "Epoch 331: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0204 - accuracy: 0.9837 - val_loss: 1.7825 - val_accuracy: 0.7742\n",
            "Epoch 332/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0234 - accuracy: 0.9800\n",
            "Epoch 332: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0228 - accuracy: 0.9837 - val_loss: 1.6251 - val_accuracy: 0.7742\n",
            "Epoch 333/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0270 - accuracy: 0.9800\n",
            "Epoch 333: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0195 - accuracy: 0.9864 - val_loss: 1.6108 - val_accuracy: 0.7742\n",
            "Epoch 334/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9864\n",
            "Epoch 334: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 84ms/step - loss: 0.0252 - accuracy: 0.9864 - val_loss: 1.9607 - val_accuracy: 0.7849\n",
            "Epoch 335/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9864\n",
            "Epoch 335: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 79ms/step - loss: 0.0239 - accuracy: 0.9864 - val_loss: 1.6861 - val_accuracy: 0.7742\n",
            "Epoch 336/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 6.4538e-04 - accuracy: 1.0000\n",
            "Epoch 336: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0222 - accuracy: 0.9864 - val_loss: 1.7184 - val_accuracy: 0.7634\n",
            "Epoch 337/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0587 - accuracy: 0.9500\n",
            "Epoch 337: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0220 - accuracy: 0.9837 - val_loss: 1.7275 - val_accuracy: 0.7742\n",
            "Epoch 338/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0259 - accuracy: 0.9900\n",
            "Epoch 338: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0174 - accuracy: 0.9891 - val_loss: 1.8570 - val_accuracy: 0.7849\n",
            "Epoch 339/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0344 - accuracy: 0.9800\n",
            "Epoch 339: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0202 - accuracy: 0.9918 - val_loss: 1.7582 - val_accuracy: 0.7527\n",
            "Epoch 340/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 340: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0200 - accuracy: 0.9891 - val_loss: 1.7039 - val_accuracy: 0.7849\n",
            "Epoch 341/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0352 - accuracy: 0.9700\n",
            "Epoch 341: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 1.7378 - val_accuracy: 0.7957\n",
            "Epoch 342/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0158 - accuracy: 0.9900\n",
            "Epoch 342: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0238 - accuracy: 0.9864 - val_loss: 1.8087 - val_accuracy: 0.7742\n",
            "Epoch 343/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0129 - accuracy: 0.9900\n",
            "Epoch 343: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0331 - accuracy: 0.9837 - val_loss: 1.7747 - val_accuracy: 0.7634\n",
            "Epoch 344/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0232 - accuracy: 0.9800\n",
            "Epoch 344: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0226 - accuracy: 0.9810 - val_loss: 1.6937 - val_accuracy: 0.7527\n",
            "Epoch 345/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0245 - accuracy: 0.9800\n",
            "Epoch 345: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0189 - accuracy: 0.9891 - val_loss: 1.7842 - val_accuracy: 0.7742\n",
            "Epoch 346/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0140 - accuracy: 0.9900\n",
            "Epoch 346: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 63ms/step - loss: 0.0229 - accuracy: 0.9837 - val_loss: 1.7009 - val_accuracy: 0.7527\n",
            "Epoch 347/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 347: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0204 - accuracy: 0.9891 - val_loss: 1.7424 - val_accuracy: 0.7527\n",
            "Epoch 348/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 348: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0222 - accuracy: 0.9864 - val_loss: 1.7086 - val_accuracy: 0.7634\n",
            "Epoch 349/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 349: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0246 - accuracy: 0.9864 - val_loss: 1.6048 - val_accuracy: 0.7634\n",
            "Epoch 350/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9837\n",
            "Epoch 350: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 0.0220 - accuracy: 0.9837 - val_loss: 1.7534 - val_accuracy: 0.7742\n",
            "Epoch 351/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0330 - accuracy: 0.9800\n",
            "Epoch 351: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0244 - accuracy: 0.9864 - val_loss: 1.5910 - val_accuracy: 0.7527\n",
            "Epoch 352/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0222 - accuracy: 0.9900\n",
            "Epoch 352: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0227 - accuracy: 0.9864 - val_loss: 1.7397 - val_accuracy: 0.7742\n",
            "Epoch 353/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0214 - accuracy: 0.9800\n",
            "Epoch 353: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0208 - accuracy: 0.9864 - val_loss: 1.8828 - val_accuracy: 0.7527\n",
            "Epoch 354/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 354: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0331 - accuracy: 0.9891 - val_loss: 1.4364 - val_accuracy: 0.7634\n",
            "Epoch 355/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0241 - accuracy: 0.9900\n",
            "Epoch 355: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0218 - accuracy: 0.9837 - val_loss: 1.4808 - val_accuracy: 0.7742\n",
            "Epoch 356/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 356: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0232 - accuracy: 0.9864 - val_loss: 1.6395 - val_accuracy: 0.7527\n",
            "Epoch 357/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0097 - accuracy: 0.9900\n",
            "Epoch 357: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0245 - accuracy: 0.9837 - val_loss: 1.5558 - val_accuracy: 0.7527\n",
            "Epoch 358/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0400 - accuracy: 0.9800\n",
            "Epoch 358: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0280 - accuracy: 0.9810 - val_loss: 1.5898 - val_accuracy: 0.7419\n",
            "Epoch 359/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 359: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0191 - accuracy: 0.9891 - val_loss: 1.5367 - val_accuracy: 0.7527\n",
            "Epoch 360/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0086 - accuracy: 0.9900\n",
            "Epoch 360: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0228 - accuracy: 0.9810 - val_loss: 1.5878 - val_accuracy: 0.7527\n",
            "Epoch 361/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 361: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0217 - accuracy: 0.9891 - val_loss: 1.5109 - val_accuracy: 0.7312\n",
            "Epoch 362/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0399 - accuracy: 0.9700\n",
            "Epoch 362: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0212 - accuracy: 0.9837 - val_loss: 1.5224 - val_accuracy: 0.7312\n",
            "Epoch 363/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0162 - accuracy: 0.9900\n",
            "Epoch 363: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0214 - accuracy: 0.9864 - val_loss: 1.5209 - val_accuracy: 0.7419\n",
            "Epoch 364/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0206 - accuracy: 0.9900\n",
            "Epoch 364: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0201 - accuracy: 0.9837 - val_loss: 1.5304 - val_accuracy: 0.7634\n",
            "Epoch 365/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0077 - accuracy: 0.9900\n",
            "Epoch 365: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0178 - accuracy: 0.9891 - val_loss: 1.6987 - val_accuracy: 0.7634\n",
            "Epoch 366/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0317 - accuracy: 0.9900\n",
            "Epoch 366: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0230 - accuracy: 0.9891 - val_loss: 1.4517 - val_accuracy: 0.7742\n",
            "Epoch 367/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 367: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0186 - accuracy: 0.9891 - val_loss: 1.3977 - val_accuracy: 0.7634\n",
            "Epoch 368/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0328 - accuracy: 0.9800\n",
            "Epoch 368: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0435 - accuracy: 0.9837 - val_loss: 1.8803 - val_accuracy: 0.7204\n",
            "Epoch 369/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0094 - accuracy: 0.9900\n",
            "Epoch 369: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0198 - accuracy: 0.9918 - val_loss: 2.0798 - val_accuracy: 0.7312\n",
            "Epoch 370/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0284 - accuracy: 0.9800\n",
            "Epoch 370: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0247 - accuracy: 0.9891 - val_loss: 1.6995 - val_accuracy: 0.7419\n",
            "Epoch 371/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 371: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0187 - accuracy: 0.9891 - val_loss: 1.8300 - val_accuracy: 0.7527\n",
            "Epoch 372/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0154 - accuracy: 1.0000\n",
            "Epoch 372: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0206 - accuracy: 0.9891 - val_loss: 1.7551 - val_accuracy: 0.7634\n",
            "Epoch 373/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0112 - accuracy: 0.9900\n",
            "Epoch 373: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0220 - accuracy: 0.9837 - val_loss: 1.7008 - val_accuracy: 0.7634\n",
            "Epoch 374/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 374: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0216 - accuracy: 0.9864 - val_loss: 1.7670 - val_accuracy: 0.7634\n",
            "Epoch 375/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0306 - accuracy: 0.9900\n",
            "Epoch 375: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0258 - accuracy: 0.9891 - val_loss: 1.6755 - val_accuracy: 0.7742\n",
            "Epoch 376/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0127 - accuracy: 1.0000\n",
            "Epoch 376: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0199 - accuracy: 0.9918 - val_loss: 1.6927 - val_accuracy: 0.7634\n",
            "Epoch 377/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0242 - accuracy: 0.9800\n",
            "Epoch 377: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0230 - accuracy: 0.9864 - val_loss: 1.6524 - val_accuracy: 0.7742\n",
            "Epoch 378/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 378: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0220 - accuracy: 0.9891 - val_loss: 1.6485 - val_accuracy: 0.7634\n",
            "Epoch 379/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0120 - accuracy: 0.9900\n",
            "Epoch 379: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0218 - accuracy: 0.9837 - val_loss: 1.6964 - val_accuracy: 0.7742\n",
            "Epoch 380/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0352 - accuracy: 0.9800\n",
            "Epoch 380: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0196 - accuracy: 0.9918 - val_loss: 1.7325 - val_accuracy: 0.7634\n",
            "Epoch 381/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0218 - accuracy: 1.0000\n",
            "Epoch 381: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0201 - accuracy: 0.9891 - val_loss: 1.7570 - val_accuracy: 0.7634\n",
            "Epoch 382/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0201 - accuracy: 0.9800\n",
            "Epoch 382: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.0220 - accuracy: 0.9837 - val_loss: 1.6714 - val_accuracy: 0.7634\n",
            "Epoch 383/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0340 - accuracy: 0.9700\n",
            "Epoch 383: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0205 - accuracy: 0.9837 - val_loss: 1.8830 - val_accuracy: 0.7634\n",
            "Epoch 384/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0166 - accuracy: 0.9900\n",
            "Epoch 384: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0198 - accuracy: 0.9864 - val_loss: 1.6960 - val_accuracy: 0.7419\n",
            "Epoch 385/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0075 - accuracy: 0.9900\n",
            "Epoch 385: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0229 - accuracy: 0.9891 - val_loss: 1.8610 - val_accuracy: 0.7527\n",
            "Epoch 386/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 8.5395e-04 - accuracy: 1.0000\n",
            "Epoch 386: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0206 - accuracy: 0.9891 - val_loss: 1.9480 - val_accuracy: 0.7419\n",
            "Epoch 387/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0209 - accuracy: 1.0000\n",
            "Epoch 387: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0198 - accuracy: 0.9891 - val_loss: 1.8437 - val_accuracy: 0.7419\n",
            "Epoch 388/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0260 - accuracy: 0.9900\n",
            "Epoch 388: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0178 - accuracy: 0.9891 - val_loss: 1.7680 - val_accuracy: 0.7634\n",
            "Epoch 389/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0154 - accuracy: 1.0000\n",
            "Epoch 389: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0204 - accuracy: 0.9891 - val_loss: 1.8291 - val_accuracy: 0.7634\n",
            "Epoch 390/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0248 - accuracy: 0.9800\n",
            "Epoch 390: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0257 - accuracy: 0.9837 - val_loss: 1.7407 - val_accuracy: 0.7634\n",
            "Epoch 391/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9891\n",
            "Epoch 391: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.0194 - accuracy: 0.9891 - val_loss: 1.6184 - val_accuracy: 0.7634\n",
            "Epoch 392/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0254 - accuracy: 0.9800\n",
            "Epoch 392: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0201 - accuracy: 0.9864 - val_loss: 1.7300 - val_accuracy: 0.7527\n",
            "Epoch 393/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 393: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0252 - accuracy: 0.9810 - val_loss: 1.6447 - val_accuracy: 0.7634\n",
            "Epoch 394/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0277 - accuracy: 0.9800\n",
            "Epoch 394: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0228 - accuracy: 0.9864 - val_loss: 1.6925 - val_accuracy: 0.7742\n",
            "Epoch 395/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9700\n",
            "Epoch 395: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0213 - accuracy: 0.9837 - val_loss: 1.8801 - val_accuracy: 0.7634\n",
            "Epoch 396/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0304 - accuracy: 0.9800\n",
            "Epoch 396: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0208 - accuracy: 0.9864 - val_loss: 1.7144 - val_accuracy: 0.7742\n",
            "Epoch 397/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0300 - accuracy: 0.9800\n",
            "Epoch 397: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0181 - accuracy: 0.9891 - val_loss: 1.7807 - val_accuracy: 0.7527\n",
            "Epoch 398/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0259 - accuracy: 0.9900\n",
            "Epoch 398: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0230 - accuracy: 0.9864 - val_loss: 1.5774 - val_accuracy: 0.7527\n",
            "Epoch 399/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0219 - accuracy: 0.9900\n",
            "Epoch 399: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0195 - accuracy: 0.9918 - val_loss: 1.9092 - val_accuracy: 0.7634\n",
            "Epoch 400/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0144 - accuracy: 0.9900\n",
            "Epoch 400: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0244 - accuracy: 0.9864 - val_loss: 1.8510 - val_accuracy: 0.7527\n",
            "Epoch 401/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9900\n",
            "Epoch 401: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0231 - accuracy: 0.9891 - val_loss: 1.6008 - val_accuracy: 0.7849\n",
            "Epoch 402/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0206 - accuracy: 0.9900\n",
            "Epoch 402: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0220 - accuracy: 0.9810 - val_loss: 1.6813 - val_accuracy: 0.7849\n",
            "Epoch 403/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0355 - accuracy: 0.9800\n",
            "Epoch 403: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0256 - accuracy: 0.9891 - val_loss: 1.6420 - val_accuracy: 0.7849\n",
            "Epoch 404/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0246 - accuracy: 0.9800\n",
            "Epoch 404: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0242 - accuracy: 0.9837 - val_loss: 1.4542 - val_accuracy: 0.7742\n",
            "Epoch 405/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0300 - accuracy: 0.9900\n",
            "Epoch 405: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0211 - accuracy: 0.9864 - val_loss: 1.8534 - val_accuracy: 0.7634\n",
            "Epoch 406/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0223 - accuracy: 0.9900\n",
            "Epoch 406: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0230 - accuracy: 0.9837 - val_loss: 1.6013 - val_accuracy: 0.7742\n",
            "Epoch 407/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0132 - accuracy: 1.0000\n",
            "Epoch 407: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0221 - accuracy: 0.9864 - val_loss: 1.8655 - val_accuracy: 0.7419\n",
            "Epoch 408/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 408: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0189 - accuracy: 0.9864 - val_loss: 1.8114 - val_accuracy: 0.7527\n",
            "Epoch 409/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0145 - accuracy: 0.9900\n",
            "Epoch 409: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0187 - accuracy: 0.9891 - val_loss: 2.0961 - val_accuracy: 0.7634\n",
            "Epoch 410/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0146 - accuracy: 1.0000\n",
            "Epoch 410: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0215 - accuracy: 0.9891 - val_loss: 1.9482 - val_accuracy: 0.7634\n",
            "Epoch 411/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0218 - accuracy: 0.9900\n",
            "Epoch 411: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0223 - accuracy: 0.9864 - val_loss: 1.8053 - val_accuracy: 0.7527\n",
            "Epoch 412/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0164 - accuracy: 0.9900\n",
            "Epoch 412: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0264 - accuracy: 0.9891 - val_loss: 1.7650 - val_accuracy: 0.7527\n",
            "Epoch 413/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0167 - accuracy: 0.9900\n",
            "Epoch 413: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0232 - accuracy: 0.9864 - val_loss: 1.7523 - val_accuracy: 0.7419\n",
            "Epoch 414/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 414: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0217 - accuracy: 0.9837 - val_loss: 1.7502 - val_accuracy: 0.7527\n",
            "Epoch 415/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0108 - accuracy: 0.9900\n",
            "Epoch 415: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0241 - accuracy: 0.9837 - val_loss: 1.5894 - val_accuracy: 0.7634\n",
            "Epoch 416/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 416: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0195 - accuracy: 0.9918 - val_loss: 1.6801 - val_accuracy: 0.7634\n",
            "Epoch 417/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0290 - accuracy: 0.9800\n",
            "Epoch 417: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0234 - accuracy: 0.9864 - val_loss: 1.6445 - val_accuracy: 0.7634\n",
            "Epoch 418/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0208 - accuracy: 0.9800\n",
            "Epoch 418: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0217 - accuracy: 0.9810 - val_loss: 1.6160 - val_accuracy: 0.7527\n",
            "Epoch 419/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0336 - accuracy: 0.9800\n",
            "Epoch 419: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0188 - accuracy: 0.9891 - val_loss: 1.8951 - val_accuracy: 0.7527\n",
            "Epoch 420/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0133 - accuracy: 0.9900\n",
            "Epoch 420: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0255 - accuracy: 0.9837 - val_loss: 1.6602 - val_accuracy: 0.7849\n",
            "Epoch 421/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 421: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0178 - accuracy: 0.9918 - val_loss: 1.8637 - val_accuracy: 0.7634\n",
            "Epoch 422/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0297 - accuracy: 0.9800\n",
            "Epoch 422: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0205 - accuracy: 0.9891 - val_loss: 1.7887 - val_accuracy: 0.7634\n",
            "Epoch 423/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0422 - accuracy: 0.9800\n",
            "Epoch 423: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0223 - accuracy: 0.9864 - val_loss: 1.7706 - val_accuracy: 0.7634\n",
            "Epoch 424/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9918\n",
            "Epoch 424: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 91ms/step - loss: 0.0242 - accuracy: 0.9918 - val_loss: 1.7976 - val_accuracy: 0.7634\n",
            "Epoch 425/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0127 - accuracy: 0.9900\n",
            "Epoch 425: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0214 - accuracy: 0.9864 - val_loss: 1.5875 - val_accuracy: 0.7634\n",
            "Epoch 426/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0213 - accuracy: 0.9900\n",
            "Epoch 426: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0226 - accuracy: 0.9864 - val_loss: 1.7077 - val_accuracy: 0.7527\n",
            "Epoch 427/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 427: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0211 - accuracy: 0.9891 - val_loss: 1.7698 - val_accuracy: 0.7849\n",
            "Epoch 428/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 428: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0244 - accuracy: 0.9864 - val_loss: 1.8070 - val_accuracy: 0.7742\n",
            "Epoch 429/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0104 - accuracy: 0.9900\n",
            "Epoch 429: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0205 - accuracy: 0.9810 - val_loss: 1.6051 - val_accuracy: 0.7634\n",
            "Epoch 430/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0190 - accuracy: 1.0000\n",
            "Epoch 430: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0236 - accuracy: 0.9864 - val_loss: 1.8188 - val_accuracy: 0.7312\n",
            "Epoch 431/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 431: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0230 - accuracy: 0.9891 - val_loss: 1.5996 - val_accuracy: 0.7527\n",
            "Epoch 432/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0355 - accuracy: 0.9800\n",
            "Epoch 432: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0214 - accuracy: 0.9864 - val_loss: 1.8045 - val_accuracy: 0.7527\n",
            "Epoch 433/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0159 - accuracy: 0.9900\n",
            "Epoch 433: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0208 - accuracy: 0.9837 - val_loss: 1.7703 - val_accuracy: 0.7419\n",
            "Epoch 434/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0354 - accuracy: 0.9700\n",
            "Epoch 434: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0211 - accuracy: 0.9864 - val_loss: 1.7392 - val_accuracy: 0.7527\n",
            "Epoch 435/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0425 - accuracy: 0.9800\n",
            "Epoch 435: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0175 - accuracy: 0.9918 - val_loss: 1.7718 - val_accuracy: 0.7634\n",
            "Epoch 436/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0144 - accuracy: 1.0000\n",
            "Epoch 436: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0213 - accuracy: 0.9918 - val_loss: 2.1508 - val_accuracy: 0.7634\n",
            "Epoch 437/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0249 - accuracy: 0.9900\n",
            "Epoch 437: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0255 - accuracy: 0.9864 - val_loss: 1.6417 - val_accuracy: 0.7312\n",
            "Epoch 438/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9946\n",
            "Epoch 438: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 0.0209 - accuracy: 0.9946 - val_loss: 1.7704 - val_accuracy: 0.7419\n",
            "Epoch 439/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9891\n",
            "Epoch 439: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 0.0231 - accuracy: 0.9891 - val_loss: 1.7881 - val_accuracy: 0.7527\n",
            "Epoch 440/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0196 - accuracy: 0.9900\n",
            "Epoch 440: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 0.0197 - accuracy: 0.9891 - val_loss: 1.7774 - val_accuracy: 0.7419\n",
            "Epoch 441/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 441: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0229 - accuracy: 0.9837 - val_loss: 1.7226 - val_accuracy: 0.7419\n",
            "Epoch 442/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0077 - accuracy: 0.9900\n",
            "Epoch 442: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0225 - accuracy: 0.9810 - val_loss: 1.6527 - val_accuracy: 0.7419\n",
            "Epoch 443/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9918\n",
            "Epoch 443: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.0273 - accuracy: 0.9918 - val_loss: 1.6054 - val_accuracy: 0.7634\n",
            "Epoch 444/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0086 - accuracy: 0.9900\n",
            "Epoch 444: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0232 - accuracy: 0.9864 - val_loss: 1.6359 - val_accuracy: 0.7419\n",
            "Epoch 445/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0302 - accuracy: 0.9800\n",
            "Epoch 445: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0224 - accuracy: 0.9864 - val_loss: 1.7611 - val_accuracy: 0.7527\n",
            "Epoch 446/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0217 - accuracy: 0.9900\n",
            "Epoch 446: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0233 - accuracy: 0.9810 - val_loss: 1.6941 - val_accuracy: 0.7527\n",
            "Epoch 447/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9891\n",
            "Epoch 447: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 0.0200 - accuracy: 0.9891 - val_loss: 1.8724 - val_accuracy: 0.7419\n",
            "Epoch 448/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0226 - accuracy: 0.9900\n",
            "Epoch 448: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0223 - accuracy: 0.9891 - val_loss: 2.0758 - val_accuracy: 0.7527\n",
            "Epoch 449/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0162 - accuracy: 1.0000\n",
            "Epoch 449: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0204 - accuracy: 0.9891 - val_loss: 1.8399 - val_accuracy: 0.7742\n",
            "Epoch 450/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 450: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0190 - accuracy: 0.9864 - val_loss: 1.7549 - val_accuracy: 0.7742\n",
            "Epoch 451/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0224 - accuracy: 0.9900\n",
            "Epoch 451: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0220 - accuracy: 0.9864 - val_loss: 1.7095 - val_accuracy: 0.7742\n",
            "Epoch 452/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 452: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0186 - accuracy: 0.9891 - val_loss: 1.8254 - val_accuracy: 0.7742\n",
            "Epoch 453/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0074 - accuracy: 0.9900\n",
            "Epoch 453: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0263 - accuracy: 0.9891 - val_loss: 1.8195 - val_accuracy: 0.7742\n",
            "Epoch 454/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 454: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0193 - accuracy: 0.9864 - val_loss: 1.8098 - val_accuracy: 0.7634\n",
            "Epoch 455/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 4.8134e-05 - accuracy: 1.0000\n",
            "Epoch 455: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0215 - accuracy: 0.9864 - val_loss: 1.7838 - val_accuracy: 0.7634\n",
            "Epoch 456/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0275 - accuracy: 0.9800\n",
            "Epoch 456: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0230 - accuracy: 0.9810 - val_loss: 1.6995 - val_accuracy: 0.7527\n",
            "Epoch 457/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0453 - accuracy: 0.9600\n",
            "Epoch 457: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0193 - accuracy: 0.9864 - val_loss: 1.7690 - val_accuracy: 0.7527\n",
            "Epoch 458/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9800\n",
            "Epoch 458: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0226 - accuracy: 0.9891 - val_loss: 1.7000 - val_accuracy: 0.7634\n",
            "Epoch 459/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0342 - accuracy: 0.9700\n",
            "Epoch 459: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.0205 - accuracy: 0.9837 - val_loss: 1.7895 - val_accuracy: 0.7634\n",
            "Epoch 460/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0283 - accuracy: 0.9800\n",
            "Epoch 460: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.0250 - accuracy: 0.9837 - val_loss: 1.7168 - val_accuracy: 0.7957\n",
            "Epoch 461/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 461: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.0216 - accuracy: 0.9864 - val_loss: 1.6451 - val_accuracy: 0.7742\n",
            "Epoch 462/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0287 - accuracy: 0.9800\n",
            "Epoch 462: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0211 - accuracy: 0.9864 - val_loss: 1.7370 - val_accuracy: 0.7527\n",
            "Epoch 463/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9800\n",
            "Epoch 463: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0183 - accuracy: 0.9891 - val_loss: 1.7967 - val_accuracy: 0.7634\n",
            "Epoch 464/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0137 - accuracy: 0.9900\n",
            "Epoch 464: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0211 - accuracy: 0.9891 - val_loss: 1.7939 - val_accuracy: 0.7742\n",
            "Epoch 465/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0429 - accuracy: 0.9700\n",
            "Epoch 465: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0190 - accuracy: 0.9864 - val_loss: 2.1371 - val_accuracy: 0.7527\n",
            "Epoch 466/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0180 - accuracy: 0.9900\n",
            "Epoch 466: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0371 - accuracy: 0.9864 - val_loss: 1.6643 - val_accuracy: 0.7742\n",
            "Epoch 467/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0161 - accuracy: 0.9800\n",
            "Epoch 467: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0207 - accuracy: 0.9864 - val_loss: 1.7044 - val_accuracy: 0.7849\n",
            "Epoch 468/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0077 - accuracy: 0.9900\n",
            "Epoch 468: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0205 - accuracy: 0.9864 - val_loss: 1.6699 - val_accuracy: 0.7742\n",
            "Epoch 469/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0154 - accuracy: 0.9800\n",
            "Epoch 469: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0239 - accuracy: 0.9837 - val_loss: 1.6366 - val_accuracy: 0.7742\n",
            "Epoch 470/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0179 - accuracy: 0.9900\n",
            "Epoch 470: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0226 - accuracy: 0.9864 - val_loss: 1.6377 - val_accuracy: 0.7849\n",
            "Epoch 471/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0212 - accuracy: 0.9900\n",
            "Epoch 471: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0217 - accuracy: 0.9891 - val_loss: 1.6887 - val_accuracy: 0.7742\n",
            "Epoch 472/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0240 - accuracy: 0.9800\n",
            "Epoch 472: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0208 - accuracy: 0.9891 - val_loss: 1.7305 - val_accuracy: 0.7527\n",
            "Epoch 473/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0310 - accuracy: 0.9800\n",
            "Epoch 473: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0208 - accuracy: 0.9891 - val_loss: 1.6853 - val_accuracy: 0.7742\n",
            "Epoch 474/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0248 - accuracy: 0.9900\n",
            "Epoch 474: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0209 - accuracy: 0.9918 - val_loss: 1.7747 - val_accuracy: 0.7742\n",
            "Epoch 475/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 475: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0226 - accuracy: 0.9837 - val_loss: 1.6163 - val_accuracy: 0.7742\n",
            "Epoch 476/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 476: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0191 - accuracy: 0.9891 - val_loss: 1.7133 - val_accuracy: 0.7742\n",
            "Epoch 477/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0194 - accuracy: 1.0000\n",
            "Epoch 477: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0199 - accuracy: 0.9891 - val_loss: 1.6399 - val_accuracy: 0.7634\n",
            "Epoch 478/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0147 - accuracy: 1.0000\n",
            "Epoch 478: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0189 - accuracy: 0.9918 - val_loss: 1.7318 - val_accuracy: 0.7849\n",
            "Epoch 479/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0165 - accuracy: 0.9800\n",
            "Epoch 479: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0227 - accuracy: 0.9837 - val_loss: 1.5407 - val_accuracy: 0.7634\n",
            "Epoch 480/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0311 - accuracy: 0.9800\n",
            "Epoch 480: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0205 - accuracy: 0.9891 - val_loss: 1.6661 - val_accuracy: 0.7957\n",
            "Epoch 481/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0207 - accuracy: 0.9850\n",
            "Epoch 481: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 95ms/step - loss: 0.0202 - accuracy: 0.9891 - val_loss: 1.6228 - val_accuracy: 0.7742\n",
            "Epoch 482/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0336 - accuracy: 0.9800\n",
            "Epoch 482: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0234 - accuracy: 0.9864 - val_loss: 1.7626 - val_accuracy: 0.7742\n",
            "Epoch 483/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0150 - accuracy: 0.9967\n",
            "Epoch 483: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 83ms/step - loss: 0.0183 - accuracy: 0.9918 - val_loss: 1.8337 - val_accuracy: 0.7527\n",
            "Epoch 484/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0255 - accuracy: 0.9900\n",
            "Epoch 484: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0228 - accuracy: 0.9864 - val_loss: 1.8151 - val_accuracy: 0.7527\n",
            "Epoch 485/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0341 - accuracy: 0.9700\n",
            "Epoch 485: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0210 - accuracy: 0.9837 - val_loss: 1.8390 - val_accuracy: 0.7527\n",
            "Epoch 486/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9864\n",
            "Epoch 486: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 84ms/step - loss: 0.0221 - accuracy: 0.9864 - val_loss: 1.7160 - val_accuracy: 0.7634\n",
            "Epoch 487/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0081 - accuracy: 0.9900\n",
            "Epoch 487: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0218 - accuracy: 0.9837 - val_loss: 1.6620 - val_accuracy: 0.7634\n",
            "Epoch 488/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0233 - accuracy: 0.9800\n",
            "Epoch 488: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0194 - accuracy: 0.9864 - val_loss: 1.6187 - val_accuracy: 0.7634\n",
            "Epoch 489/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 489: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0213 - accuracy: 0.9891 - val_loss: 1.9247 - val_accuracy: 0.7419\n",
            "Epoch 490/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 490: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0239 - accuracy: 0.9864 - val_loss: 2.3355 - val_accuracy: 0.7419\n",
            "Epoch 491/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0182 - accuracy: 0.9900\n",
            "Epoch 491: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0392 - accuracy: 0.9783 - val_loss: 1.9971 - val_accuracy: 0.7419\n",
            "Epoch 492/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9864\n",
            "Epoch 492: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 91ms/step - loss: 0.0189 - accuracy: 0.9864 - val_loss: 2.0475 - val_accuracy: 0.7527\n",
            "Epoch 493/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0285 - accuracy: 0.9800\n",
            "Epoch 493: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0182 - accuracy: 0.9891 - val_loss: 2.0489 - val_accuracy: 0.7419\n",
            "Epoch 494/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0166 - accuracy: 0.9900\n",
            "Epoch 494: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0213 - accuracy: 0.9891 - val_loss: 1.9283 - val_accuracy: 0.7419\n",
            "Epoch 495/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0173 - accuracy: 0.9900\n",
            "Epoch 495: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.0206 - accuracy: 0.9891 - val_loss: 1.9479 - val_accuracy: 0.7204\n",
            "Epoch 496/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0349 - accuracy: 0.9800\n",
            "Epoch 496: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0215 - accuracy: 0.9837 - val_loss: 1.9476 - val_accuracy: 0.7204\n",
            "Epoch 497/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 497: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0198 - accuracy: 0.9837 - val_loss: 1.9549 - val_accuracy: 0.7527\n",
            "Epoch 498/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 498: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0238 - accuracy: 0.9864 - val_loss: 2.0174 - val_accuracy: 0.7419\n",
            "Epoch 499/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 499: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0203 - accuracy: 0.9918 - val_loss: 1.8384 - val_accuracy: 0.7312\n",
            "Epoch 500/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0179 - accuracy: 0.9900\n",
            "Epoch 500: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 1.8703 - val_accuracy: 0.7312\n",
            "Epoch 501/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0311 - accuracy: 0.9800\n",
            "Epoch 501: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0179 - accuracy: 0.9946 - val_loss: 1.9924 - val_accuracy: 0.7419\n",
            "Epoch 502/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0189 - accuracy: 0.9900\n",
            "Epoch 502: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0200 - accuracy: 0.9864 - val_loss: 1.9194 - val_accuracy: 0.7204\n",
            "Epoch 503/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0218 - accuracy: 0.9800\n",
            "Epoch 503: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0189 - accuracy: 0.9891 - val_loss: 1.8615 - val_accuracy: 0.7419\n",
            "Epoch 504/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 4.9436e-05 - accuracy: 1.0000\n",
            "Epoch 504: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0197 - accuracy: 0.9946 - val_loss: 2.1019 - val_accuracy: 0.7312\n",
            "Epoch 505/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0111 - accuracy: 1.0000\n",
            "Epoch 505: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0195 - accuracy: 0.9891 - val_loss: 1.8551 - val_accuracy: 0.7634\n",
            "Epoch 506/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 506: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0209 - accuracy: 0.9864 - val_loss: 1.7323 - val_accuracy: 0.7742\n",
            "Epoch 507/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0277 - accuracy: 0.9800\n",
            "Epoch 507: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0216 - accuracy: 0.9864 - val_loss: 1.9552 - val_accuracy: 0.7742\n",
            "Epoch 508/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0136 - accuracy: 0.9900\n",
            "Epoch 508: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0193 - accuracy: 0.9891 - val_loss: 1.9770 - val_accuracy: 0.7742\n",
            "Epoch 509/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 509: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0209 - accuracy: 0.9891 - val_loss: 2.0810 - val_accuracy: 0.7634\n",
            "Epoch 510/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 510: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0229 - accuracy: 0.9891 - val_loss: 1.7378 - val_accuracy: 0.7419\n",
            "Epoch 511/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0191 - accuracy: 1.0000\n",
            "Epoch 511: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0178 - accuracy: 0.9918 - val_loss: 1.6175 - val_accuracy: 0.7634\n",
            "Epoch 512/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0259 - accuracy: 0.9800\n",
            "Epoch 512: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0204 - accuracy: 0.9891 - val_loss: 1.5021 - val_accuracy: 0.7634\n",
            "Epoch 513/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0244 - accuracy: 0.9900\n",
            "Epoch 513: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0230 - accuracy: 0.9864 - val_loss: 1.9503 - val_accuracy: 0.7634\n",
            "Epoch 514/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0136 - accuracy: 0.9900\n",
            "Epoch 514: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0240 - accuracy: 0.9864 - val_loss: 1.8804 - val_accuracy: 0.7527\n",
            "Epoch 515/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0389 - accuracy: 0.9800\n",
            "Epoch 515: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0200 - accuracy: 0.9891 - val_loss: 1.8993 - val_accuracy: 0.7419\n",
            "Epoch 516/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 516: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0238 - accuracy: 0.9864 - val_loss: 1.5594 - val_accuracy: 0.7419\n",
            "Epoch 517/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0230 - accuracy: 0.9800\n",
            "Epoch 517: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0235 - accuracy: 0.9864 - val_loss: 1.6628 - val_accuracy: 0.7742\n",
            "Epoch 518/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9891\n",
            "Epoch 518: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 0.0209 - accuracy: 0.9891 - val_loss: 1.6173 - val_accuracy: 0.7849\n",
            "Epoch 519/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0164 - accuracy: 0.9900\n",
            "Epoch 519: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0196 - accuracy: 0.9918 - val_loss: 1.8343 - val_accuracy: 0.7849\n",
            "Epoch 520/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 520: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0208 - accuracy: 0.9891 - val_loss: 1.6963 - val_accuracy: 0.7527\n",
            "Epoch 521/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9700\n",
            "Epoch 521: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0189 - accuracy: 0.9891 - val_loss: 1.8814 - val_accuracy: 0.7957\n",
            "Epoch 522/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0239 - accuracy: 0.9800\n",
            "Epoch 522: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0220 - accuracy: 0.9837 - val_loss: 1.7520 - val_accuracy: 0.7634\n",
            "Epoch 523/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0331 - accuracy: 0.9800\n",
            "Epoch 523: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0228 - accuracy: 0.9864 - val_loss: 1.7191 - val_accuracy: 0.7742\n",
            "Epoch 524/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0246 - accuracy: 0.9800\n",
            "Epoch 524: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0226 - accuracy: 0.9837 - val_loss: 1.7064 - val_accuracy: 0.8065\n",
            "Epoch 525/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0126 - accuracy: 0.9900\n",
            "Epoch 525: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0195 - accuracy: 0.9891 - val_loss: 1.6728 - val_accuracy: 0.7849\n",
            "Epoch 526/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0391 - accuracy: 0.9700\n",
            "Epoch 526: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0211 - accuracy: 0.9891 - val_loss: 1.6005 - val_accuracy: 0.7957\n",
            "Epoch 527/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 527: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0168 - accuracy: 0.9918 - val_loss: 1.9088 - val_accuracy: 0.7527\n",
            "Epoch 528/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0152 - accuracy: 0.9900\n",
            "Epoch 528: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0256 - accuracy: 0.9891 - val_loss: 1.7340 - val_accuracy: 0.7634\n",
            "Epoch 529/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0362 - accuracy: 0.9800\n",
            "Epoch 529: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0211 - accuracy: 0.9864 - val_loss: 1.7946 - val_accuracy: 0.7849\n",
            "Epoch 530/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0120 - accuracy: 0.9900\n",
            "Epoch 530: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0223 - accuracy: 0.9837 - val_loss: 1.8964 - val_accuracy: 0.7634\n",
            "Epoch 531/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0290 - accuracy: 0.9800\n",
            "Epoch 531: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0224 - accuracy: 0.9837 - val_loss: 1.8671 - val_accuracy: 0.7742\n",
            "Epoch 532/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 3.9495e-05 - accuracy: 1.0000\n",
            "Epoch 532: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0227 - accuracy: 0.9891 - val_loss: 1.8957 - val_accuracy: 0.7419\n",
            "Epoch 533/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 533: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0196 - accuracy: 0.9891 - val_loss: 1.8760 - val_accuracy: 0.7634\n",
            "Epoch 534/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0333 - accuracy: 0.9900\n",
            "Epoch 534: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0223 - accuracy: 0.9891 - val_loss: 1.8876 - val_accuracy: 0.7312\n",
            "Epoch 535/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0174 - accuracy: 1.0000\n",
            "Epoch 535: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0232 - accuracy: 0.9864 - val_loss: 1.9686 - val_accuracy: 0.7312\n",
            "Epoch 536/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0464 - accuracy: 0.9900\n",
            "Epoch 536: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0334 - accuracy: 0.9946 - val_loss: 2.3621 - val_accuracy: 0.7419\n",
            "Epoch 537/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0278 - accuracy: 0.9800\n",
            "Epoch 537: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0249 - accuracy: 0.9891 - val_loss: 2.0954 - val_accuracy: 0.7849\n",
            "Epoch 538/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.9476e-04 - accuracy: 1.0000\n",
            "Epoch 538: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0214 - accuracy: 0.9864 - val_loss: 2.1712 - val_accuracy: 0.7527\n",
            "Epoch 539/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0302 - accuracy: 0.9800\n",
            "Epoch 539: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0201 - accuracy: 0.9864 - val_loss: 2.1748 - val_accuracy: 0.7312\n",
            "Epoch 540/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 540: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0192 - accuracy: 0.9891 - val_loss: 2.0946 - val_accuracy: 0.7419\n",
            "Epoch 541/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0475 - accuracy: 0.9800\n",
            "Epoch 541: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0221 - accuracy: 0.9864 - val_loss: 2.1264 - val_accuracy: 0.7312\n",
            "Epoch 542/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 542: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0221 - accuracy: 0.9864 - val_loss: 1.9509 - val_accuracy: 0.7419\n",
            "Epoch 543/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0219 - accuracy: 0.9800\n",
            "Epoch 543: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0190 - accuracy: 0.9891 - val_loss: 2.1208 - val_accuracy: 0.7527\n",
            "Epoch 544/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0218 - accuracy: 0.9900\n",
            "Epoch 544: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0201 - accuracy: 0.9891 - val_loss: 2.0975 - val_accuracy: 0.7419\n",
            "Epoch 545/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0148 - accuracy: 0.9900\n",
            "Epoch 545: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0224 - accuracy: 0.9837 - val_loss: 2.1761 - val_accuracy: 0.7634\n",
            "Epoch 546/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0233 - accuracy: 0.9800\n",
            "Epoch 546: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0231 - accuracy: 0.9918 - val_loss: 1.8584 - val_accuracy: 0.7527\n",
            "Epoch 547/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0240 - accuracy: 0.9800\n",
            "Epoch 547: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0206 - accuracy: 0.9864 - val_loss: 1.8464 - val_accuracy: 0.7527\n",
            "Epoch 548/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0218 - accuracy: 0.9900\n",
            "Epoch 548: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0195 - accuracy: 0.9918 - val_loss: 1.8499 - val_accuracy: 0.7849\n",
            "Epoch 549/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9837\n",
            "Epoch 549: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 99ms/step - loss: 0.0245 - accuracy: 0.9837 - val_loss: 1.8007 - val_accuracy: 0.7419\n",
            "Epoch 550/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0477 - accuracy: 0.9700\n",
            "Epoch 550: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0233 - accuracy: 0.9891 - val_loss: 1.7877 - val_accuracy: 0.7849\n",
            "Epoch 551/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0117 - accuracy: 0.9900\n",
            "Epoch 551: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0214 - accuracy: 0.9864 - val_loss: 1.8273 - val_accuracy: 0.7742\n",
            "Epoch 552/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0122 - accuracy: 0.9900\n",
            "Epoch 552: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0207 - accuracy: 0.9810 - val_loss: 1.8020 - val_accuracy: 0.7742\n",
            "Epoch 553/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 553: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0201 - accuracy: 0.9891 - val_loss: 1.8800 - val_accuracy: 0.7527\n",
            "Epoch 554/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0331 - accuracy: 0.9800\n",
            "Epoch 554: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0239 - accuracy: 0.9864 - val_loss: 1.7378 - val_accuracy: 0.7742\n",
            "Epoch 555/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0173 - accuracy: 0.9900\n",
            "Epoch 555: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0201 - accuracy: 0.9918 - val_loss: 1.7821 - val_accuracy: 0.7849\n",
            "Epoch 556/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9891\n",
            "Epoch 556: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.0208 - accuracy: 0.9891 - val_loss: 1.7843 - val_accuracy: 0.7957\n",
            "Epoch 557/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0303 - accuracy: 0.9700\n",
            "Epoch 557: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0197 - accuracy: 0.9837 - val_loss: 1.8449 - val_accuracy: 0.7634\n",
            "Epoch 558/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0652 - accuracy: 0.9700\n",
            "Epoch 558: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0277 - accuracy: 0.9837 - val_loss: 1.5259 - val_accuracy: 0.7957\n",
            "Epoch 559/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0242 - accuracy: 0.9800\n",
            "Epoch 559: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0213 - accuracy: 0.9864 - val_loss: 1.5612 - val_accuracy: 0.7634\n",
            "Epoch 560/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.1911e-04 - accuracy: 1.0000\n",
            "Epoch 560: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0198 - accuracy: 0.9891 - val_loss: 1.6591 - val_accuracy: 0.7527\n",
            "Epoch 561/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0183 - accuracy: 0.9900\n",
            "Epoch 561: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0331 - accuracy: 0.9864 - val_loss: 1.8214 - val_accuracy: 0.7634\n",
            "Epoch 562/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0194 - accuracy: 0.9900\n",
            "Epoch 562: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0197 - accuracy: 0.9918 - val_loss: 1.7501 - val_accuracy: 0.7849\n",
            "Epoch 563/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0121 - accuracy: 0.9900\n",
            "Epoch 563: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0228 - accuracy: 0.9864 - val_loss: 1.5780 - val_accuracy: 0.7634\n",
            "Epoch 564/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0295 - accuracy: 0.9800\n",
            "Epoch 564: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0196 - accuracy: 0.9864 - val_loss: 1.5826 - val_accuracy: 0.7849\n",
            "Epoch 565/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0277 - accuracy: 0.9800\n",
            "Epoch 565: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0187 - accuracy: 0.9891 - val_loss: 1.7188 - val_accuracy: 0.7634\n",
            "Epoch 566/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0202 - accuracy: 0.9800\n",
            "Epoch 566: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0203 - accuracy: 0.9864 - val_loss: 1.8061 - val_accuracy: 0.7634\n",
            "Epoch 567/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0172 - accuracy: 0.9900\n",
            "Epoch 567: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0183 - accuracy: 0.9891 - val_loss: 1.6952 - val_accuracy: 0.7527\n",
            "Epoch 568/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0219 - accuracy: 0.9900\n",
            "Epoch 568: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0261 - accuracy: 0.9837 - val_loss: 1.7501 - val_accuracy: 0.7419\n",
            "Epoch 569/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0100 - accuracy: 0.9900\n",
            "Epoch 569: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0185 - accuracy: 0.9891 - val_loss: 1.7818 - val_accuracy: 0.7527\n",
            "Epoch 570/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0172 - accuracy: 0.9900\n",
            "Epoch 570: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0229 - accuracy: 0.9837 - val_loss: 1.7629 - val_accuracy: 0.7419\n",
            "Epoch 571/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0176 - accuracy: 0.9800\n",
            "Epoch 571: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0215 - accuracy: 0.9837 - val_loss: 1.7919 - val_accuracy: 0.7419\n",
            "Epoch 572/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0091 - accuracy: 0.9900\n",
            "Epoch 572: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0200 - accuracy: 0.9837 - val_loss: 1.9031 - val_accuracy: 0.7634\n",
            "Epoch 573/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0141 - accuracy: 1.0000\n",
            "Epoch 573: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0201 - accuracy: 0.9918 - val_loss: 1.8328 - val_accuracy: 0.7634\n",
            "Epoch 574/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0232 - accuracy: 0.9900\n",
            "Epoch 574: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0196 - accuracy: 0.9891 - val_loss: 1.8565 - val_accuracy: 0.7527\n",
            "Epoch 575/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0074 - accuracy: 0.9900\n",
            "Epoch 575: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0206 - accuracy: 0.9837 - val_loss: 1.8335 - val_accuracy: 0.7634\n",
            "Epoch 576/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 576: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0206 - accuracy: 0.9864 - val_loss: 1.9700 - val_accuracy: 0.7527\n",
            "Epoch 577/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 577: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0178 - accuracy: 0.9918 - val_loss: 1.9019 - val_accuracy: 0.7634\n",
            "Epoch 578/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0217 - accuracy: 0.9900\n",
            "Epoch 578: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0209 - accuracy: 0.9891 - val_loss: 1.7744 - val_accuracy: 0.7527\n",
            "Epoch 579/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 579: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0299 - accuracy: 0.9918 - val_loss: 1.5999 - val_accuracy: 0.7527\n",
            "Epoch 580/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0317 - accuracy: 0.9800\n",
            "Epoch 580: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0205 - accuracy: 0.9891 - val_loss: 1.8327 - val_accuracy: 0.7849\n",
            "Epoch 581/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0212 - accuracy: 0.9900\n",
            "Epoch 581: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0187 - accuracy: 0.9864 - val_loss: 1.8759 - val_accuracy: 0.7634\n",
            "Epoch 582/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 582: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0214 - accuracy: 0.9918 - val_loss: 2.1613 - val_accuracy: 0.7634\n",
            "Epoch 583/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 583: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0426 - accuracy: 0.9810 - val_loss: 1.7314 - val_accuracy: 0.7419\n",
            "Epoch 584/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0447 - accuracy: 0.9700\n",
            "Epoch 584: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0235 - accuracy: 0.9891 - val_loss: 1.7612 - val_accuracy: 0.7527\n",
            "Epoch 585/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0150 - accuracy: 0.9900\n",
            "Epoch 585: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0191 - accuracy: 0.9891 - val_loss: 1.7912 - val_accuracy: 0.7634\n",
            "Epoch 586/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 586: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0187 - accuracy: 0.9864 - val_loss: 1.8004 - val_accuracy: 0.7742\n",
            "Epoch 587/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0219 - accuracy: 0.9850\n",
            "Epoch 587: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.0237 - accuracy: 0.9837 - val_loss: 1.8454 - val_accuracy: 0.7742\n",
            "Epoch 588/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9800\n",
            "Epoch 588: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0190 - accuracy: 0.9918 - val_loss: 1.9487 - val_accuracy: 0.7742\n",
            "Epoch 589/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0124 - accuracy: 0.9900\n",
            "Epoch 589: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0225 - accuracy: 0.9864 - val_loss: 1.8411 - val_accuracy: 0.7634\n",
            "Epoch 590/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0388 - accuracy: 0.9800\n",
            "Epoch 590: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0218 - accuracy: 0.9864 - val_loss: 1.7846 - val_accuracy: 0.7634\n",
            "Epoch 591/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0370 - accuracy: 0.9800\n",
            "Epoch 591: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0202 - accuracy: 0.9837 - val_loss: 1.7513 - val_accuracy: 0.7742\n",
            "Epoch 592/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 592: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0197 - accuracy: 0.9891 - val_loss: 1.8353 - val_accuracy: 0.7742\n",
            "Epoch 593/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0194 - accuracy: 0.9800\n",
            "Epoch 593: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0225 - accuracy: 0.9783 - val_loss: 1.7106 - val_accuracy: 0.7742\n",
            "Epoch 594/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0119 - accuracy: 0.9900\n",
            "Epoch 594: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0205 - accuracy: 0.9837 - val_loss: 1.7007 - val_accuracy: 0.7849\n",
            "Epoch 595/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0071 - accuracy: 0.9900\n",
            "Epoch 595: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0195 - accuracy: 0.9918 - val_loss: 1.5919 - val_accuracy: 0.7742\n",
            "Epoch 596/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 596: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0195 - accuracy: 0.9918 - val_loss: 1.9281 - val_accuracy: 0.7527\n",
            "Epoch 597/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0253 - accuracy: 0.9900\n",
            "Epoch 597: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0188 - accuracy: 0.9918 - val_loss: 1.9840 - val_accuracy: 0.7634\n",
            "Epoch 598/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0267 - accuracy: 0.9900\n",
            "Epoch 598: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0232 - accuracy: 0.9864 - val_loss: 1.8412 - val_accuracy: 0.7742\n",
            "Epoch 599/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 599: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0196 - accuracy: 0.9891 - val_loss: 1.8705 - val_accuracy: 0.7742\n",
            "Epoch 600/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0138 - accuracy: 1.0000\n",
            "Epoch 600: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0203 - accuracy: 0.9837 - val_loss: 1.8733 - val_accuracy: 0.7742\n",
            "Epoch 601/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9891    \n",
            "Epoch 601: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 0.0190 - accuracy: 0.9891 - val_loss: 1.8473 - val_accuracy: 0.7742\n",
            "Epoch 602/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 602: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0206 - accuracy: 0.9918 - val_loss: 1.8261 - val_accuracy: 0.7527\n",
            "Epoch 603/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0202 - accuracy: 0.9900\n",
            "Epoch 603: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0208 - accuracy: 0.9837 - val_loss: 1.8262 - val_accuracy: 0.7634\n",
            "Epoch 604/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0172 - accuracy: 0.9900\n",
            "Epoch 604: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0216 - accuracy: 0.9891 - val_loss: 1.9506 - val_accuracy: 0.7849\n",
            "Epoch 605/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 605: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0212 - accuracy: 0.9891 - val_loss: 2.1131 - val_accuracy: 0.7634\n",
            "Epoch 606/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 606: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0220 - accuracy: 0.9864 - val_loss: 1.8857 - val_accuracy: 0.7634\n",
            "Epoch 607/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0096 - accuracy: 0.9900\n",
            "Epoch 607: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0221 - accuracy: 0.9837 - val_loss: 1.9358 - val_accuracy: 0.7742\n",
            "Epoch 608/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0285 - accuracy: 0.9900\n",
            "Epoch 608: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0247 - accuracy: 0.9837 - val_loss: 1.9001 - val_accuracy: 0.7742\n",
            "Epoch 609/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 609: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0196 - accuracy: 0.9891 - val_loss: 1.9701 - val_accuracy: 0.7634\n",
            "Epoch 610/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0175 - accuracy: 0.9900\n",
            "Epoch 610: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0273 - accuracy: 0.9891 - val_loss: 1.8821 - val_accuracy: 0.7742\n",
            "Epoch 611/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 611: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0224 - accuracy: 0.9891 - val_loss: 1.9135 - val_accuracy: 0.7742\n",
            "Epoch 612/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0224 - accuracy: 0.9900\n",
            "Epoch 612: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0206 - accuracy: 0.9891 - val_loss: 1.9823 - val_accuracy: 0.7849\n",
            "Epoch 613/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0204 - accuracy: 0.9900\n",
            "Epoch 613: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0200 - accuracy: 0.9864 - val_loss: 1.8865 - val_accuracy: 0.7742\n",
            "Epoch 614/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0153 - accuracy: 0.9900\n",
            "Epoch 614: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0244 - accuracy: 0.9891 - val_loss: 1.6943 - val_accuracy: 0.7634\n",
            "Epoch 615/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0423 - accuracy: 0.9500\n",
            "Epoch 615: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0202 - accuracy: 0.9837 - val_loss: 1.8838 - val_accuracy: 0.7634\n",
            "Epoch 616/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0165 - accuracy: 1.0000\n",
            "Epoch 616: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0200 - accuracy: 0.9891 - val_loss: 2.1206 - val_accuracy: 0.7957\n",
            "Epoch 617/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 4.7806e-05 - accuracy: 1.0000\n",
            "Epoch 617: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0191 - accuracy: 0.9946 - val_loss: 2.1807 - val_accuracy: 0.7849\n",
            "Epoch 618/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 618: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0289 - accuracy: 0.9918 - val_loss: 1.9309 - val_accuracy: 0.7742\n",
            "Epoch 619/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9837\n",
            "Epoch 619: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 63ms/step - loss: 0.0209 - accuracy: 0.9837 - val_loss: 2.1130 - val_accuracy: 0.7957\n",
            "Epoch 620/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 620: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0252 - accuracy: 0.9891 - val_loss: 1.7193 - val_accuracy: 0.7634\n",
            "Epoch 621/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0233 - accuracy: 0.9900\n",
            "Epoch 621: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0198 - accuracy: 0.9891 - val_loss: 1.7945 - val_accuracy: 0.7742\n",
            "Epoch 622/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0329 - accuracy: 0.9800\n",
            "Epoch 622: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.0209 - accuracy: 0.9891 - val_loss: 1.8187 - val_accuracy: 0.7634\n",
            "Epoch 623/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0307 - accuracy: 0.9800\n",
            "Epoch 623: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0183 - accuracy: 0.9891 - val_loss: 1.8329 - val_accuracy: 0.7742\n",
            "Epoch 624/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 624: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 92ms/step - loss: 0.0197 - accuracy: 0.9918 - val_loss: 1.9109 - val_accuracy: 0.7634\n",
            "Epoch 625/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 625: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0195 - accuracy: 0.9891 - val_loss: 2.0169 - val_accuracy: 0.7634\n",
            "Epoch 626/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0157 - accuracy: 0.9900\n",
            "Epoch 626: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.0237 - accuracy: 0.9864 - val_loss: 1.7533 - val_accuracy: 0.7742\n",
            "Epoch 627/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0339 - accuracy: 0.9800\n",
            "Epoch 627: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0209 - accuracy: 0.9837 - val_loss: 1.8422 - val_accuracy: 0.7849\n",
            "Epoch 628/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0161 - accuracy: 1.0000\n",
            "Epoch 628: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.0197 - accuracy: 0.9864 - val_loss: 2.0558 - val_accuracy: 0.7634\n",
            "Epoch 629/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0199 - accuracy: 0.9800\n",
            "Epoch 629: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0182 - accuracy: 0.9891 - val_loss: 2.0630 - val_accuracy: 0.7742\n",
            "Epoch 630/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 630: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.0216 - accuracy: 0.9891 - val_loss: 2.0250 - val_accuracy: 0.7742\n",
            "Epoch 631/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9900\n",
            "Epoch 631: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0192 - accuracy: 0.9918 - val_loss: 2.0332 - val_accuracy: 0.7742\n",
            "Epoch 632/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 632: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0229 - accuracy: 0.9864 - val_loss: 1.7767 - val_accuracy: 0.7742\n",
            "Epoch 633/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0246 - accuracy: 0.9800\n",
            "Epoch 633: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0215 - accuracy: 0.9864 - val_loss: 1.7177 - val_accuracy: 0.7849\n",
            "Epoch 634/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 634: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0167 - accuracy: 0.9918 - val_loss: 1.8074 - val_accuracy: 0.7742\n",
            "Epoch 635/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 9.9050e-04 - accuracy: 1.0000\n",
            "Epoch 635: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0237 - accuracy: 0.9891 - val_loss: 1.5376 - val_accuracy: 0.7742\n",
            "Epoch 636/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0144 - accuracy: 1.0000\n",
            "Epoch 636: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0225 - accuracy: 0.9918 - val_loss: 1.6493 - val_accuracy: 0.7634\n",
            "Epoch 637/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9810\n",
            "Epoch 637: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.0248 - accuracy: 0.9810 - val_loss: 1.4655 - val_accuracy: 0.7634\n",
            "Epoch 638/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0177 - accuracy: 0.9800\n",
            "Epoch 638: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0193 - accuracy: 0.9918 - val_loss: 1.8376 - val_accuracy: 0.7527\n",
            "Epoch 639/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0309 - accuracy: 0.9800\n",
            "Epoch 639: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0233 - accuracy: 0.9837 - val_loss: 1.9763 - val_accuracy: 0.7634\n",
            "Epoch 640/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0155 - accuracy: 0.9900\n",
            "Epoch 640: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0241 - accuracy: 0.9837 - val_loss: 1.9516 - val_accuracy: 0.7419\n",
            "Epoch 641/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0199 - accuracy: 0.9900\n",
            "Epoch 641: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0198 - accuracy: 0.9891 - val_loss: 1.8872 - val_accuracy: 0.7527\n",
            "Epoch 642/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 642: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.0196 - accuracy: 0.9918 - val_loss: 2.0112 - val_accuracy: 0.7419\n",
            "Epoch 643/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 643: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0175 - accuracy: 0.9946 - val_loss: 2.1070 - val_accuracy: 0.7527\n",
            "Epoch 644/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0347 - accuracy: 0.9700\n",
            "Epoch 644: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0206 - accuracy: 0.9864 - val_loss: 2.0737 - val_accuracy: 0.7527\n",
            "Epoch 645/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0203 - accuracy: 0.9900\n",
            "Epoch 645: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0206 - accuracy: 0.9891 - val_loss: 1.9598 - val_accuracy: 0.7634\n",
            "Epoch 646/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0232 - accuracy: 0.9800\n",
            "Epoch 646: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0218 - accuracy: 0.9837 - val_loss: 1.8165 - val_accuracy: 0.7527\n",
            "Epoch 647/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0298 - accuracy: 0.9800\n",
            "Epoch 647: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0223 - accuracy: 0.9837 - val_loss: 1.8021 - val_accuracy: 0.7527\n",
            "Epoch 648/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0341 - accuracy: 0.9800\n",
            "Epoch 648: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0201 - accuracy: 0.9891 - val_loss: 1.9344 - val_accuracy: 0.7527\n",
            "Epoch 649/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0228 - accuracy: 1.0000\n",
            "Epoch 649: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0178 - accuracy: 0.9918 - val_loss: 1.9191 - val_accuracy: 0.7527\n",
            "Epoch 650/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0289 - accuracy: 0.9900\n",
            "Epoch 650: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0228 - accuracy: 0.9891 - val_loss: 2.0252 - val_accuracy: 0.7849\n",
            "Epoch 651/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 651: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0223 - accuracy: 0.9891 - val_loss: 2.1193 - val_accuracy: 0.7742\n",
            "Epoch 652/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0171 - accuracy: 0.9900\n",
            "Epoch 652: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.0164 - accuracy: 0.9891 - val_loss: 2.2059 - val_accuracy: 0.7742\n",
            "Epoch 653/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0116 - accuracy: 0.9900\n",
            "Epoch 653: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0233 - accuracy: 0.9891 - val_loss: 2.0543 - val_accuracy: 0.7634\n",
            "Epoch 654/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0245 - accuracy: 0.9900\n",
            "Epoch 654: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0239 - accuracy: 0.9891 - val_loss: 2.0809 - val_accuracy: 0.7634\n",
            "Epoch 655/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0120 - accuracy: 0.9900\n",
            "Epoch 655: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0210 - accuracy: 0.9864 - val_loss: 2.2637 - val_accuracy: 0.7527\n",
            "Epoch 656/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0479 - accuracy: 0.9800\n",
            "Epoch 656: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0319 - accuracy: 0.9837 - val_loss: 2.0498 - val_accuracy: 0.7849\n",
            "Epoch 657/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0240 - accuracy: 0.9900\n",
            "Epoch 657: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.0209 - accuracy: 0.9891 - val_loss: 2.0242 - val_accuracy: 0.7742\n",
            "Epoch 658/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0207 - accuracy: 0.9867\n",
            "Epoch 658: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 99ms/step - loss: 0.0191 - accuracy: 0.9891 - val_loss: 1.9125 - val_accuracy: 0.7849\n",
            "Epoch 659/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 659: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0240 - accuracy: 0.9891 - val_loss: 1.8757 - val_accuracy: 0.7419\n",
            "Epoch 660/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 660: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0190 - accuracy: 0.9864 - val_loss: 1.9360 - val_accuracy: 0.7527\n",
            "Epoch 661/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0110 - accuracy: 0.9900\n",
            "Epoch 661: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0205 - accuracy: 0.9918 - val_loss: 1.9663 - val_accuracy: 0.7742\n",
            "Epoch 662/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0170 - accuracy: 0.9800\n",
            "Epoch 662: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0226 - accuracy: 0.9864 - val_loss: 1.7645 - val_accuracy: 0.7742\n",
            "Epoch 663/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0176 - accuracy: 1.0000\n",
            "Epoch 663: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0235 - accuracy: 0.9891 - val_loss: 1.8050 - val_accuracy: 0.7419\n",
            "Epoch 664/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000\n",
            "Epoch 664: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0213 - accuracy: 0.9891 - val_loss: 1.9136 - val_accuracy: 0.7312\n",
            "Epoch 665/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0142 - accuracy: 0.9900\n",
            "Epoch 665: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0199 - accuracy: 0.9891 - val_loss: 1.9562 - val_accuracy: 0.7527\n",
            "Epoch 666/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0303 - accuracy: 0.9800\n",
            "Epoch 666: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0252 - accuracy: 0.9837 - val_loss: 1.8059 - val_accuracy: 0.7419\n",
            "Epoch 667/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0225 - accuracy: 0.9900\n",
            "Epoch 667: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0210 - accuracy: 0.9891 - val_loss: 1.7964 - val_accuracy: 0.7634\n",
            "Epoch 668/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0222 - accuracy: 0.9800\n",
            "Epoch 668: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0199 - accuracy: 0.9864 - val_loss: 1.8397 - val_accuracy: 0.7419\n",
            "Epoch 669/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0093 - accuracy: 0.9900\n",
            "Epoch 669: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0188 - accuracy: 0.9864 - val_loss: 1.8408 - val_accuracy: 0.7419\n",
            "Epoch 670/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 670: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0203 - accuracy: 0.9891 - val_loss: 1.9129 - val_accuracy: 0.7312\n",
            "Epoch 671/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0150 - accuracy: 0.9900\n",
            "Epoch 671: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0244 - accuracy: 0.9837 - val_loss: 1.8271 - val_accuracy: 0.7312\n",
            "Epoch 672/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 672: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0224 - accuracy: 0.9891 - val_loss: 1.8456 - val_accuracy: 0.7419\n",
            "Epoch 673/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0094 - accuracy: 0.9900\n",
            "Epoch 673: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0208 - accuracy: 0.9837 - val_loss: 1.8593 - val_accuracy: 0.7312\n",
            "Epoch 674/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0224 - accuracy: 0.9900\n",
            "Epoch 674: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0211 - accuracy: 0.9837 - val_loss: 1.7966 - val_accuracy: 0.7097\n",
            "Epoch 675/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0204 - accuracy: 0.9900\n",
            "Epoch 675: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0215 - accuracy: 0.9891 - val_loss: 2.2716 - val_accuracy: 0.7419\n",
            "Epoch 676/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 676: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0218 - accuracy: 0.9864 - val_loss: 2.0640 - val_accuracy: 0.7634\n",
            "Epoch 677/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0114 - accuracy: 0.9900\n",
            "Epoch 677: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0244 - accuracy: 0.9837 - val_loss: 1.8474 - val_accuracy: 0.7849\n",
            "Epoch 678/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0139 - accuracy: 1.0000\n",
            "Epoch 678: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0204 - accuracy: 0.9918 - val_loss: 1.7531 - val_accuracy: 0.7957\n",
            "Epoch 679/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0211 - accuracy: 0.9800\n",
            "Epoch 679: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0192 - accuracy: 0.9918 - val_loss: 1.8247 - val_accuracy: 0.7849\n",
            "Epoch 680/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0353 - accuracy: 0.9700\n",
            "Epoch 680: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0214 - accuracy: 0.9864 - val_loss: 1.8061 - val_accuracy: 0.7849\n",
            "Epoch 681/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0164 - accuracy: 0.9900\n",
            "Epoch 681: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0209 - accuracy: 0.9891 - val_loss: 1.8287 - val_accuracy: 0.7957\n",
            "Epoch 682/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0127 - accuracy: 0.9900\n",
            "Epoch 682: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0226 - accuracy: 0.9891 - val_loss: 1.6232 - val_accuracy: 0.7634\n",
            "Epoch 683/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 9.1857e-06 - accuracy: 1.0000\n",
            "Epoch 683: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0200 - accuracy: 0.9891 - val_loss: 1.7695 - val_accuracy: 0.7742\n",
            "Epoch 684/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0170 - accuracy: 1.0000\n",
            "Epoch 684: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0218 - accuracy: 0.9918 - val_loss: 1.7896 - val_accuracy: 0.7634\n",
            "Epoch 685/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0224 - accuracy: 0.9800\n",
            "Epoch 685: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0202 - accuracy: 0.9891 - val_loss: 1.9583 - val_accuracy: 0.7634\n",
            "Epoch 686/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0137 - accuracy: 0.9900\n",
            "Epoch 686: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0350 - accuracy: 0.9837 - val_loss: 1.7808 - val_accuracy: 0.7634\n",
            "Epoch 687/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0368 - accuracy: 0.9800\n",
            "Epoch 687: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 1.8823 - val_accuracy: 0.7527\n",
            "Epoch 688/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 688: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0191 - accuracy: 0.9837 - val_loss: 2.0433 - val_accuracy: 0.7312\n",
            "Epoch 689/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0284 - accuracy: 0.9800\n",
            "Epoch 689: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0211 - accuracy: 0.9891 - val_loss: 1.9338 - val_accuracy: 0.7527\n",
            "Epoch 690/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0220 - accuracy: 0.9900\n",
            "Epoch 690: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0196 - accuracy: 0.9891 - val_loss: 1.8670 - val_accuracy: 0.7527\n",
            "Epoch 691/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0414 - accuracy: 0.9700\n",
            "Epoch 691: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0198 - accuracy: 0.9864 - val_loss: 1.9614 - val_accuracy: 0.7419\n",
            "Epoch 692/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0209 - accuracy: 0.9900\n",
            "Epoch 692: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0195 - accuracy: 0.9864 - val_loss: 2.0409 - val_accuracy: 0.7419\n",
            "Epoch 693/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0319 - accuracy: 0.9900\n",
            "Epoch 693: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0221 - accuracy: 0.9918 - val_loss: 2.1025 - val_accuracy: 0.7312\n",
            "Epoch 694/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0120 - accuracy: 1.0000\n",
            "Epoch 694: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0330 - accuracy: 0.9864 - val_loss: 1.7330 - val_accuracy: 0.7742\n",
            "Epoch 695/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0215 - accuracy: 0.9800\n",
            "Epoch 695: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0178 - accuracy: 0.9891 - val_loss: 1.8468 - val_accuracy: 0.7527\n",
            "Epoch 696/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0123 - accuracy: 0.9900\n",
            "Epoch 696: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0201 - accuracy: 0.9891 - val_loss: 1.8254 - val_accuracy: 0.7419\n",
            "Epoch 697/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0160 - accuracy: 0.9800\n",
            "Epoch 697: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0193 - accuracy: 0.9864 - val_loss: 1.7875 - val_accuracy: 0.7634\n",
            "Epoch 698/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0220 - accuracy: 0.9900\n",
            "Epoch 698: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0209 - accuracy: 0.9891 - val_loss: 1.6375 - val_accuracy: 0.7742\n",
            "Epoch 699/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0075 - accuracy: 0.9900\n",
            "Epoch 699: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0222 - accuracy: 0.9864 - val_loss: 1.6968 - val_accuracy: 0.7742\n",
            "Epoch 700/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0363 - accuracy: 0.9700\n",
            "Epoch 700: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0224 - accuracy: 0.9837 - val_loss: 1.6517 - val_accuracy: 0.7634\n",
            "Epoch 701/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0332 - accuracy: 0.9800\n",
            "Epoch 701: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0213 - accuracy: 0.9864 - val_loss: 1.7684 - val_accuracy: 0.7634\n",
            "Epoch 702/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0140 - accuracy: 0.9900\n",
            "Epoch 702: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0173 - accuracy: 0.9891 - val_loss: 1.8592 - val_accuracy: 0.7634\n",
            "Epoch 703/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0197 - accuracy: 0.9800\n",
            "Epoch 703: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0219 - accuracy: 0.9810 - val_loss: 1.8535 - val_accuracy: 0.7527\n",
            "Epoch 704/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0329 - accuracy: 0.9800\n",
            "Epoch 704: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0206 - accuracy: 0.9864 - val_loss: 1.9369 - val_accuracy: 0.7634\n",
            "Epoch 705/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 705: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0203 - accuracy: 0.9864 - val_loss: 1.8941 - val_accuracy: 0.7312\n",
            "Epoch 706/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9837\n",
            "Epoch 706: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 63ms/step - loss: 0.0246 - accuracy: 0.9837 - val_loss: 1.9198 - val_accuracy: 0.7742\n",
            "Epoch 707/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0106 - accuracy: 0.9900\n",
            "Epoch 707: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0225 - accuracy: 0.9864 - val_loss: 1.8443 - val_accuracy: 0.7527\n",
            "Epoch 708/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.0068e-04 - accuracy: 1.0000\n",
            "Epoch 708: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0225 - accuracy: 0.9891 - val_loss: 1.6744 - val_accuracy: 0.7419\n",
            "Epoch 709/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0319 - accuracy: 0.9800\n",
            "Epoch 709: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0193 - accuracy: 0.9946 - val_loss: 1.8685 - val_accuracy: 0.7312\n",
            "Epoch 710/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0117 - accuracy: 1.0000\n",
            "Epoch 710: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0192 - accuracy: 0.9946 - val_loss: 2.0647 - val_accuracy: 0.7204\n",
            "Epoch 711/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0200 - accuracy: 1.0000\n",
            "Epoch 711: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0206 - accuracy: 0.9918 - val_loss: 1.9439 - val_accuracy: 0.7634\n",
            "Epoch 712/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0226 - accuracy: 0.9800\n",
            "Epoch 712: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0203 - accuracy: 0.9864 - val_loss: 2.0448 - val_accuracy: 0.7527\n",
            "Epoch 713/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0131 - accuracy: 0.9900\n",
            "Epoch 713: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0192 - accuracy: 0.9891 - val_loss: 2.0592 - val_accuracy: 0.7527\n",
            "Epoch 714/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0413 - accuracy: 0.9700\n",
            "Epoch 714: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0232 - accuracy: 0.9837 - val_loss: 1.8426 - val_accuracy: 0.7742\n",
            "Epoch 715/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0269 - accuracy: 0.9900\n",
            "Epoch 715: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0179 - accuracy: 0.9891 - val_loss: 1.8579 - val_accuracy: 0.7742\n",
            "Epoch 716/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 716: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0212 - accuracy: 0.9918 - val_loss: 1.9869 - val_accuracy: 0.7634\n",
            "Epoch 717/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0304 - accuracy: 0.9900\n",
            "Epoch 717: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0196 - accuracy: 0.9918 - val_loss: 2.0041 - val_accuracy: 0.7634\n",
            "Epoch 718/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0288 - accuracy: 0.9800\n",
            "Epoch 718: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0219 - accuracy: 0.9837 - val_loss: 1.8242 - val_accuracy: 0.7527\n",
            "Epoch 719/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0307 - accuracy: 0.9700\n",
            "Epoch 719: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0199 - accuracy: 0.9864 - val_loss: 1.7883 - val_accuracy: 0.7742\n",
            "Epoch 720/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 6.7330e-05 - accuracy: 1.0000\n",
            "Epoch 720: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0155 - accuracy: 0.9946 - val_loss: 1.8695 - val_accuracy: 0.7849\n",
            "Epoch 721/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 721: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0213 - accuracy: 0.9864 - val_loss: 1.6414 - val_accuracy: 0.8065\n",
            "Epoch 722/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0206 - accuracy: 0.9900\n",
            "Epoch 722: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0216 - accuracy: 0.9864 - val_loss: 1.6799 - val_accuracy: 0.7634\n",
            "Epoch 723/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0201 - accuracy: 1.0000\n",
            "Epoch 723: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0195 - accuracy: 0.9864 - val_loss: 1.8242 - val_accuracy: 0.7849\n",
            "Epoch 724/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0103 - accuracy: 1.0000\n",
            "Epoch 724: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0218 - accuracy: 0.9891 - val_loss: 1.8539 - val_accuracy: 0.7742\n",
            "Epoch 725/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9918\n",
            "Epoch 725: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 0.0170 - accuracy: 0.9918 - val_loss: 1.9038 - val_accuracy: 0.7527\n",
            "Epoch 726/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0274 - accuracy: 0.9800\n",
            "Epoch 726: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0211 - accuracy: 0.9891 - val_loss: 1.9652 - val_accuracy: 0.7312\n",
            "Epoch 727/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0163 - accuracy: 1.0000\n",
            "Epoch 727: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 2.3134 - val_accuracy: 0.7527\n",
            "Epoch 728/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0258 - accuracy: 0.9900\n",
            "Epoch 728: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0183 - accuracy: 0.9946 - val_loss: 2.1978 - val_accuracy: 0.7634\n",
            "Epoch 729/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0183 - accuracy: 0.9900\n",
            "Epoch 729: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0203 - accuracy: 0.9891 - val_loss: 2.2557 - val_accuracy: 0.7742\n",
            "Epoch 730/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0082 - accuracy: 0.9900\n",
            "Epoch 730: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0199 - accuracy: 0.9891 - val_loss: 2.0200 - val_accuracy: 0.7527\n",
            "Epoch 731/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 3.1897e-04 - accuracy: 1.0000\n",
            "Epoch 731: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0217 - accuracy: 0.9891 - val_loss: 1.9781 - val_accuracy: 0.7849\n",
            "Epoch 732/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0348 - accuracy: 0.9800\n",
            "Epoch 732: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 1.9976 - val_accuracy: 0.7957\n",
            "Epoch 733/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0187 - accuracy: 0.9900\n",
            "Epoch 733: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0206 - accuracy: 0.9864 - val_loss: 2.0936 - val_accuracy: 0.7742\n",
            "Epoch 734/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0276 - accuracy: 0.9900\n",
            "Epoch 734: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0182 - accuracy: 0.9918 - val_loss: 2.0524 - val_accuracy: 0.7742\n",
            "Epoch 735/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0309 - accuracy: 0.9900\n",
            "Epoch 735: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0204 - accuracy: 0.9891 - val_loss: 1.8766 - val_accuracy: 0.7419\n",
            "Epoch 736/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0226 - accuracy: 0.9800\n",
            "Epoch 736: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0163 - accuracy: 0.9891 - val_loss: 1.9000 - val_accuracy: 0.7849\n",
            "Epoch 737/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 737: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0215 - accuracy: 0.9891 - val_loss: 2.0437 - val_accuracy: 0.7634\n",
            "Epoch 738/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 738: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0218 - accuracy: 0.9864 - val_loss: 1.7902 - val_accuracy: 0.7419\n",
            "Epoch 739/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0461 - accuracy: 0.9600\n",
            "Epoch 739: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0203 - accuracy: 0.9837 - val_loss: 1.8694 - val_accuracy: 0.7742\n",
            "Epoch 740/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9837\n",
            "Epoch 740: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 0.0209 - accuracy: 0.9837 - val_loss: 1.9724 - val_accuracy: 0.7742\n",
            "Epoch 741/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0231 - accuracy: 0.9900\n",
            "Epoch 741: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0213 - accuracy: 0.9891 - val_loss: 1.6742 - val_accuracy: 0.7527\n",
            "Epoch 742/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0191 - accuracy: 0.9900\n",
            "Epoch 742: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0221 - accuracy: 0.9837 - val_loss: 1.7603 - val_accuracy: 0.7957\n",
            "Epoch 743/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 743: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0204 - accuracy: 0.9891 - val_loss: 1.9926 - val_accuracy: 0.7957\n",
            "Epoch 744/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 744: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0209 - accuracy: 0.9891 - val_loss: 1.9899 - val_accuracy: 0.7957\n",
            "Epoch 745/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0239 - accuracy: 0.9800\n",
            "Epoch 745: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0201 - accuracy: 0.9891 - val_loss: 1.9042 - val_accuracy: 0.8065\n",
            "Epoch 746/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 746: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0183 - accuracy: 0.9891 - val_loss: 2.0258 - val_accuracy: 0.7957\n",
            "Epoch 747/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0103 - accuracy: 1.0000\n",
            "Epoch 747: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0291 - accuracy: 0.9864 - val_loss: 2.1120 - val_accuracy: 0.7527\n",
            "Epoch 748/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0456 - accuracy: 0.9800\n",
            "Epoch 748: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0188 - accuracy: 0.9891 - val_loss: 2.0074 - val_accuracy: 0.7527\n",
            "Epoch 749/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0146 - accuracy: 0.9900\n",
            "Epoch 749: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0227 - accuracy: 0.9864 - val_loss: 2.0704 - val_accuracy: 0.8065\n",
            "Epoch 750/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9864\n",
            "Epoch 750: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.0221 - accuracy: 0.9864 - val_loss: 1.9625 - val_accuracy: 0.7957\n",
            "Epoch 751/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0183 - accuracy: 0.9800\n",
            "Epoch 751: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0224 - accuracy: 0.9864 - val_loss: 1.9187 - val_accuracy: 0.8172\n",
            "Epoch 752/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 752: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0248 - accuracy: 0.9864 - val_loss: 1.9742 - val_accuracy: 0.7849\n",
            "Epoch 753/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0334 - accuracy: 0.9800\n",
            "Epoch 753: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0191 - accuracy: 0.9864 - val_loss: 1.9745 - val_accuracy: 0.8065\n",
            "Epoch 754/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0157 - accuracy: 0.9900\n",
            "Epoch 754: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0193 - accuracy: 0.9864 - val_loss: 1.8960 - val_accuracy: 0.8065\n",
            "Epoch 755/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0158 - accuracy: 0.9900\n",
            "Epoch 755: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0198 - accuracy: 0.9891 - val_loss: 1.9341 - val_accuracy: 0.8065\n",
            "Epoch 756/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 756: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0206 - accuracy: 0.9891 - val_loss: 1.8861 - val_accuracy: 0.7634\n",
            "Epoch 757/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0174 - accuracy: 0.9900\n",
            "Epoch 757: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0189 - accuracy: 0.9891 - val_loss: 1.9394 - val_accuracy: 0.7742\n",
            "Epoch 758/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 758: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0218 - accuracy: 0.9891 - val_loss: 1.9509 - val_accuracy: 0.7957\n",
            "Epoch 759/800\n",
            "2/4 [==============>...............] - ETA: 0s - loss: 0.0303 - accuracy: 0.9800\n",
            "Epoch 759: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 65ms/step - loss: 0.0194 - accuracy: 0.9864 - val_loss: 1.9504 - val_accuracy: 0.8065\n",
            "Epoch 760/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0249 - accuracy: 0.9800\n",
            "Epoch 760: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0221 - accuracy: 0.9837 - val_loss: 1.8976 - val_accuracy: 0.8172\n",
            "Epoch 761/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0289 - accuracy: 0.9800\n",
            "Epoch 761: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0227 - accuracy: 0.9891 - val_loss: 1.7565 - val_accuracy: 0.7849\n",
            "Epoch 762/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 762: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0211 - accuracy: 0.9891 - val_loss: 1.8391 - val_accuracy: 0.8172\n",
            "Epoch 763/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 1.8132e-05 - accuracy: 1.0000\n",
            "Epoch 763: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0209 - accuracy: 0.9864 - val_loss: 1.7978 - val_accuracy: 0.8065\n",
            "Epoch 764/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0205 - accuracy: 0.9800\n",
            "Epoch 764: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0208 - accuracy: 0.9864 - val_loss: 1.7802 - val_accuracy: 0.8065\n",
            "Epoch 765/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0264 - accuracy: 0.9800\n",
            "Epoch 765: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.0199 - accuracy: 0.9918 - val_loss: 1.8429 - val_accuracy: 0.8065\n",
            "Epoch 766/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 766: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0199 - accuracy: 0.9891 - val_loss: 1.8437 - val_accuracy: 0.7957\n",
            "Epoch 767/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0222 - accuracy: 0.9900\n",
            "Epoch 767: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0194 - accuracy: 0.9891 - val_loss: 1.9211 - val_accuracy: 0.7742\n",
            "Epoch 768/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0242 - accuracy: 0.9900\n",
            "Epoch 768: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0204 - accuracy: 0.9864 - val_loss: 1.7715 - val_accuracy: 0.7849\n",
            "Epoch 769/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0447 - accuracy: 0.9900\n",
            "Epoch 769: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0238 - accuracy: 0.9891 - val_loss: 1.8270 - val_accuracy: 0.7849\n",
            "Epoch 770/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0158 - accuracy: 0.9900\n",
            "Epoch 770: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0181 - accuracy: 0.9891 - val_loss: 1.8433 - val_accuracy: 0.7957\n",
            "Epoch 771/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0226 - accuracy: 0.9900\n",
            "Epoch 771: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0216 - accuracy: 0.9864 - val_loss: 1.8370 - val_accuracy: 0.7957\n",
            "Epoch 772/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0270 - accuracy: 0.9800\n",
            "Epoch 772: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0211 - accuracy: 0.9891 - val_loss: 1.8238 - val_accuracy: 0.7957\n",
            "Epoch 773/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0235 - accuracy: 0.9900\n",
            "Epoch 773: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 1.8119 - val_accuracy: 0.8065\n",
            "Epoch 774/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 774: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0195 - accuracy: 0.9918 - val_loss: 1.9770 - val_accuracy: 0.7849\n",
            "Epoch 775/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0213 - accuracy: 0.9900\n",
            "Epoch 775: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0211 - accuracy: 0.9864 - val_loss: 1.8950 - val_accuracy: 0.7957\n",
            "Epoch 776/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0235 - accuracy: 0.9900\n",
            "Epoch 776: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0232 - accuracy: 0.9891 - val_loss: 1.7594 - val_accuracy: 0.7742\n",
            "Epoch 777/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 777: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0207 - accuracy: 0.9864 - val_loss: 1.7693 - val_accuracy: 0.7957\n",
            "Epoch 778/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0083 - accuracy: 0.9900\n",
            "Epoch 778: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0245 - accuracy: 0.9837 - val_loss: 1.8903 - val_accuracy: 0.8172\n",
            "Epoch 779/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 5.3514e-05 - accuracy: 1.0000\n",
            "Epoch 779: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0230 - accuracy: 0.9864 - val_loss: 1.7715 - val_accuracy: 0.8065\n",
            "Epoch 780/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 780: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0199 - accuracy: 0.9891 - val_loss: 1.7251 - val_accuracy: 0.7957\n",
            "Epoch 781/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0359 - accuracy: 0.9800\n",
            "Epoch 781: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0218 - accuracy: 0.9864 - val_loss: 1.7569 - val_accuracy: 0.7957\n",
            "Epoch 782/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0133 - accuracy: 0.9900\n",
            "Epoch 782: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0207 - accuracy: 0.9864 - val_loss: 1.7068 - val_accuracy: 0.7957\n",
            "Epoch 783/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0246 - accuracy: 0.9900\n",
            "Epoch 783: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0198 - accuracy: 0.9891 - val_loss: 1.6976 - val_accuracy: 0.7957\n",
            "Epoch 784/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0184 - accuracy: 0.9800\n",
            "Epoch 784: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0197 - accuracy: 0.9891 - val_loss: 1.8578 - val_accuracy: 0.7957\n",
            "Epoch 785/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0147 - accuracy: 0.9900\n",
            "Epoch 785: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0214 - accuracy: 0.9891 - val_loss: 1.9406 - val_accuracy: 0.7957\n",
            "Epoch 786/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 786: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0208 - accuracy: 0.9837 - val_loss: 2.0563 - val_accuracy: 0.7849\n",
            "Epoch 787/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0164 - accuracy: 0.9800\n",
            "Epoch 787: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0169 - accuracy: 0.9891 - val_loss: 2.1504 - val_accuracy: 0.7957\n",
            "Epoch 788/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0313 - accuracy: 0.9800\n",
            "Epoch 788: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0211 - accuracy: 0.9864 - val_loss: 2.0838 - val_accuracy: 0.7742\n",
            "Epoch 789/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0376 - accuracy: 0.9700\n",
            "Epoch 789: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0216 - accuracy: 0.9864 - val_loss: 1.8291 - val_accuracy: 0.7634\n",
            "Epoch 790/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0220 - accuracy: 0.9900\n",
            "Epoch 790: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.0192 - accuracy: 0.9891 - val_loss: 2.0066 - val_accuracy: 0.7957\n",
            "Epoch 791/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0123 - accuracy: 0.9900\n",
            "Epoch 791: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0191 - accuracy: 0.9891 - val_loss: 2.0556 - val_accuracy: 0.7849\n",
            "Epoch 792/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0283 - accuracy: 0.9900\n",
            "Epoch 792: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0190 - accuracy: 0.9918 - val_loss: 2.1331 - val_accuracy: 0.7957\n",
            "Epoch 793/800\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9864\n",
            "Epoch 793: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 93ms/step - loss: 0.0214 - accuracy: 0.9864 - val_loss: 1.9561 - val_accuracy: 0.7527\n",
            "Epoch 794/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0220 - accuracy: 0.9900\n",
            "Epoch 794: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.0214 - accuracy: 0.9918 - val_loss: 2.1939 - val_accuracy: 0.7527\n",
            "Epoch 795/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0175 - accuracy: 0.9900\n",
            "Epoch 795: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0235 - accuracy: 0.9891 - val_loss: 2.1110 - val_accuracy: 0.7634\n",
            "Epoch 796/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0325 - accuracy: 0.9900\n",
            "Epoch 796: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.0244 - accuracy: 0.9918 - val_loss: 1.8626 - val_accuracy: 0.7957\n",
            "Epoch 797/800\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.0179 - accuracy: 0.9933\n",
            "Epoch 797: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.0191 - accuracy: 0.9918 - val_loss: 1.7964 - val_accuracy: 0.7742\n",
            "Epoch 798/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0148 - accuracy: 1.0000\n",
            "Epoch 798: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0206 - accuracy: 0.9864 - val_loss: 1.7080 - val_accuracy: 0.7849\n",
            "Epoch 799/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0203 - accuracy: 0.9900\n",
            "Epoch 799: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0179 - accuracy: 0.9918 - val_loss: 1.7044 - val_accuracy: 0.8065\n",
            "Epoch 800/800\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 800: val_accuracy did not improve from 0.82796\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0207 - accuracy: 0.9891 - val_loss: 1.6963 - val_accuracy: 0.8065\n",
            "Training time: 105.33 seconds\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 150, 50)           100450    \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 146, 64)           16064     \n",
            "                                                                 \n",
            " max_pooling1d_7 (MaxPoolin  (None, 29, 64)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 25, 32)            10272     \n",
            "                                                                 \n",
            " max_pooling1d_8 (MaxPoolin  (None, 5, 32)             0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 160)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 128)               20608     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 147781 (577.27 KB)\n",
            "Trainable params: 147781 (577.27 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(CNN_history.history['accuracy'])\n",
        "plt.title('CNN Model accuracy with class=3')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "hKfVPqJjnrDI",
        "outputId": "6e659da5-e834-4ac3-8cd7-48e0f3eae3ff"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss values\n",
        "plt.plot(CNN_history.history['loss'])\n",
        "plt.title('CNN Model loss with class=3')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "RIRZUXj9nys6",
        "outputId": "a92a4b28-4d11-4e55-90db-775a29444866"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "tGjmiQZ8nwEC"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_CNN = CNN_model.predict(feature_valid)\n",
        "\n",
        "# convert the validation vector\n",
        "valid_y_CNN = y_valid_CNN.copy()\n",
        "for i in range(len(y_valid_CNN)):\n",
        "    j = np.where(y_valid_CNN[i] == np.amax(y_valid_CNN[i]))\n",
        "    valid_y_CNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q79beMaknzlW",
        "outputId": "3c442450-b4f0-4992-bbec-5407a7180c8b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7bdd1c207010> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 4ms/step\n",
            "0.8064516129032258\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.71      0.77        17\n",
            "           1       0.80      0.91      0.85        43\n",
            "           2       0.80      0.73      0.76        33\n",
            "\n",
            "   micro avg       0.81      0.81      0.81        93\n",
            "   macro avg       0.82      0.78      0.79        93\n",
            "weighted avg       0.81      0.81      0.80        93\n",
            " samples avg       0.81      0.81      0.81        93\n",
            "\n",
            "auc score:  0.8356360058210445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "\n",
        "# 假设您已经有了预测结果y_valid_CNN和真实标签label_valid_y\n",
        "\n",
        "# 将标签转换为二进制形式(假设是三分类问题)\n",
        "y_valid_binary = label_binarize(label_valid_y, classes=[0, 1, 2])\n",
        "\n",
        "# 计算每个类别的ROC曲线和AUC\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "n_classes = 3\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_valid_binary[:, i], y_valid_CNN[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# 绘制ROC曲线\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = ['blue', 'red', 'green']\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2, label='ROC curve of class {0} (AUC = {1:0.2f})'.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([-0.05, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic (ROC) curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "EJgv4_zrW-jx",
        "outputId": "14388cce-c4b6-41e9-c776-2ef1a9ac93e1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN2"
      ],
      "metadata": {
        "id": "F9CJlnUspfDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def create_CNN_model():\n",
        "    CNN = Sequential()\n",
        "    CNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=True))\n",
        "\n",
        "    CNN.add(Convolution1D(256, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Convolution1D(128, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Convolution1D(64, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Flatten())\n",
        "    CNN.add(Dense(units = 512 , activation = 'relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(Dropout(0.3))\n",
        "    CNN.add(Dense(units = 256 , activation = 'relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(Dropout(0.3))\n",
        "    CNN.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "    opt = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "    CNN.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    return CNN\n",
        "\n",
        "\n",
        "CNN_model = create_CNN_model()\n",
        "\n",
        "# 创建ModelCheckpoint回调函数\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5',\n",
        "                             monitor='val_accuracy',\n",
        "                             save_best_only=True,\n",
        "                             mode='max',\n",
        "                             verbose=1)\n",
        "\n",
        "class_weights = {0: 1.0, 1: 0.5, 2: 3.0}\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "CNN_history = CNN_model.fit(feature_train, label_train_y,\n",
        "                            epochs=800, batch_size=128,\n",
        "                            validation_data=(feature_valid, label_valid_y),\n",
        "                            class_weight=class_weights,\n",
        "                            callbacks=[checkpoint])  # 将回调函数传递给fit函数\n",
        "\n",
        "\n",
        "# 记录结束时间\n",
        "end_time = time.time()\n",
        "\n",
        "# 计算训练时间\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"Training time: {training_time:.2f} seconds\")\n",
        "CNN_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R59r4luen4HJ",
        "outputId": "30d8c39f-e0b3-415e-8762-d4d33d53ec25"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 8.0939 - accuracy: 0.3560\n",
            "Epoch 1: val_accuracy improved from -inf to 0.35484, saving model to best_model.h5\n",
            "3/3 [==============================] - 5s 502ms/step - loss: 8.0939 - accuracy: 0.3560 - val_loss: 7.8991 - val_accuracy: 0.3548\n",
            "Epoch 2/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.8533 - accuracy: 0.4219"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - ETA: 0s - loss: 7.7257 - accuracy: 0.5027\n",
            "Epoch 2: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 129ms/step - loss: 7.7257 - accuracy: 0.5027 - val_loss: 7.8685 - val_accuracy: 0.3548\n",
            "Epoch 3/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.5179 - accuracy: 0.5815\n",
            "Epoch 3: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 121ms/step - loss: 7.5179 - accuracy: 0.5815 - val_loss: 7.8116 - val_accuracy: 0.3548\n",
            "Epoch 4/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.2981 - accuracy: 0.7364\n",
            "Epoch 4: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 123ms/step - loss: 7.2981 - accuracy: 0.7364 - val_loss: 7.7559 - val_accuracy: 0.3548\n",
            "Epoch 5/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.0997 - accuracy: 0.7636\n",
            "Epoch 5: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 7.0997 - accuracy: 0.7636 - val_loss: 7.6966 - val_accuracy: 0.3548\n",
            "Epoch 6/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.0171 - accuracy: 0.7908\n",
            "Epoch 6: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 7.0171 - accuracy: 0.7908 - val_loss: 7.6384 - val_accuracy: 0.3548\n",
            "Epoch 7/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.8758 - accuracy: 0.8342\n",
            "Epoch 7: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 119ms/step - loss: 6.8758 - accuracy: 0.8342 - val_loss: 7.5703 - val_accuracy: 0.3548\n",
            "Epoch 8/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.6996 - accuracy: 0.8614\n",
            "Epoch 8: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 131ms/step - loss: 6.6996 - accuracy: 0.8614 - val_loss: 7.4817 - val_accuracy: 0.3548\n",
            "Epoch 9/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 6.6200 - accuracy: 0.9336\n",
            "Epoch 9: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 6.5942 - accuracy: 0.9348 - val_loss: 7.4128 - val_accuracy: 0.3548\n",
            "Epoch 10/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.4818 - accuracy: 0.9293\n",
            "Epoch 10: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 123ms/step - loss: 6.4818 - accuracy: 0.9293 - val_loss: 7.3391 - val_accuracy: 0.3548\n",
            "Epoch 11/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.3891 - accuracy: 0.9429\n",
            "Epoch 11: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 129ms/step - loss: 6.3891 - accuracy: 0.9429 - val_loss: 7.2619 - val_accuracy: 0.3548\n",
            "Epoch 12/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.2999 - accuracy: 0.9429\n",
            "Epoch 12: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 123ms/step - loss: 6.2999 - accuracy: 0.9429 - val_loss: 7.1827 - val_accuracy: 0.3548\n",
            "Epoch 13/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.2003 - accuracy: 0.9511\n",
            "Epoch 13: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 123ms/step - loss: 6.2003 - accuracy: 0.9511 - val_loss: 7.1120 - val_accuracy: 0.3548\n",
            "Epoch 14/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.1440 - accuracy: 0.9565\n",
            "Epoch 14: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 126ms/step - loss: 6.1440 - accuracy: 0.9565 - val_loss: 7.0453 - val_accuracy: 0.3548\n",
            "Epoch 15/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.0576 - accuracy: 0.9620\n",
            "Epoch 15: val_accuracy did not improve from 0.35484\n",
            "3/3 [==============================] - 0s 131ms/step - loss: 6.0576 - accuracy: 0.9620 - val_loss: 6.9702 - val_accuracy: 0.3548\n",
            "Epoch 16/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.0124 - accuracy: 0.9647\n",
            "Epoch 16: val_accuracy improved from 0.35484 to 0.37634, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 6.0124 - accuracy: 0.9647 - val_loss: 6.8954 - val_accuracy: 0.3763\n",
            "Epoch 17/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.8863 - accuracy: 0.9810\n",
            "Epoch 17: val_accuracy improved from 0.37634 to 0.38710, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 5.8863 - accuracy: 0.9810 - val_loss: 6.8235 - val_accuracy: 0.3871\n",
            "Epoch 18/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.8995 - accuracy: 0.9701\n",
            "Epoch 18: val_accuracy improved from 0.38710 to 0.40860, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 5.8995 - accuracy: 0.9701 - val_loss: 6.7551 - val_accuracy: 0.4086\n",
            "Epoch 19/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.7563 - accuracy: 0.9647\n",
            "Epoch 19: val_accuracy improved from 0.40860 to 0.45161, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 5.7563 - accuracy: 0.9647 - val_loss: 6.6904 - val_accuracy: 0.4516\n",
            "Epoch 20/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.6731 - accuracy: 0.9783\n",
            "Epoch 20: val_accuracy improved from 0.45161 to 0.46237, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 5.6731 - accuracy: 0.9783 - val_loss: 6.6155 - val_accuracy: 0.4624\n",
            "Epoch 21/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.6874 - accuracy: 0.9375\n",
            "Epoch 21: val_accuracy did not improve from 0.46237\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 5.6508 - accuracy: 0.9647 - val_loss: 6.5377 - val_accuracy: 0.4301\n",
            "Epoch 22/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.5447 - accuracy: 0.9755\n",
            "Epoch 22: val_accuracy did not improve from 0.46237\n",
            "3/3 [==============================] - 0s 132ms/step - loss: 5.5447 - accuracy: 0.9755 - val_loss: 6.4644 - val_accuracy: 0.4301\n",
            "Epoch 23/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.4625 - accuracy: 0.9783\n",
            "Epoch 23: val_accuracy improved from 0.46237 to 0.49462, saving model to best_model.h5\n",
            "3/3 [==============================] - 1s 217ms/step - loss: 5.4625 - accuracy: 0.9783 - val_loss: 6.3920 - val_accuracy: 0.4946\n",
            "Epoch 24/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.4132 - accuracy: 0.9783\n",
            "Epoch 24: val_accuracy improved from 0.49462 to 0.50538, saving model to best_model.h5\n",
            "3/3 [==============================] - 1s 219ms/step - loss: 5.4132 - accuracy: 0.9783 - val_loss: 6.3166 - val_accuracy: 0.5054\n",
            "Epoch 25/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.3461 - accuracy: 0.9783\n",
            "Epoch 25: val_accuracy improved from 0.50538 to 0.56989, saving model to best_model.h5\n",
            "3/3 [==============================] - 1s 224ms/step - loss: 5.3461 - accuracy: 0.9783 - val_loss: 6.2434 - val_accuracy: 0.5699\n",
            "Epoch 26/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.2749 - accuracy: 0.9783\n",
            "Epoch 26: val_accuracy did not improve from 0.56989\n",
            "3/3 [==============================] - 1s 201ms/step - loss: 5.2749 - accuracy: 0.9783 - val_loss: 6.1780 - val_accuracy: 0.5161\n",
            "Epoch 27/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.2151 - accuracy: 0.9810\n",
            "Epoch 27: val_accuracy did not improve from 0.56989\n",
            "3/3 [==============================] - 0s 123ms/step - loss: 5.2151 - accuracy: 0.9810 - val_loss: 6.1106 - val_accuracy: 0.5054\n",
            "Epoch 28/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.1635 - accuracy: 0.9755\n",
            "Epoch 28: val_accuracy did not improve from 0.56989\n",
            "3/3 [==============================] - 0s 191ms/step - loss: 5.1635 - accuracy: 0.9755 - val_loss: 6.0453 - val_accuracy: 0.5161\n",
            "Epoch 29/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.0843 - accuracy: 0.9783\n",
            "Epoch 29: val_accuracy did not improve from 0.56989\n",
            "3/3 [==============================] - 0s 196ms/step - loss: 5.0843 - accuracy: 0.9783 - val_loss: 5.9776 - val_accuracy: 0.5484\n",
            "Epoch 30/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.0151 - accuracy: 0.9783\n",
            "Epoch 30: val_accuracy did not improve from 0.56989\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 5.0151 - accuracy: 0.9783 - val_loss: 5.9086 - val_accuracy: 0.5591\n",
            "Epoch 31/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.9547 - accuracy: 0.9864\n",
            "Epoch 31: val_accuracy did not improve from 0.56989\n",
            "3/3 [==============================] - 0s 133ms/step - loss: 4.9547 - accuracy: 0.9864 - val_loss: 5.8420 - val_accuracy: 0.5699\n",
            "Epoch 32/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.8905 - accuracy: 1.0000\n",
            "Epoch 32: val_accuracy improved from 0.56989 to 0.61290, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 4.8806 - accuracy: 0.9837 - val_loss: 5.7733 - val_accuracy: 0.6129\n",
            "Epoch 33/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.8297 - accuracy: 0.9864\n",
            "Epoch 33: val_accuracy did not improve from 0.61290\n",
            "3/3 [==============================] - 0s 128ms/step - loss: 4.8297 - accuracy: 0.9864 - val_loss: 5.7036 - val_accuracy: 0.6129\n",
            "Epoch 34/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.7915 - accuracy: 0.9922\n",
            "Epoch 34: val_accuracy improved from 0.61290 to 0.62366, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 4.7731 - accuracy: 0.9837 - val_loss: 5.6345 - val_accuracy: 0.6237\n",
            "Epoch 35/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.7106 - accuracy: 0.9810\n",
            "Epoch 35: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 124ms/step - loss: 4.7106 - accuracy: 0.9810 - val_loss: 5.5688 - val_accuracy: 0.5914\n",
            "Epoch 36/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.6727 - accuracy: 0.9783\n",
            "Epoch 36: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 126ms/step - loss: 4.6727 - accuracy: 0.9783 - val_loss: 5.5010 - val_accuracy: 0.6022\n",
            "Epoch 37/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 4.6253 - accuracy: 0.9766\n",
            "Epoch 37: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 4.6066 - accuracy: 0.9810 - val_loss: 5.4360 - val_accuracy: 0.5806\n",
            "Epoch 38/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.5546 - accuracy: 0.9810\n",
            "Epoch 38: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 4.5546 - accuracy: 0.9810 - val_loss: 5.3787 - val_accuracy: 0.5376\n",
            "Epoch 39/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.5012 - accuracy: 0.9810\n",
            "Epoch 39: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 4.5012 - accuracy: 0.9810 - val_loss: 5.3256 - val_accuracy: 0.5269\n",
            "Epoch 40/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.4740 - accuracy: 0.9766\n",
            "Epoch 40: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 4.4483 - accuracy: 0.9783 - val_loss: 5.2697 - val_accuracy: 0.5591\n",
            "Epoch 41/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.3785 - accuracy: 0.9810\n",
            "Epoch 41: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 4.3785 - accuracy: 0.9810 - val_loss: 5.2213 - val_accuracy: 0.5269\n",
            "Epoch 42/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.3142 - accuracy: 1.0000\n",
            "Epoch 42: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 4.3255 - accuracy: 0.9837 - val_loss: 5.1732 - val_accuracy: 0.5054\n",
            "Epoch 43/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.3327 - accuracy: 0.9688\n",
            "Epoch 43: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 4.2910 - accuracy: 0.9755 - val_loss: 5.1285 - val_accuracy: 0.4946\n",
            "Epoch 44/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.2296 - accuracy: 0.9922\n",
            "Epoch 44: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 4.2322 - accuracy: 0.9810 - val_loss: 5.0861 - val_accuracy: 0.4731\n",
            "Epoch 45/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.1856 - accuracy: 0.9783\n",
            "Epoch 45: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 4.1856 - accuracy: 0.9783 - val_loss: 5.0390 - val_accuracy: 0.4731\n",
            "Epoch 46/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 4.1358 - accuracy: 0.9844\n",
            "Epoch 46: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 4.1291 - accuracy: 0.9810 - val_loss: 4.9995 - val_accuracy: 0.4731\n",
            "Epoch 47/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.0816 - accuracy: 0.9783\n",
            "Epoch 47: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 4.0816 - accuracy: 0.9783 - val_loss: 4.9703 - val_accuracy: 0.4731\n",
            "Epoch 48/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.0198 - accuracy: 0.9922\n",
            "Epoch 48: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 4.0273 - accuracy: 0.9810 - val_loss: 4.9466 - val_accuracy: 0.4624\n",
            "Epoch 49/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.9837 - accuracy: 0.9805\n",
            "Epoch 49: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 3.9707 - accuracy: 0.9837 - val_loss: 4.9178 - val_accuracy: 0.4624\n",
            "Epoch 50/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.9541 - accuracy: 0.9766\n",
            "Epoch 50: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 3.9358 - accuracy: 0.9810 - val_loss: 4.8772 - val_accuracy: 0.4624\n",
            "Epoch 51/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.8927 - accuracy: 0.9844\n",
            "Epoch 51: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 3.8863 - accuracy: 0.9810 - val_loss: 4.8372 - val_accuracy: 0.4516\n",
            "Epoch 52/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.8303 - accuracy: 0.9837\n",
            "Epoch 52: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 3.8303 - accuracy: 0.9837 - val_loss: 4.7931 - val_accuracy: 0.4624\n",
            "Epoch 53/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.7930 - accuracy: 0.9810\n",
            "Epoch 53: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 128ms/step - loss: 3.7930 - accuracy: 0.9810 - val_loss: 4.7551 - val_accuracy: 0.4624\n",
            "Epoch 54/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.7502 - accuracy: 0.9844\n",
            "Epoch 54: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 3.7359 - accuracy: 0.9864 - val_loss: 4.7234 - val_accuracy: 0.4624\n",
            "Epoch 55/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.7022 - accuracy: 0.9783\n",
            "Epoch 55: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 3.7022 - accuracy: 0.9783 - val_loss: 4.6915 - val_accuracy: 0.4624\n",
            "Epoch 56/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.6573 - accuracy: 0.9837\n",
            "Epoch 56: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 126ms/step - loss: 3.6573 - accuracy: 0.9837 - val_loss: 4.6575 - val_accuracy: 0.4624\n",
            "Epoch 57/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.6181 - accuracy: 0.9844\n",
            "Epoch 57: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 3.6126 - accuracy: 0.9810 - val_loss: 4.6245 - val_accuracy: 0.4624\n",
            "Epoch 58/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.5808 - accuracy: 0.9844\n",
            "Epoch 58: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 3.5742 - accuracy: 0.9810 - val_loss: 4.5809 - val_accuracy: 0.4624\n",
            "Epoch 59/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.5173 - accuracy: 0.9810\n",
            "Epoch 59: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 3.5173 - accuracy: 0.9810 - val_loss: 4.5417 - val_accuracy: 0.4624\n",
            "Epoch 60/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4948 - accuracy: 0.9844\n",
            "Epoch 60: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 3.4833 - accuracy: 0.9837 - val_loss: 4.5091 - val_accuracy: 0.4624\n",
            "Epoch 61/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4319 - accuracy: 0.9922\n",
            "Epoch 61: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 3.4409 - accuracy: 0.9783 - val_loss: 4.4741 - val_accuracy: 0.4624\n",
            "Epoch 62/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4485 - accuracy: 0.9688\n",
            "Epoch 62: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 3.4126 - accuracy: 0.9755 - val_loss: 4.4412 - val_accuracy: 0.4624\n",
            "Epoch 63/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.3587 - accuracy: 0.9810\n",
            "Epoch 63: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 3.3587 - accuracy: 0.9810 - val_loss: 4.3991 - val_accuracy: 0.4624\n",
            "Epoch 64/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.3280 - accuracy: 0.9844\n",
            "Epoch 64: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 3.3173 - accuracy: 0.9837 - val_loss: 4.3592 - val_accuracy: 0.4624\n",
            "Epoch 65/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.2886 - accuracy: 0.9805\n",
            "Epoch 65: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 3.2811 - accuracy: 0.9810 - val_loss: 4.3264 - val_accuracy: 0.4624\n",
            "Epoch 66/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.2385 - accuracy: 0.9810\n",
            "Epoch 66: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 132ms/step - loss: 3.2385 - accuracy: 0.9810 - val_loss: 4.2969 - val_accuracy: 0.4624\n",
            "Epoch 67/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.2005 - accuracy: 0.9810\n",
            "Epoch 67: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 3.2005 - accuracy: 0.9810 - val_loss: 4.2673 - val_accuracy: 0.4624\n",
            "Epoch 68/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.1600 - accuracy: 0.9810\n",
            "Epoch 68: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 3.1600 - accuracy: 0.9810 - val_loss: 4.2378 - val_accuracy: 0.4624\n",
            "Epoch 69/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1192 - accuracy: 0.9844\n",
            "Epoch 69: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 3.1163 - accuracy: 0.9837 - val_loss: 4.2047 - val_accuracy: 0.4624\n",
            "Epoch 70/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1231 - accuracy: 0.9688\n",
            "Epoch 70: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 3.1032 - accuracy: 0.9783 - val_loss: 4.1619 - val_accuracy: 0.4731\n",
            "Epoch 71/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.0628 - accuracy: 0.9844\n",
            "Epoch 71: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 3.0515 - accuracy: 0.9810 - val_loss: 4.1239 - val_accuracy: 0.4731\n",
            "Epoch 72/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0148 - accuracy: 0.9810\n",
            "Epoch 72: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 3.0148 - accuracy: 0.9810 - val_loss: 4.0882 - val_accuracy: 0.4839\n",
            "Epoch 73/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9834 - accuracy: 0.9844\n",
            "Epoch 73: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 2.9765 - accuracy: 0.9810 - val_loss: 4.0482 - val_accuracy: 0.4946\n",
            "Epoch 74/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9468 - accuracy: 0.9922\n",
            "Epoch 74: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 2.9426 - accuracy: 0.9810 - val_loss: 4.0085 - val_accuracy: 0.4946\n",
            "Epoch 75/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9094 - accuracy: 0.9844\n",
            "Epoch 75: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 2.9100 - accuracy: 0.9810 - val_loss: 3.9672 - val_accuracy: 0.4946\n",
            "Epoch 76/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.8785 - accuracy: 0.9837\n",
            "Epoch 76: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 2.8785 - accuracy: 0.9837 - val_loss: 3.9290 - val_accuracy: 0.4946\n",
            "Epoch 77/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.8391 - accuracy: 0.9837\n",
            "Epoch 77: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 2.8391 - accuracy: 0.9837 - val_loss: 3.8942 - val_accuracy: 0.4946\n",
            "Epoch 78/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.8334 - accuracy: 0.9766\n",
            "Epoch 78: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 2.8043 - accuracy: 0.9837 - val_loss: 3.8503 - val_accuracy: 0.4946\n",
            "Epoch 79/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7705 - accuracy: 0.9922\n",
            "Epoch 79: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 2.7809 - accuracy: 0.9810 - val_loss: 3.7953 - val_accuracy: 0.4946\n",
            "Epoch 80/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.7437 - accuracy: 0.9810\n",
            "Epoch 80: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 129ms/step - loss: 2.7437 - accuracy: 0.9810 - val_loss: 3.7492 - val_accuracy: 0.5054\n",
            "Epoch 81/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7115 - accuracy: 0.9844\n",
            "Epoch 81: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 2.7073 - accuracy: 0.9810 - val_loss: 3.7158 - val_accuracy: 0.5054\n",
            "Epoch 82/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6875 - accuracy: 0.9688\n",
            "Epoch 82: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 2.6802 - accuracy: 0.9837 - val_loss: 3.6812 - val_accuracy: 0.5054\n",
            "Epoch 83/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6458 - accuracy: 0.9864\n",
            "Epoch 83: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 177ms/step - loss: 2.6458 - accuracy: 0.9864 - val_loss: 3.6149 - val_accuracy: 0.5376\n",
            "Epoch 84/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6058 - accuracy: 0.9864\n",
            "Epoch 84: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 2.6058 - accuracy: 0.9864 - val_loss: 3.5650 - val_accuracy: 0.5376\n",
            "Epoch 85/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5745 - accuracy: 0.9864\n",
            "Epoch 85: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 117ms/step - loss: 2.5745 - accuracy: 0.9864 - val_loss: 3.5285 - val_accuracy: 0.5376\n",
            "Epoch 86/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5438 - accuracy: 0.9864\n",
            "Epoch 86: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 126ms/step - loss: 2.5438 - accuracy: 0.9864 - val_loss: 3.4830 - val_accuracy: 0.5591\n",
            "Epoch 87/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.5147 - accuracy: 0.9922\n",
            "Epoch 87: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 2.5138 - accuracy: 0.9864 - val_loss: 3.4493 - val_accuracy: 0.5376\n",
            "Epoch 88/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4941 - accuracy: 0.9810\n",
            "Epoch 88: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 118ms/step - loss: 2.4941 - accuracy: 0.9810 - val_loss: 3.4253 - val_accuracy: 0.5484\n",
            "Epoch 89/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4754 - accuracy: 0.9688\n",
            "Epoch 89: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 2.4580 - accuracy: 0.9864 - val_loss: 3.3884 - val_accuracy: 0.5484\n",
            "Epoch 90/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.4384 - accuracy: 0.9805\n",
            "Epoch 90: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 129ms/step - loss: 2.4294 - accuracy: 0.9864 - val_loss: 3.3547 - val_accuracy: 0.5484\n",
            "Epoch 91/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4088 - accuracy: 0.9766\n",
            "Epoch 91: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 2.3913 - accuracy: 0.9864 - val_loss: 3.3175 - val_accuracy: 0.5806\n",
            "Epoch 92/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.3740 - accuracy: 0.9864\n",
            "Epoch 92: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 117ms/step - loss: 2.3740 - accuracy: 0.9864 - val_loss: 3.2933 - val_accuracy: 0.5914\n",
            "Epoch 93/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3446 - accuracy: 0.9922\n",
            "Epoch 93: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 2.3425 - accuracy: 0.9864 - val_loss: 3.2866 - val_accuracy: 0.5806\n",
            "Epoch 94/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.3165 - accuracy: 0.9837\n",
            "Epoch 94: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 203ms/step - loss: 2.3165 - accuracy: 0.9837 - val_loss: 3.2780 - val_accuracy: 0.5914\n",
            "Epoch 95/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.3041 - accuracy: 0.9766\n",
            "Epoch 95: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 2.2883 - accuracy: 0.9837 - val_loss: 3.2757 - val_accuracy: 0.5699\n",
            "Epoch 96/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.2559 - accuracy: 0.9810\n",
            "Epoch 96: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 125ms/step - loss: 2.2559 - accuracy: 0.9810 - val_loss: 3.2621 - val_accuracy: 0.5699\n",
            "Epoch 97/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2445 - accuracy: 0.9922\n",
            "Epoch 97: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 2.2326 - accuracy: 0.9864 - val_loss: 3.2380 - val_accuracy: 0.5806\n",
            "Epoch 98/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2099 - accuracy: 0.9844\n",
            "Epoch 98: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 2.2083 - accuracy: 0.9864 - val_loss: 3.1927 - val_accuracy: 0.5806\n",
            "Epoch 99/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1941 - accuracy: 0.9688\n",
            "Epoch 99: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 2.1722 - accuracy: 0.9837 - val_loss: 3.1562 - val_accuracy: 0.6022\n",
            "Epoch 100/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.1585 - accuracy: 0.9844\n",
            "Epoch 100: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 2.1516 - accuracy: 0.9864 - val_loss: 3.1121 - val_accuracy: 0.6022\n",
            "Epoch 101/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1230 - accuracy: 0.9922\n",
            "Epoch 101: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 2.1248 - accuracy: 0.9864 - val_loss: 3.0717 - val_accuracy: 0.6022\n",
            "Epoch 102/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0967 - accuracy: 0.9922\n",
            "Epoch 102: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 2.1016 - accuracy: 0.9864 - val_loss: 3.0364 - val_accuracy: 0.6129\n",
            "Epoch 103/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0737 - accuracy: 0.9922\n",
            "Epoch 103: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 2.0799 - accuracy: 0.9864 - val_loss: 3.0043 - val_accuracy: 0.6129\n",
            "Epoch 104/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.0548 - accuracy: 0.9883\n",
            "Epoch 104: val_accuracy did not improve from 0.62366\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 2.0519 - accuracy: 0.9864 - val_loss: 2.9720 - val_accuracy: 0.6237\n",
            "Epoch 105/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0346 - accuracy: 0.9766\n",
            "Epoch 105: val_accuracy improved from 0.62366 to 0.65591, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 2.0235 - accuracy: 0.9864 - val_loss: 2.9495 - val_accuracy: 0.6559\n",
            "Epoch 106/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0072 - accuracy: 0.9922\n",
            "Epoch 106: val_accuracy did not improve from 0.65591\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.9997 - accuracy: 0.9891 - val_loss: 2.9293 - val_accuracy: 0.6559\n",
            "Epoch 107/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9877 - accuracy: 0.9844\n",
            "Epoch 107: val_accuracy did not improve from 0.65591\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.9812 - accuracy: 0.9864 - val_loss: 2.9009 - val_accuracy: 0.6559\n",
            "Epoch 108/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9531 - accuracy: 0.9864\n",
            "Epoch 108: val_accuracy improved from 0.65591 to 0.66667, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 1.9531 - accuracy: 0.9864 - val_loss: 2.8729 - val_accuracy: 0.6667\n",
            "Epoch 109/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9387 - accuracy: 0.9844\n",
            "Epoch 109: val_accuracy did not improve from 0.66667\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.9321 - accuracy: 0.9864 - val_loss: 2.8528 - val_accuracy: 0.6667\n",
            "Epoch 110/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9333 - accuracy: 0.9844\n",
            "Epoch 110: val_accuracy did not improve from 0.66667\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.9124 - accuracy: 0.9837 - val_loss: 2.8216 - val_accuracy: 0.6667\n",
            "Epoch 111/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.8899 - accuracy: 0.9837\n",
            "Epoch 111: val_accuracy did not improve from 0.66667\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 1.8899 - accuracy: 0.9837 - val_loss: 2.8012 - val_accuracy: 0.6667\n",
            "Epoch 112/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.8671 - accuracy: 0.9864\n",
            "Epoch 112: val_accuracy improved from 0.66667 to 0.67742, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 1.8671 - accuracy: 0.9864 - val_loss: 2.7719 - val_accuracy: 0.6774\n",
            "Epoch 113/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.8394 - accuracy: 0.9922\n",
            "Epoch 113: val_accuracy improved from 0.67742 to 0.68817, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 1.8406 - accuracy: 0.9864 - val_loss: 2.7462 - val_accuracy: 0.6882\n",
            "Epoch 114/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8169 - accuracy: 0.9922\n",
            "Epoch 114: val_accuracy did not improve from 0.68817\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.8208 - accuracy: 0.9864 - val_loss: 2.7241 - val_accuracy: 0.6882\n",
            "Epoch 115/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.7969 - accuracy: 0.9864\n",
            "Epoch 115: val_accuracy did not improve from 0.68817\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 1.7969 - accuracy: 0.9864 - val_loss: 2.7099 - val_accuracy: 0.6774\n",
            "Epoch 116/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7663 - accuracy: 0.9922\n",
            "Epoch 116: val_accuracy did not improve from 0.68817\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.7819 - accuracy: 0.9864 - val_loss: 2.6914 - val_accuracy: 0.6774\n",
            "Epoch 117/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.7547 - accuracy: 0.9864\n",
            "Epoch 117: val_accuracy improved from 0.68817 to 0.69892, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 1.7547 - accuracy: 0.9864 - val_loss: 2.6707 - val_accuracy: 0.6989\n",
            "Epoch 118/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7575 - accuracy: 0.9688\n",
            "Epoch 118: val_accuracy did not improve from 0.69892\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.7398 - accuracy: 0.9864 - val_loss: 2.6527 - val_accuracy: 0.6989\n",
            "Epoch 119/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7452 - accuracy: 0.9609\n",
            "Epoch 119: val_accuracy did not improve from 0.69892\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.7123 - accuracy: 0.9864 - val_loss: 2.6247 - val_accuracy: 0.6989\n",
            "Epoch 120/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.7008 - accuracy: 0.9844\n",
            "Epoch 120: val_accuracy improved from 0.69892 to 0.74194, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 1.6928 - accuracy: 0.9864 - val_loss: 2.5853 - val_accuracy: 0.7419\n",
            "Epoch 121/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6790 - accuracy: 0.9844\n",
            "Epoch 121: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.6756 - accuracy: 0.9864 - val_loss: 2.5462 - val_accuracy: 0.7419\n",
            "Epoch 122/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.6551 - accuracy: 0.9864\n",
            "Epoch 122: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 125ms/step - loss: 1.6551 - accuracy: 0.9864 - val_loss: 2.5014 - val_accuracy: 0.7419\n",
            "Epoch 123/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.6367 - accuracy: 0.9883\n",
            "Epoch 123: val_accuracy improved from 0.74194 to 0.75269, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 1.6372 - accuracy: 0.9864 - val_loss: 2.4604 - val_accuracy: 0.7527\n",
            "Epoch 124/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.6156 - accuracy: 0.9864\n",
            "Epoch 124: val_accuracy improved from 0.75269 to 0.76344, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 1.6156 - accuracy: 0.9864 - val_loss: 2.4204 - val_accuracy: 0.7634\n",
            "Epoch 125/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6023 - accuracy: 1.0000\n",
            "Epoch 125: val_accuracy did not improve from 0.76344\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 1.5980 - accuracy: 0.9864 - val_loss: 2.3981 - val_accuracy: 0.7634\n",
            "Epoch 126/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5753 - accuracy: 0.9922\n",
            "Epoch 126: val_accuracy did not improve from 0.76344\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.5796 - accuracy: 0.9864 - val_loss: 2.3697 - val_accuracy: 0.7634\n",
            "Epoch 127/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5617 - accuracy: 0.9844\n",
            "Epoch 127: val_accuracy did not improve from 0.76344\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.5607 - accuracy: 0.9864 - val_loss: 2.3327 - val_accuracy: 0.7634\n",
            "Epoch 128/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5446 - accuracy: 0.9922\n",
            "Epoch 128: val_accuracy did not improve from 0.76344\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.5403 - accuracy: 0.9864 - val_loss: 2.2898 - val_accuracy: 0.7634\n",
            "Epoch 129/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.5244 - accuracy: 0.9864\n",
            "Epoch 129: val_accuracy improved from 0.76344 to 0.77419, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 1.5244 - accuracy: 0.9864 - val_loss: 2.2540 - val_accuracy: 0.7742\n",
            "Epoch 130/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4862 - accuracy: 1.0000\n",
            "Epoch 130: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.5038 - accuracy: 0.9864 - val_loss: 2.2237 - val_accuracy: 0.7742\n",
            "Epoch 131/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4829 - accuracy: 0.9844\n",
            "Epoch 131: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 1.4855 - accuracy: 0.9864 - val_loss: 2.2007 - val_accuracy: 0.7742\n",
            "Epoch 132/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4894 - accuracy: 0.9766\n",
            "Epoch 132: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.4758 - accuracy: 0.9864 - val_loss: 2.1827 - val_accuracy: 0.7742\n",
            "Epoch 133/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4628 - accuracy: 0.9922\n",
            "Epoch 133: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 1.4530 - accuracy: 0.9864 - val_loss: 2.1711 - val_accuracy: 0.7742\n",
            "Epoch 134/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4420 - accuracy: 0.9922\n",
            "Epoch 134: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 1.4367 - accuracy: 0.9864 - val_loss: 2.1494 - val_accuracy: 0.7742\n",
            "Epoch 135/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4176 - accuracy: 0.9922\n",
            "Epoch 135: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.4196 - accuracy: 0.9864 - val_loss: 2.1307 - val_accuracy: 0.7742\n",
            "Epoch 136/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4134 - accuracy: 0.9844\n",
            "Epoch 136: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.4041 - accuracy: 0.9864 - val_loss: 2.1054 - val_accuracy: 0.7742\n",
            "Epoch 137/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.3863 - accuracy: 0.9864\n",
            "Epoch 137: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 1.3863 - accuracy: 0.9864 - val_loss: 2.0822 - val_accuracy: 0.7742\n",
            "Epoch 138/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.3761 - accuracy: 0.9883\n",
            "Epoch 138: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 1.3711 - accuracy: 0.9864 - val_loss: 2.0573 - val_accuracy: 0.7742\n",
            "Epoch 139/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.3565 - accuracy: 0.9864\n",
            "Epoch 139: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 1.3565 - accuracy: 0.9864 - val_loss: 2.0440 - val_accuracy: 0.7742\n",
            "Epoch 140/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3230 - accuracy: 1.0000\n",
            "Epoch 140: val_accuracy improved from 0.77419 to 0.78495, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 1.3335 - accuracy: 0.9864 - val_loss: 2.0227 - val_accuracy: 0.7849\n",
            "Epoch 141/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3173 - accuracy: 1.0000\n",
            "Epoch 141: val_accuracy did not improve from 0.78495\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 1.3233 - accuracy: 0.9864 - val_loss: 1.9972 - val_accuracy: 0.7849\n",
            "Epoch 142/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3154 - accuracy: 0.9844\n",
            "Epoch 142: val_accuracy did not improve from 0.78495\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.3133 - accuracy: 0.9864 - val_loss: 1.9715 - val_accuracy: 0.7849\n",
            "Epoch 143/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.2900 - accuracy: 0.9864\n",
            "Epoch 143: val_accuracy did not improve from 0.78495\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 1.2900 - accuracy: 0.9864 - val_loss: 1.9438 - val_accuracy: 0.7849\n",
            "Epoch 144/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.2884 - accuracy: 0.9805\n",
            "Epoch 144: val_accuracy improved from 0.78495 to 0.79570, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 1.2772 - accuracy: 0.9864 - val_loss: 1.9147 - val_accuracy: 0.7957\n",
            "Epoch 145/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2697 - accuracy: 0.9844\n",
            "Epoch 145: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.2629 - accuracy: 0.9864 - val_loss: 1.8950 - val_accuracy: 0.7957\n",
            "Epoch 146/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2599 - accuracy: 0.9688\n",
            "Epoch 146: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 1.2438 - accuracy: 0.9864 - val_loss: 1.8823 - val_accuracy: 0.7849\n",
            "Epoch 147/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2411 - accuracy: 0.9766\n",
            "Epoch 147: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.2294 - accuracy: 0.9864 - val_loss: 1.8764 - val_accuracy: 0.7957\n",
            "Epoch 148/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.2087 - accuracy: 0.9922\n",
            "Epoch 148: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 1.2191 - accuracy: 0.9864 - val_loss: 1.8695 - val_accuracy: 0.7957\n",
            "Epoch 149/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2113 - accuracy: 0.9922\n",
            "Epoch 149: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 1.2077 - accuracy: 0.9864 - val_loss: 1.8643 - val_accuracy: 0.7957\n",
            "Epoch 150/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1907 - accuracy: 0.9922\n",
            "Epoch 150: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.1895 - accuracy: 0.9864 - val_loss: 1.8581 - val_accuracy: 0.7957\n",
            "Epoch 151/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.1746 - accuracy: 0.9864\n",
            "Epoch 151: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 1.1746 - accuracy: 0.9864 - val_loss: 1.8516 - val_accuracy: 0.7957\n",
            "Epoch 152/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1704 - accuracy: 0.9844\n",
            "Epoch 152: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.1620 - accuracy: 0.9864 - val_loss: 1.8433 - val_accuracy: 0.7957\n",
            "Epoch 153/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1623 - accuracy: 0.9922\n",
            "Epoch 153: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.1524 - accuracy: 0.9864 - val_loss: 1.8332 - val_accuracy: 0.7957\n",
            "Epoch 154/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.1374 - accuracy: 0.9864\n",
            "Epoch 154: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 1.1374 - accuracy: 0.9864 - val_loss: 1.8181 - val_accuracy: 0.7957\n",
            "Epoch 155/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1077 - accuracy: 1.0000\n",
            "Epoch 155: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.1236 - accuracy: 0.9864 - val_loss: 1.8018 - val_accuracy: 0.7957\n",
            "Epoch 156/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1128 - accuracy: 0.9844\n",
            "Epoch 156: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.1121 - accuracy: 0.9864 - val_loss: 1.7829 - val_accuracy: 0.7957\n",
            "Epoch 157/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0878 - accuracy: 0.9922\n",
            "Epoch 157: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.0970 - accuracy: 0.9864 - val_loss: 1.7625 - val_accuracy: 0.7849\n",
            "Epoch 158/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0917 - accuracy: 0.9844\n",
            "Epoch 158: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.0814 - accuracy: 0.9864 - val_loss: 1.7457 - val_accuracy: 0.7849\n",
            "Epoch 159/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0694 - accuracy: 0.9844\n",
            "Epoch 159: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.0703 - accuracy: 0.9864 - val_loss: 1.7292 - val_accuracy: 0.7849\n",
            "Epoch 160/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0640 - accuracy: 0.9844\n",
            "Epoch 160: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 1.0588 - accuracy: 0.9864 - val_loss: 1.7152 - val_accuracy: 0.7849\n",
            "Epoch 161/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0682 - accuracy: 0.9766\n",
            "Epoch 161: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 1.0459 - accuracy: 0.9864 - val_loss: 1.6998 - val_accuracy: 0.7849\n",
            "Epoch 162/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0372 - accuracy: 0.9922\n",
            "Epoch 162: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.0399 - accuracy: 0.9864 - val_loss: 1.6802 - val_accuracy: 0.7742\n",
            "Epoch 163/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0290 - accuracy: 0.9844\n",
            "Epoch 163: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 1.0228 - accuracy: 0.9891 - val_loss: 1.6664 - val_accuracy: 0.7742\n",
            "Epoch 164/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.0083 - accuracy: 0.9844\n",
            "Epoch 164: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 1.0110 - accuracy: 0.9864 - val_loss: 1.6552 - val_accuracy: 0.7849\n",
            "Epoch 165/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9975 - accuracy: 0.9844\n",
            "Epoch 165: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 1.0013 - accuracy: 0.9864 - val_loss: 1.6461 - val_accuracy: 0.7849\n",
            "Epoch 166/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9879 - accuracy: 0.9844\n",
            "Epoch 166: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.9920 - accuracy: 0.9864 - val_loss: 1.6343 - val_accuracy: 0.7849\n",
            "Epoch 167/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9674 - accuracy: 0.9922\n",
            "Epoch 167: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.9737 - accuracy: 0.9864 - val_loss: 1.6259 - val_accuracy: 0.7849\n",
            "Epoch 168/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9510 - accuracy: 0.9922\n",
            "Epoch 168: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.9637 - accuracy: 0.9864 - val_loss: 1.6166 - val_accuracy: 0.7849\n",
            "Epoch 169/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9450 - accuracy: 0.9922\n",
            "Epoch 169: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.9498 - accuracy: 0.9864 - val_loss: 1.6081 - val_accuracy: 0.7849\n",
            "Epoch 170/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9353 - accuracy: 0.9922\n",
            "Epoch 170: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.9379 - accuracy: 0.9864 - val_loss: 1.6006 - val_accuracy: 0.7849\n",
            "Epoch 171/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9291 - accuracy: 0.9844\n",
            "Epoch 171: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.9324 - accuracy: 0.9864 - val_loss: 1.5892 - val_accuracy: 0.7849\n",
            "Epoch 172/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9183 - accuracy: 0.9844\n",
            "Epoch 172: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.9185 - accuracy: 0.9864 - val_loss: 1.5743 - val_accuracy: 0.7849\n",
            "Epoch 173/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.9131 - accuracy: 0.9844\n",
            "Epoch 173: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 124ms/step - loss: 0.9072 - accuracy: 0.9864 - val_loss: 1.5657 - val_accuracy: 0.7849\n",
            "Epoch 174/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8930 - accuracy: 0.9844\n",
            "Epoch 174: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.8955 - accuracy: 0.9864 - val_loss: 1.5593 - val_accuracy: 0.7849\n",
            "Epoch 175/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8924 - accuracy: 0.9844\n",
            "Epoch 175: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.8926 - accuracy: 0.9864 - val_loss: 1.5529 - val_accuracy: 0.7849\n",
            "Epoch 176/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8895 - accuracy: 0.9844\n",
            "Epoch 176: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.8761 - accuracy: 0.9864 - val_loss: 1.5413 - val_accuracy: 0.7849\n",
            "Epoch 177/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8671 - accuracy: 0.9864\n",
            "Epoch 177: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 0.8671 - accuracy: 0.9864 - val_loss: 1.5345 - val_accuracy: 0.7849\n",
            "Epoch 178/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8716 - accuracy: 0.9844\n",
            "Epoch 178: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.8602 - accuracy: 0.9864 - val_loss: 1.5237 - val_accuracy: 0.7742\n",
            "Epoch 179/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8626 - accuracy: 0.9844\n",
            "Epoch 179: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.8453 - accuracy: 0.9864 - val_loss: 1.5069 - val_accuracy: 0.7742\n",
            "Epoch 180/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8451 - accuracy: 0.9766\n",
            "Epoch 180: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.8402 - accuracy: 0.9864 - val_loss: 1.5009 - val_accuracy: 0.7634\n",
            "Epoch 181/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8089 - accuracy: 1.0000\n",
            "Epoch 181: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.8348 - accuracy: 0.9864 - val_loss: 1.4927 - val_accuracy: 0.7527\n",
            "Epoch 182/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8143 - accuracy: 0.9922\n",
            "Epoch 182: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.8199 - accuracy: 0.9864 - val_loss: 1.4861 - val_accuracy: 0.7527\n",
            "Epoch 183/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8254 - accuracy: 0.9844\n",
            "Epoch 183: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.8125 - accuracy: 0.9864 - val_loss: 1.4800 - val_accuracy: 0.7634\n",
            "Epoch 184/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8071 - accuracy: 0.9766\n",
            "Epoch 184: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.8024 - accuracy: 0.9864 - val_loss: 1.4718 - val_accuracy: 0.7742\n",
            "Epoch 185/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8017 - accuracy: 0.9844\n",
            "Epoch 185: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.7921 - accuracy: 0.9864 - val_loss: 1.4621 - val_accuracy: 0.7742\n",
            "Epoch 186/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7617 - accuracy: 1.0000\n",
            "Epoch 186: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.7832 - accuracy: 0.9864 - val_loss: 1.4535 - val_accuracy: 0.7634\n",
            "Epoch 187/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7920 - accuracy: 0.9766\n",
            "Epoch 187: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.7741 - accuracy: 0.9864 - val_loss: 1.4485 - val_accuracy: 0.7634\n",
            "Epoch 188/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7885 - accuracy: 0.9766\n",
            "Epoch 188: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.7644 - accuracy: 0.9864 - val_loss: 1.4404 - val_accuracy: 0.7742\n",
            "Epoch 189/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7754 - accuracy: 0.9688\n",
            "Epoch 189: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.7591 - accuracy: 0.9864 - val_loss: 1.4347 - val_accuracy: 0.7742\n",
            "Epoch 190/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7464 - accuracy: 0.9844\n",
            "Epoch 190: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.7473 - accuracy: 0.9864 - val_loss: 1.4288 - val_accuracy: 0.7742\n",
            "Epoch 191/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7238 - accuracy: 1.0000\n",
            "Epoch 191: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.7350 - accuracy: 0.9864 - val_loss: 1.4259 - val_accuracy: 0.7742\n",
            "Epoch 192/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7298 - accuracy: 0.9864\n",
            "Epoch 192: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 128ms/step - loss: 0.7298 - accuracy: 0.9864 - val_loss: 1.4206 - val_accuracy: 0.7742\n",
            "Epoch 193/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7258 - accuracy: 0.9844\n",
            "Epoch 193: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7219 - accuracy: 0.9864 - val_loss: 1.4103 - val_accuracy: 0.7742\n",
            "Epoch 194/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7138 - accuracy: 0.9864\n",
            "Epoch 194: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.7138 - accuracy: 0.9864 - val_loss: 1.3970 - val_accuracy: 0.7742\n",
            "Epoch 195/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6936 - accuracy: 1.0000\n",
            "Epoch 195: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.7096 - accuracy: 0.9864 - val_loss: 1.3868 - val_accuracy: 0.7634\n",
            "Epoch 196/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.9864\n",
            "Epoch 196: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.6941 - accuracy: 0.9864 - val_loss: 1.3768 - val_accuracy: 0.7634\n",
            "Epoch 197/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7079 - accuracy: 0.9766\n",
            "Epoch 197: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6910 - accuracy: 0.9864 - val_loss: 1.3678 - val_accuracy: 0.7634\n",
            "Epoch 198/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6710 - accuracy: 0.9922\n",
            "Epoch 198: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.6818 - accuracy: 0.9864 - val_loss: 1.3559 - val_accuracy: 0.7634\n",
            "Epoch 199/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6981 - accuracy: 0.9766\n",
            "Epoch 199: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6777 - accuracy: 0.9864 - val_loss: 1.3465 - val_accuracy: 0.7634\n",
            "Epoch 200/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6795 - accuracy: 0.9766\n",
            "Epoch 200: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6675 - accuracy: 0.9864 - val_loss: 1.3403 - val_accuracy: 0.7634\n",
            "Epoch 201/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6548 - accuracy: 0.9844\n",
            "Epoch 201: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6580 - accuracy: 0.9864 - val_loss: 1.3410 - val_accuracy: 0.7742\n",
            "Epoch 202/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6435 - accuracy: 0.9922\n",
            "Epoch 202: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6531 - accuracy: 0.9837 - val_loss: 1.3403 - val_accuracy: 0.7742\n",
            "Epoch 203/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6384 - accuracy: 0.9922\n",
            "Epoch 203: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6456 - accuracy: 0.9837 - val_loss: 1.3422 - val_accuracy: 0.7742\n",
            "Epoch 204/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6329 - accuracy: 0.9922\n",
            "Epoch 204: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6418 - accuracy: 0.9864 - val_loss: 1.3452 - val_accuracy: 0.7849\n",
            "Epoch 205/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6143 - accuracy: 1.0000\n",
            "Epoch 205: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.6273 - accuracy: 0.9864 - val_loss: 1.3452 - val_accuracy: 0.7742\n",
            "Epoch 206/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6154 - accuracy: 0.9922\n",
            "Epoch 206: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6232 - accuracy: 0.9864 - val_loss: 1.3326 - val_accuracy: 0.7742\n",
            "Epoch 207/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6192 - accuracy: 0.9844\n",
            "Epoch 207: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.6168 - accuracy: 0.9864 - val_loss: 1.3223 - val_accuracy: 0.7849\n",
            "Epoch 208/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6108 - accuracy: 0.9922\n",
            "Epoch 208: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6112 - accuracy: 0.9864 - val_loss: 1.3060 - val_accuracy: 0.7849\n",
            "Epoch 209/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5956 - accuracy: 0.9922\n",
            "Epoch 209: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6066 - accuracy: 0.9864 - val_loss: 1.2869 - val_accuracy: 0.7849\n",
            "Epoch 210/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6128 - accuracy: 0.9766\n",
            "Epoch 210: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5942 - accuracy: 0.9864 - val_loss: 1.2718 - val_accuracy: 0.7634\n",
            "Epoch 211/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5946 - accuracy: 0.9844\n",
            "Epoch 211: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5916 - accuracy: 0.9864 - val_loss: 1.2547 - val_accuracy: 0.7634\n",
            "Epoch 212/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5827 - accuracy: 0.9844\n",
            "Epoch 212: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5842 - accuracy: 0.9864 - val_loss: 1.2426 - val_accuracy: 0.7742\n",
            "Epoch 213/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5572 - accuracy: 1.0000\n",
            "Epoch 213: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5781 - accuracy: 0.9864 - val_loss: 1.2353 - val_accuracy: 0.7742\n",
            "Epoch 214/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5668 - accuracy: 0.9922\n",
            "Epoch 214: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5738 - accuracy: 0.9864 - val_loss: 1.2324 - val_accuracy: 0.7849\n",
            "Epoch 215/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5608 - accuracy: 0.9844\n",
            "Epoch 215: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5666 - accuracy: 0.9864 - val_loss: 1.2277 - val_accuracy: 0.7742\n",
            "Epoch 216/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5510 - accuracy: 0.9922\n",
            "Epoch 216: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5547 - accuracy: 0.9864 - val_loss: 1.2250 - val_accuracy: 0.7742\n",
            "Epoch 217/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5537 - accuracy: 0.9922\n",
            "Epoch 217: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5516 - accuracy: 0.9864 - val_loss: 1.2218 - val_accuracy: 0.7742\n",
            "Epoch 218/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5436 - accuracy: 0.9864\n",
            "Epoch 218: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.5436 - accuracy: 0.9864 - val_loss: 1.2186 - val_accuracy: 0.7742\n",
            "Epoch 219/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5363 - accuracy: 0.9844\n",
            "Epoch 219: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5374 - accuracy: 0.9864 - val_loss: 1.2149 - val_accuracy: 0.7742\n",
            "Epoch 220/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5315 - accuracy: 0.9864\n",
            "Epoch 220: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.5315 - accuracy: 0.9864 - val_loss: 1.2100 - val_accuracy: 0.7742\n",
            "Epoch 221/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5238 - accuracy: 0.9922\n",
            "Epoch 221: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.5266 - accuracy: 0.9864 - val_loss: 1.2084 - val_accuracy: 0.7742\n",
            "Epoch 222/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5022 - accuracy: 1.0000\n",
            "Epoch 222: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5240 - accuracy: 0.9864 - val_loss: 1.2073 - val_accuracy: 0.7742\n",
            "Epoch 223/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5301 - accuracy: 0.9766\n",
            "Epoch 223: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.5140 - accuracy: 0.9864 - val_loss: 1.2075 - val_accuracy: 0.7634\n",
            "Epoch 224/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5204 - accuracy: 0.9766\n",
            "Epoch 224: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5072 - accuracy: 0.9864 - val_loss: 1.2085 - val_accuracy: 0.7742\n",
            "Epoch 225/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5078 - accuracy: 0.9844\n",
            "Epoch 225: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5031 - accuracy: 0.9864 - val_loss: 1.2072 - val_accuracy: 0.7742\n",
            "Epoch 226/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4745 - accuracy: 1.0000\n",
            "Epoch 226: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.4942 - accuracy: 0.9864 - val_loss: 1.2054 - val_accuracy: 0.7742\n",
            "Epoch 227/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4955 - accuracy: 0.9844\n",
            "Epoch 227: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4899 - accuracy: 0.9864 - val_loss: 1.2023 - val_accuracy: 0.7742\n",
            "Epoch 228/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5060 - accuracy: 0.9688\n",
            "Epoch 228: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4882 - accuracy: 0.9864 - val_loss: 1.1960 - val_accuracy: 0.7742\n",
            "Epoch 229/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4727 - accuracy: 0.9844\n",
            "Epoch 229: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4812 - accuracy: 0.9864 - val_loss: 1.1940 - val_accuracy: 0.7742\n",
            "Epoch 230/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4890 - accuracy: 0.9766\n",
            "Epoch 230: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4795 - accuracy: 0.9864 - val_loss: 1.1891 - val_accuracy: 0.7742\n",
            "Epoch 231/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4864 - accuracy: 0.9766\n",
            "Epoch 231: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4698 - accuracy: 0.9864 - val_loss: 1.1834 - val_accuracy: 0.7742\n",
            "Epoch 232/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4770 - accuracy: 0.9844\n",
            "Epoch 232: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4689 - accuracy: 0.9864 - val_loss: 1.1765 - val_accuracy: 0.7742\n",
            "Epoch 233/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4747 - accuracy: 0.9766\n",
            "Epoch 233: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4577 - accuracy: 0.9864 - val_loss: 1.1695 - val_accuracy: 0.7634\n",
            "Epoch 234/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4584 - accuracy: 0.9864\n",
            "Epoch 234: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4584 - accuracy: 0.9864 - val_loss: 1.1638 - val_accuracy: 0.7634\n",
            "Epoch 235/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4450 - accuracy: 0.9891\n",
            "Epoch 235: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.4450 - accuracy: 0.9891 - val_loss: 1.1603 - val_accuracy: 0.7634\n",
            "Epoch 236/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4682 - accuracy: 0.9688\n",
            "Epoch 236: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4435 - accuracy: 0.9864 - val_loss: 1.1599 - val_accuracy: 0.7634\n",
            "Epoch 237/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4409 - accuracy: 0.9864\n",
            "Epoch 237: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.4409 - accuracy: 0.9864 - val_loss: 1.1574 - val_accuracy: 0.7634\n",
            "Epoch 238/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4231 - accuracy: 0.9922\n",
            "Epoch 238: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4370 - accuracy: 0.9864 - val_loss: 1.1563 - val_accuracy: 0.7634\n",
            "Epoch 239/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4185 - accuracy: 0.9922\n",
            "Epoch 239: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4374 - accuracy: 0.9837 - val_loss: 1.1507 - val_accuracy: 0.7634\n",
            "Epoch 240/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4108 - accuracy: 1.0000\n",
            "Epoch 240: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4305 - accuracy: 0.9864 - val_loss: 1.1458 - val_accuracy: 0.7849\n",
            "Epoch 241/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4200 - accuracy: 0.9864\n",
            "Epoch 241: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.4200 - accuracy: 0.9864 - val_loss: 1.1426 - val_accuracy: 0.7849\n",
            "Epoch 242/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4162 - accuracy: 0.9922\n",
            "Epoch 242: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4176 - accuracy: 0.9864 - val_loss: 1.1356 - val_accuracy: 0.7849\n",
            "Epoch 243/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4139 - accuracy: 0.9864\n",
            "Epoch 243: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4139 - accuracy: 0.9864 - val_loss: 1.1267 - val_accuracy: 0.7849\n",
            "Epoch 244/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4360 - accuracy: 0.9766\n",
            "Epoch 244: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4119 - accuracy: 0.9864 - val_loss: 1.1173 - val_accuracy: 0.7849\n",
            "Epoch 245/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3992 - accuracy: 0.9922\n",
            "Epoch 245: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4041 - accuracy: 0.9864 - val_loss: 1.1092 - val_accuracy: 0.7849\n",
            "Epoch 246/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4140 - accuracy: 0.9844\n",
            "Epoch 246: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4036 - accuracy: 0.9864 - val_loss: 1.1022 - val_accuracy: 0.7849\n",
            "Epoch 247/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4013 - accuracy: 0.9844\n",
            "Epoch 247: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3963 - accuracy: 0.9864 - val_loss: 1.0984 - val_accuracy: 0.7849\n",
            "Epoch 248/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3918 - accuracy: 0.9844\n",
            "Epoch 248: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3949 - accuracy: 0.9864 - val_loss: 1.0939 - val_accuracy: 0.7849\n",
            "Epoch 249/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3697 - accuracy: 1.0000\n",
            "Epoch 249: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3853 - accuracy: 0.9864 - val_loss: 1.0935 - val_accuracy: 0.7849\n",
            "Epoch 250/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3865 - accuracy: 0.9844\n",
            "Epoch 250: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3823 - accuracy: 0.9864 - val_loss: 1.0938 - val_accuracy: 0.7849\n",
            "Epoch 251/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3804 - accuracy: 0.9864\n",
            "Epoch 251: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 129ms/step - loss: 0.3804 - accuracy: 0.9864 - val_loss: 1.0933 - val_accuracy: 0.7849\n",
            "Epoch 252/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3683 - accuracy: 0.9922\n",
            "Epoch 252: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3748 - accuracy: 0.9864 - val_loss: 1.0911 - val_accuracy: 0.7849\n",
            "Epoch 253/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3718 - accuracy: 0.9844\n",
            "Epoch 253: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3685 - accuracy: 0.9864 - val_loss: 1.0893 - val_accuracy: 0.7849\n",
            "Epoch 254/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3659 - accuracy: 0.9844\n",
            "Epoch 254: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3676 - accuracy: 0.9864 - val_loss: 1.0894 - val_accuracy: 0.7742\n",
            "Epoch 255/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3636 - accuracy: 0.9844\n",
            "Epoch 255: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3615 - accuracy: 0.9864 - val_loss: 1.0904 - val_accuracy: 0.7742\n",
            "Epoch 256/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3769 - accuracy: 0.9766\n",
            "Epoch 256: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3595 - accuracy: 0.9864 - val_loss: 1.0878 - val_accuracy: 0.7634\n",
            "Epoch 257/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3499 - accuracy: 0.9844\n",
            "Epoch 257: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3515 - accuracy: 0.9864 - val_loss: 1.0844 - val_accuracy: 0.7634\n",
            "Epoch 258/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3578 - accuracy: 0.9844\n",
            "Epoch 258: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3508 - accuracy: 0.9864 - val_loss: 1.0808 - val_accuracy: 0.7634\n",
            "Epoch 259/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.9864\n",
            "Epoch 259: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3471 - accuracy: 0.9864 - val_loss: 1.0775 - val_accuracy: 0.7634\n",
            "Epoch 260/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3320 - accuracy: 0.9922\n",
            "Epoch 260: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3422 - accuracy: 0.9864 - val_loss: 1.0726 - val_accuracy: 0.7634\n",
            "Epoch 261/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3344 - accuracy: 0.9864\n",
            "Epoch 261: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3344 - accuracy: 0.9864 - val_loss: 1.0695 - val_accuracy: 0.7742\n",
            "Epoch 262/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3451 - accuracy: 0.9844\n",
            "Epoch 262: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3423 - accuracy: 0.9864 - val_loss: 1.0648 - val_accuracy: 0.7742\n",
            "Epoch 263/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3475 - accuracy: 0.9766\n",
            "Epoch 263: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3338 - accuracy: 0.9864 - val_loss: 1.0615 - val_accuracy: 0.7742\n",
            "Epoch 264/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3304 - accuracy: 0.9922\n",
            "Epoch 264: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3335 - accuracy: 0.9864 - val_loss: 1.0584 - val_accuracy: 0.7742\n",
            "Epoch 265/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3157 - accuracy: 0.9922\n",
            "Epoch 265: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3292 - accuracy: 0.9864 - val_loss: 1.0524 - val_accuracy: 0.7742\n",
            "Epoch 266/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3345 - accuracy: 0.9766\n",
            "Epoch 266: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3242 - accuracy: 0.9864 - val_loss: 1.0469 - val_accuracy: 0.7742\n",
            "Epoch 267/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2980 - accuracy: 1.0000\n",
            "Epoch 267: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3211 - accuracy: 0.9864 - val_loss: 1.0416 - val_accuracy: 0.7742\n",
            "Epoch 268/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2961 - accuracy: 1.0000\n",
            "Epoch 268: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3136 - accuracy: 0.9864 - val_loss: 1.0393 - val_accuracy: 0.7742\n",
            "Epoch 269/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3100 - accuracy: 0.9844\n",
            "Epoch 269: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3143 - accuracy: 0.9864 - val_loss: 1.0387 - val_accuracy: 0.7742\n",
            "Epoch 270/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2915 - accuracy: 1.0000\n",
            "Epoch 270: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3069 - accuracy: 0.9864 - val_loss: 1.0372 - val_accuracy: 0.7742\n",
            "Epoch 271/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.9864\n",
            "Epoch 271: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3066 - accuracy: 0.9864 - val_loss: 1.0391 - val_accuracy: 0.7742\n",
            "Epoch 272/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3004 - accuracy: 0.9844\n",
            "Epoch 272: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3022 - accuracy: 0.9864 - val_loss: 1.0392 - val_accuracy: 0.7634\n",
            "Epoch 273/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2956 - accuracy: 0.9922\n",
            "Epoch 273: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3019 - accuracy: 0.9864 - val_loss: 1.0401 - val_accuracy: 0.7742\n",
            "Epoch 274/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2985 - accuracy: 0.9922\n",
            "Epoch 274: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3008 - accuracy: 0.9864 - val_loss: 1.0357 - val_accuracy: 0.7742\n",
            "Epoch 275/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3013 - accuracy: 0.9844\n",
            "Epoch 275: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2953 - accuracy: 0.9864 - val_loss: 1.0287 - val_accuracy: 0.7957\n",
            "Epoch 276/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2976 - accuracy: 0.9766\n",
            "Epoch 276: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2912 - accuracy: 0.9864 - val_loss: 1.0226 - val_accuracy: 0.7957\n",
            "Epoch 277/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2772 - accuracy: 0.9922\n",
            "Epoch 277: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2853 - accuracy: 0.9864 - val_loss: 1.0208 - val_accuracy: 0.7957\n",
            "Epoch 278/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2840 - accuracy: 0.9864\n",
            "Epoch 278: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.2840 - accuracy: 0.9864 - val_loss: 1.0203 - val_accuracy: 0.7957\n",
            "Epoch 279/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2850 - accuracy: 0.9844\n",
            "Epoch 279: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2834 - accuracy: 0.9864 - val_loss: 1.0173 - val_accuracy: 0.7957\n",
            "Epoch 280/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2716 - accuracy: 0.9922\n",
            "Epoch 280: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2775 - accuracy: 0.9864 - val_loss: 1.0143 - val_accuracy: 0.7957\n",
            "Epoch 281/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2712 - accuracy: 0.9922\n",
            "Epoch 281: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2778 - accuracy: 0.9864 - val_loss: 1.0134 - val_accuracy: 0.7742\n",
            "Epoch 282/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2636 - accuracy: 0.9922\n",
            "Epoch 282: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2697 - accuracy: 0.9864 - val_loss: 1.0081 - val_accuracy: 0.7742\n",
            "Epoch 283/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2835 - accuracy: 0.9766\n",
            "Epoch 283: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2711 - accuracy: 0.9864 - val_loss: 0.9997 - val_accuracy: 0.7849\n",
            "Epoch 284/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2652 - accuracy: 0.9922\n",
            "Epoch 284: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2691 - accuracy: 0.9864 - val_loss: 0.9965 - val_accuracy: 0.7849\n",
            "Epoch 285/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2766 - accuracy: 0.9766\n",
            "Epoch 285: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2661 - accuracy: 0.9864 - val_loss: 0.9911 - val_accuracy: 0.7849\n",
            "Epoch 286/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2928 - accuracy: 0.9688\n",
            "Epoch 286: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2649 - accuracy: 0.9864 - val_loss: 0.9859 - val_accuracy: 0.7849\n",
            "Epoch 287/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2561 - accuracy: 0.9844\n",
            "Epoch 287: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2580 - accuracy: 0.9864 - val_loss: 0.9832 - val_accuracy: 0.7849\n",
            "Epoch 288/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2523 - accuracy: 0.9922\n",
            "Epoch 288: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2612 - accuracy: 0.9864 - val_loss: 0.9802 - val_accuracy: 0.7742\n",
            "Epoch 289/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2557 - accuracy: 0.9922\n",
            "Epoch 289: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2581 - accuracy: 0.9864 - val_loss: 0.9773 - val_accuracy: 0.7742\n",
            "Epoch 290/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2478 - accuracy: 0.9922\n",
            "Epoch 290: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2521 - accuracy: 0.9864 - val_loss: 0.9773 - val_accuracy: 0.7634\n",
            "Epoch 291/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2265 - accuracy: 1.0000\n",
            "Epoch 291: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2486 - accuracy: 0.9864 - val_loss: 0.9790 - val_accuracy: 0.7634\n",
            "Epoch 292/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2401 - accuracy: 0.9922\n",
            "Epoch 292: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.2461 - accuracy: 0.9864 - val_loss: 0.9803 - val_accuracy: 0.7634\n",
            "Epoch 293/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2285 - accuracy: 1.0000\n",
            "Epoch 293: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.2438 - accuracy: 0.9864 - val_loss: 0.9810 - val_accuracy: 0.7634\n",
            "Epoch 294/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2390 - accuracy: 0.9844\n",
            "Epoch 294: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2422 - accuracy: 0.9864 - val_loss: 0.9837 - val_accuracy: 0.7634\n",
            "Epoch 295/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2294 - accuracy: 0.9922\n",
            "Epoch 295: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2412 - accuracy: 0.9864 - val_loss: 0.9865 - val_accuracy: 0.7849\n",
            "Epoch 296/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2164 - accuracy: 1.0000\n",
            "Epoch 296: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2370 - accuracy: 0.9864 - val_loss: 0.9913 - val_accuracy: 0.7849\n",
            "Epoch 297/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2383 - accuracy: 0.9844\n",
            "Epoch 297: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2365 - accuracy: 0.9864 - val_loss: 0.9946 - val_accuracy: 0.7849\n",
            "Epoch 298/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2370 - accuracy: 0.9844\n",
            "Epoch 298: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.2329 - accuracy: 0.9864 - val_loss: 0.9931 - val_accuracy: 0.7849\n",
            "Epoch 299/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2262 - accuracy: 0.9844\n",
            "Epoch 299: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2295 - accuracy: 0.9864 - val_loss: 0.9931 - val_accuracy: 0.7849\n",
            "Epoch 300/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2306 - accuracy: 0.9844\n",
            "Epoch 300: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2323 - accuracy: 0.9864 - val_loss: 0.9910 - val_accuracy: 0.7849\n",
            "Epoch 301/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2154 - accuracy: 0.9922\n",
            "Epoch 301: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2272 - accuracy: 0.9864 - val_loss: 0.9888 - val_accuracy: 0.7849\n",
            "Epoch 302/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2112 - accuracy: 0.9922\n",
            "Epoch 302: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2228 - accuracy: 0.9864 - val_loss: 0.9849 - val_accuracy: 0.7742\n",
            "Epoch 303/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2172 - accuracy: 0.9864\n",
            "Epoch 303: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 118ms/step - loss: 0.2172 - accuracy: 0.9864 - val_loss: 0.9786 - val_accuracy: 0.7742\n",
            "Epoch 304/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2062 - accuracy: 0.9922\n",
            "Epoch 304: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2182 - accuracy: 0.9864 - val_loss: 0.9750 - val_accuracy: 0.7742\n",
            "Epoch 305/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2109 - accuracy: 0.9922\n",
            "Epoch 305: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2198 - accuracy: 0.9864 - val_loss: 0.9733 - val_accuracy: 0.7634\n",
            "Epoch 306/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2308 - accuracy: 0.9766\n",
            "Epoch 306: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2171 - accuracy: 0.9864 - val_loss: 0.9722 - val_accuracy: 0.7742\n",
            "Epoch 307/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2137 - accuracy: 0.9844\n",
            "Epoch 307: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2130 - accuracy: 0.9864 - val_loss: 0.9698 - val_accuracy: 0.7742\n",
            "Epoch 308/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1905 - accuracy: 1.0000\n",
            "Epoch 308: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2119 - accuracy: 0.9864 - val_loss: 0.9648 - val_accuracy: 0.7742\n",
            "Epoch 309/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1889 - accuracy: 1.0000\n",
            "Epoch 309: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2121 - accuracy: 0.9864 - val_loss: 0.9584 - val_accuracy: 0.7742\n",
            "Epoch 310/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2282 - accuracy: 0.9766\n",
            "Epoch 310: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2049 - accuracy: 0.9864 - val_loss: 0.9553 - val_accuracy: 0.7742\n",
            "Epoch 311/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1956 - accuracy: 0.9922\n",
            "Epoch 311: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2053 - accuracy: 0.9864 - val_loss: 0.9535 - val_accuracy: 0.7742\n",
            "Epoch 312/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2075 - accuracy: 0.9844\n",
            "Epoch 312: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2018 - accuracy: 0.9864 - val_loss: 0.9483 - val_accuracy: 0.7634\n",
            "Epoch 313/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1774 - accuracy: 1.0000\n",
            "Epoch 313: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2027 - accuracy: 0.9864 - val_loss: 0.9440 - val_accuracy: 0.7634\n",
            "Epoch 314/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1898 - accuracy: 0.9844\n",
            "Epoch 314: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.1982 - accuracy: 0.9864 - val_loss: 0.9385 - val_accuracy: 0.7634\n",
            "Epoch 315/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1975 - accuracy: 0.9864\n",
            "Epoch 315: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.1975 - accuracy: 0.9864 - val_loss: 0.9343 - val_accuracy: 0.7634\n",
            "Epoch 316/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1898 - accuracy: 0.9844\n",
            "Epoch 316: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.1935 - accuracy: 0.9864 - val_loss: 0.9314 - val_accuracy: 0.7634\n",
            "Epoch 317/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2042 - accuracy: 0.9688\n",
            "Epoch 317: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.1947 - accuracy: 0.9864 - val_loss: 0.9322 - val_accuracy: 0.7634\n",
            "Epoch 318/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1935 - accuracy: 0.9844\n",
            "Epoch 318: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1944 - accuracy: 0.9864 - val_loss: 0.9347 - val_accuracy: 0.7742\n",
            "Epoch 319/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2105 - accuracy: 0.9766\n",
            "Epoch 319: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1928 - accuracy: 0.9864 - val_loss: 0.9406 - val_accuracy: 0.7742\n",
            "Epoch 320/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1886 - accuracy: 0.9922\n",
            "Epoch 320: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.1889 - accuracy: 0.9864 - val_loss: 0.9420 - val_accuracy: 0.7742\n",
            "Epoch 321/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1649 - accuracy: 1.0000\n",
            "Epoch 321: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1847 - accuracy: 0.9864 - val_loss: 0.9420 - val_accuracy: 0.7742\n",
            "Epoch 322/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1660 - accuracy: 1.0000\n",
            "Epoch 322: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1837 - accuracy: 0.9864 - val_loss: 0.9401 - val_accuracy: 0.7742\n",
            "Epoch 323/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1904 - accuracy: 0.9766\n",
            "Epoch 323: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1870 - accuracy: 0.9864 - val_loss: 0.9388 - val_accuracy: 0.7742\n",
            "Epoch 324/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1791 - accuracy: 0.9922\n",
            "Epoch 324: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1842 - accuracy: 0.9864 - val_loss: 0.9374 - val_accuracy: 0.7742\n",
            "Epoch 325/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1703 - accuracy: 0.9922\n",
            "Epoch 325: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1795 - accuracy: 0.9864 - val_loss: 0.9332 - val_accuracy: 0.7742\n",
            "Epoch 326/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2063 - accuracy: 0.9766\n",
            "Epoch 326: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1796 - accuracy: 0.9864 - val_loss: 0.9309 - val_accuracy: 0.7849\n",
            "Epoch 327/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1555 - accuracy: 1.0000\n",
            "Epoch 327: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1763 - accuracy: 0.9864 - val_loss: 0.9304 - val_accuracy: 0.7849\n",
            "Epoch 328/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1828 - accuracy: 0.9922\n",
            "Epoch 328: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1766 - accuracy: 0.9864 - val_loss: 0.9292 - val_accuracy: 0.7742\n",
            "Epoch 329/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1788 - accuracy: 0.9844\n",
            "Epoch 329: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1736 - accuracy: 0.9864 - val_loss: 0.9251 - val_accuracy: 0.7742\n",
            "Epoch 330/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1770 - accuracy: 0.9844\n",
            "Epoch 330: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1722 - accuracy: 0.9864 - val_loss: 0.9205 - val_accuracy: 0.7742\n",
            "Epoch 331/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1743 - accuracy: 0.9844\n",
            "Epoch 331: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1696 - accuracy: 0.9864 - val_loss: 0.9155 - val_accuracy: 0.7742\n",
            "Epoch 332/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1735 - accuracy: 0.9844\n",
            "Epoch 332: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1686 - accuracy: 0.9864 - val_loss: 0.9106 - val_accuracy: 0.7634\n",
            "Epoch 333/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1699 - accuracy: 0.9766\n",
            "Epoch 333: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1720 - accuracy: 0.9864 - val_loss: 0.9087 - val_accuracy: 0.7634\n",
            "Epoch 334/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1643 - accuracy: 0.9922\n",
            "Epoch 334: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1684 - accuracy: 0.9864 - val_loss: 0.9080 - val_accuracy: 0.7527\n",
            "Epoch 335/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1464 - accuracy: 1.0000\n",
            "Epoch 335: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1665 - accuracy: 0.9864 - val_loss: 0.9070 - val_accuracy: 0.7527\n",
            "Epoch 336/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1755 - accuracy: 0.9844\n",
            "Epoch 336: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1647 - accuracy: 0.9864 - val_loss: 0.9082 - val_accuracy: 0.7634\n",
            "Epoch 337/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1534 - accuracy: 0.9922\n",
            "Epoch 337: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1610 - accuracy: 0.9864 - val_loss: 0.9027 - val_accuracy: 0.7742\n",
            "Epoch 338/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1554 - accuracy: 0.9922\n",
            "Epoch 338: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1603 - accuracy: 0.9864 - val_loss: 0.8971 - val_accuracy: 0.7742\n",
            "Epoch 339/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1584 - accuracy: 0.9844\n",
            "Epoch 339: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1592 - accuracy: 0.9864 - val_loss: 0.8945 - val_accuracy: 0.7742\n",
            "Epoch 340/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1729 - accuracy: 0.9766\n",
            "Epoch 340: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1567 - accuracy: 0.9864 - val_loss: 0.8931 - val_accuracy: 0.7742\n",
            "Epoch 341/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1547 - accuracy: 0.9844\n",
            "Epoch 341: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1561 - accuracy: 0.9864 - val_loss: 0.8939 - val_accuracy: 0.7742\n",
            "Epoch 342/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1530 - accuracy: 0.9844\n",
            "Epoch 342: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1552 - accuracy: 0.9864 - val_loss: 0.8961 - val_accuracy: 0.7742\n",
            "Epoch 343/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1696 - accuracy: 0.9766\n",
            "Epoch 343: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1541 - accuracy: 0.9864 - val_loss: 0.8972 - val_accuracy: 0.7742\n",
            "Epoch 344/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1460 - accuracy: 0.9922\n",
            "Epoch 344: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1511 - accuracy: 0.9864 - val_loss: 0.8980 - val_accuracy: 0.7742\n",
            "Epoch 345/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1542 - accuracy: 0.9844\n",
            "Epoch 345: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.1507 - accuracy: 0.9864 - val_loss: 0.8984 - val_accuracy: 0.7742\n",
            "Epoch 346/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1396 - accuracy: 0.9922\n",
            "Epoch 346: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1491 - accuracy: 0.9864 - val_loss: 0.8979 - val_accuracy: 0.7742\n",
            "Epoch 347/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1293 - accuracy: 1.0000\n",
            "Epoch 347: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1470 - accuracy: 0.9864 - val_loss: 0.9005 - val_accuracy: 0.7742\n",
            "Epoch 348/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1441 - accuracy: 0.9922\n",
            "Epoch 348: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1461 - accuracy: 0.9864 - val_loss: 0.9039 - val_accuracy: 0.7742\n",
            "Epoch 349/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1499 - accuracy: 0.9844\n",
            "Epoch 349: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1436 - accuracy: 0.9864 - val_loss: 0.9069 - val_accuracy: 0.7742\n",
            "Epoch 350/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1411 - accuracy: 0.9844\n",
            "Epoch 350: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1428 - accuracy: 0.9864 - val_loss: 0.9085 - val_accuracy: 0.7742\n",
            "Epoch 351/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1583 - accuracy: 0.9688\n",
            "Epoch 351: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1404 - accuracy: 0.9864 - val_loss: 0.9080 - val_accuracy: 0.7742\n",
            "Epoch 352/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1439 - accuracy: 0.9844\n",
            "Epoch 352: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1411 - accuracy: 0.9864 - val_loss: 0.9045 - val_accuracy: 0.7742\n",
            "Epoch 353/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1319 - accuracy: 0.9922\n",
            "Epoch 353: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1400 - accuracy: 0.9864 - val_loss: 0.9014 - val_accuracy: 0.7742\n",
            "Epoch 354/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1424 - accuracy: 0.9844\n",
            "Epoch 354: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1373 - accuracy: 0.9864 - val_loss: 0.8992 - val_accuracy: 0.7742\n",
            "Epoch 355/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1466 - accuracy: 0.9766\n",
            "Epoch 355: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1365 - accuracy: 0.9864 - val_loss: 0.8962 - val_accuracy: 0.7742\n",
            "Epoch 356/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1401 - accuracy: 0.9844\n",
            "Epoch 356: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1337 - accuracy: 0.9864 - val_loss: 0.8949 - val_accuracy: 0.7742\n",
            "Epoch 357/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1436 - accuracy: 0.9766\n",
            "Epoch 357: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1343 - accuracy: 0.9864 - val_loss: 0.8918 - val_accuracy: 0.7742\n",
            "Epoch 358/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1187 - accuracy: 1.0000\n",
            "Epoch 358: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1358 - accuracy: 0.9864 - val_loss: 0.8889 - val_accuracy: 0.7634\n",
            "Epoch 359/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1344 - accuracy: 0.9805\n",
            "Epoch 359: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1342 - accuracy: 0.9864 - val_loss: 0.8818 - val_accuracy: 0.7634\n",
            "Epoch 360/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1439 - accuracy: 0.9766\n",
            "Epoch 360: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1369 - accuracy: 0.9837 - val_loss: 0.8804 - val_accuracy: 0.7634\n",
            "Epoch 361/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1395 - accuracy: 0.9766\n",
            "Epoch 361: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1331 - accuracy: 0.9864 - val_loss: 0.8828 - val_accuracy: 0.7742\n",
            "Epoch 362/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1333 - accuracy: 0.9844\n",
            "Epoch 362: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1304 - accuracy: 0.9864 - val_loss: 0.8840 - val_accuracy: 0.7742\n",
            "Epoch 363/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1192 - accuracy: 0.9922\n",
            "Epoch 363: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1333 - accuracy: 0.9864 - val_loss: 0.8814 - val_accuracy: 0.7742\n",
            "Epoch 364/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1254 - accuracy: 0.9844\n",
            "Epoch 364: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.1266 - accuracy: 0.9864 - val_loss: 0.8780 - val_accuracy: 0.7742\n",
            "Epoch 365/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1233 - accuracy: 0.9922\n",
            "Epoch 365: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1261 - accuracy: 0.9864 - val_loss: 0.8765 - val_accuracy: 0.7742\n",
            "Epoch 366/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1153 - accuracy: 1.0000\n",
            "Epoch 366: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1272 - accuracy: 0.9864 - val_loss: 0.8717 - val_accuracy: 0.7742\n",
            "Epoch 367/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1111 - accuracy: 1.0000\n",
            "Epoch 367: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1246 - accuracy: 0.9864 - val_loss: 0.8656 - val_accuracy: 0.7634\n",
            "Epoch 368/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1240 - accuracy: 0.9922\n",
            "Epoch 368: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1241 - accuracy: 0.9864 - val_loss: 0.8627 - val_accuracy: 0.7634\n",
            "Epoch 369/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1279 - accuracy: 0.9844\n",
            "Epoch 369: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1242 - accuracy: 0.9864 - val_loss: 0.8571 - val_accuracy: 0.7634\n",
            "Epoch 370/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1303 - accuracy: 0.9766\n",
            "Epoch 370: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1184 - accuracy: 0.9864 - val_loss: 0.8572 - val_accuracy: 0.7634\n",
            "Epoch 371/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1073 - accuracy: 0.9922\n",
            "Epoch 371: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1198 - accuracy: 0.9864 - val_loss: 0.8566 - val_accuracy: 0.7634\n",
            "Epoch 372/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1230 - accuracy: 0.9844\n",
            "Epoch 372: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1225 - accuracy: 0.9864 - val_loss: 0.8604 - val_accuracy: 0.7634\n",
            "Epoch 373/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1262 - accuracy: 0.9766\n",
            "Epoch 373: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1194 - accuracy: 0.9864 - val_loss: 0.8631 - val_accuracy: 0.7634\n",
            "Epoch 374/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0983 - accuracy: 1.0000\n",
            "Epoch 374: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1189 - accuracy: 0.9864 - val_loss: 0.8648 - val_accuracy: 0.7634\n",
            "Epoch 375/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.9864\n",
            "Epoch 375: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1207 - accuracy: 0.9864 - val_loss: 0.8622 - val_accuracy: 0.7527\n",
            "Epoch 376/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1155 - accuracy: 0.9844\n",
            "Epoch 376: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1158 - accuracy: 0.9864 - val_loss: 0.8621 - val_accuracy: 0.7527\n",
            "Epoch 377/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1058 - accuracy: 0.9922\n",
            "Epoch 377: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1177 - accuracy: 0.9864 - val_loss: 0.8605 - val_accuracy: 0.7527\n",
            "Epoch 378/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1169 - accuracy: 0.9844\n",
            "Epoch 378: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1164 - accuracy: 0.9864 - val_loss: 0.8593 - val_accuracy: 0.7527\n",
            "Epoch 379/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1041 - accuracy: 0.9922\n",
            "Epoch 379: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1136 - accuracy: 0.9864 - val_loss: 0.8579 - val_accuracy: 0.7527\n",
            "Epoch 380/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1174 - accuracy: 0.9766\n",
            "Epoch 380: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1133 - accuracy: 0.9864 - val_loss: 0.8588 - val_accuracy: 0.7527\n",
            "Epoch 381/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1222 - accuracy: 0.9766\n",
            "Epoch 381: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1089 - accuracy: 0.9864 - val_loss: 0.8611 - val_accuracy: 0.7527\n",
            "Epoch 382/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1179 - accuracy: 0.9844\n",
            "Epoch 382: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1117 - accuracy: 0.9864 - val_loss: 0.8630 - val_accuracy: 0.7527\n",
            "Epoch 383/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1028 - accuracy: 0.9922\n",
            "Epoch 383: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1149 - accuracy: 0.9864 - val_loss: 0.8634 - val_accuracy: 0.7527\n",
            "Epoch 384/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1117 - accuracy: 0.9922\n",
            "Epoch 384: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1088 - accuracy: 0.9864 - val_loss: 0.8615 - val_accuracy: 0.7527\n",
            "Epoch 385/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1024 - accuracy: 0.9922\n",
            "Epoch 385: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1103 - accuracy: 0.9864 - val_loss: 0.8620 - val_accuracy: 0.7527\n",
            "Epoch 386/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0992 - accuracy: 1.0000\n",
            "Epoch 386: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1116 - accuracy: 0.9864 - val_loss: 0.8610 - val_accuracy: 0.7634\n",
            "Epoch 387/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1127 - accuracy: 0.9844\n",
            "Epoch 387: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1100 - accuracy: 0.9864 - val_loss: 0.8567 - val_accuracy: 0.7634\n",
            "Epoch 388/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1096 - accuracy: 0.9844\n",
            "Epoch 388: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1069 - accuracy: 0.9864 - val_loss: 0.8550 - val_accuracy: 0.7634\n",
            "Epoch 389/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1199 - accuracy: 0.9766\n",
            "Epoch 389: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1082 - accuracy: 0.9864 - val_loss: 0.8561 - val_accuracy: 0.7634\n",
            "Epoch 390/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1064 - accuracy: 0.9844\n",
            "Epoch 390: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1057 - accuracy: 0.9864 - val_loss: 0.8543 - val_accuracy: 0.7634\n",
            "Epoch 391/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1173 - accuracy: 0.9766\n",
            "Epoch 391: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1044 - accuracy: 0.9864 - val_loss: 0.8552 - val_accuracy: 0.7634\n",
            "Epoch 392/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1040 - accuracy: 0.9922\n",
            "Epoch 392: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1037 - accuracy: 0.9864 - val_loss: 0.8543 - val_accuracy: 0.7634\n",
            "Epoch 393/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0991 - accuracy: 0.9922\n",
            "Epoch 393: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1024 - accuracy: 0.9864 - val_loss: 0.8530 - val_accuracy: 0.7527\n",
            "Epoch 394/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0954 - accuracy: 0.9922\n",
            "Epoch 394: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1050 - accuracy: 0.9864 - val_loss: 0.8510 - val_accuracy: 0.7634\n",
            "Epoch 395/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1008 - accuracy: 0.9844\n",
            "Epoch 395: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0993 - accuracy: 0.9864 - val_loss: 0.8485 - val_accuracy: 0.7634\n",
            "Epoch 396/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0956 - accuracy: 0.9844\n",
            "Epoch 396: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0995 - accuracy: 0.9864 - val_loss: 0.8465 - val_accuracy: 0.7634\n",
            "Epoch 397/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9864\n",
            "Epoch 397: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1003 - accuracy: 0.9864 - val_loss: 0.8456 - val_accuracy: 0.7527\n",
            "Epoch 398/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0893 - accuracy: 0.9922\n",
            "Epoch 398: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0990 - accuracy: 0.9864 - val_loss: 0.8441 - val_accuracy: 0.7634\n",
            "Epoch 399/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0881 - accuracy: 0.9922\n",
            "Epoch 399: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0986 - accuracy: 0.9864 - val_loss: 0.8424 - val_accuracy: 0.7634\n",
            "Epoch 400/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0947 - accuracy: 0.9922\n",
            "Epoch 400: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1002 - accuracy: 0.9864 - val_loss: 0.8424 - val_accuracy: 0.7634\n",
            "Epoch 401/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0854 - accuracy: 0.9922\n",
            "Epoch 401: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0953 - accuracy: 0.9864 - val_loss: 0.8448 - val_accuracy: 0.7634\n",
            "Epoch 402/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0815 - accuracy: 1.0000\n",
            "Epoch 402: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0952 - accuracy: 0.9864 - val_loss: 0.8432 - val_accuracy: 0.7634\n",
            "Epoch 403/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0886 - accuracy: 0.9883\n",
            "Epoch 403: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0927 - accuracy: 0.9864 - val_loss: 0.8395 - val_accuracy: 0.7634\n",
            "Epoch 404/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0966 - accuracy: 0.9922\n",
            "Epoch 404: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0975 - accuracy: 0.9864 - val_loss: 0.8369 - val_accuracy: 0.7634\n",
            "Epoch 405/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0875 - accuracy: 0.9844\n",
            "Epoch 405: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0948 - accuracy: 0.9864 - val_loss: 0.8351 - val_accuracy: 0.7527\n",
            "Epoch 406/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0813 - accuracy: 0.9922\n",
            "Epoch 406: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0926 - accuracy: 0.9864 - val_loss: 0.8391 - val_accuracy: 0.7527\n",
            "Epoch 407/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0735 - accuracy: 1.0000\n",
            "Epoch 407: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0931 - accuracy: 0.9864 - val_loss: 0.8419 - val_accuracy: 0.7527\n",
            "Epoch 408/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0792 - accuracy: 0.9922\n",
            "Epoch 408: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0930 - accuracy: 0.9864 - val_loss: 0.8441 - val_accuracy: 0.7527\n",
            "Epoch 409/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0961 - accuracy: 0.9844\n",
            "Epoch 409: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0933 - accuracy: 0.9864 - val_loss: 0.8438 - val_accuracy: 0.7527\n",
            "Epoch 410/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1168 - accuracy: 0.9688\n",
            "Epoch 410: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1001 - accuracy: 0.9864 - val_loss: 0.8443 - val_accuracy: 0.7527\n",
            "Epoch 411/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0899 - accuracy: 0.9844\n",
            "Epoch 411: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0915 - accuracy: 0.9864 - val_loss: 0.8426 - val_accuracy: 0.7527\n",
            "Epoch 412/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0857 - accuracy: 0.9922\n",
            "Epoch 412: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0923 - accuracy: 0.9864 - val_loss: 0.8401 - val_accuracy: 0.7527\n",
            "Epoch 413/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0800 - accuracy: 1.0000\n",
            "Epoch 413: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0914 - accuracy: 0.9864 - val_loss: 0.8395 - val_accuracy: 0.7527\n",
            "Epoch 414/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0845 - accuracy: 0.9922\n",
            "Epoch 414: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0901 - accuracy: 0.9864 - val_loss: 0.8383 - val_accuracy: 0.7634\n",
            "Epoch 415/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9864\n",
            "Epoch 415: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 130ms/step - loss: 0.0914 - accuracy: 0.9864 - val_loss: 0.8345 - val_accuracy: 0.7634\n",
            "Epoch 416/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0725 - accuracy: 1.0000\n",
            "Epoch 416: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0887 - accuracy: 0.9864 - val_loss: 0.8321 - val_accuracy: 0.7634\n",
            "Epoch 417/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0704 - accuracy: 1.0000\n",
            "Epoch 417: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0890 - accuracy: 0.9864 - val_loss: 0.8301 - val_accuracy: 0.7634\n",
            "Epoch 418/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0931 - accuracy: 0.9766\n",
            "Epoch 418: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0871 - accuracy: 0.9864 - val_loss: 0.8288 - val_accuracy: 0.7634\n",
            "Epoch 419/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0914 - accuracy: 0.9844\n",
            "Epoch 419: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0858 - accuracy: 0.9864 - val_loss: 0.8274 - val_accuracy: 0.7634\n",
            "Epoch 420/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1013 - accuracy: 0.9766\n",
            "Epoch 420: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0835 - accuracy: 0.9864 - val_loss: 0.8293 - val_accuracy: 0.7634\n",
            "Epoch 421/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0695 - accuracy: 1.0000\n",
            "Epoch 421: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0887 - accuracy: 0.9864 - val_loss: 0.8341 - val_accuracy: 0.7634\n",
            "Epoch 422/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0663 - accuracy: 1.0000\n",
            "Epoch 422: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0851 - accuracy: 0.9864 - val_loss: 0.8380 - val_accuracy: 0.7634\n",
            "Epoch 423/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0910 - accuracy: 0.9844\n",
            "Epoch 423: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0836 - accuracy: 0.9864 - val_loss: 0.8383 - val_accuracy: 0.7634\n",
            "Epoch 424/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0621 - accuracy: 1.0000\n",
            "Epoch 424: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0876 - accuracy: 0.9864 - val_loss: 0.8391 - val_accuracy: 0.7634\n",
            "Epoch 425/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0759 - accuracy: 0.9844\n",
            "Epoch 425: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0837 - accuracy: 0.9864 - val_loss: 0.8418 - val_accuracy: 0.7634\n",
            "Epoch 426/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0947 - accuracy: 0.9766\n",
            "Epoch 426: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0836 - accuracy: 0.9864 - val_loss: 0.8439 - val_accuracy: 0.7634\n",
            "Epoch 427/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0800 - accuracy: 0.9844\n",
            "Epoch 427: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0836 - accuracy: 0.9864 - val_loss: 0.8428 - val_accuracy: 0.7634\n",
            "Epoch 428/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0871 - accuracy: 0.9844\n",
            "Epoch 428: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0841 - accuracy: 0.9864 - val_loss: 0.8428 - val_accuracy: 0.7634\n",
            "Epoch 429/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0742 - accuracy: 0.9922\n",
            "Epoch 429: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0813 - accuracy: 0.9864 - val_loss: 0.8405 - val_accuracy: 0.7634\n",
            "Epoch 430/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0605 - accuracy: 1.0000\n",
            "Epoch 430: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0801 - accuracy: 0.9864 - val_loss: 0.8375 - val_accuracy: 0.7634\n",
            "Epoch 431/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0661 - accuracy: 1.0000\n",
            "Epoch 431: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0792 - accuracy: 0.9864 - val_loss: 0.8373 - val_accuracy: 0.7634\n",
            "Epoch 432/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0729 - accuracy: 0.9922\n",
            "Epoch 432: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0801 - accuracy: 0.9864 - val_loss: 0.8331 - val_accuracy: 0.7634\n",
            "Epoch 433/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0839 - accuracy: 0.9844\n",
            "Epoch 433: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0828 - accuracy: 0.9864 - val_loss: 0.8309 - val_accuracy: 0.7634\n",
            "Epoch 434/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0856 - accuracy: 0.9688\n",
            "Epoch 434: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0770 - accuracy: 0.9864 - val_loss: 0.8284 - val_accuracy: 0.7634\n",
            "Epoch 435/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9864\n",
            "Epoch 435: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 144ms/step - loss: 0.0805 - accuracy: 0.9864 - val_loss: 0.8302 - val_accuracy: 0.7634\n",
            "Epoch 436/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0894 - accuracy: 0.9766\n",
            "Epoch 436: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0806 - accuracy: 0.9864 - val_loss: 0.8300 - val_accuracy: 0.7634\n",
            "Epoch 437/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0884 - accuracy: 0.9766\n",
            "Epoch 437: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0774 - accuracy: 0.9864 - val_loss: 0.8305 - val_accuracy: 0.7634\n",
            "Epoch 438/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0807 - accuracy: 0.9844\n",
            "Epoch 438: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0750 - accuracy: 0.9864 - val_loss: 0.8316 - val_accuracy: 0.7634\n",
            "Epoch 439/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0904 - accuracy: 0.9844\n",
            "Epoch 439: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0813 - accuracy: 0.9864 - val_loss: 0.8345 - val_accuracy: 0.7742\n",
            "Epoch 440/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0818 - accuracy: 0.9766\n",
            "Epoch 440: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0784 - accuracy: 0.9864 - val_loss: 0.8357 - val_accuracy: 0.7742\n",
            "Epoch 441/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0762 - accuracy: 0.9766\n",
            "Epoch 441: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0793 - accuracy: 0.9864 - val_loss: 0.8413 - val_accuracy: 0.7742\n",
            "Epoch 442/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0756 - accuracy: 0.9844\n",
            "Epoch 442: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0762 - accuracy: 0.9864 - val_loss: 0.8454 - val_accuracy: 0.7742\n",
            "Epoch 443/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0888 - accuracy: 0.9844\n",
            "Epoch 443: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0763 - accuracy: 0.9864 - val_loss: 0.8464 - val_accuracy: 0.7634\n",
            "Epoch 444/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0677 - accuracy: 0.9922\n",
            "Epoch 444: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0727 - accuracy: 0.9864 - val_loss: 0.8428 - val_accuracy: 0.7634\n",
            "Epoch 445/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0769 - accuracy: 0.9844\n",
            "Epoch 445: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0744 - accuracy: 0.9864 - val_loss: 0.8344 - val_accuracy: 0.7634\n",
            "Epoch 446/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0892 - accuracy: 0.9688\n",
            "Epoch 446: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0747 - accuracy: 0.9864 - val_loss: 0.8278 - val_accuracy: 0.7634\n",
            "Epoch 447/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9864\n",
            "Epoch 447: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.0739 - accuracy: 0.9864 - val_loss: 0.8220 - val_accuracy: 0.7527\n",
            "Epoch 448/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0893 - accuracy: 0.9688\n",
            "Epoch 448: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0770 - accuracy: 0.9864 - val_loss: 0.8158 - val_accuracy: 0.7527\n",
            "Epoch 449/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0885 - accuracy: 0.9688\n",
            "Epoch 449: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0741 - accuracy: 0.9864 - val_loss: 0.8101 - val_accuracy: 0.7527\n",
            "Epoch 450/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0880 - accuracy: 0.9688\n",
            "Epoch 450: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0738 - accuracy: 0.9864 - val_loss: 0.8093 - val_accuracy: 0.7634\n",
            "Epoch 451/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9864\n",
            "Epoch 451: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 0.0734 - accuracy: 0.9864 - val_loss: 0.8120 - val_accuracy: 0.7634\n",
            "Epoch 452/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0750 - accuracy: 0.9844\n",
            "Epoch 452: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0710 - accuracy: 0.9864 - val_loss: 0.8146 - val_accuracy: 0.7634\n",
            "Epoch 453/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0792 - accuracy: 0.9766\n",
            "Epoch 453: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0711 - accuracy: 0.9864 - val_loss: 0.8150 - val_accuracy: 0.7634\n",
            "Epoch 454/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0885 - accuracy: 0.9766\n",
            "Epoch 454: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0704 - accuracy: 0.9864 - val_loss: 0.8166 - val_accuracy: 0.7634\n",
            "Epoch 455/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0884 - accuracy: 0.9688\n",
            "Epoch 455: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0710 - accuracy: 0.9864 - val_loss: 0.8188 - val_accuracy: 0.7634\n",
            "Epoch 456/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0736 - accuracy: 0.9766\n",
            "Epoch 456: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0692 - accuracy: 0.9864 - val_loss: 0.8226 - val_accuracy: 0.7634\n",
            "Epoch 457/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0660 - accuracy: 0.9844\n",
            "Epoch 457: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0718 - accuracy: 0.9864 - val_loss: 0.8260 - val_accuracy: 0.7634\n",
            "Epoch 458/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0848 - accuracy: 0.9766\n",
            "Epoch 458: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0735 - accuracy: 0.9864 - val_loss: 0.8251 - val_accuracy: 0.7634\n",
            "Epoch 459/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0975 - accuracy: 0.9609\n",
            "Epoch 459: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0673 - accuracy: 0.9864 - val_loss: 0.8232 - val_accuracy: 0.7634\n",
            "Epoch 460/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0599 - accuracy: 1.0000\n",
            "Epoch 460: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0705 - accuracy: 0.9864 - val_loss: 0.8232 - val_accuracy: 0.7634\n",
            "Epoch 461/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0635 - accuracy: 0.9922\n",
            "Epoch 461: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0692 - accuracy: 0.9864 - val_loss: 0.8207 - val_accuracy: 0.7634\n",
            "Epoch 462/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0587 - accuracy: 0.9922\n",
            "Epoch 462: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0677 - accuracy: 0.9864 - val_loss: 0.8202 - val_accuracy: 0.7634\n",
            "Epoch 463/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0772 - accuracy: 0.9766\n",
            "Epoch 463: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0664 - accuracy: 0.9864 - val_loss: 0.8204 - val_accuracy: 0.7634\n",
            "Epoch 464/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0493 - accuracy: 1.0000\n",
            "Epoch 464: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0691 - accuracy: 0.9864 - val_loss: 0.8228 - val_accuracy: 0.7634\n",
            "Epoch 465/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0810 - accuracy: 0.9766\n",
            "Epoch 465: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0685 - accuracy: 0.9864 - val_loss: 0.8230 - val_accuracy: 0.7634\n",
            "Epoch 466/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9864\n",
            "Epoch 466: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 129ms/step - loss: 0.0651 - accuracy: 0.9864 - val_loss: 0.8239 - val_accuracy: 0.7634\n",
            "Epoch 467/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9922\n",
            "Epoch 467: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0649 - accuracy: 0.9864 - val_loss: 0.8255 - val_accuracy: 0.7634\n",
            "Epoch 468/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0663 - accuracy: 0.9844\n",
            "Epoch 468: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0689 - accuracy: 0.9864 - val_loss: 0.8250 - val_accuracy: 0.7527\n",
            "Epoch 469/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0574 - accuracy: 1.0000\n",
            "Epoch 469: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0697 - accuracy: 0.9864 - val_loss: 0.8239 - val_accuracy: 0.7634\n",
            "Epoch 470/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0681 - accuracy: 0.9922\n",
            "Epoch 470: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0663 - accuracy: 0.9864 - val_loss: 0.8202 - val_accuracy: 0.7634\n",
            "Epoch 471/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0556 - accuracy: 0.9922\n",
            "Epoch 471: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0649 - accuracy: 0.9864 - val_loss: 0.8197 - val_accuracy: 0.7527\n",
            "Epoch 472/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0736 - accuracy: 0.9844\n",
            "Epoch 472: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0656 - accuracy: 0.9864 - val_loss: 0.8201 - val_accuracy: 0.7527\n",
            "Epoch 473/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0524 - accuracy: 0.9922\n",
            "Epoch 473: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0652 - accuracy: 0.9864 - val_loss: 0.8211 - val_accuracy: 0.7634\n",
            "Epoch 474/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0824 - accuracy: 0.9688\n",
            "Epoch 474: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0629 - accuracy: 0.9864 - val_loss: 0.8201 - val_accuracy: 0.7634\n",
            "Epoch 475/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0523 - accuracy: 0.9922\n",
            "Epoch 475: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0644 - accuracy: 0.9864 - val_loss: 0.8188 - val_accuracy: 0.7634\n",
            "Epoch 476/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0499 - accuracy: 1.0000\n",
            "Epoch 476: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0641 - accuracy: 0.9864 - val_loss: 0.8186 - val_accuracy: 0.7527\n",
            "Epoch 477/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0713 - accuracy: 0.9844\n",
            "Epoch 477: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0650 - accuracy: 0.9864 - val_loss: 0.8178 - val_accuracy: 0.7527\n",
            "Epoch 478/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0400 - accuracy: 1.0000\n",
            "Epoch 478: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0630 - accuracy: 0.9864 - val_loss: 0.8197 - val_accuracy: 0.7527\n",
            "Epoch 479/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0535 - accuracy: 0.9922\n",
            "Epoch 479: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0642 - accuracy: 0.9864 - val_loss: 0.8250 - val_accuracy: 0.7527\n",
            "Epoch 480/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0419 - accuracy: 1.0000\n",
            "Epoch 480: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0629 - accuracy: 0.9864 - val_loss: 0.8298 - val_accuracy: 0.7527\n",
            "Epoch 481/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0571 - accuracy: 0.9922\n",
            "Epoch 481: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0655 - accuracy: 0.9864 - val_loss: 0.8304 - val_accuracy: 0.7527\n",
            "Epoch 482/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0744 - accuracy: 0.9766\n",
            "Epoch 482: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0623 - accuracy: 0.9864 - val_loss: 0.8300 - val_accuracy: 0.7634\n",
            "Epoch 483/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0600 - accuracy: 0.9922\n",
            "Epoch 483: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0645 - accuracy: 0.9864 - val_loss: 0.8310 - val_accuracy: 0.7634\n",
            "Epoch 484/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0556 - accuracy: 0.9844\n",
            "Epoch 484: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0630 - accuracy: 0.9864 - val_loss: 0.8325 - val_accuracy: 0.7634\n",
            "Epoch 485/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0668 - accuracy: 0.9766\n",
            "Epoch 485: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0589 - accuracy: 0.9864 - val_loss: 0.8322 - val_accuracy: 0.7634\n",
            "Epoch 486/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0589 - accuracy: 0.9922\n",
            "Epoch 486: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0616 - accuracy: 0.9864 - val_loss: 0.8327 - val_accuracy: 0.7634\n",
            "Epoch 487/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0650 - accuracy: 0.9844\n",
            "Epoch 487: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0645 - accuracy: 0.9864 - val_loss: 0.8297 - val_accuracy: 0.7634\n",
            "Epoch 488/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0583 - accuracy: 0.9844\n",
            "Epoch 488: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0596 - accuracy: 0.9864 - val_loss: 0.8290 - val_accuracy: 0.7634\n",
            "Epoch 489/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9864\n",
            "Epoch 489: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 127ms/step - loss: 0.0591 - accuracy: 0.9864 - val_loss: 0.8277 - val_accuracy: 0.7634\n",
            "Epoch 490/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0655 - accuracy: 0.9844\n",
            "Epoch 490: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0621 - accuracy: 0.9864 - val_loss: 0.8258 - val_accuracy: 0.7527\n",
            "Epoch 491/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0509 - accuracy: 0.9922\n",
            "Epoch 491: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0593 - accuracy: 0.9864 - val_loss: 0.8240 - val_accuracy: 0.7527\n",
            "Epoch 492/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0593 - accuracy: 0.9844\n",
            "Epoch 492: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0605 - accuracy: 0.9864 - val_loss: 0.8228 - val_accuracy: 0.7527\n",
            "Epoch 493/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0559 - accuracy: 0.9844\n",
            "Epoch 493: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0609 - accuracy: 0.9864 - val_loss: 0.8214 - val_accuracy: 0.7527\n",
            "Epoch 494/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0690 - accuracy: 0.9766\n",
            "Epoch 494: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0582 - accuracy: 0.9864 - val_loss: 0.8198 - val_accuracy: 0.7527\n",
            "Epoch 495/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0572 - accuracy: 0.9844\n",
            "Epoch 495: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0587 - accuracy: 0.9864 - val_loss: 0.8204 - val_accuracy: 0.7527\n",
            "Epoch 496/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0642 - accuracy: 0.9766\n",
            "Epoch 496: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0572 - accuracy: 0.9864 - val_loss: 0.8213 - val_accuracy: 0.7527\n",
            "Epoch 497/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0555 - accuracy: 0.9844\n",
            "Epoch 497: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0574 - accuracy: 0.9864 - val_loss: 0.8221 - val_accuracy: 0.7527\n",
            "Epoch 498/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0531 - accuracy: 0.9922\n",
            "Epoch 498: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0606 - accuracy: 0.9864 - val_loss: 0.8255 - val_accuracy: 0.7634\n",
            "Epoch 499/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0828 - accuracy: 0.9609\n",
            "Epoch 499: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0560 - accuracy: 0.9864 - val_loss: 0.8269 - val_accuracy: 0.7634\n",
            "Epoch 500/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0524 - accuracy: 0.9922\n",
            "Epoch 500: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0589 - accuracy: 0.9864 - val_loss: 0.8299 - val_accuracy: 0.7634\n",
            "Epoch 501/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0622 - accuracy: 0.9844\n",
            "Epoch 501: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0574 - accuracy: 0.9864 - val_loss: 0.8343 - val_accuracy: 0.7634\n",
            "Epoch 502/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0338 - accuracy: 1.0000\n",
            "Epoch 502: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0553 - accuracy: 0.9864 - val_loss: 0.8367 - val_accuracy: 0.7634\n",
            "Epoch 503/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0515 - accuracy: 0.9922\n",
            "Epoch 503: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0563 - accuracy: 0.9864 - val_loss: 0.8386 - val_accuracy: 0.7527\n",
            "Epoch 504/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0800 - accuracy: 0.9688\n",
            "Epoch 504: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0588 - accuracy: 0.9864 - val_loss: 0.8365 - val_accuracy: 0.7527\n",
            "Epoch 505/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0737 - accuracy: 0.9766\n",
            "Epoch 505: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0585 - accuracy: 0.9864 - val_loss: 0.8353 - val_accuracy: 0.7527\n",
            "Epoch 506/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0526 - accuracy: 0.9922\n",
            "Epoch 506: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0546 - accuracy: 0.9864 - val_loss: 0.8354 - val_accuracy: 0.7527\n",
            "Epoch 507/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0648 - accuracy: 0.9844\n",
            "Epoch 507: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0593 - accuracy: 0.9864 - val_loss: 0.8358 - val_accuracy: 0.7634\n",
            "Epoch 508/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0356 - accuracy: 1.0000\n",
            "Epoch 508: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0581 - accuracy: 0.9864 - val_loss: 0.8342 - val_accuracy: 0.7634\n",
            "Epoch 509/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0475 - accuracy: 1.0000\n",
            "Epoch 509: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0593 - accuracy: 0.9864 - val_loss: 0.8340 - val_accuracy: 0.7634\n",
            "Epoch 510/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0712 - accuracy: 0.9688\n",
            "Epoch 510: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0530 - accuracy: 0.9864 - val_loss: 0.8331 - val_accuracy: 0.7634\n",
            "Epoch 511/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0787 - accuracy: 0.9688\n",
            "Epoch 511: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0583 - accuracy: 0.9864 - val_loss: 0.8315 - val_accuracy: 0.7634\n",
            "Epoch 512/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0412 - accuracy: 0.9922\n",
            "Epoch 512: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0576 - accuracy: 0.9864 - val_loss: 0.8330 - val_accuracy: 0.7634\n",
            "Epoch 513/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0593 - accuracy: 0.9844\n",
            "Epoch 513: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0527 - accuracy: 0.9864 - val_loss: 0.8337 - val_accuracy: 0.7634\n",
            "Epoch 514/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0336 - accuracy: 1.0000\n",
            "Epoch 514: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0589 - accuracy: 0.9864 - val_loss: 0.8353 - val_accuracy: 0.7634\n",
            "Epoch 515/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0679 - accuracy: 0.9766\n",
            "Epoch 515: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0521 - accuracy: 0.9864 - val_loss: 0.8357 - val_accuracy: 0.7634\n",
            "Epoch 516/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0887 - accuracy: 0.9688\n",
            "Epoch 516: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0576 - accuracy: 0.9864 - val_loss: 0.8360 - val_accuracy: 0.7634\n",
            "Epoch 517/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0696 - accuracy: 0.9844\n",
            "Epoch 517: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0573 - accuracy: 0.9864 - val_loss: 0.8354 - val_accuracy: 0.7634\n",
            "Epoch 518/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0531 - accuracy: 0.9844\n",
            "Epoch 518: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0516 - accuracy: 0.9864 - val_loss: 0.8363 - val_accuracy: 0.7634\n",
            "Epoch 519/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0622 - accuracy: 0.9844\n",
            "Epoch 519: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0565 - accuracy: 0.9864 - val_loss: 0.8369 - val_accuracy: 0.7527\n",
            "Epoch 520/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0715 - accuracy: 0.9766\n",
            "Epoch 520: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0542 - accuracy: 0.9864 - val_loss: 0.8367 - val_accuracy: 0.7527\n",
            "Epoch 521/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0506 - accuracy: 0.9844\n",
            "Epoch 521: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0539 - accuracy: 0.9864 - val_loss: 0.8357 - val_accuracy: 0.7634\n",
            "Epoch 522/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0440 - accuracy: 0.9922\n",
            "Epoch 522: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0549 - accuracy: 0.9864 - val_loss: 0.8349 - val_accuracy: 0.7527\n",
            "Epoch 523/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9864\n",
            "Epoch 523: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 129ms/step - loss: 0.0505 - accuracy: 0.9864 - val_loss: 0.8349 - val_accuracy: 0.7634\n",
            "Epoch 524/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0445 - accuracy: 0.9922\n",
            "Epoch 524: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0547 - accuracy: 0.9864 - val_loss: 0.8334 - val_accuracy: 0.7634\n",
            "Epoch 525/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0623 - accuracy: 0.9766\n",
            "Epoch 525: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0548 - accuracy: 0.9864 - val_loss: 0.8318 - val_accuracy: 0.7634\n",
            "Epoch 526/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0490 - accuracy: 0.9922\n",
            "Epoch 526: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0514 - accuracy: 0.9864 - val_loss: 0.8332 - val_accuracy: 0.7634\n",
            "Epoch 527/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0720 - accuracy: 0.9766\n",
            "Epoch 527: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0542 - accuracy: 0.9864 - val_loss: 0.8322 - val_accuracy: 0.7634\n",
            "Epoch 528/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0381 - accuracy: 0.9922\n",
            "Epoch 528: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0531 - accuracy: 0.9864 - val_loss: 0.8310 - val_accuracy: 0.7634\n",
            "Epoch 529/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0415 - accuracy: 0.9922\n",
            "Epoch 529: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0512 - accuracy: 0.9864 - val_loss: 0.8313 - val_accuracy: 0.7634\n",
            "Epoch 530/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0441 - accuracy: 0.9922\n",
            "Epoch 530: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0527 - accuracy: 0.9864 - val_loss: 0.8308 - val_accuracy: 0.7634\n",
            "Epoch 531/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0353 - accuracy: 1.0000\n",
            "Epoch 531: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0518 - accuracy: 0.9864 - val_loss: 0.8291 - val_accuracy: 0.7634\n",
            "Epoch 532/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0444 - accuracy: 0.9922\n",
            "Epoch 532: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0520 - accuracy: 0.9864 - val_loss: 0.8268 - val_accuracy: 0.7634\n",
            "Epoch 533/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0469 - accuracy: 0.9922\n",
            "Epoch 533: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0545 - accuracy: 0.9864 - val_loss: 0.8250 - val_accuracy: 0.7634\n",
            "Epoch 534/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0692 - accuracy: 0.9766\n",
            "Epoch 534: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0515 - accuracy: 0.9864 - val_loss: 0.8236 - val_accuracy: 0.7527\n",
            "Epoch 535/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0506 - accuracy: 0.9844\n",
            "Epoch 535: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0500 - accuracy: 0.9864 - val_loss: 0.8232 - val_accuracy: 0.7527\n",
            "Epoch 536/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0848 - accuracy: 0.9688\n",
            "Epoch 536: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0552 - accuracy: 0.9864 - val_loss: 0.8239 - val_accuracy: 0.7527\n",
            "Epoch 537/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0451 - accuracy: 0.9844\n",
            "Epoch 537: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0510 - accuracy: 0.9864 - val_loss: 0.8262 - val_accuracy: 0.7634\n",
            "Epoch 538/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0454 - accuracy: 0.9844\n",
            "Epoch 538: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0517 - accuracy: 0.9864 - val_loss: 0.8270 - val_accuracy: 0.7634\n",
            "Epoch 539/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0615 - accuracy: 0.9922\n",
            "Epoch 539: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0531 - accuracy: 0.9864 - val_loss: 0.8277 - val_accuracy: 0.7634\n",
            "Epoch 540/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0764 - accuracy: 0.9688\n",
            "Epoch 540: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0506 - accuracy: 0.9864 - val_loss: 0.8274 - val_accuracy: 0.7634\n",
            "Epoch 541/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0686 - accuracy: 0.9688\n",
            "Epoch 541: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0503 - accuracy: 0.9864 - val_loss: 0.8281 - val_accuracy: 0.7634\n",
            "Epoch 542/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9864\n",
            "Epoch 542: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0532 - accuracy: 0.9864 - val_loss: 0.8284 - val_accuracy: 0.7634\n",
            "Epoch 543/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0372 - accuracy: 0.9922\n",
            "Epoch 543: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0490 - accuracy: 0.9864 - val_loss: 0.8276 - val_accuracy: 0.7634\n",
            "Epoch 544/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0426 - accuracy: 0.9922\n",
            "Epoch 544: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0475 - accuracy: 0.9864 - val_loss: 0.8275 - val_accuracy: 0.7634\n",
            "Epoch 545/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0315 - accuracy: 1.0000\n",
            "Epoch 545: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0499 - accuracy: 0.9864 - val_loss: 0.8271 - val_accuracy: 0.7634\n",
            "Epoch 546/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0782 - accuracy: 0.9688\n",
            "Epoch 546: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0509 - accuracy: 0.9864 - val_loss: 0.8260 - val_accuracy: 0.7634\n",
            "Epoch 547/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0361 - accuracy: 0.9922\n",
            "Epoch 547: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0501 - accuracy: 0.9864 - val_loss: 0.8259 - val_accuracy: 0.7634\n",
            "Epoch 548/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0469 - accuracy: 0.9922\n",
            "Epoch 548: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0513 - accuracy: 0.9864 - val_loss: 0.8245 - val_accuracy: 0.7527\n",
            "Epoch 549/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0513 - accuracy: 0.9844\n",
            "Epoch 549: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0520 - accuracy: 0.9864 - val_loss: 0.8247 - val_accuracy: 0.7527\n",
            "Epoch 550/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0559 - accuracy: 0.9844\n",
            "Epoch 550: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0529 - accuracy: 0.9864 - val_loss: 0.8229 - val_accuracy: 0.7527\n",
            "Epoch 551/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0560 - accuracy: 0.9844\n",
            "Epoch 551: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0564 - accuracy: 0.9864 - val_loss: 0.8234 - val_accuracy: 0.7634\n",
            "Epoch 552/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0491 - accuracy: 0.9922\n",
            "Epoch 552: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0519 - accuracy: 0.9864 - val_loss: 0.8246 - val_accuracy: 0.7634\n",
            "Epoch 553/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0474 - accuracy: 0.9883\n",
            "Epoch 553: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0504 - accuracy: 0.9864 - val_loss: 0.8236 - val_accuracy: 0.7634\n",
            "Epoch 554/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0644 - accuracy: 0.9688\n",
            "Epoch 554: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0517 - accuracy: 0.9864 - val_loss: 0.8228 - val_accuracy: 0.7634\n",
            "Epoch 555/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0490 - accuracy: 0.9844\n",
            "Epoch 555: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0509 - accuracy: 0.9864 - val_loss: 0.8218 - val_accuracy: 0.7742\n",
            "Epoch 556/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0398 - accuracy: 0.9922\n",
            "Epoch 556: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0521 - accuracy: 0.9864 - val_loss: 0.8166 - val_accuracy: 0.7742\n",
            "Epoch 557/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0550 - accuracy: 0.9766\n",
            "Epoch 557: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0484 - accuracy: 0.9864 - val_loss: 0.8150 - val_accuracy: 0.7742\n",
            "Epoch 558/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0603 - accuracy: 0.9766\n",
            "Epoch 558: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0465 - accuracy: 0.9864 - val_loss: 0.8147 - val_accuracy: 0.7742\n",
            "Epoch 559/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9864\n",
            "Epoch 559: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.0496 - accuracy: 0.9864 - val_loss: 0.8137 - val_accuracy: 0.7634\n",
            "Epoch 560/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0333 - accuracy: 0.9922\n",
            "Epoch 560: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0508 - accuracy: 0.9864 - val_loss: 0.8138 - val_accuracy: 0.7634\n",
            "Epoch 561/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0488 - accuracy: 0.9844\n",
            "Epoch 561: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0513 - accuracy: 0.9864 - val_loss: 0.8154 - val_accuracy: 0.7634\n",
            "Epoch 562/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0656 - accuracy: 0.9844\n",
            "Epoch 562: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0513 - accuracy: 0.9864 - val_loss: 0.8194 - val_accuracy: 0.7634\n",
            "Epoch 563/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0430 - accuracy: 0.9844\n",
            "Epoch 563: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0508 - accuracy: 0.9864 - val_loss: 0.8220 - val_accuracy: 0.7634\n",
            "Epoch 564/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0769 - accuracy: 0.9766\n",
            "Epoch 564: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0526 - accuracy: 0.9864 - val_loss: 0.8244 - val_accuracy: 0.7634\n",
            "Epoch 565/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0318 - accuracy: 1.0000\n",
            "Epoch 565: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0496 - accuracy: 0.9864 - val_loss: 0.8248 - val_accuracy: 0.7634\n",
            "Epoch 566/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0647 - accuracy: 0.9688\n",
            "Epoch 566: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0492 - accuracy: 0.9864 - val_loss: 0.8225 - val_accuracy: 0.7634\n",
            "Epoch 567/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0578 - accuracy: 0.9766\n",
            "Epoch 567: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0463 - accuracy: 0.9864 - val_loss: 0.8197 - val_accuracy: 0.7634\n",
            "Epoch 568/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0535 - accuracy: 0.9766\n",
            "Epoch 568: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0489 - accuracy: 0.9864 - val_loss: 0.8184 - val_accuracy: 0.7527\n",
            "Epoch 569/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0621 - accuracy: 0.9766\n",
            "Epoch 569: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0469 - accuracy: 0.9864 - val_loss: 0.8188 - val_accuracy: 0.7527\n",
            "Epoch 570/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0477 - accuracy: 0.9922\n",
            "Epoch 570: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0494 - accuracy: 0.9864 - val_loss: 0.8174 - val_accuracy: 0.7527\n",
            "Epoch 571/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0243 - accuracy: 1.0000\n",
            "Epoch 571: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0453 - accuracy: 0.9864 - val_loss: 0.8144 - val_accuracy: 0.7527\n",
            "Epoch 572/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0500 - accuracy: 0.9844\n",
            "Epoch 572: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0451 - accuracy: 0.9891 - val_loss: 0.8136 - val_accuracy: 0.7527\n",
            "Epoch 573/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0488 - accuracy: 0.9844\n",
            "Epoch 573: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0472 - accuracy: 0.9864 - val_loss: 0.8142 - val_accuracy: 0.7634\n",
            "Epoch 574/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
            "Epoch 574: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0490 - accuracy: 0.9864 - val_loss: 0.8177 - val_accuracy: 0.7634\n",
            "Epoch 575/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0571 - accuracy: 0.9766\n",
            "Epoch 575: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0493 - accuracy: 0.9864 - val_loss: 0.8195 - val_accuracy: 0.7634\n",
            "Epoch 576/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0703 - accuracy: 0.9844\n",
            "Epoch 576: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0519 - accuracy: 0.9864 - val_loss: 0.8280 - val_accuracy: 0.7634\n",
            "Epoch 577/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0490 - accuracy: 0.9844\n",
            "Epoch 577: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 0.0470 - accuracy: 0.9864 - val_loss: 0.8317 - val_accuracy: 0.7634\n",
            "Epoch 578/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9844\n",
            "Epoch 578: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0464 - accuracy: 0.9864 - val_loss: 0.8328 - val_accuracy: 0.7634\n",
            "Epoch 579/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0619 - accuracy: 0.9766\n",
            "Epoch 579: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0486 - accuracy: 0.9864 - val_loss: 0.8323 - val_accuracy: 0.7634\n",
            "Epoch 580/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0580 - accuracy: 0.9844\n",
            "Epoch 580: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0515 - accuracy: 0.9864 - val_loss: 0.8332 - val_accuracy: 0.7634\n",
            "Epoch 581/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0573 - accuracy: 0.9844\n",
            "Epoch 581: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0498 - accuracy: 0.9864 - val_loss: 0.8319 - val_accuracy: 0.7634\n",
            "Epoch 582/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0540 - accuracy: 0.9766\n",
            "Epoch 582: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0474 - accuracy: 0.9864 - val_loss: 0.8282 - val_accuracy: 0.7634\n",
            "Epoch 583/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0262 - accuracy: 1.0000\n",
            "Epoch 583: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0478 - accuracy: 0.9864 - val_loss: 0.8243 - val_accuracy: 0.7634\n",
            "Epoch 584/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
            "Epoch 584: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0476 - accuracy: 0.9864 - val_loss: 0.8190 - val_accuracy: 0.7634\n",
            "Epoch 585/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0364 - accuracy: 0.9922\n",
            "Epoch 585: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0460 - accuracy: 0.9864 - val_loss: 0.8163 - val_accuracy: 0.7634\n",
            "Epoch 586/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0636 - accuracy: 0.9766\n",
            "Epoch 586: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0528 - accuracy: 0.9837 - val_loss: 0.8122 - val_accuracy: 0.7634\n",
            "Epoch 587/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0599 - accuracy: 0.9844\n",
            "Epoch 587: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0504 - accuracy: 0.9864 - val_loss: 0.8132 - val_accuracy: 0.7634\n",
            "Epoch 588/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0396 - accuracy: 0.9844\n",
            "Epoch 588: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0452 - accuracy: 0.9864 - val_loss: 0.8162 - val_accuracy: 0.7634\n",
            "Epoch 589/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0509 - accuracy: 0.9766\n",
            "Epoch 589: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0446 - accuracy: 0.9864 - val_loss: 0.8178 - val_accuracy: 0.7634\n",
            "Epoch 590/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0455 - accuracy: 0.9844\n",
            "Epoch 590: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0466 - accuracy: 0.9864 - val_loss: 0.8209 - val_accuracy: 0.7634\n",
            "Epoch 591/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0603 - accuracy: 0.9844\n",
            "Epoch 591: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0458 - accuracy: 0.9864 - val_loss: 0.8241 - val_accuracy: 0.7634\n",
            "Epoch 592/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0351 - accuracy: 0.9922\n",
            "Epoch 592: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0456 - accuracy: 0.9864 - val_loss: 0.8254 - val_accuracy: 0.7634\n",
            "Epoch 593/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0422 - accuracy: 0.9844\n",
            "Epoch 593: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0454 - accuracy: 0.9864 - val_loss: 0.8263 - val_accuracy: 0.7634\n",
            "Epoch 594/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0653 - accuracy: 0.9766\n",
            "Epoch 594: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0500 - accuracy: 0.9864 - val_loss: 0.8289 - val_accuracy: 0.7634\n",
            "Epoch 595/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0301 - accuracy: 0.9922\n",
            "Epoch 595: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0462 - accuracy: 0.9864 - val_loss: 0.8337 - val_accuracy: 0.7634\n",
            "Epoch 596/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0503 - accuracy: 0.9844\n",
            "Epoch 596: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0413 - accuracy: 0.9864 - val_loss: 0.8386 - val_accuracy: 0.7634\n",
            "Epoch 597/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0356 - accuracy: 0.9922\n",
            "Epoch 597: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0466 - accuracy: 0.9864 - val_loss: 0.8402 - val_accuracy: 0.7634\n",
            "Epoch 598/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0456 - accuracy: 0.9922\n",
            "Epoch 598: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0488 - accuracy: 0.9864 - val_loss: 0.8385 - val_accuracy: 0.7634\n",
            "Epoch 599/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0680 - accuracy: 0.9688\n",
            "Epoch 599: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0429 - accuracy: 0.9864 - val_loss: 0.8331 - val_accuracy: 0.7634\n",
            "Epoch 600/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000\n",
            "Epoch 600: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0463 - accuracy: 0.9864 - val_loss: 0.8285 - val_accuracy: 0.7634\n",
            "Epoch 601/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0607 - accuracy: 0.9766\n",
            "Epoch 601: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0458 - accuracy: 0.9864 - val_loss: 0.8197 - val_accuracy: 0.7634\n",
            "Epoch 602/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0439 - accuracy: 0.9844\n",
            "Epoch 602: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0456 - accuracy: 0.9864 - val_loss: 0.8149 - val_accuracy: 0.7634\n",
            "Epoch 603/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0254 - accuracy: 1.0000\n",
            "Epoch 603: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0430 - accuracy: 0.9864 - val_loss: 0.8110 - val_accuracy: 0.7742\n",
            "Epoch 604/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0521 - accuracy: 0.9766\n",
            "Epoch 604: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0443 - accuracy: 0.9864 - val_loss: 0.8077 - val_accuracy: 0.7742\n",
            "Epoch 605/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0470 - accuracy: 0.9844\n",
            "Epoch 605: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0442 - accuracy: 0.9864 - val_loss: 0.8096 - val_accuracy: 0.7742\n",
            "Epoch 606/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0553 - accuracy: 0.9766\n",
            "Epoch 606: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0429 - accuracy: 0.9864 - val_loss: 0.8085 - val_accuracy: 0.7742\n",
            "Epoch 607/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0442 - accuracy: 0.9844\n",
            "Epoch 607: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0429 - accuracy: 0.9864 - val_loss: 0.8101 - val_accuracy: 0.7742\n",
            "Epoch 608/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0559 - accuracy: 0.9766\n",
            "Epoch 608: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 0.8147 - val_accuracy: 0.7742\n",
            "Epoch 609/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0428 - accuracy: 0.9844\n",
            "Epoch 609: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0475 - accuracy: 0.9864 - val_loss: 0.8171 - val_accuracy: 0.7742\n",
            "Epoch 610/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0510 - accuracy: 0.9844\n",
            "Epoch 610: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0451 - accuracy: 0.9864 - val_loss: 0.8225 - val_accuracy: 0.7742\n",
            "Epoch 611/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0385 - accuracy: 0.9922\n",
            "Epoch 611: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0474 - accuracy: 0.9864 - val_loss: 0.8261 - val_accuracy: 0.7742\n",
            "Epoch 612/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000\n",
            "Epoch 612: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0446 - accuracy: 0.9864 - val_loss: 0.8296 - val_accuracy: 0.7634\n",
            "Epoch 613/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0355 - accuracy: 0.9922\n",
            "Epoch 613: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0459 - accuracy: 0.9864 - val_loss: 0.8322 - val_accuracy: 0.7634\n",
            "Epoch 614/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9864\n",
            "Epoch 614: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.0436 - accuracy: 0.9864 - val_loss: 0.8302 - val_accuracy: 0.7634\n",
            "Epoch 615/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0403 - accuracy: 0.9844\n",
            "Epoch 615: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0432 - accuracy: 0.9864 - val_loss: 0.8313 - val_accuracy: 0.7634\n",
            "Epoch 616/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0545 - accuracy: 0.9766\n",
            "Epoch 616: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0472 - accuracy: 0.9864 - val_loss: 0.8283 - val_accuracy: 0.7634\n",
            "Epoch 617/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0307 - accuracy: 0.9922\n",
            "Epoch 617: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0426 - accuracy: 0.9864 - val_loss: 0.8290 - val_accuracy: 0.7634\n",
            "Epoch 618/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0688 - accuracy: 0.9688\n",
            "Epoch 618: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0426 - accuracy: 0.9864 - val_loss: 0.8294 - val_accuracy: 0.7634\n",
            "Epoch 619/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0503 - accuracy: 0.9766\n",
            "Epoch 619: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0442 - accuracy: 0.9864 - val_loss: 0.8300 - val_accuracy: 0.7634\n",
            "Epoch 620/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0426 - accuracy: 0.9844\n",
            "Epoch 620: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0444 - accuracy: 0.9864 - val_loss: 0.8320 - val_accuracy: 0.7634\n",
            "Epoch 621/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9864\n",
            "Epoch 621: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0434 - accuracy: 0.9864 - val_loss: 0.8325 - val_accuracy: 0.7634\n",
            "Epoch 622/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0464 - accuracy: 0.9844\n",
            "Epoch 622: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0459 - accuracy: 0.9864 - val_loss: 0.8300 - val_accuracy: 0.7634\n",
            "Epoch 623/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0512 - accuracy: 0.9844\n",
            "Epoch 623: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0435 - accuracy: 0.9864 - val_loss: 0.8232 - val_accuracy: 0.7634\n",
            "Epoch 624/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 624: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0487 - accuracy: 0.9864 - val_loss: 0.8233 - val_accuracy: 0.7634\n",
            "Epoch 625/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0341 - accuracy: 0.9922\n",
            "Epoch 625: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0467 - accuracy: 0.9864 - val_loss: 0.8229 - val_accuracy: 0.7634\n",
            "Epoch 626/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0561 - accuracy: 0.9766\n",
            "Epoch 626: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0436 - accuracy: 0.9864 - val_loss: 0.8231 - val_accuracy: 0.7634\n",
            "Epoch 627/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0472 - accuracy: 0.9844\n",
            "Epoch 627: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0454 - accuracy: 0.9864 - val_loss: 0.8232 - val_accuracy: 0.7634\n",
            "Epoch 628/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0588 - accuracy: 0.9766\n",
            "Epoch 628: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0453 - accuracy: 0.9864 - val_loss: 0.8222 - val_accuracy: 0.7634\n",
            "Epoch 629/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0574 - accuracy: 0.9688\n",
            "Epoch 629: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 0.8215 - val_accuracy: 0.7634\n",
            "Epoch 630/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0440 - accuracy: 0.9844\n",
            "Epoch 630: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0443 - accuracy: 0.9864 - val_loss: 0.8229 - val_accuracy: 0.7634\n",
            "Epoch 631/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0528 - accuracy: 0.9766\n",
            "Epoch 631: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0446 - accuracy: 0.9864 - val_loss: 0.8229 - val_accuracy: 0.7634\n",
            "Epoch 632/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0351 - accuracy: 0.9922\n",
            "Epoch 632: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0443 - accuracy: 0.9864 - val_loss: 0.8226 - val_accuracy: 0.7742\n",
            "Epoch 633/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0495 - accuracy: 0.9844\n",
            "Epoch 633: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0443 - accuracy: 0.9864 - val_loss: 0.8225 - val_accuracy: 0.7742\n",
            "Epoch 634/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0328 - accuracy: 0.9922\n",
            "Epoch 634: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0468 - accuracy: 0.9864 - val_loss: 0.8224 - val_accuracy: 0.7742\n",
            "Epoch 635/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0236 - accuracy: 1.0000\n",
            "Epoch 635: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0446 - accuracy: 0.9864 - val_loss: 0.8212 - val_accuracy: 0.7742\n",
            "Epoch 636/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0480 - accuracy: 0.9766\n",
            "Epoch 636: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0424 - accuracy: 0.9864 - val_loss: 0.8197 - val_accuracy: 0.7634\n",
            "Epoch 637/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0411 - accuracy: 0.9766\n",
            "Epoch 637: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 0.8202 - val_accuracy: 0.7742\n",
            "Epoch 638/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0641 - accuracy: 0.9844\n",
            "Epoch 638: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0474 - accuracy: 0.9864 - val_loss: 0.8191 - val_accuracy: 0.7634\n",
            "Epoch 639/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0576 - accuracy: 0.9766\n",
            "Epoch 639: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0441 - accuracy: 0.9864 - val_loss: 0.8200 - val_accuracy: 0.7634\n",
            "Epoch 640/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0540 - accuracy: 0.9844\n",
            "Epoch 640: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0448 - accuracy: 0.9864 - val_loss: 0.8175 - val_accuracy: 0.7742\n",
            "Epoch 641/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0456 - accuracy: 0.9844\n",
            "Epoch 641: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0468 - accuracy: 0.9864 - val_loss: 0.8170 - val_accuracy: 0.7742\n",
            "Epoch 642/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0442 - accuracy: 0.9844\n",
            "Epoch 642: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0440 - accuracy: 0.9864 - val_loss: 0.8170 - val_accuracy: 0.7742\n",
            "Epoch 643/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0557 - accuracy: 0.9766\n",
            "Epoch 643: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0441 - accuracy: 0.9864 - val_loss: 0.8181 - val_accuracy: 0.7742\n",
            "Epoch 644/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0555 - accuracy: 0.9766\n",
            "Epoch 644: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0436 - accuracy: 0.9864 - val_loss: 0.8188 - val_accuracy: 0.7634\n",
            "Epoch 645/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0406 - accuracy: 0.9922\n",
            "Epoch 645: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0432 - accuracy: 0.9864 - val_loss: 0.8230 - val_accuracy: 0.7634\n",
            "Epoch 646/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9922\n",
            "Epoch 646: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0445 - accuracy: 0.9864 - val_loss: 0.8220 - val_accuracy: 0.7634\n",
            "Epoch 647/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0337 - accuracy: 0.9922\n",
            "Epoch 647: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0414 - accuracy: 0.9864 - val_loss: 0.8224 - val_accuracy: 0.7634\n",
            "Epoch 648/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0443 - accuracy: 0.9922\n",
            "Epoch 648: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0431 - accuracy: 0.9864 - val_loss: 0.8247 - val_accuracy: 0.7634\n",
            "Epoch 649/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0442 - accuracy: 0.9844\n",
            "Epoch 649: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0418 - accuracy: 0.9864 - val_loss: 0.8250 - val_accuracy: 0.7634\n",
            "Epoch 650/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0287 - accuracy: 1.0000\n",
            "Epoch 650: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0445 - accuracy: 0.9864 - val_loss: 0.8253 - val_accuracy: 0.7634\n",
            "Epoch 651/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9864\n",
            "Epoch 651: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0427 - accuracy: 0.9864 - val_loss: 0.8260 - val_accuracy: 0.7634\n",
            "Epoch 652/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9864\n",
            "Epoch 652: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0421 - accuracy: 0.9864 - val_loss: 0.8259 - val_accuracy: 0.7634\n",
            "Epoch 653/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0635 - accuracy: 0.9688\n",
            "Epoch 653: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0427 - accuracy: 0.9864 - val_loss: 0.8207 - val_accuracy: 0.7634\n",
            "Epoch 654/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0443 - accuracy: 0.9844\n",
            "Epoch 654: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0422 - accuracy: 0.9864 - val_loss: 0.8199 - val_accuracy: 0.7634\n",
            "Epoch 655/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0247 - accuracy: 0.9922\n",
            "Epoch 655: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0414 - accuracy: 0.9864 - val_loss: 0.8223 - val_accuracy: 0.7634\n",
            "Epoch 656/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9864\n",
            "Epoch 656: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.0424 - accuracy: 0.9864 - val_loss: 0.8259 - val_accuracy: 0.7742\n",
            "Epoch 657/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0493 - accuracy: 0.9844\n",
            "Epoch 657: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0439 - accuracy: 0.9864 - val_loss: 0.8284 - val_accuracy: 0.7742\n",
            "Epoch 658/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0375 - accuracy: 0.9922\n",
            "Epoch 658: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0431 - accuracy: 0.9864 - val_loss: 0.8303 - val_accuracy: 0.7742\n",
            "Epoch 659/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0549 - accuracy: 0.9844\n",
            "Epoch 659: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0454 - accuracy: 0.9864 - val_loss: 0.8276 - val_accuracy: 0.7634\n",
            "Epoch 660/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0533 - accuracy: 0.9766\n",
            "Epoch 660: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0437 - accuracy: 0.9864 - val_loss: 0.8270 - val_accuracy: 0.7634\n",
            "Epoch 661/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0303 - accuracy: 0.9844\n",
            "Epoch 661: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0458 - accuracy: 0.9864 - val_loss: 0.8270 - val_accuracy: 0.7634\n",
            "Epoch 662/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0486 - accuracy: 0.9922\n",
            "Epoch 662: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0434 - accuracy: 0.9864 - val_loss: 0.8266 - val_accuracy: 0.7742\n",
            "Epoch 663/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0414 - accuracy: 0.9922\n",
            "Epoch 663: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0448 - accuracy: 0.9864 - val_loss: 0.8275 - val_accuracy: 0.7742\n",
            "Epoch 664/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9864\n",
            "Epoch 664: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0413 - accuracy: 0.9864 - val_loss: 0.8249 - val_accuracy: 0.7742\n",
            "Epoch 665/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0477 - accuracy: 0.9844\n",
            "Epoch 665: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0403 - accuracy: 0.9864 - val_loss: 0.8256 - val_accuracy: 0.7742\n",
            "Epoch 666/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0333 - accuracy: 0.9922\n",
            "Epoch 666: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0410 - accuracy: 0.9864 - val_loss: 0.8276 - val_accuracy: 0.7634\n",
            "Epoch 667/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0391 - accuracy: 0.9844\n",
            "Epoch 667: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0422 - accuracy: 0.9864 - val_loss: 0.8317 - val_accuracy: 0.7634\n",
            "Epoch 668/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0273 - accuracy: 0.9922\n",
            "Epoch 668: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0397 - accuracy: 0.9864 - val_loss: 0.8353 - val_accuracy: 0.7634\n",
            "Epoch 669/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0496 - accuracy: 0.9844\n",
            "Epoch 669: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0430 - accuracy: 0.9864 - val_loss: 0.8371 - val_accuracy: 0.7634\n",
            "Epoch 670/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0383 - accuracy: 0.9844\n",
            "Epoch 670: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0446 - accuracy: 0.9864 - val_loss: 0.8375 - val_accuracy: 0.7634\n",
            "Epoch 671/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0547 - accuracy: 0.9766\n",
            "Epoch 671: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0443 - accuracy: 0.9864 - val_loss: 0.8350 - val_accuracy: 0.7742\n",
            "Epoch 672/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0484 - accuracy: 0.9844\n",
            "Epoch 672: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0440 - accuracy: 0.9864 - val_loss: 0.8310 - val_accuracy: 0.7742\n",
            "Epoch 673/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0561 - accuracy: 0.9766\n",
            "Epoch 673: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0434 - accuracy: 0.9864 - val_loss: 0.8296 - val_accuracy: 0.7742\n",
            "Epoch 674/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0364 - accuracy: 0.9922\n",
            "Epoch 674: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0407 - accuracy: 0.9864 - val_loss: 0.8293 - val_accuracy: 0.7742\n",
            "Epoch 675/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9844\n",
            "Epoch 675: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0421 - accuracy: 0.9864 - val_loss: 0.8296 - val_accuracy: 0.7742\n",
            "Epoch 676/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0447 - accuracy: 0.9844\n",
            "Epoch 676: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0424 - accuracy: 0.9864 - val_loss: 0.8313 - val_accuracy: 0.7742\n",
            "Epoch 677/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9766\n",
            "Epoch 677: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0406 - accuracy: 0.9864 - val_loss: 0.8301 - val_accuracy: 0.7742\n",
            "Epoch 678/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0384 - accuracy: 0.9922\n",
            "Epoch 678: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 0.8345 - val_accuracy: 0.7634\n",
            "Epoch 679/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0401 - accuracy: 0.9922\n",
            "Epoch 679: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0415 - accuracy: 0.9864 - val_loss: 0.8369 - val_accuracy: 0.7634\n",
            "Epoch 680/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0429 - accuracy: 0.9844\n",
            "Epoch 680: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0415 - accuracy: 0.9864 - val_loss: 0.8382 - val_accuracy: 0.7634\n",
            "Epoch 681/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0513 - accuracy: 0.9844\n",
            "Epoch 681: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0418 - accuracy: 0.9864 - val_loss: 0.8383 - val_accuracy: 0.7634\n",
            "Epoch 682/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0518 - accuracy: 0.9766\n",
            "Epoch 682: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0440 - accuracy: 0.9864 - val_loss: 0.8376 - val_accuracy: 0.7634\n",
            "Epoch 683/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0375 - accuracy: 0.9844\n",
            "Epoch 683: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0403 - accuracy: 0.9864 - val_loss: 0.8352 - val_accuracy: 0.7634\n",
            "Epoch 684/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0362 - accuracy: 0.9922\n",
            "Epoch 684: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0414 - accuracy: 0.9864 - val_loss: 0.8361 - val_accuracy: 0.7634\n",
            "Epoch 685/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0405 - accuracy: 0.9922\n",
            "Epoch 685: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0427 - accuracy: 0.9864 - val_loss: 0.8339 - val_accuracy: 0.7634\n",
            "Epoch 686/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0430 - accuracy: 0.9844\n",
            "Epoch 686: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0440 - accuracy: 0.9864 - val_loss: 0.8334 - val_accuracy: 0.7634\n",
            "Epoch 687/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0516 - accuracy: 0.9766\n",
            "Epoch 687: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0430 - accuracy: 0.9864 - val_loss: 0.8349 - val_accuracy: 0.7634\n",
            "Epoch 688/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0816 - accuracy: 0.9609\n",
            "Epoch 688: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0408 - accuracy: 0.9864 - val_loss: 0.8404 - val_accuracy: 0.7634\n",
            "Epoch 689/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0264 - accuracy: 0.9922\n",
            "Epoch 689: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0407 - accuracy: 0.9864 - val_loss: 0.8426 - val_accuracy: 0.7742\n",
            "Epoch 690/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0375 - accuracy: 0.9922\n",
            "Epoch 690: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0431 - accuracy: 0.9864 - val_loss: 0.8434 - val_accuracy: 0.7742\n",
            "Epoch 691/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0614 - accuracy: 0.9688\n",
            "Epoch 691: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0415 - accuracy: 0.9864 - val_loss: 0.8444 - val_accuracy: 0.7634\n",
            "Epoch 692/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0648 - accuracy: 0.9766\n",
            "Epoch 692: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0445 - accuracy: 0.9864 - val_loss: 0.8415 - val_accuracy: 0.7634\n",
            "Epoch 693/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0385 - accuracy: 0.9844\n",
            "Epoch 693: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0423 - accuracy: 0.9864 - val_loss: 0.8386 - val_accuracy: 0.7634\n",
            "Epoch 694/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0341 - accuracy: 0.9922\n",
            "Epoch 694: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0403 - accuracy: 0.9864 - val_loss: 0.8331 - val_accuracy: 0.7634\n",
            "Epoch 695/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0382 - accuracy: 0.9922\n",
            "Epoch 695: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0433 - accuracy: 0.9864 - val_loss: 0.8283 - val_accuracy: 0.7634\n",
            "Epoch 696/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0327 - accuracy: 0.9844\n",
            "Epoch 696: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0396 - accuracy: 0.9864 - val_loss: 0.8276 - val_accuracy: 0.7634\n",
            "Epoch 697/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0423 - accuracy: 0.9922\n",
            "Epoch 697: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0390 - accuracy: 0.9864 - val_loss: 0.8285 - val_accuracy: 0.7634\n",
            "Epoch 698/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0432 - accuracy: 0.9922\n",
            "Epoch 698: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0410 - accuracy: 0.9864 - val_loss: 0.8286 - val_accuracy: 0.7634\n",
            "Epoch 699/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0382 - accuracy: 0.9844\n",
            "Epoch 699: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0431 - accuracy: 0.9864 - val_loss: 0.8267 - val_accuracy: 0.7634\n",
            "Epoch 700/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0239 - accuracy: 0.9922\n",
            "Epoch 700: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0460 - accuracy: 0.9864 - val_loss: 0.8256 - val_accuracy: 0.7634\n",
            "Epoch 701/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0381 - accuracy: 0.9844\n",
            "Epoch 701: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0414 - accuracy: 0.9864 - val_loss: 0.8231 - val_accuracy: 0.7742\n",
            "Epoch 702/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0370 - accuracy: 0.9922\n",
            "Epoch 702: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0401 - accuracy: 0.9864 - val_loss: 0.8208 - val_accuracy: 0.7742\n",
            "Epoch 703/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0445 - accuracy: 0.9922\n",
            "Epoch 703: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0413 - accuracy: 0.9864 - val_loss: 0.8201 - val_accuracy: 0.7849\n",
            "Epoch 704/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0451 - accuracy: 0.9766\n",
            "Epoch 704: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0416 - accuracy: 0.9864 - val_loss: 0.8205 - val_accuracy: 0.7849\n",
            "Epoch 705/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0378 - accuracy: 0.9922\n",
            "Epoch 705: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 0.8222 - val_accuracy: 0.7849\n",
            "Epoch 706/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0543 - accuracy: 0.9766\n",
            "Epoch 706: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0441 - accuracy: 0.9864 - val_loss: 0.8232 - val_accuracy: 0.7849\n",
            "Epoch 707/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0499 - accuracy: 0.9766\n",
            "Epoch 707: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0419 - accuracy: 0.9864 - val_loss: 0.8215 - val_accuracy: 0.7849\n",
            "Epoch 708/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0369 - accuracy: 0.9922\n",
            "Epoch 708: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0421 - accuracy: 0.9864 - val_loss: 0.8211 - val_accuracy: 0.7849\n",
            "Epoch 709/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0356 - accuracy: 0.9883\n",
            "Epoch 709: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 121ms/step - loss: 0.0400 - accuracy: 0.9864 - val_loss: 0.8196 - val_accuracy: 0.7849\n",
            "Epoch 710/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0389 - accuracy: 0.9922\n",
            "Epoch 710: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0401 - accuracy: 0.9864 - val_loss: 0.8182 - val_accuracy: 0.7849\n",
            "Epoch 711/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0438 - accuracy: 0.9844\n",
            "Epoch 711: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0424 - accuracy: 0.9864 - val_loss: 0.8163 - val_accuracy: 0.7849\n",
            "Epoch 712/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0261 - accuracy: 0.9922\n",
            "Epoch 712: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0389 - accuracy: 0.9864 - val_loss: 0.8165 - val_accuracy: 0.7849\n",
            "Epoch 713/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0505 - accuracy: 0.9844\n",
            "Epoch 713: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 0.8162 - val_accuracy: 0.7849\n",
            "Epoch 714/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0363 - accuracy: 0.9844\n",
            "Epoch 714: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0411 - accuracy: 0.9864 - val_loss: 0.8168 - val_accuracy: 0.7849\n",
            "Epoch 715/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0401 - accuracy: 0.9844\n",
            "Epoch 715: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0406 - accuracy: 0.9864 - val_loss: 0.8189 - val_accuracy: 0.7742\n",
            "Epoch 716/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0545 - accuracy: 0.9844\n",
            "Epoch 716: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0419 - accuracy: 0.9864 - val_loss: 0.8199 - val_accuracy: 0.7634\n",
            "Epoch 717/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0196 - accuracy: 1.0000\n",
            "Epoch 717: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0397 - accuracy: 0.9864 - val_loss: 0.8216 - val_accuracy: 0.7634\n",
            "Epoch 718/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0280 - accuracy: 0.9922\n",
            "Epoch 718: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0408 - accuracy: 0.9864 - val_loss: 0.8226 - val_accuracy: 0.7634\n",
            "Epoch 719/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0330 - accuracy: 0.9844\n",
            "Epoch 719: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0398 - accuracy: 0.9864 - val_loss: 0.8245 - val_accuracy: 0.7634\n",
            "Epoch 720/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9864\n",
            "Epoch 720: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.0396 - accuracy: 0.9864 - val_loss: 0.8292 - val_accuracy: 0.7742\n",
            "Epoch 721/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0603 - accuracy: 0.9688\n",
            "Epoch 721: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 0.8300 - val_accuracy: 0.7634\n",
            "Epoch 722/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0391 - accuracy: 0.9844\n",
            "Epoch 722: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0376 - accuracy: 0.9864 - val_loss: 0.8320 - val_accuracy: 0.7634\n",
            "Epoch 723/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0251 - accuracy: 1.0000\n",
            "Epoch 723: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0388 - accuracy: 0.9864 - val_loss: 0.8350 - val_accuracy: 0.7634\n",
            "Epoch 724/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0525 - accuracy: 0.9766\n",
            "Epoch 724: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0407 - accuracy: 0.9864 - val_loss: 0.8351 - val_accuracy: 0.7634\n",
            "Epoch 725/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0440 - accuracy: 0.9844\n",
            "Epoch 725: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0427 - accuracy: 0.9864 - val_loss: 0.8325 - val_accuracy: 0.7742\n",
            "Epoch 726/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0541 - accuracy: 0.9766\n",
            "Epoch 726: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0379 - accuracy: 0.9864 - val_loss: 0.8326 - val_accuracy: 0.7742\n",
            "Epoch 727/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0475 - accuracy: 0.9844\n",
            "Epoch 727: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0426 - accuracy: 0.9864 - val_loss: 0.8290 - val_accuracy: 0.7742\n",
            "Epoch 728/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0493 - accuracy: 0.9844\n",
            "Epoch 728: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0427 - accuracy: 0.9864 - val_loss: 0.8288 - val_accuracy: 0.7742\n",
            "Epoch 729/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0517 - accuracy: 0.9844\n",
            "Epoch 729: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 0.8297 - val_accuracy: 0.7742\n",
            "Epoch 730/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9922\n",
            "Epoch 730: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0422 - accuracy: 0.9864 - val_loss: 0.8275 - val_accuracy: 0.7742\n",
            "Epoch 731/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0395 - accuracy: 0.9922\n",
            "Epoch 731: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0401 - accuracy: 0.9864 - val_loss: 0.8270 - val_accuracy: 0.7742\n",
            "Epoch 732/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0480 - accuracy: 0.9766\n",
            "Epoch 732: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0352 - accuracy: 0.9864 - val_loss: 0.8293 - val_accuracy: 0.7742\n",
            "Epoch 733/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0483 - accuracy: 0.9844\n",
            "Epoch 733: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0410 - accuracy: 0.9864 - val_loss: 0.8312 - val_accuracy: 0.7634\n",
            "Epoch 734/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9864\n",
            "Epoch 734: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0422 - accuracy: 0.9864 - val_loss: 0.8283 - val_accuracy: 0.7742\n",
            "Epoch 735/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0533 - accuracy: 0.9844\n",
            "Epoch 735: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0416 - accuracy: 0.9864 - val_loss: 0.8292 - val_accuracy: 0.7742\n",
            "Epoch 736/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9688\n",
            "Epoch 736: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0385 - accuracy: 0.9864 - val_loss: 0.8303 - val_accuracy: 0.7742\n",
            "Epoch 737/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0507 - accuracy: 0.9844\n",
            "Epoch 737: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0402 - accuracy: 0.9864 - val_loss: 0.8308 - val_accuracy: 0.7742\n",
            "Epoch 738/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0342 - accuracy: 0.9922\n",
            "Epoch 738: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0421 - accuracy: 0.9864 - val_loss: 0.8293 - val_accuracy: 0.7742\n",
            "Epoch 739/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0388 - accuracy: 0.9922\n",
            "Epoch 739: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0419 - accuracy: 0.9864 - val_loss: 0.8296 - val_accuracy: 0.7634\n",
            "Epoch 740/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0322 - accuracy: 0.9844\n",
            "Epoch 740: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0431 - accuracy: 0.9864 - val_loss: 0.8304 - val_accuracy: 0.7634\n",
            "Epoch 741/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0473 - accuracy: 0.9766\n",
            "Epoch 741: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0414 - accuracy: 0.9864 - val_loss: 0.8319 - val_accuracy: 0.7742\n",
            "Epoch 742/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0241 - accuracy: 1.0000\n",
            "Epoch 742: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0401 - accuracy: 0.9864 - val_loss: 0.8355 - val_accuracy: 0.7742\n",
            "Epoch 743/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0441 - accuracy: 0.9844\n",
            "Epoch 743: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0415 - accuracy: 0.9864 - val_loss: 0.8355 - val_accuracy: 0.7742\n",
            "Epoch 744/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0626 - accuracy: 0.9688\n",
            "Epoch 744: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0423 - accuracy: 0.9864 - val_loss: 0.8368 - val_accuracy: 0.7742\n",
            "Epoch 745/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0371 - accuracy: 0.9844\n",
            "Epoch 745: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0403 - accuracy: 0.9864 - val_loss: 0.8375 - val_accuracy: 0.7742\n",
            "Epoch 746/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0423 - accuracy: 0.9766\n",
            "Epoch 746: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0387 - accuracy: 0.9864 - val_loss: 0.8383 - val_accuracy: 0.7742\n",
            "Epoch 747/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0328 - accuracy: 0.9844\n",
            "Epoch 747: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0412 - accuracy: 0.9864 - val_loss: 0.8383 - val_accuracy: 0.7742\n",
            "Epoch 748/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0402 - accuracy: 0.9844\n",
            "Epoch 748: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0406 - accuracy: 0.9864 - val_loss: 0.8350 - val_accuracy: 0.7742\n",
            "Epoch 749/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0622 - accuracy: 0.9766\n",
            "Epoch 749: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0415 - accuracy: 0.9864 - val_loss: 0.8313 - val_accuracy: 0.7634\n",
            "Epoch 750/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9922\n",
            "Epoch 750: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0404 - accuracy: 0.9864 - val_loss: 0.8319 - val_accuracy: 0.7634\n",
            "Epoch 751/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0382 - accuracy: 0.9844\n",
            "Epoch 751: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0432 - accuracy: 0.9864 - val_loss: 0.8324 - val_accuracy: 0.7634\n",
            "Epoch 752/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0532 - accuracy: 0.9688\n",
            "Epoch 752: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0392 - accuracy: 0.9864 - val_loss: 0.8316 - val_accuracy: 0.7634\n",
            "Epoch 753/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0295 - accuracy: 0.9922\n",
            "Epoch 753: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0374 - accuracy: 0.9864 - val_loss: 0.8315 - val_accuracy: 0.7634\n",
            "Epoch 754/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0296 - accuracy: 0.9922\n",
            "Epoch 754: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0406 - accuracy: 0.9864 - val_loss: 0.8346 - val_accuracy: 0.7634\n",
            "Epoch 755/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0276 - accuracy: 0.9922\n",
            "Epoch 755: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0384 - accuracy: 0.9864 - val_loss: 0.8364 - val_accuracy: 0.7527\n",
            "Epoch 756/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9864\n",
            "Epoch 756: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 144ms/step - loss: 0.0404 - accuracy: 0.9864 - val_loss: 0.8383 - val_accuracy: 0.7527\n",
            "Epoch 757/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0253 - accuracy: 0.9922\n",
            "Epoch 757: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0406 - accuracy: 0.9864 - val_loss: 0.8402 - val_accuracy: 0.7527\n",
            "Epoch 758/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0241 - accuracy: 0.9922\n",
            "Epoch 758: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0413 - accuracy: 0.9864 - val_loss: 0.8400 - val_accuracy: 0.7527\n",
            "Epoch 759/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0438 - accuracy: 0.9766\n",
            "Epoch 759: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0401 - accuracy: 0.9864 - val_loss: 0.8394 - val_accuracy: 0.7527\n",
            "Epoch 760/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0255 - accuracy: 0.9922\n",
            "Epoch 760: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0429 - accuracy: 0.9864 - val_loss: 0.8406 - val_accuracy: 0.7527\n",
            "Epoch 761/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0577 - accuracy: 0.9844\n",
            "Epoch 761: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0421 - accuracy: 0.9864 - val_loss: 0.8433 - val_accuracy: 0.7634\n",
            "Epoch 762/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0464 - accuracy: 0.9844\n",
            "Epoch 762: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0404 - accuracy: 0.9864 - val_loss: 0.8401 - val_accuracy: 0.7527\n",
            "Epoch 763/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0428 - accuracy: 0.9844\n",
            "Epoch 763: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0390 - accuracy: 0.9864 - val_loss: 0.8380 - val_accuracy: 0.7742\n",
            "Epoch 764/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9864\n",
            "Epoch 764: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0387 - accuracy: 0.9864 - val_loss: 0.8371 - val_accuracy: 0.7849\n",
            "Epoch 765/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9864\n",
            "Epoch 765: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0400 - accuracy: 0.9864 - val_loss: 0.8337 - val_accuracy: 0.7849\n",
            "Epoch 766/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0421 - accuracy: 0.9922\n",
            "Epoch 766: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0423 - accuracy: 0.9864 - val_loss: 0.8309 - val_accuracy: 0.7849\n",
            "Epoch 767/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0227 - accuracy: 0.9922\n",
            "Epoch 767: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0386 - accuracy: 0.9864 - val_loss: 0.8295 - val_accuracy: 0.7849\n",
            "Epoch 768/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0391 - accuracy: 0.9844\n",
            "Epoch 768: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0398 - accuracy: 0.9864 - val_loss: 0.8288 - val_accuracy: 0.7849\n",
            "Epoch 769/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0278 - accuracy: 0.9922\n",
            "Epoch 769: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0398 - accuracy: 0.9864 - val_loss: 0.8305 - val_accuracy: 0.7849\n",
            "Epoch 770/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 770: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0416 - accuracy: 0.9864 - val_loss: 0.8351 - val_accuracy: 0.7742\n",
            "Epoch 771/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0206 - accuracy: 1.0000\n",
            "Epoch 771: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0392 - accuracy: 0.9864 - val_loss: 0.8364 - val_accuracy: 0.7742\n",
            "Epoch 772/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0405 - accuracy: 0.9844\n",
            "Epoch 772: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0411 - accuracy: 0.9864 - val_loss: 0.8349 - val_accuracy: 0.7527\n",
            "Epoch 773/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0415 - accuracy: 0.9844\n",
            "Epoch 773: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0416 - accuracy: 0.9864 - val_loss: 0.8339 - val_accuracy: 0.7634\n",
            "Epoch 774/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9864\n",
            "Epoch 774: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0365 - accuracy: 0.9864 - val_loss: 0.8322 - val_accuracy: 0.7634\n",
            "Epoch 775/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0310 - accuracy: 0.9922\n",
            "Epoch 775: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0384 - accuracy: 0.9864 - val_loss: 0.8343 - val_accuracy: 0.7634\n",
            "Epoch 776/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0317 - accuracy: 0.9922\n",
            "Epoch 776: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0381 - accuracy: 0.9864 - val_loss: 0.8371 - val_accuracy: 0.7634\n",
            "Epoch 777/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0174 - accuracy: 1.0000\n",
            "Epoch 777: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 0.8392 - val_accuracy: 0.7634\n",
            "Epoch 778/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0262 - accuracy: 0.9922\n",
            "Epoch 778: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0387 - accuracy: 0.9864 - val_loss: 0.8397 - val_accuracy: 0.7634\n",
            "Epoch 779/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0593 - accuracy: 0.9766\n",
            "Epoch 779: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0445 - accuracy: 0.9864 - val_loss: 0.8384 - val_accuracy: 0.7742\n",
            "Epoch 780/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0399 - accuracy: 0.9922\n",
            "Epoch 780: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0413 - accuracy: 0.9864 - val_loss: 0.8401 - val_accuracy: 0.7742\n",
            "Epoch 781/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0471 - accuracy: 0.9844\n",
            "Epoch 781: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0402 - accuracy: 0.9864 - val_loss: 0.8433 - val_accuracy: 0.7742\n",
            "Epoch 782/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0350 - accuracy: 0.9922\n",
            "Epoch 782: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0407 - accuracy: 0.9864 - val_loss: 0.8460 - val_accuracy: 0.7527\n",
            "Epoch 783/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0467 - accuracy: 0.9844\n",
            "Epoch 783: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 0.8449 - val_accuracy: 0.7634\n",
            "Epoch 784/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0344 - accuracy: 0.9922\n",
            "Epoch 784: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0378 - accuracy: 0.9864 - val_loss: 0.8455 - val_accuracy: 0.7634\n",
            "Epoch 785/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0443 - accuracy: 0.9844\n",
            "Epoch 785: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0395 - accuracy: 0.9864 - val_loss: 0.8463 - val_accuracy: 0.7634\n",
            "Epoch 786/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0582 - accuracy: 0.9688\n",
            "Epoch 786: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0405 - accuracy: 0.9864 - val_loss: 0.8457 - val_accuracy: 0.7634\n",
            "Epoch 787/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0306 - accuracy: 0.9844\n",
            "Epoch 787: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0417 - accuracy: 0.9864 - val_loss: 0.8443 - val_accuracy: 0.7634\n",
            "Epoch 788/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0310 - accuracy: 0.9922\n",
            "Epoch 788: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0351 - accuracy: 0.9864 - val_loss: 0.8453 - val_accuracy: 0.7634\n",
            "Epoch 789/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0465 - accuracy: 0.9766\n",
            "Epoch 789: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0413 - accuracy: 0.9864 - val_loss: 0.8452 - val_accuracy: 0.7634\n",
            "Epoch 790/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0587 - accuracy: 0.9844\n",
            "Epoch 790: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0424 - accuracy: 0.9864 - val_loss: 0.8449 - val_accuracy: 0.7634\n",
            "Epoch 791/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0401 - accuracy: 0.9844\n",
            "Epoch 791: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0412 - accuracy: 0.9864 - val_loss: 0.8442 - val_accuracy: 0.7634\n",
            "Epoch 792/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0425 - accuracy: 0.9844\n",
            "Epoch 792: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0413 - accuracy: 0.9864 - val_loss: 0.8422 - val_accuracy: 0.7634\n",
            "Epoch 793/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0428 - accuracy: 0.9844\n",
            "Epoch 793: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0369 - accuracy: 0.9864 - val_loss: 0.8440 - val_accuracy: 0.7527\n",
            "Epoch 794/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0305 - accuracy: 0.9922\n",
            "Epoch 794: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0396 - accuracy: 0.9864 - val_loss: 0.8437 - val_accuracy: 0.7634\n",
            "Epoch 795/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0503 - accuracy: 0.9766\n",
            "Epoch 795: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0413 - accuracy: 0.9864 - val_loss: 0.8446 - val_accuracy: 0.7634\n",
            "Epoch 796/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0246 - accuracy: 1.0000\n",
            "Epoch 796: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0399 - accuracy: 0.9864 - val_loss: 0.8448 - val_accuracy: 0.7634\n",
            "Epoch 797/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0444 - accuracy: 0.9766\n",
            "Epoch 797: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0387 - accuracy: 0.9864 - val_loss: 0.8417 - val_accuracy: 0.7634\n",
            "Epoch 798/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0383 - accuracy: 0.9922\n",
            "Epoch 798: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0389 - accuracy: 0.9864 - val_loss: 0.8368 - val_accuracy: 0.7634\n",
            "Epoch 799/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0230 - accuracy: 1.0000\n",
            "Epoch 799: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0385 - accuracy: 0.9864 - val_loss: 0.8355 - val_accuracy: 0.7634\n",
            "Epoch 800/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0421 - accuracy: 0.9844\n",
            "Epoch 800: val_accuracy did not improve from 0.79570\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0398 - accuracy: 0.9864 - val_loss: 0.8347 - val_accuracy: 0.7634\n",
            "Training time: 144.10 seconds\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 150, 50)           100450    \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 148, 256)          38656     \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 148, 256)          1024      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 49, 256)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 47, 128)           98432     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 47, 128)           512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPoolin  (None, 15, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 13, 64)            24640     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 13, 64)            256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPoolin  (None, 4, 64)             0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 3)                 771       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 527653 (2.01 MB)\n",
            "Trainable params: 526757 (2.01 MB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_CNN = CNN_model.predict(feature_valid)\n",
        "\n",
        "# convert the validation vector\n",
        "valid_y_CNN = y_valid_CNN.copy()\n",
        "for i in range(len(y_valid_CNN)):\n",
        "    j = np.where(y_valid_CNN[i] == np.amax(y_valid_CNN[i]))\n",
        "    valid_y_CNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKWWfV1epI_V",
        "outputId": "628100f2-be2c-45e6-eca7-0de26420edee"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 63ms/step\n",
            "0.7634408602150538\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.65      0.79        17\n",
            "           1       0.72      0.88      0.79        43\n",
            "           2       0.76      0.67      0.71        33\n",
            "\n",
            "   micro avg       0.76      0.76      0.76        93\n",
            "   macro avg       0.83      0.73      0.76        93\n",
            "weighted avg       0.78      0.76      0.76        93\n",
            " samples avg       0.76      0.76      0.76        93\n",
            "\n",
            "auc score:  0.7967966256269948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "\n",
        "# 假设您已经有了预测结果y_valid_CNN和真实标签label_valid_y\n",
        "\n",
        "# 将标签转换为二进制形式(假设是三分类问题)\n",
        "y_valid_binary = label_binarize(label_valid_y, classes=[0, 1, 2])\n",
        "\n",
        "# 计算每个类别的ROC曲线和AUC\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "n_classes = 3\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_valid_binary[:, i], y_valid_CNN[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# 绘制ROC曲线\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = ['blue', 'red', 'green']\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2, label='ROC curve of class {0} (AUC = {1:0.2f})'.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([-0.05, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic (ROC) curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "sNi9_wYMRodv",
        "outputId": "85d8ecee-d456-4a0e-81d4-2bd98fa00082"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss values\n",
        "plt.plot(CNN_history.history['loss'])\n",
        "plt.title('CNN Model loss with class=3')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "eWdp9k_FpLh_",
        "outputId": "b02dc89d-5e09-4a56-d492-04fba059688c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(CNN_history.history['accuracy'])\n",
        "plt.title('CNN Model accuracy with class=3')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "fg_oFql1pOLj",
        "outputId": "5b14c089-05e8-4d42-f7a1-8724779c1482"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "HdzQaiuWpTdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def create_RNN_model():\n",
        "    RNN = Sequential()\n",
        "    RNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False))\n",
        "    RNN.add(Bidirectional(LSTM(word_dimension, return_sequences=True)))\n",
        "    RNN.add(Dropout(0.2))\n",
        "    RNN.add(Bidirectional(LSTM(word_dimension, return_sequences=True)))\n",
        "    RNN.add(Dropout(0.2))\n",
        "    RNN.add(Bidirectional(LSTM(word_dimension)))\n",
        "    RNN.add(Dense(word_dimension, activation='relu'))\n",
        "    RNN.add(Dropout(0.2))\n",
        "    RNN.add(Dense(3, activation='softmax'))\n",
        "    RNN.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "    return RNN\n",
        "\n",
        "RNN_model = create_RNN_model()\n",
        "\n",
        "# 创建 ModelCheckpoint 回调函数\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "RNN_history = RNN_model.fit(feature_train, label_train_y, epochs=800, batch_size=128, validation_data=(feature_valid, label_valid_y), callbacks=[checkpoint])\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Training time: {training_time:.2f} seconds\")\n",
        "RNN_model.summary()\n",
        "# 加载最佳模型的权重\n",
        "RNN_model.load_weights('best_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k01iAvgoqzKT",
        "outputId": "6403948b-3b67-4648-8776-17501b317871"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.0322 - accuracy: 0.5435\n",
            "Epoch 1: val_accuracy improved from -inf to 0.46237, saving model to best_model.h5\n",
            "3/3 [==============================] - 13s 1s/step - loss: 1.0322 - accuracy: 0.5435 - val_loss: 1.0875 - val_accuracy: 0.4624\n",
            "Epoch 2/800\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0141 - accuracy: 0.5234"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - ETA: 0s - loss: 0.9977 - accuracy: 0.5543\n",
            "Epoch 2: val_accuracy did not improve from 0.46237\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.9977 - accuracy: 0.5543 - val_loss: 1.0359 - val_accuracy: 0.4624\n",
            "Epoch 3/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.9839 - accuracy: 0.5508\n",
            "Epoch 3: val_accuracy did not improve from 0.46237\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.9838 - accuracy: 0.5543 - val_loss: 1.0426 - val_accuracy: 0.4624\n",
            "Epoch 4/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9794 - accuracy: 0.5543\n",
            "Epoch 4: val_accuracy did not improve from 0.46237\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.9794 - accuracy: 0.5543 - val_loss: 1.0501 - val_accuracy: 0.4624\n",
            "Epoch 5/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9655 - accuracy: 0.5543\n",
            "Epoch 5: val_accuracy did not improve from 0.46237\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.9655 - accuracy: 0.5543 - val_loss: 1.0419 - val_accuracy: 0.4624\n",
            "Epoch 6/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9543 - accuracy: 0.5543\n",
            "Epoch 6: val_accuracy did not improve from 0.46237\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.9543 - accuracy: 0.5543 - val_loss: 1.0340 - val_accuracy: 0.4624\n",
            "Epoch 7/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9526 - accuracy: 0.5707\n",
            "Epoch 7: val_accuracy improved from 0.46237 to 0.48387, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.9526 - accuracy: 0.5707 - val_loss: 1.0310 - val_accuracy: 0.4839\n",
            "Epoch 8/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9399 - accuracy: 0.5707\n",
            "Epoch 8: val_accuracy improved from 0.48387 to 0.50538, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.9399 - accuracy: 0.5707 - val_loss: 1.0231 - val_accuracy: 0.5054\n",
            "Epoch 9/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9156 - accuracy: 0.5951\n",
            "Epoch 9: val_accuracy did not improve from 0.50538\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.9156 - accuracy: 0.5951 - val_loss: 1.0234 - val_accuracy: 0.4946\n",
            "Epoch 10/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9019 - accuracy: 0.5897\n",
            "Epoch 10: val_accuracy did not improve from 0.50538\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.9019 - accuracy: 0.5897 - val_loss: 1.0226 - val_accuracy: 0.4946\n",
            "Epoch 11/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8832 - accuracy: 0.6060\n",
            "Epoch 11: val_accuracy did not improve from 0.50538\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.8832 - accuracy: 0.6060 - val_loss: 1.0012 - val_accuracy: 0.4731\n",
            "Epoch 12/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8580 - accuracy: 0.6005\n",
            "Epoch 12: val_accuracy did not improve from 0.50538\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.8580 - accuracy: 0.6005 - val_loss: 0.9824 - val_accuracy: 0.5054\n",
            "Epoch 13/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8413 - accuracy: 0.6114\n",
            "Epoch 13: val_accuracy did not improve from 0.50538\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.8413 - accuracy: 0.6114 - val_loss: 0.9770 - val_accuracy: 0.4624\n",
            "Epoch 14/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7908 - accuracy: 0.6413\n",
            "Epoch 14: val_accuracy did not improve from 0.50538\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.7908 - accuracy: 0.6413 - val_loss: 0.9565 - val_accuracy: 0.5054\n",
            "Epoch 15/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7736 - accuracy: 0.6413\n",
            "Epoch 15: val_accuracy improved from 0.50538 to 0.61290, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.7736 - accuracy: 0.6413 - val_loss: 0.9106 - val_accuracy: 0.6129\n",
            "Epoch 16/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7291 - accuracy: 0.6984\n",
            "Epoch 16: val_accuracy did not improve from 0.61290\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.7291 - accuracy: 0.6984 - val_loss: 0.8886 - val_accuracy: 0.6129\n",
            "Epoch 17/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.7255\n",
            "Epoch 17: val_accuracy did not improve from 0.61290\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.6921 - accuracy: 0.7255 - val_loss: 0.9237 - val_accuracy: 0.5914\n",
            "Epoch 18/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6968 - accuracy: 0.7038\n",
            "Epoch 18: val_accuracy improved from 0.61290 to 0.65591, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.6968 - accuracy: 0.7038 - val_loss: 0.8282 - val_accuracy: 0.6559\n",
            "Epoch 19/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6404 - accuracy: 0.7636\n",
            "Epoch 19: val_accuracy improved from 0.65591 to 0.66667, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.6404 - accuracy: 0.7636 - val_loss: 0.8328 - val_accuracy: 0.6667\n",
            "Epoch 20/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6073 - accuracy: 0.7853\n",
            "Epoch 20: val_accuracy did not improve from 0.66667\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.6073 - accuracy: 0.7853 - val_loss: 0.7887 - val_accuracy: 0.6559\n",
            "Epoch 21/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5986 - accuracy: 0.7636\n",
            "Epoch 21: val_accuracy did not improve from 0.66667\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.5986 - accuracy: 0.7636 - val_loss: 0.8067 - val_accuracy: 0.6452\n",
            "Epoch 22/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5681 - accuracy: 0.7989\n",
            "Epoch 22: val_accuracy did not improve from 0.66667\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.5681 - accuracy: 0.7989 - val_loss: 0.7890 - val_accuracy: 0.6559\n",
            "Epoch 23/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.7935\n",
            "Epoch 23: val_accuracy did not improve from 0.66667\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.5497 - accuracy: 0.7935 - val_loss: 0.8318 - val_accuracy: 0.6237\n",
            "Epoch 24/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.7880\n",
            "Epoch 24: val_accuracy did not improve from 0.66667\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.5554 - accuracy: 0.7880 - val_loss: 0.8115 - val_accuracy: 0.6667\n",
            "Epoch 25/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5590 - accuracy: 0.7772\n",
            "Epoch 25: val_accuracy improved from 0.66667 to 0.68817, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.5590 - accuracy: 0.7772 - val_loss: 0.7437 - val_accuracy: 0.6882\n",
            "Epoch 26/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5252 - accuracy: 0.7989\n",
            "Epoch 26: val_accuracy did not improve from 0.68817\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.5252 - accuracy: 0.7989 - val_loss: 0.8342 - val_accuracy: 0.6559\n",
            "Epoch 27/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.8152\n",
            "Epoch 27: val_accuracy did not improve from 0.68817\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.5094 - accuracy: 0.8152 - val_loss: 0.7655 - val_accuracy: 0.6882\n",
            "Epoch 28/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5009 - accuracy: 0.7989\n",
            "Epoch 28: val_accuracy improved from 0.68817 to 0.69892, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.5009 - accuracy: 0.7989 - val_loss: 0.7820 - val_accuracy: 0.6989\n",
            "Epoch 29/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4559 - accuracy: 0.8370\n",
            "Epoch 29: val_accuracy did not improve from 0.69892\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4559 - accuracy: 0.8370 - val_loss: 0.8457 - val_accuracy: 0.6129\n",
            "Epoch 30/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4425 - accuracy: 0.8315\n",
            "Epoch 30: val_accuracy did not improve from 0.69892\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4425 - accuracy: 0.8315 - val_loss: 0.9071 - val_accuracy: 0.6344\n",
            "Epoch 31/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.8152\n",
            "Epoch 31: val_accuracy did not improve from 0.69892\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.4990 - accuracy: 0.8152 - val_loss: 0.8202 - val_accuracy: 0.6559\n",
            "Epoch 32/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4543 - accuracy: 0.8098\n",
            "Epoch 32: val_accuracy did not improve from 0.69892\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.4543 - accuracy: 0.8098 - val_loss: 0.9086 - val_accuracy: 0.6452\n",
            "Epoch 33/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.8451\n",
            "Epoch 33: val_accuracy did not improve from 0.69892\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4356 - accuracy: 0.8451 - val_loss: 0.7855 - val_accuracy: 0.6774\n",
            "Epoch 34/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4198 - accuracy: 0.8451\n",
            "Epoch 34: val_accuracy did not improve from 0.69892\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.4198 - accuracy: 0.8451 - val_loss: 0.8657 - val_accuracy: 0.6989\n",
            "Epoch 35/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4317 - accuracy: 0.8397\n",
            "Epoch 35: val_accuracy did not improve from 0.69892\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.4317 - accuracy: 0.8397 - val_loss: 0.8395 - val_accuracy: 0.6559\n",
            "Epoch 36/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.8125\n",
            "Epoch 36: val_accuracy did not improve from 0.69892\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4669 - accuracy: 0.8125 - val_loss: 0.7759 - val_accuracy: 0.6667\n",
            "Epoch 37/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4211 - accuracy: 0.8315\n",
            "Epoch 37: val_accuracy did not improve from 0.69892\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.4211 - accuracy: 0.8315 - val_loss: 0.9226 - val_accuracy: 0.6344\n",
            "Epoch 38/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4585 - accuracy: 0.8152\n",
            "Epoch 38: val_accuracy improved from 0.69892 to 0.72043, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 0.4585 - accuracy: 0.8152 - val_loss: 0.7360 - val_accuracy: 0.7204\n",
            "Epoch 39/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3664 - accuracy: 0.8587\n",
            "Epoch 39: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3664 - accuracy: 0.8587 - val_loss: 0.7723 - val_accuracy: 0.6667\n",
            "Epoch 40/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4178 - accuracy: 0.8451\n",
            "Epoch 40: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.4178 - accuracy: 0.8451 - val_loss: 0.8188 - val_accuracy: 0.6452\n",
            "Epoch 41/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3953 - accuracy: 0.8641\n",
            "Epoch 41: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3953 - accuracy: 0.8641 - val_loss: 0.8777 - val_accuracy: 0.6774\n",
            "Epoch 42/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3388 - accuracy: 0.8711\n",
            "Epoch 42: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3428 - accuracy: 0.8668 - val_loss: 0.8088 - val_accuracy: 0.6989\n",
            "Epoch 43/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3481 - accuracy: 0.8560\n",
            "Epoch 43: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.3481 - accuracy: 0.8560 - val_loss: 0.8466 - val_accuracy: 0.6559\n",
            "Epoch 44/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3500 - accuracy: 0.8750\n",
            "Epoch 44: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.3500 - accuracy: 0.8750 - val_loss: 0.9213 - val_accuracy: 0.6882\n",
            "Epoch 45/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3439 - accuracy: 0.8696\n",
            "Epoch 45: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.3439 - accuracy: 0.8696 - val_loss: 0.9348 - val_accuracy: 0.6774\n",
            "Epoch 46/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3831 - accuracy: 0.8533\n",
            "Epoch 46: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.3831 - accuracy: 0.8533 - val_loss: 0.9091 - val_accuracy: 0.7097\n",
            "Epoch 47/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.8668\n",
            "Epoch 47: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.3474 - accuracy: 0.8668 - val_loss: 0.9420 - val_accuracy: 0.6882\n",
            "Epoch 48/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3398 - accuracy: 0.8804\n",
            "Epoch 48: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.3398 - accuracy: 0.8804 - val_loss: 0.8758 - val_accuracy: 0.7097\n",
            "Epoch 49/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.8750\n",
            "Epoch 49: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.2861 - accuracy: 0.8750 - val_loss: 0.9139 - val_accuracy: 0.7097\n",
            "Epoch 50/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.8995\n",
            "Epoch 50: val_accuracy did not improve from 0.72043\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.2883 - accuracy: 0.8995 - val_loss: 0.9370 - val_accuracy: 0.7097\n",
            "Epoch 51/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9076\n",
            "Epoch 51: val_accuracy improved from 0.72043 to 0.73118, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.2579 - accuracy: 0.9076 - val_loss: 0.8854 - val_accuracy: 0.7312\n",
            "Epoch 52/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2397 - accuracy: 0.9158\n",
            "Epoch 52: val_accuracy did not improve from 0.73118\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.2397 - accuracy: 0.9158 - val_loss: 0.9387 - val_accuracy: 0.6882\n",
            "Epoch 53/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9076\n",
            "Epoch 53: val_accuracy did not improve from 0.73118\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.2205 - accuracy: 0.9076 - val_loss: 0.9514 - val_accuracy: 0.7097\n",
            "Epoch 54/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.9103\n",
            "Epoch 54: val_accuracy did not improve from 0.73118\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.2183 - accuracy: 0.9103 - val_loss: 1.1256 - val_accuracy: 0.6882\n",
            "Epoch 55/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9103\n",
            "Epoch 55: val_accuracy did not improve from 0.73118\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.2316 - accuracy: 0.9103 - val_loss: 1.1772 - val_accuracy: 0.6237\n",
            "Epoch 56/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 0.9158\n",
            "Epoch 56: val_accuracy did not improve from 0.73118\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.2274 - accuracy: 0.9158 - val_loss: 1.0816 - val_accuracy: 0.6882\n",
            "Epoch 57/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2331 - accuracy: 0.9103\n",
            "Epoch 57: val_accuracy improved from 0.73118 to 0.74194, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.2331 - accuracy: 0.9103 - val_loss: 1.0227 - val_accuracy: 0.7419\n",
            "Epoch 58/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.8750\n",
            "Epoch 58: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.3332 - accuracy: 0.8750 - val_loss: 1.1534 - val_accuracy: 0.6559\n",
            "Epoch 59/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4008 - accuracy: 0.8533\n",
            "Epoch 59: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.4008 - accuracy: 0.8533 - val_loss: 1.0584 - val_accuracy: 0.6882\n",
            "Epoch 60/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3957 - accuracy: 0.8370\n",
            "Epoch 60: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.3957 - accuracy: 0.8370 - val_loss: 0.7900 - val_accuracy: 0.6882\n",
            "Epoch 61/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.8641\n",
            "Epoch 61: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3848 - accuracy: 0.8641 - val_loss: 1.0396 - val_accuracy: 0.6667\n",
            "Epoch 62/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3436 - accuracy: 0.8614\n",
            "Epoch 62: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.3436 - accuracy: 0.8614 - val_loss: 1.0869 - val_accuracy: 0.6452\n",
            "Epoch 63/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2928 - accuracy: 0.8995\n",
            "Epoch 63: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.2928 - accuracy: 0.8995 - val_loss: 0.8950 - val_accuracy: 0.6989\n",
            "Epoch 64/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9049\n",
            "Epoch 64: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.2699 - accuracy: 0.9049 - val_loss: 0.9862 - val_accuracy: 0.6989\n",
            "Epoch 65/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2314 - accuracy: 0.9103\n",
            "Epoch 65: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.2314 - accuracy: 0.9103 - val_loss: 1.0488 - val_accuracy: 0.6452\n",
            "Epoch 66/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2300 - accuracy: 0.9076\n",
            "Epoch 66: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.2300 - accuracy: 0.9076 - val_loss: 0.9389 - val_accuracy: 0.6989\n",
            "Epoch 67/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.9321\n",
            "Epoch 67: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.2009 - accuracy: 0.9321 - val_loss: 0.9718 - val_accuracy: 0.6989\n",
            "Epoch 68/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1753 - accuracy: 0.9402\n",
            "Epoch 68: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1753 - accuracy: 0.9402 - val_loss: 1.0497 - val_accuracy: 0.6774\n",
            "Epoch 69/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9484\n",
            "Epoch 69: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.1615 - accuracy: 0.9484 - val_loss: 1.0934 - val_accuracy: 0.6774\n",
            "Epoch 70/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9375\n",
            "Epoch 70: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1683 - accuracy: 0.9375 - val_loss: 1.0646 - val_accuracy: 0.6882\n",
            "Epoch 71/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.9484\n",
            "Epoch 71: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1534 - accuracy: 0.9484 - val_loss: 1.1975 - val_accuracy: 0.6774\n",
            "Epoch 72/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9429\n",
            "Epoch 72: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1337 - accuracy: 0.9429 - val_loss: 1.2226 - val_accuracy: 0.6774\n",
            "Epoch 73/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.9538\n",
            "Epoch 73: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1359 - accuracy: 0.9538 - val_loss: 1.1972 - val_accuracy: 0.6667\n",
            "Epoch 74/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1382 - accuracy: 0.9429\n",
            "Epoch 74: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1382 - accuracy: 0.9429 - val_loss: 1.1441 - val_accuracy: 0.6774\n",
            "Epoch 75/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9484\n",
            "Epoch 75: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1161 - accuracy: 0.9484 - val_loss: 1.2321 - val_accuracy: 0.6882\n",
            "Epoch 76/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1178 - accuracy: 0.9620\n",
            "Epoch 76: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1178 - accuracy: 0.9620 - val_loss: 1.1950 - val_accuracy: 0.7097\n",
            "Epoch 77/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9565\n",
            "Epoch 77: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.1146 - accuracy: 0.9565 - val_loss: 1.2767 - val_accuracy: 0.6559\n",
            "Epoch 78/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9728\n",
            "Epoch 78: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0952 - accuracy: 0.9728 - val_loss: 1.2905 - val_accuracy: 0.6452\n",
            "Epoch 79/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.9484\n",
            "Epoch 79: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1274 - accuracy: 0.9484 - val_loss: 1.1788 - val_accuracy: 0.7204\n",
            "Epoch 80/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1594 - accuracy: 0.9266\n",
            "Epoch 80: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1594 - accuracy: 0.9266 - val_loss: 1.2494 - val_accuracy: 0.7097\n",
            "Epoch 81/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2383 - accuracy: 0.9185\n",
            "Epoch 81: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.2383 - accuracy: 0.9185 - val_loss: 1.3400 - val_accuracy: 0.6667\n",
            "Epoch 82/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9076\n",
            "Epoch 82: val_accuracy did not improve from 0.74194\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.2155 - accuracy: 0.9076 - val_loss: 1.1373 - val_accuracy: 0.7204\n",
            "Epoch 83/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.8940\n",
            "Epoch 83: val_accuracy improved from 0.74194 to 0.75269, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.2666 - accuracy: 0.8940 - val_loss: 0.8982 - val_accuracy: 0.7527\n",
            "Epoch 84/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.8940\n",
            "Epoch 84: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.2633 - accuracy: 0.8940 - val_loss: 1.3759 - val_accuracy: 0.6882\n",
            "Epoch 85/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.8940\n",
            "Epoch 85: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.2861 - accuracy: 0.8940 - val_loss: 1.3088 - val_accuracy: 0.6774\n",
            "Epoch 86/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9185\n",
            "Epoch 86: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1987 - accuracy: 0.9185 - val_loss: 1.0477 - val_accuracy: 0.6989\n",
            "Epoch 87/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.9103\n",
            "Epoch 87: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.2183 - accuracy: 0.9103 - val_loss: 1.2244 - val_accuracy: 0.6882\n",
            "Epoch 88/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9293\n",
            "Epoch 88: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.1859 - accuracy: 0.9293 - val_loss: 1.2709 - val_accuracy: 0.6774\n",
            "Epoch 89/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1661 - accuracy: 0.9375\n",
            "Epoch 89: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1661 - accuracy: 0.9375 - val_loss: 1.0532 - val_accuracy: 0.7097\n",
            "Epoch 90/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9429\n",
            "Epoch 90: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1771 - accuracy: 0.9429 - val_loss: 1.0735 - val_accuracy: 0.7312\n",
            "Epoch 91/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9429\n",
            "Epoch 91: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1602 - accuracy: 0.9429 - val_loss: 1.1516 - val_accuracy: 0.7097\n",
            "Epoch 92/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9457\n",
            "Epoch 92: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1388 - accuracy: 0.9457 - val_loss: 1.2145 - val_accuracy: 0.7097\n",
            "Epoch 93/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9511\n",
            "Epoch 93: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1277 - accuracy: 0.9511 - val_loss: 1.1705 - val_accuracy: 0.7204\n",
            "Epoch 94/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9620\n",
            "Epoch 94: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1077 - accuracy: 0.9620 - val_loss: 1.1717 - val_accuracy: 0.7097\n",
            "Epoch 95/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9647\n",
            "Epoch 95: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1024 - accuracy: 0.9647 - val_loss: 1.1913 - val_accuracy: 0.7097\n",
            "Epoch 96/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9620\n",
            "Epoch 96: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1013 - accuracy: 0.9620 - val_loss: 1.1887 - val_accuracy: 0.7097\n",
            "Epoch 97/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9592\n",
            "Epoch 97: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1073 - accuracy: 0.9592 - val_loss: 1.1808 - val_accuracy: 0.6989\n",
            "Epoch 98/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0968 - accuracy: 0.9755\n",
            "Epoch 98: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0968 - accuracy: 0.9755 - val_loss: 1.2325 - val_accuracy: 0.6989\n",
            "Epoch 99/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9701\n",
            "Epoch 99: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0812 - accuracy: 0.9701 - val_loss: 1.1965 - val_accuracy: 0.7312\n",
            "Epoch 100/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9674\n",
            "Epoch 100: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0896 - accuracy: 0.9674 - val_loss: 1.2519 - val_accuracy: 0.6774\n",
            "Epoch 101/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9728\n",
            "Epoch 101: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0820 - accuracy: 0.9728 - val_loss: 1.3672 - val_accuracy: 0.6667\n",
            "Epoch 102/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9755\n",
            "Epoch 102: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0761 - accuracy: 0.9755 - val_loss: 1.3105 - val_accuracy: 0.6882\n",
            "Epoch 103/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9701\n",
            "Epoch 103: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0783 - accuracy: 0.9701 - val_loss: 1.2722 - val_accuracy: 0.6989\n",
            "Epoch 104/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9701\n",
            "Epoch 104: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0759 - accuracy: 0.9701 - val_loss: 1.3790 - val_accuracy: 0.6559\n",
            "Epoch 105/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9810\n",
            "Epoch 105: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0754 - accuracy: 0.9810 - val_loss: 1.3623 - val_accuracy: 0.6774\n",
            "Epoch 106/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9755\n",
            "Epoch 106: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0749 - accuracy: 0.9755 - val_loss: 1.4279 - val_accuracy: 0.6667\n",
            "Epoch 107/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9728\n",
            "Epoch 107: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0697 - accuracy: 0.9728 - val_loss: 1.4718 - val_accuracy: 0.6774\n",
            "Epoch 108/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9728\n",
            "Epoch 108: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0682 - accuracy: 0.9728 - val_loss: 1.3402 - val_accuracy: 0.6774\n",
            "Epoch 109/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9755\n",
            "Epoch 109: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0699 - accuracy: 0.9755 - val_loss: 1.3155 - val_accuracy: 0.6989\n",
            "Epoch 110/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9538\n",
            "Epoch 110: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.0852 - accuracy: 0.9538 - val_loss: 1.4095 - val_accuracy: 0.6882\n",
            "Epoch 111/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9755\n",
            "Epoch 111: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0673 - accuracy: 0.9755 - val_loss: 1.3340 - val_accuracy: 0.6882\n",
            "Epoch 112/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9728\n",
            "Epoch 112: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0740 - accuracy: 0.9728 - val_loss: 1.3474 - val_accuracy: 0.7204\n",
            "Epoch 113/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9810\n",
            "Epoch 113: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0650 - accuracy: 0.9810 - val_loss: 1.4712 - val_accuracy: 0.6882\n",
            "Epoch 114/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9755\n",
            "Epoch 114: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0808 - accuracy: 0.9755 - val_loss: 1.4321 - val_accuracy: 0.7204\n",
            "Epoch 115/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9755\n",
            "Epoch 115: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0622 - accuracy: 0.9755 - val_loss: 1.3847 - val_accuracy: 0.7097\n",
            "Epoch 116/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9810\n",
            "Epoch 116: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0550 - accuracy: 0.9810 - val_loss: 1.3667 - val_accuracy: 0.7312\n",
            "Epoch 117/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9783\n",
            "Epoch 117: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.0541 - accuracy: 0.9783 - val_loss: 1.3955 - val_accuracy: 0.7097\n",
            "Epoch 118/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9837\n",
            "Epoch 118: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.0533 - accuracy: 0.9837 - val_loss: 1.3878 - val_accuracy: 0.6989\n",
            "Epoch 119/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0644 - accuracy: 0.9688\n",
            "Epoch 119: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.0517 - accuracy: 0.9783 - val_loss: 1.4007 - val_accuracy: 0.7204\n",
            "Epoch 120/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9783\n",
            "Epoch 120: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.0602 - accuracy: 0.9783 - val_loss: 1.3532 - val_accuracy: 0.7204\n",
            "Epoch 121/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9837\n",
            "Epoch 121: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.0513 - accuracy: 0.9837 - val_loss: 1.3494 - val_accuracy: 0.7204\n",
            "Epoch 122/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9755\n",
            "Epoch 122: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0581 - accuracy: 0.9755 - val_loss: 1.3998 - val_accuracy: 0.7204\n",
            "Epoch 123/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9837\n",
            "Epoch 123: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0487 - accuracy: 0.9837 - val_loss: 1.5524 - val_accuracy: 0.6882\n",
            "Epoch 124/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9864\n",
            "Epoch 124: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0465 - accuracy: 0.9864 - val_loss: 1.6250 - val_accuracy: 0.6774\n",
            "Epoch 125/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 0.9755\n",
            "Epoch 125: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0592 - accuracy: 0.9755 - val_loss: 1.4172 - val_accuracy: 0.6989\n",
            "Epoch 126/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9837\n",
            "Epoch 126: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0516 - accuracy: 0.9837 - val_loss: 1.3411 - val_accuracy: 0.7419\n",
            "Epoch 127/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9728\n",
            "Epoch 127: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0681 - accuracy: 0.9728 - val_loss: 1.6552 - val_accuracy: 0.6882\n",
            "Epoch 128/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9701\n",
            "Epoch 128: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0796 - accuracy: 0.9701 - val_loss: 1.6938 - val_accuracy: 0.6989\n",
            "Epoch 129/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9647\n",
            "Epoch 129: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0950 - accuracy: 0.9647 - val_loss: 1.4515 - val_accuracy: 0.6989\n",
            "Epoch 130/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9674\n",
            "Epoch 130: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0971 - accuracy: 0.9674 - val_loss: 1.5095 - val_accuracy: 0.7097\n",
            "Epoch 131/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1135 - accuracy: 0.9674\n",
            "Epoch 131: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1135 - accuracy: 0.9674 - val_loss: 1.3971 - val_accuracy: 0.7097\n",
            "Epoch 132/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9592\n",
            "Epoch 132: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0970 - accuracy: 0.9592 - val_loss: 1.3656 - val_accuracy: 0.7204\n",
            "Epoch 133/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0965 - accuracy: 0.9592\n",
            "Epoch 133: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0965 - accuracy: 0.9592 - val_loss: 1.5565 - val_accuracy: 0.6882\n",
            "Epoch 134/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9402\n",
            "Epoch 134: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1773 - accuracy: 0.9402 - val_loss: 1.4375 - val_accuracy: 0.7312\n",
            "Epoch 135/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9538\n",
            "Epoch 135: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.1223 - accuracy: 0.9538 - val_loss: 1.3171 - val_accuracy: 0.7312\n",
            "Epoch 136/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1607 - accuracy: 0.9402\n",
            "Epoch 136: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1607 - accuracy: 0.9402 - val_loss: 1.4659 - val_accuracy: 0.6882\n",
            "Epoch 137/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9484\n",
            "Epoch 137: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.1542 - accuracy: 0.9484 - val_loss: 1.5987 - val_accuracy: 0.7204\n",
            "Epoch 138/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9565\n",
            "Epoch 138: val_accuracy did not improve from 0.75269\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.1447 - accuracy: 0.9565 - val_loss: 1.5727 - val_accuracy: 0.6774\n",
            "Epoch 139/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9511\n",
            "Epoch 139: val_accuracy improved from 0.75269 to 0.77419, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.1300 - accuracy: 0.9511 - val_loss: 0.9983 - val_accuracy: 0.7742\n",
            "Epoch 140/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9484\n",
            "Epoch 140: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1365 - accuracy: 0.9484 - val_loss: 1.4153 - val_accuracy: 0.6774\n",
            "Epoch 141/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.9620\n",
            "Epoch 141: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1162 - accuracy: 0.9620 - val_loss: 1.5181 - val_accuracy: 0.6452\n",
            "Epoch 142/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1911 - accuracy: 0.9348\n",
            "Epoch 142: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.1911 - accuracy: 0.9348 - val_loss: 1.4255 - val_accuracy: 0.6882\n",
            "Epoch 143/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 0.9348\n",
            "Epoch 143: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1735 - accuracy: 0.9348 - val_loss: 1.1937 - val_accuracy: 0.7097\n",
            "Epoch 144/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9674\n",
            "Epoch 144: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.1334 - accuracy: 0.9674 - val_loss: 1.4102 - val_accuracy: 0.6882\n",
            "Epoch 145/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.9402\n",
            "Epoch 145: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.1350 - accuracy: 0.9402 - val_loss: 1.4221 - val_accuracy: 0.7204\n",
            "Epoch 146/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9484\n",
            "Epoch 146: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.1043 - accuracy: 0.9484 - val_loss: 1.3483 - val_accuracy: 0.7419\n",
            "Epoch 147/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.9484\n",
            "Epoch 147: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1313 - accuracy: 0.9484 - val_loss: 1.3816 - val_accuracy: 0.7204\n",
            "Epoch 148/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9620\n",
            "Epoch 148: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.1021 - accuracy: 0.9620 - val_loss: 1.3763 - val_accuracy: 0.6989\n",
            "Epoch 149/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9701\n",
            "Epoch 149: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0771 - accuracy: 0.9701 - val_loss: 1.3728 - val_accuracy: 0.6989\n",
            "Epoch 150/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9728\n",
            "Epoch 150: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0803 - accuracy: 0.9728 - val_loss: 1.3666 - val_accuracy: 0.6667\n",
            "Epoch 151/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9701\n",
            "Epoch 151: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0714 - accuracy: 0.9701 - val_loss: 1.4195 - val_accuracy: 0.6774\n",
            "Epoch 152/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9783\n",
            "Epoch 152: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0605 - accuracy: 0.9783 - val_loss: 1.4413 - val_accuracy: 0.6882\n",
            "Epoch 153/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9810\n",
            "Epoch 153: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0703 - accuracy: 0.9810 - val_loss: 1.4282 - val_accuracy: 0.6989\n",
            "Epoch 154/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9810\n",
            "Epoch 154: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0583 - accuracy: 0.9810 - val_loss: 1.3799 - val_accuracy: 0.7204\n",
            "Epoch 155/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9728\n",
            "Epoch 155: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0554 - accuracy: 0.9728 - val_loss: 1.4197 - val_accuracy: 0.7097\n",
            "Epoch 156/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9837\n",
            "Epoch 156: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0510 - accuracy: 0.9837 - val_loss: 1.5158 - val_accuracy: 0.6989\n",
            "Epoch 157/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9755\n",
            "Epoch 157: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0600 - accuracy: 0.9755 - val_loss: 1.5126 - val_accuracy: 0.6989\n",
            "Epoch 158/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0547 - accuracy: 0.9844\n",
            "Epoch 158: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0630 - accuracy: 0.9810 - val_loss: 1.4152 - val_accuracy: 0.7204\n",
            "Epoch 159/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9783\n",
            "Epoch 159: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0579 - accuracy: 0.9783 - val_loss: 1.4138 - val_accuracy: 0.7312\n",
            "Epoch 160/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9783\n",
            "Epoch 160: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0500 - accuracy: 0.9783 - val_loss: 1.4697 - val_accuracy: 0.7097\n",
            "Epoch 161/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9728\n",
            "Epoch 161: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0536 - accuracy: 0.9728 - val_loss: 1.5244 - val_accuracy: 0.7204\n",
            "Epoch 162/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9810\n",
            "Epoch 162: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0446 - accuracy: 0.9810 - val_loss: 1.5406 - val_accuracy: 0.7204\n",
            "Epoch 163/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9810\n",
            "Epoch 163: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0451 - accuracy: 0.9810 - val_loss: 1.5512 - val_accuracy: 0.7097\n",
            "Epoch 164/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9783\n",
            "Epoch 164: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0529 - accuracy: 0.9783 - val_loss: 1.5963 - val_accuracy: 0.7097\n",
            "Epoch 165/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9728\n",
            "Epoch 165: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0589 - accuracy: 0.9728 - val_loss: 1.5925 - val_accuracy: 0.6989\n",
            "Epoch 166/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9810\n",
            "Epoch 166: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0449 - accuracy: 0.9810 - val_loss: 1.5248 - val_accuracy: 0.7419\n",
            "Epoch 167/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9810\n",
            "Epoch 167: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0435 - accuracy: 0.9810 - val_loss: 1.5309 - val_accuracy: 0.7419\n",
            "Epoch 168/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9891\n",
            "Epoch 168: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0319 - accuracy: 0.9891 - val_loss: 1.5551 - val_accuracy: 0.7204\n",
            "Epoch 169/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9864\n",
            "Epoch 169: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0391 - accuracy: 0.9864 - val_loss: 1.5920 - val_accuracy: 0.7204\n",
            "Epoch 170/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9837\n",
            "Epoch 170: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0440 - accuracy: 0.9837 - val_loss: 1.6123 - val_accuracy: 0.7204\n",
            "Epoch 171/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9837\n",
            "Epoch 171: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0449 - accuracy: 0.9837 - val_loss: 1.6316 - val_accuracy: 0.7204\n",
            "Epoch 172/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9891\n",
            "Epoch 172: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0374 - accuracy: 0.9891 - val_loss: 1.6599 - val_accuracy: 0.7312\n",
            "Epoch 173/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9810\n",
            "Epoch 173: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0451 - accuracy: 0.9810 - val_loss: 1.6913 - val_accuracy: 0.7419\n",
            "Epoch 174/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9891\n",
            "Epoch 174: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0380 - accuracy: 0.9891 - val_loss: 1.6922 - val_accuracy: 0.7204\n",
            "Epoch 175/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9810\n",
            "Epoch 175: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0392 - accuracy: 0.9810 - val_loss: 1.6777 - val_accuracy: 0.7204\n",
            "Epoch 176/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9810\n",
            "Epoch 176: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0402 - accuracy: 0.9810 - val_loss: 1.6538 - val_accuracy: 0.7419\n",
            "Epoch 177/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9755\n",
            "Epoch 177: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0416 - accuracy: 0.9755 - val_loss: 1.6355 - val_accuracy: 0.7527\n",
            "Epoch 178/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9864\n",
            "Epoch 178: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0326 - accuracy: 0.9864 - val_loss: 1.6402 - val_accuracy: 0.7527\n",
            "Epoch 179/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9864\n",
            "Epoch 179: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0359 - accuracy: 0.9864 - val_loss: 1.6617 - val_accuracy: 0.7312\n",
            "Epoch 180/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9864\n",
            "Epoch 180: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0363 - accuracy: 0.9864 - val_loss: 1.6918 - val_accuracy: 0.7312\n",
            "Epoch 181/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9918\n",
            "Epoch 181: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0385 - accuracy: 0.9918 - val_loss: 1.7107 - val_accuracy: 0.7527\n",
            "Epoch 182/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0320 - accuracy: 0.9805\n",
            "Epoch 182: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0371 - accuracy: 0.9783 - val_loss: 1.7315 - val_accuracy: 0.7312\n",
            "Epoch 183/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0191 - accuracy: 0.9922\n",
            "Epoch 183: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0327 - accuracy: 0.9864 - val_loss: 1.7426 - val_accuracy: 0.7312\n",
            "Epoch 184/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9837\n",
            "Epoch 184: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0337 - accuracy: 0.9837 - val_loss: 1.7256 - val_accuracy: 0.7312\n",
            "Epoch 185/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9837\n",
            "Epoch 185: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0361 - accuracy: 0.9837 - val_loss: 1.7043 - val_accuracy: 0.7527\n",
            "Epoch 186/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9837\n",
            "Epoch 186: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0330 - accuracy: 0.9837 - val_loss: 1.7150 - val_accuracy: 0.7527\n",
            "Epoch 187/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9783\n",
            "Epoch 187: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0403 - accuracy: 0.9783 - val_loss: 1.7333 - val_accuracy: 0.7527\n",
            "Epoch 188/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9918\n",
            "Epoch 188: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.0320 - accuracy: 0.9918 - val_loss: 1.7351 - val_accuracy: 0.7527\n",
            "Epoch 189/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9810\n",
            "Epoch 189: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0374 - accuracy: 0.9810 - val_loss: 1.7467 - val_accuracy: 0.7419\n",
            "Epoch 190/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9864\n",
            "Epoch 190: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0309 - accuracy: 0.9864 - val_loss: 1.7721 - val_accuracy: 0.7419\n",
            "Epoch 191/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0274 - accuracy: 0.9922\n",
            "Epoch 191: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0371 - accuracy: 0.9837 - val_loss: 1.7732 - val_accuracy: 0.7527\n",
            "Epoch 192/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9837\n",
            "Epoch 192: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0397 - accuracy: 0.9837 - val_loss: 1.8007 - val_accuracy: 0.7419\n",
            "Epoch 193/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9837\n",
            "Epoch 193: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0355 - accuracy: 0.9837 - val_loss: 1.8524 - val_accuracy: 0.6989\n",
            "Epoch 194/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9891\n",
            "Epoch 194: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0300 - accuracy: 0.9891 - val_loss: 1.8831 - val_accuracy: 0.6989\n",
            "Epoch 195/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9864\n",
            "Epoch 195: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0325 - accuracy: 0.9864 - val_loss: 1.8005 - val_accuracy: 0.7204\n",
            "Epoch 196/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0442 - accuracy: 0.9844\n",
            "Epoch 196: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0357 - accuracy: 0.9864 - val_loss: 1.7849 - val_accuracy: 0.7419\n",
            "Epoch 197/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9810\n",
            "Epoch 197: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0459 - accuracy: 0.9810 - val_loss: 1.8235 - val_accuracy: 0.7204\n",
            "Epoch 198/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9864\n",
            "Epoch 198: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.0258 - accuracy: 0.9864 - val_loss: 1.8895 - val_accuracy: 0.6989\n",
            "Epoch 199/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9864\n",
            "Epoch 199: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0342 - accuracy: 0.9864 - val_loss: 1.8625 - val_accuracy: 0.6989\n",
            "Epoch 200/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0380 - accuracy: 0.9766\n",
            "Epoch 200: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0352 - accuracy: 0.9783 - val_loss: 1.8440 - val_accuracy: 0.7312\n",
            "Epoch 201/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9837\n",
            "Epoch 201: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0404 - accuracy: 0.9837 - val_loss: 1.8276 - val_accuracy: 0.7312\n",
            "Epoch 202/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9918\n",
            "Epoch 202: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0280 - accuracy: 0.9918 - val_loss: 1.7990 - val_accuracy: 0.7097\n",
            "Epoch 203/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9837\n",
            "Epoch 203: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0305 - accuracy: 0.9837 - val_loss: 1.7685 - val_accuracy: 0.7312\n",
            "Epoch 204/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9837\n",
            "Epoch 204: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0284 - accuracy: 0.9837 - val_loss: 1.7784 - val_accuracy: 0.7419\n",
            "Epoch 205/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9810\n",
            "Epoch 205: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0380 - accuracy: 0.9810 - val_loss: 1.7823 - val_accuracy: 0.7419\n",
            "Epoch 206/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9891\n",
            "Epoch 206: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0280 - accuracy: 0.9891 - val_loss: 1.7751 - val_accuracy: 0.7204\n",
            "Epoch 207/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9891\n",
            "Epoch 207: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0321 - accuracy: 0.9891 - val_loss: 1.7982 - val_accuracy: 0.7097\n",
            "Epoch 208/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9837\n",
            "Epoch 208: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0407 - accuracy: 0.9837 - val_loss: 1.7874 - val_accuracy: 0.7097\n",
            "Epoch 209/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9891\n",
            "Epoch 209: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0335 - accuracy: 0.9891 - val_loss: 1.7700 - val_accuracy: 0.7634\n",
            "Epoch 210/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9755\n",
            "Epoch 210: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0470 - accuracy: 0.9755 - val_loss: 1.7261 - val_accuracy: 0.7742\n",
            "Epoch 211/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9783\n",
            "Epoch 211: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0467 - accuracy: 0.9783 - val_loss: 1.7599 - val_accuracy: 0.7419\n",
            "Epoch 212/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9810\n",
            "Epoch 212: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0349 - accuracy: 0.9810 - val_loss: 1.8433 - val_accuracy: 0.7419\n",
            "Epoch 213/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9891\n",
            "Epoch 213: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0361 - accuracy: 0.9891 - val_loss: 1.8411 - val_accuracy: 0.7204\n",
            "Epoch 214/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9864\n",
            "Epoch 214: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0352 - accuracy: 0.9864 - val_loss: 1.8541 - val_accuracy: 0.7097\n",
            "Epoch 215/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9810\n",
            "Epoch 215: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0421 - accuracy: 0.9810 - val_loss: 1.8351 - val_accuracy: 0.7097\n",
            "Epoch 216/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9728\n",
            "Epoch 216: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0588 - accuracy: 0.9728 - val_loss: 1.8210 - val_accuracy: 0.7419\n",
            "Epoch 217/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.9484\n",
            "Epoch 217: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.1670 - accuracy: 0.9484 - val_loss: 1.9733 - val_accuracy: 0.6774\n",
            "Epoch 218/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3184 - accuracy: 0.9076\n",
            "Epoch 218: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.3184 - accuracy: 0.9076 - val_loss: 1.7750 - val_accuracy: 0.6344\n",
            "Epoch 219/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3996 - accuracy: 0.8641\n",
            "Epoch 219: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.3996 - accuracy: 0.8641 - val_loss: 1.6733 - val_accuracy: 0.6667\n",
            "Epoch 220/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.8614\n",
            "Epoch 220: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.4271 - accuracy: 0.8614 - val_loss: 1.4165 - val_accuracy: 0.7097\n",
            "Epoch 221/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.9022\n",
            "Epoch 221: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.3082 - accuracy: 0.9022 - val_loss: 1.1547 - val_accuracy: 0.7419\n",
            "Epoch 222/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8913\n",
            "Epoch 222: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.2628 - accuracy: 0.8913 - val_loss: 1.3271 - val_accuracy: 0.7419\n",
            "Epoch 223/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.9212\n",
            "Epoch 223: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.2276 - accuracy: 0.9212 - val_loss: 1.3294 - val_accuracy: 0.7097\n",
            "Epoch 224/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1947 - accuracy: 0.9239\n",
            "Epoch 224: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1947 - accuracy: 0.9239 - val_loss: 1.4392 - val_accuracy: 0.6989\n",
            "Epoch 225/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.8940\n",
            "Epoch 225: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.2567 - accuracy: 0.8940 - val_loss: 1.1224 - val_accuracy: 0.7204\n",
            "Epoch 226/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9484\n",
            "Epoch 226: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.1362 - accuracy: 0.9484 - val_loss: 1.0140 - val_accuracy: 0.7419\n",
            "Epoch 227/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 0.9212\n",
            "Epoch 227: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1576 - accuracy: 0.9212 - val_loss: 0.9618 - val_accuracy: 0.7634\n",
            "Epoch 228/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9538\n",
            "Epoch 228: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.1075 - accuracy: 0.9538 - val_loss: 1.0347 - val_accuracy: 0.7527\n",
            "Epoch 229/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9592\n",
            "Epoch 229: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1014 - accuracy: 0.9592 - val_loss: 1.1325 - val_accuracy: 0.7527\n",
            "Epoch 230/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9755\n",
            "Epoch 230: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0851 - accuracy: 0.9755 - val_loss: 1.0423 - val_accuracy: 0.7097\n",
            "Epoch 231/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9674\n",
            "Epoch 231: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0947 - accuracy: 0.9674 - val_loss: 1.0552 - val_accuracy: 0.7312\n",
            "Epoch 232/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9783\n",
            "Epoch 232: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0764 - accuracy: 0.9783 - val_loss: 1.0942 - val_accuracy: 0.7204\n",
            "Epoch 233/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9837\n",
            "Epoch 233: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0544 - accuracy: 0.9837 - val_loss: 1.1836 - val_accuracy: 0.7204\n",
            "Epoch 234/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9728\n",
            "Epoch 234: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0601 - accuracy: 0.9728 - val_loss: 1.3018 - val_accuracy: 0.7312\n",
            "Epoch 235/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9918\n",
            "Epoch 235: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0433 - accuracy: 0.9918 - val_loss: 1.3696 - val_accuracy: 0.7312\n",
            "Epoch 236/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9783\n",
            "Epoch 236: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0547 - accuracy: 0.9783 - val_loss: 1.2662 - val_accuracy: 0.7312\n",
            "Epoch 237/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9864\n",
            "Epoch 237: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0441 - accuracy: 0.9864 - val_loss: 1.1974 - val_accuracy: 0.7527\n",
            "Epoch 238/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9891\n",
            "Epoch 238: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0387 - accuracy: 0.9891 - val_loss: 1.1432 - val_accuracy: 0.7527\n",
            "Epoch 239/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9837\n",
            "Epoch 239: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0425 - accuracy: 0.9837 - val_loss: 1.1843 - val_accuracy: 0.7312\n",
            "Epoch 240/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9918\n",
            "Epoch 240: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0369 - accuracy: 0.9918 - val_loss: 1.3241 - val_accuracy: 0.7204\n",
            "Epoch 241/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9837\n",
            "Epoch 241: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0376 - accuracy: 0.9837 - val_loss: 1.3821 - val_accuracy: 0.7419\n",
            "Epoch 242/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9864\n",
            "Epoch 242: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0410 - accuracy: 0.9864 - val_loss: 1.3918 - val_accuracy: 0.7419\n",
            "Epoch 243/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9810\n",
            "Epoch 243: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0385 - accuracy: 0.9810 - val_loss: 1.4101 - val_accuracy: 0.7312\n",
            "Epoch 244/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9891\n",
            "Epoch 244: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0360 - accuracy: 0.9891 - val_loss: 1.4188 - val_accuracy: 0.7312\n",
            "Epoch 245/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9891\n",
            "Epoch 245: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0298 - accuracy: 0.9891 - val_loss: 1.3742 - val_accuracy: 0.7419\n",
            "Epoch 246/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9837\n",
            "Epoch 246: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0413 - accuracy: 0.9837 - val_loss: 1.3555 - val_accuracy: 0.7527\n",
            "Epoch 247/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9864\n",
            "Epoch 247: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0392 - accuracy: 0.9864 - val_loss: 1.3690 - val_accuracy: 0.7419\n",
            "Epoch 248/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9864\n",
            "Epoch 248: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0395 - accuracy: 0.9864 - val_loss: 1.4526 - val_accuracy: 0.7312\n",
            "Epoch 249/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9810\n",
            "Epoch 249: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0367 - accuracy: 0.9810 - val_loss: 1.4502 - val_accuracy: 0.7312\n",
            "Epoch 250/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9783\n",
            "Epoch 250: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0394 - accuracy: 0.9783 - val_loss: 1.4359 - val_accuracy: 0.7419\n",
            "Epoch 251/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9864\n",
            "Epoch 251: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0325 - accuracy: 0.9864 - val_loss: 1.4574 - val_accuracy: 0.7312\n",
            "Epoch 252/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9891\n",
            "Epoch 252: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0313 - accuracy: 0.9891 - val_loss: 1.4744 - val_accuracy: 0.7312\n",
            "Epoch 253/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9891\n",
            "Epoch 253: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0329 - accuracy: 0.9891 - val_loss: 1.4610 - val_accuracy: 0.7312\n",
            "Epoch 254/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9891\n",
            "Epoch 254: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0303 - accuracy: 0.9891 - val_loss: 1.4079 - val_accuracy: 0.7419\n",
            "Epoch 255/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9810\n",
            "Epoch 255: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0312 - accuracy: 0.9810 - val_loss: 1.5481 - val_accuracy: 0.7097\n",
            "Epoch 256/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9810\n",
            "Epoch 256: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0355 - accuracy: 0.9810 - val_loss: 1.5962 - val_accuracy: 0.7097\n",
            "Epoch 257/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9837\n",
            "Epoch 257: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0346 - accuracy: 0.9837 - val_loss: 1.5092 - val_accuracy: 0.7204\n",
            "Epoch 258/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9783\n",
            "Epoch 258: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0359 - accuracy: 0.9783 - val_loss: 1.3966 - val_accuracy: 0.7419\n",
            "Epoch 259/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9864\n",
            "Epoch 259: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0319 - accuracy: 0.9864 - val_loss: 1.3660 - val_accuracy: 0.7527\n",
            "Epoch 260/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9864\n",
            "Epoch 260: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0368 - accuracy: 0.9864 - val_loss: 1.3846 - val_accuracy: 0.7634\n",
            "Epoch 261/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9837\n",
            "Epoch 261: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.0348 - accuracy: 0.9837 - val_loss: 1.4081 - val_accuracy: 0.7634\n",
            "Epoch 262/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9891\n",
            "Epoch 262: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0337 - accuracy: 0.9891 - val_loss: 1.4314 - val_accuracy: 0.7634\n",
            "Epoch 263/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9864\n",
            "Epoch 263: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 158ms/step - loss: 0.0337 - accuracy: 0.9864 - val_loss: 1.4518 - val_accuracy: 0.7634\n",
            "Epoch 264/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9810\n",
            "Epoch 264: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 0.0388 - accuracy: 0.9810 - val_loss: 1.4815 - val_accuracy: 0.7527\n",
            "Epoch 265/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0269 - accuracy: 0.9922\n",
            "Epoch 265: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.0322 - accuracy: 0.9891 - val_loss: 1.4910 - val_accuracy: 0.7527\n",
            "Epoch 266/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9946\n",
            "Epoch 266: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.0256 - accuracy: 0.9946 - val_loss: 1.4923 - val_accuracy: 0.7527\n",
            "Epoch 267/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9864\n",
            "Epoch 267: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.0272 - accuracy: 0.9864 - val_loss: 1.4920 - val_accuracy: 0.7527\n",
            "Epoch 268/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9837\n",
            "Epoch 268: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 201ms/step - loss: 0.0349 - accuracy: 0.9837 - val_loss: 1.5018 - val_accuracy: 0.7527\n",
            "Epoch 269/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9864\n",
            "Epoch 269: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 124ms/step - loss: 0.0291 - accuracy: 0.9864 - val_loss: 1.5062 - val_accuracy: 0.7527\n",
            "Epoch 270/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9864\n",
            "Epoch 270: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 131ms/step - loss: 0.0279 - accuracy: 0.9864 - val_loss: 1.5056 - val_accuracy: 0.7527\n",
            "Epoch 271/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9918\n",
            "Epoch 271: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 160ms/step - loss: 0.0228 - accuracy: 0.9918 - val_loss: 1.4988 - val_accuracy: 0.7634\n",
            "Epoch 272/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9891\n",
            "Epoch 272: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 186ms/step - loss: 0.0269 - accuracy: 0.9891 - val_loss: 1.4969 - val_accuracy: 0.7634\n",
            "Epoch 273/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9891\n",
            "Epoch 273: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 207ms/step - loss: 0.0233 - accuracy: 0.9891 - val_loss: 1.5013 - val_accuracy: 0.7634\n",
            "Epoch 274/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9891\n",
            "Epoch 274: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 172ms/step - loss: 0.0263 - accuracy: 0.9891 - val_loss: 1.4982 - val_accuracy: 0.7634\n",
            "Epoch 275/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9864\n",
            "Epoch 275: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 0.0298 - accuracy: 0.9864 - val_loss: 1.5120 - val_accuracy: 0.7527\n",
            "Epoch 276/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9946\n",
            "Epoch 276: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 135ms/step - loss: 0.0231 - accuracy: 0.9946 - val_loss: 1.5389 - val_accuracy: 0.7419\n",
            "Epoch 277/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9837\n",
            "Epoch 277: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.0292 - accuracy: 0.9837 - val_loss: 1.5387 - val_accuracy: 0.7527\n",
            "Epoch 278/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9891\n",
            "Epoch 278: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0271 - accuracy: 0.9891 - val_loss: 1.5476 - val_accuracy: 0.7527\n",
            "Epoch 279/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9918\n",
            "Epoch 279: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 127ms/step - loss: 0.0215 - accuracy: 0.9918 - val_loss: 1.5507 - val_accuracy: 0.7527\n",
            "Epoch 280/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0225 - accuracy: 0.9922\n",
            "Epoch 280: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.0273 - accuracy: 0.9891 - val_loss: 1.5406 - val_accuracy: 0.7527\n",
            "Epoch 281/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9864\n",
            "Epoch 281: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0293 - accuracy: 0.9864 - val_loss: 1.5356 - val_accuracy: 0.7527\n",
            "Epoch 282/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9946\n",
            "Epoch 282: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0227 - accuracy: 0.9946 - val_loss: 1.5421 - val_accuracy: 0.7419\n",
            "Epoch 283/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9918\n",
            "Epoch 283: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0247 - accuracy: 0.9918 - val_loss: 1.5459 - val_accuracy: 0.7312\n",
            "Epoch 284/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9891\n",
            "Epoch 284: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0294 - accuracy: 0.9891 - val_loss: 1.5469 - val_accuracy: 0.7312\n",
            "Epoch 285/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9891\n",
            "Epoch 285: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0258 - accuracy: 0.9891 - val_loss: 1.5656 - val_accuracy: 0.7312\n",
            "Epoch 286/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9918\n",
            "Epoch 286: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0204 - accuracy: 0.9918 - val_loss: 1.5751 - val_accuracy: 0.7312\n",
            "Epoch 287/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9891\n",
            "Epoch 287: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0267 - accuracy: 0.9891 - val_loss: 1.5611 - val_accuracy: 0.7312\n",
            "Epoch 288/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9864\n",
            "Epoch 288: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0309 - accuracy: 0.9864 - val_loss: 1.5621 - val_accuracy: 0.7312\n",
            "Epoch 289/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9837\n",
            "Epoch 289: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0300 - accuracy: 0.9837 - val_loss: 1.5362 - val_accuracy: 0.7312\n",
            "Epoch 290/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9783\n",
            "Epoch 290: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0319 - accuracy: 0.9783 - val_loss: 1.5212 - val_accuracy: 0.7312\n",
            "Epoch 291/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9810\n",
            "Epoch 291: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0386 - accuracy: 0.9810 - val_loss: 1.4751 - val_accuracy: 0.7527\n",
            "Epoch 292/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9755\n",
            "Epoch 292: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0485 - accuracy: 0.9755 - val_loss: 1.4075 - val_accuracy: 0.7527\n",
            "Epoch 293/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9864\n",
            "Epoch 293: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0351 - accuracy: 0.9864 - val_loss: 1.4392 - val_accuracy: 0.7527\n",
            "Epoch 294/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9864\n",
            "Epoch 294: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0320 - accuracy: 0.9864 - val_loss: 1.5194 - val_accuracy: 0.7312\n",
            "Epoch 295/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9864\n",
            "Epoch 295: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0314 - accuracy: 0.9864 - val_loss: 1.5788 - val_accuracy: 0.7312\n",
            "Epoch 296/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9810\n",
            "Epoch 296: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0405 - accuracy: 0.9810 - val_loss: 1.6105 - val_accuracy: 0.7312\n",
            "Epoch 297/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9837\n",
            "Epoch 297: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0397 - accuracy: 0.9837 - val_loss: 1.5979 - val_accuracy: 0.7419\n",
            "Epoch 298/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9810\n",
            "Epoch 298: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0301 - accuracy: 0.9810 - val_loss: 1.5857 - val_accuracy: 0.7527\n",
            "Epoch 299/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0239 - accuracy: 0.9883\n",
            "Epoch 299: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0290 - accuracy: 0.9864 - val_loss: 1.5675 - val_accuracy: 0.7527\n",
            "Epoch 300/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9864\n",
            "Epoch 300: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0313 - accuracy: 0.9864 - val_loss: 1.5510 - val_accuracy: 0.7634\n",
            "Epoch 301/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9891\n",
            "Epoch 301: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.0300 - accuracy: 0.9891 - val_loss: 1.5485 - val_accuracy: 0.7634\n",
            "Epoch 302/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9946\n",
            "Epoch 302: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 131ms/step - loss: 0.0215 - accuracy: 0.9946 - val_loss: 1.5637 - val_accuracy: 0.7634\n",
            "Epoch 303/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9891\n",
            "Epoch 303: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 207ms/step - loss: 0.0301 - accuracy: 0.9891 - val_loss: 1.5615 - val_accuracy: 0.7634\n",
            "Epoch 304/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9864\n",
            "Epoch 304: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 196ms/step - loss: 0.0262 - accuracy: 0.9864 - val_loss: 1.5862 - val_accuracy: 0.7634\n",
            "Epoch 305/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9891\n",
            "Epoch 305: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 199ms/step - loss: 0.0275 - accuracy: 0.9891 - val_loss: 1.6107 - val_accuracy: 0.7634\n",
            "Epoch 306/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9891\n",
            "Epoch 306: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 170ms/step - loss: 0.0328 - accuracy: 0.9891 - val_loss: 1.6506 - val_accuracy: 0.7527\n",
            "Epoch 307/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9810\n",
            "Epoch 307: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 196ms/step - loss: 0.0401 - accuracy: 0.9810 - val_loss: 1.6315 - val_accuracy: 0.7634\n",
            "Epoch 308/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9891\n",
            "Epoch 308: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 134ms/step - loss: 0.0262 - accuracy: 0.9891 - val_loss: 1.6342 - val_accuracy: 0.7634\n",
            "Epoch 309/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9864\n",
            "Epoch 309: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 201ms/step - loss: 0.0279 - accuracy: 0.9864 - val_loss: 1.6465 - val_accuracy: 0.7634\n",
            "Epoch 310/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9755\n",
            "Epoch 310: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 187ms/step - loss: 0.0330 - accuracy: 0.9755 - val_loss: 1.6514 - val_accuracy: 0.7634\n",
            "Epoch 311/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9837\n",
            "Epoch 311: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 267ms/step - loss: 0.0280 - accuracy: 0.9837 - val_loss: 1.6427 - val_accuracy: 0.7634\n",
            "Epoch 312/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9891\n",
            "Epoch 312: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 263ms/step - loss: 0.0219 - accuracy: 0.9891 - val_loss: 1.6209 - val_accuracy: 0.7634\n",
            "Epoch 313/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9837\n",
            "Epoch 313: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 290ms/step - loss: 0.0297 - accuracy: 0.9837 - val_loss: 1.6142 - val_accuracy: 0.7634\n",
            "Epoch 314/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9810\n",
            "Epoch 314: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 238ms/step - loss: 0.0281 - accuracy: 0.9810 - val_loss: 1.6123 - val_accuracy: 0.7634\n",
            "Epoch 315/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9891\n",
            "Epoch 315: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 242ms/step - loss: 0.0243 - accuracy: 0.9891 - val_loss: 1.6471 - val_accuracy: 0.7419\n",
            "Epoch 316/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9891\n",
            "Epoch 316: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 225ms/step - loss: 0.0279 - accuracy: 0.9891 - val_loss: 1.6460 - val_accuracy: 0.7419\n",
            "Epoch 317/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9864\n",
            "Epoch 317: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 228ms/step - loss: 0.0251 - accuracy: 0.9864 - val_loss: 1.6316 - val_accuracy: 0.7527\n",
            "Epoch 318/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9864\n",
            "Epoch 318: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 216ms/step - loss: 0.0281 - accuracy: 0.9864 - val_loss: 1.6187 - val_accuracy: 0.7527\n",
            "Epoch 319/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9864\n",
            "Epoch 319: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 199ms/step - loss: 0.0297 - accuracy: 0.9864 - val_loss: 1.6299 - val_accuracy: 0.7527\n",
            "Epoch 320/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9891\n",
            "Epoch 320: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 211ms/step - loss: 0.0229 - accuracy: 0.9891 - val_loss: 1.6185 - val_accuracy: 0.7527\n",
            "Epoch 321/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9918\n",
            "Epoch 321: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 190ms/step - loss: 0.0189 - accuracy: 0.9918 - val_loss: 1.6238 - val_accuracy: 0.7634\n",
            "Epoch 322/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9891\n",
            "Epoch 322: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 161ms/step - loss: 0.0240 - accuracy: 0.9891 - val_loss: 1.6446 - val_accuracy: 0.7419\n",
            "Epoch 323/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9891\n",
            "Epoch 323: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 171ms/step - loss: 0.0242 - accuracy: 0.9891 - val_loss: 1.6712 - val_accuracy: 0.7419\n",
            "Epoch 324/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9946\n",
            "Epoch 324: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 166ms/step - loss: 0.0244 - accuracy: 0.9946 - val_loss: 1.7055 - val_accuracy: 0.7527\n",
            "Epoch 325/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9891\n",
            "Epoch 325: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.0246 - accuracy: 0.9891 - val_loss: 1.7098 - val_accuracy: 0.7419\n",
            "Epoch 326/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9864\n",
            "Epoch 326: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 176ms/step - loss: 0.0257 - accuracy: 0.9864 - val_loss: 1.6972 - val_accuracy: 0.7527\n",
            "Epoch 327/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9891\n",
            "Epoch 327: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 198ms/step - loss: 0.0247 - accuracy: 0.9891 - val_loss: 1.6961 - val_accuracy: 0.7419\n",
            "Epoch 328/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9918\n",
            "Epoch 328: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 159ms/step - loss: 0.0209 - accuracy: 0.9918 - val_loss: 1.7039 - val_accuracy: 0.7204\n",
            "Epoch 329/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9891\n",
            "Epoch 329: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 144ms/step - loss: 0.0241 - accuracy: 0.9891 - val_loss: 1.6863 - val_accuracy: 0.7634\n",
            "Epoch 330/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9783\n",
            "Epoch 330: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 160ms/step - loss: 0.0314 - accuracy: 0.9783 - val_loss: 1.6589 - val_accuracy: 0.7742\n",
            "Epoch 331/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9864\n",
            "Epoch 331: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 175ms/step - loss: 0.0267 - accuracy: 0.9864 - val_loss: 1.6502 - val_accuracy: 0.7742\n",
            "Epoch 332/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0265 - accuracy: 0.9844\n",
            "Epoch 332: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 123ms/step - loss: 0.0280 - accuracy: 0.9864 - val_loss: 1.6649 - val_accuracy: 0.7742\n",
            "Epoch 333/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9864\n",
            "Epoch 333: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 186ms/step - loss: 0.0304 - accuracy: 0.9864 - val_loss: 1.6589 - val_accuracy: 0.7634\n",
            "Epoch 334/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9891\n",
            "Epoch 334: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 176ms/step - loss: 0.0251 - accuracy: 0.9891 - val_loss: 1.6708 - val_accuracy: 0.7634\n",
            "Epoch 335/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9918\n",
            "Epoch 335: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 239ms/step - loss: 0.0219 - accuracy: 0.9918 - val_loss: 1.7050 - val_accuracy: 0.7634\n",
            "Epoch 336/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9918\n",
            "Epoch 336: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 161ms/step - loss: 0.0253 - accuracy: 0.9918 - val_loss: 1.7354 - val_accuracy: 0.7634\n",
            "Epoch 337/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9891\n",
            "Epoch 337: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 205ms/step - loss: 0.0250 - accuracy: 0.9891 - val_loss: 1.7545 - val_accuracy: 0.7527\n",
            "Epoch 338/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9864\n",
            "Epoch 338: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.0288 - accuracy: 0.9864 - val_loss: 1.7315 - val_accuracy: 0.7634\n",
            "Epoch 339/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9864\n",
            "Epoch 339: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 138ms/step - loss: 0.0328 - accuracy: 0.9864 - val_loss: 1.7185 - val_accuracy: 0.7634\n",
            "Epoch 340/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9837\n",
            "Epoch 340: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 166ms/step - loss: 0.0276 - accuracy: 0.9837 - val_loss: 1.7220 - val_accuracy: 0.7634\n",
            "Epoch 341/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9891\n",
            "Epoch 341: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.0199 - accuracy: 0.9891 - val_loss: 1.7220 - val_accuracy: 0.7634\n",
            "Epoch 342/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9810\n",
            "Epoch 342: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 283ms/step - loss: 0.0282 - accuracy: 0.9810 - val_loss: 1.7134 - val_accuracy: 0.7634\n",
            "Epoch 343/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9864\n",
            "Epoch 343: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 241ms/step - loss: 0.0277 - accuracy: 0.9864 - val_loss: 1.6929 - val_accuracy: 0.7527\n",
            "Epoch 344/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9918\n",
            "Epoch 344: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 211ms/step - loss: 0.0241 - accuracy: 0.9918 - val_loss: 1.6830 - val_accuracy: 0.7419\n",
            "Epoch 345/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9864\n",
            "Epoch 345: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 243ms/step - loss: 0.0227 - accuracy: 0.9864 - val_loss: 1.6750 - val_accuracy: 0.7527\n",
            "Epoch 346/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9810\n",
            "Epoch 346: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 212ms/step - loss: 0.0297 - accuracy: 0.9810 - val_loss: 1.6814 - val_accuracy: 0.7527\n",
            "Epoch 347/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9864\n",
            "Epoch 347: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 167ms/step - loss: 0.0288 - accuracy: 0.9864 - val_loss: 1.7041 - val_accuracy: 0.7419\n",
            "Epoch 348/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9891\n",
            "Epoch 348: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 184ms/step - loss: 0.0214 - accuracy: 0.9891 - val_loss: 1.7167 - val_accuracy: 0.7527\n",
            "Epoch 349/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9864\n",
            "Epoch 349: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 210ms/step - loss: 0.0324 - accuracy: 0.9864 - val_loss: 1.7403 - val_accuracy: 0.7312\n",
            "Epoch 350/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9891\n",
            "Epoch 350: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 270ms/step - loss: 0.0251 - accuracy: 0.9891 - val_loss: 1.7381 - val_accuracy: 0.7312\n",
            "Epoch 351/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9891\n",
            "Epoch 351: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 188ms/step - loss: 0.0245 - accuracy: 0.9891 - val_loss: 1.7392 - val_accuracy: 0.7312\n",
            "Epoch 352/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9891\n",
            "Epoch 352: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 173ms/step - loss: 0.0235 - accuracy: 0.9891 - val_loss: 1.7351 - val_accuracy: 0.7312\n",
            "Epoch 353/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9891\n",
            "Epoch 353: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 198ms/step - loss: 0.0251 - accuracy: 0.9891 - val_loss: 1.7371 - val_accuracy: 0.7312\n",
            "Epoch 354/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9891\n",
            "Epoch 354: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 157ms/step - loss: 0.0233 - accuracy: 0.9891 - val_loss: 1.7290 - val_accuracy: 0.7527\n",
            "Epoch 355/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9837\n",
            "Epoch 355: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 165ms/step - loss: 0.0237 - accuracy: 0.9837 - val_loss: 1.7270 - val_accuracy: 0.7527\n",
            "Epoch 356/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9918\n",
            "Epoch 356: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 178ms/step - loss: 0.0210 - accuracy: 0.9918 - val_loss: 1.7424 - val_accuracy: 0.7527\n",
            "Epoch 357/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9864\n",
            "Epoch 357: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 174ms/step - loss: 0.0223 - accuracy: 0.9864 - val_loss: 1.7573 - val_accuracy: 0.7527\n",
            "Epoch 358/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9864\n",
            "Epoch 358: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 210ms/step - loss: 0.0220 - accuracy: 0.9864 - val_loss: 1.7599 - val_accuracy: 0.7527\n",
            "Epoch 359/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9864\n",
            "Epoch 359: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 161ms/step - loss: 0.0268 - accuracy: 0.9864 - val_loss: 1.7623 - val_accuracy: 0.7527\n",
            "Epoch 360/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9891\n",
            "Epoch 360: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 166ms/step - loss: 0.0270 - accuracy: 0.9891 - val_loss: 1.7426 - val_accuracy: 0.7527\n",
            "Epoch 361/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9864\n",
            "Epoch 361: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 133ms/step - loss: 0.0242 - accuracy: 0.9864 - val_loss: 1.7180 - val_accuracy: 0.7527\n",
            "Epoch 362/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9918\n",
            "Epoch 362: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 160ms/step - loss: 0.0241 - accuracy: 0.9918 - val_loss: 1.6933 - val_accuracy: 0.7527\n",
            "Epoch 363/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9810\n",
            "Epoch 363: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 164ms/step - loss: 0.0261 - accuracy: 0.9810 - val_loss: 1.6948 - val_accuracy: 0.7527\n",
            "Epoch 364/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9810\n",
            "Epoch 364: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.0262 - accuracy: 0.9810 - val_loss: 1.7197 - val_accuracy: 0.7419\n",
            "Epoch 365/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9810\n",
            "Epoch 365: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 118ms/step - loss: 0.0304 - accuracy: 0.9810 - val_loss: 1.6734 - val_accuracy: 0.7527\n",
            "Epoch 366/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9918\n",
            "Epoch 366: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.0248 - accuracy: 0.9918 - val_loss: 1.6546 - val_accuracy: 0.7419\n",
            "Epoch 367/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9837\n",
            "Epoch 367: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 160ms/step - loss: 0.0337 - accuracy: 0.9837 - val_loss: 1.6422 - val_accuracy: 0.7634\n",
            "Epoch 368/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9864\n",
            "Epoch 368: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 199ms/step - loss: 0.0311 - accuracy: 0.9864 - val_loss: 1.6760 - val_accuracy: 0.7419\n",
            "Epoch 369/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9837\n",
            "Epoch 369: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 183ms/step - loss: 0.0407 - accuracy: 0.9837 - val_loss: 1.7588 - val_accuracy: 0.7097\n",
            "Epoch 370/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9810\n",
            "Epoch 370: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 124ms/step - loss: 0.0387 - accuracy: 0.9810 - val_loss: 1.7622 - val_accuracy: 0.7312\n",
            "Epoch 371/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9864\n",
            "Epoch 371: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.0382 - accuracy: 0.9864 - val_loss: 1.7974 - val_accuracy: 0.7527\n",
            "Epoch 372/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9864\n",
            "Epoch 372: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 162ms/step - loss: 0.0366 - accuracy: 0.9864 - val_loss: 1.7849 - val_accuracy: 0.7312\n",
            "Epoch 373/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9837\n",
            "Epoch 373: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 0.0326 - accuracy: 0.9837 - val_loss: 1.7756 - val_accuracy: 0.7097\n",
            "Epoch 374/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9891\n",
            "Epoch 374: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 133ms/step - loss: 0.0267 - accuracy: 0.9891 - val_loss: 1.7688 - val_accuracy: 0.7204\n",
            "Epoch 375/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9810\n",
            "Epoch 375: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 170ms/step - loss: 0.0334 - accuracy: 0.9810 - val_loss: 1.7659 - val_accuracy: 0.7312\n",
            "Epoch 376/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9837\n",
            "Epoch 376: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 237ms/step - loss: 0.0300 - accuracy: 0.9837 - val_loss: 1.7718 - val_accuracy: 0.7097\n",
            "Epoch 377/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9783\n",
            "Epoch 377: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 271ms/step - loss: 0.0348 - accuracy: 0.9783 - val_loss: 1.7526 - val_accuracy: 0.7419\n",
            "Epoch 378/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9837\n",
            "Epoch 378: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 223ms/step - loss: 0.0345 - accuracy: 0.9837 - val_loss: 1.7745 - val_accuracy: 0.7312\n",
            "Epoch 379/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9891\n",
            "Epoch 379: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 268ms/step - loss: 0.0204 - accuracy: 0.9891 - val_loss: 1.7621 - val_accuracy: 0.7419\n",
            "Epoch 380/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9837\n",
            "Epoch 380: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 195ms/step - loss: 0.0268 - accuracy: 0.9837 - val_loss: 1.7484 - val_accuracy: 0.7419\n",
            "Epoch 381/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9810\n",
            "Epoch 381: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 159ms/step - loss: 0.0295 - accuracy: 0.9810 - val_loss: 1.7568 - val_accuracy: 0.7419\n",
            "Epoch 382/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9864\n",
            "Epoch 382: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 161ms/step - loss: 0.0273 - accuracy: 0.9864 - val_loss: 1.7774 - val_accuracy: 0.7312\n",
            "Epoch 383/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9837\n",
            "Epoch 383: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 193ms/step - loss: 0.0279 - accuracy: 0.9837 - val_loss: 1.7877 - val_accuracy: 0.7312\n",
            "Epoch 384/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9810\n",
            "Epoch 384: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 176ms/step - loss: 0.0358 - accuracy: 0.9810 - val_loss: 1.7538 - val_accuracy: 0.7312\n",
            "Epoch 385/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9728\n",
            "Epoch 385: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 260ms/step - loss: 0.0753 - accuracy: 0.9728 - val_loss: 1.6720 - val_accuracy: 0.7312\n",
            "Epoch 386/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9755\n",
            "Epoch 386: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 1s 267ms/step - loss: 0.0492 - accuracy: 0.9755 - val_loss: 1.5722 - val_accuracy: 0.7419\n",
            "Epoch 387/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9837\n",
            "Epoch 387: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 178ms/step - loss: 0.0477 - accuracy: 0.9837 - val_loss: 1.5693 - val_accuracy: 0.7527\n",
            "Epoch 388/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9864\n",
            "Epoch 388: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 167ms/step - loss: 0.0308 - accuracy: 0.9864 - val_loss: 1.5883 - val_accuracy: 0.7419\n",
            "Epoch 389/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9783\n",
            "Epoch 389: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.0368 - accuracy: 0.9783 - val_loss: 1.6255 - val_accuracy: 0.7527\n",
            "Epoch 390/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9810\n",
            "Epoch 390: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 124ms/step - loss: 0.0337 - accuracy: 0.9810 - val_loss: 1.6493 - val_accuracy: 0.7527\n",
            "Epoch 391/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9837\n",
            "Epoch 391: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 0.0377 - accuracy: 0.9837 - val_loss: 1.6775 - val_accuracy: 0.7527\n",
            "Epoch 392/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0243 - accuracy: 0.9883\n",
            "Epoch 392: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0308 - accuracy: 0.9864 - val_loss: 1.7136 - val_accuracy: 0.7527\n",
            "Epoch 393/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9810\n",
            "Epoch 393: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0308 - accuracy: 0.9810 - val_loss: 1.7379 - val_accuracy: 0.7419\n",
            "Epoch 394/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9891\n",
            "Epoch 394: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0269 - accuracy: 0.9891 - val_loss: 1.7678 - val_accuracy: 0.7419\n",
            "Epoch 395/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9810\n",
            "Epoch 395: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0331 - accuracy: 0.9810 - val_loss: 1.7787 - val_accuracy: 0.7312\n",
            "Epoch 396/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9864\n",
            "Epoch 396: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0293 - accuracy: 0.9864 - val_loss: 1.8022 - val_accuracy: 0.7204\n",
            "Epoch 397/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9864\n",
            "Epoch 397: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0274 - accuracy: 0.9864 - val_loss: 1.7849 - val_accuracy: 0.7419\n",
            "Epoch 398/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9864\n",
            "Epoch 398: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0256 - accuracy: 0.9864 - val_loss: 1.7688 - val_accuracy: 0.7419\n",
            "Epoch 399/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9891\n",
            "Epoch 399: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0235 - accuracy: 0.9891 - val_loss: 1.7466 - val_accuracy: 0.7527\n",
            "Epoch 400/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9837\n",
            "Epoch 400: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0256 - accuracy: 0.9837 - val_loss: 1.7316 - val_accuracy: 0.7527\n",
            "Epoch 401/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9918\n",
            "Epoch 401: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0231 - accuracy: 0.9918 - val_loss: 1.7242 - val_accuracy: 0.7419\n",
            "Epoch 402/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9946\n",
            "Epoch 402: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0215 - accuracy: 0.9946 - val_loss: 1.7234 - val_accuracy: 0.7419\n",
            "Epoch 403/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9918\n",
            "Epoch 403: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0210 - accuracy: 0.9918 - val_loss: 1.7775 - val_accuracy: 0.7419\n",
            "Epoch 404/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9864\n",
            "Epoch 404: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0261 - accuracy: 0.9864 - val_loss: 1.8204 - val_accuracy: 0.7527\n",
            "Epoch 405/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9864\n",
            "Epoch 405: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0245 - accuracy: 0.9864 - val_loss: 1.8281 - val_accuracy: 0.7419\n",
            "Epoch 406/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9837\n",
            "Epoch 406: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0246 - accuracy: 0.9837 - val_loss: 1.8210 - val_accuracy: 0.7312\n",
            "Epoch 407/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9810\n",
            "Epoch 407: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0323 - accuracy: 0.9810 - val_loss: 1.7657 - val_accuracy: 0.7419\n",
            "Epoch 408/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9864\n",
            "Epoch 408: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0226 - accuracy: 0.9864 - val_loss: 1.6699 - val_accuracy: 0.7527\n",
            "Epoch 409/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9783\n",
            "Epoch 409: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0515 - accuracy: 0.9783 - val_loss: 2.0157 - val_accuracy: 0.7097\n",
            "Epoch 410/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9647\n",
            "Epoch 410: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.1678 - accuracy: 0.9647 - val_loss: 1.9930 - val_accuracy: 0.6882\n",
            "Epoch 411/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9484\n",
            "Epoch 411: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.1613 - accuracy: 0.9484 - val_loss: 2.0895 - val_accuracy: 0.6882\n",
            "Epoch 412/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9457\n",
            "Epoch 412: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.1926 - accuracy: 0.9457 - val_loss: 1.6751 - val_accuracy: 0.7312\n",
            "Epoch 413/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9538\n",
            "Epoch 413: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.1180 - accuracy: 0.9538 - val_loss: 1.9837 - val_accuracy: 0.6989\n",
            "Epoch 414/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2245 - accuracy: 0.9212\n",
            "Epoch 414: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.2245 - accuracy: 0.9212 - val_loss: 1.4074 - val_accuracy: 0.7312\n",
            "Epoch 415/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1113 - accuracy: 0.9727\n",
            "Epoch 415: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1586 - accuracy: 0.9429 - val_loss: 1.5135 - val_accuracy: 0.6667\n",
            "Epoch 416/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1866 - accuracy: 0.9402\n",
            "Epoch 416: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.1866 - accuracy: 0.9402 - val_loss: 1.3929 - val_accuracy: 0.7097\n",
            "Epoch 417/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1286 - accuracy: 0.9538\n",
            "Epoch 417: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.1286 - accuracy: 0.9538 - val_loss: 1.3813 - val_accuracy: 0.7204\n",
            "Epoch 418/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1237 - accuracy: 0.9620\n",
            "Epoch 418: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.1237 - accuracy: 0.9620 - val_loss: 1.4258 - val_accuracy: 0.7204\n",
            "Epoch 419/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9755\n",
            "Epoch 419: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0805 - accuracy: 0.9755 - val_loss: 1.4128 - val_accuracy: 0.7419\n",
            "Epoch 420/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9511\n",
            "Epoch 420: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0880 - accuracy: 0.9511 - val_loss: 1.4739 - val_accuracy: 0.7312\n",
            "Epoch 421/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9511\n",
            "Epoch 421: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.1132 - accuracy: 0.9511 - val_loss: 1.4913 - val_accuracy: 0.7419\n",
            "Epoch 422/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9783\n",
            "Epoch 422: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0799 - accuracy: 0.9783 - val_loss: 1.4371 - val_accuracy: 0.7527\n",
            "Epoch 423/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9810\n",
            "Epoch 423: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0639 - accuracy: 0.9810 - val_loss: 1.5115 - val_accuracy: 0.7204\n",
            "Epoch 424/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9783\n",
            "Epoch 424: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0652 - accuracy: 0.9783 - val_loss: 1.5030 - val_accuracy: 0.7312\n",
            "Epoch 425/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9837\n",
            "Epoch 425: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0459 - accuracy: 0.9837 - val_loss: 1.4977 - val_accuracy: 0.7312\n",
            "Epoch 426/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9864\n",
            "Epoch 426: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0460 - accuracy: 0.9864 - val_loss: 1.4334 - val_accuracy: 0.7097\n",
            "Epoch 427/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9864\n",
            "Epoch 427: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0435 - accuracy: 0.9864 - val_loss: 1.5139 - val_accuracy: 0.6989\n",
            "Epoch 428/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9918\n",
            "Epoch 428: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0323 - accuracy: 0.9918 - val_loss: 1.7250 - val_accuracy: 0.7097\n",
            "Epoch 429/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9701\n",
            "Epoch 429: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0645 - accuracy: 0.9701 - val_loss: 1.7073 - val_accuracy: 0.7312\n",
            "Epoch 430/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9810\n",
            "Epoch 430: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0563 - accuracy: 0.9810 - val_loss: 1.6293 - val_accuracy: 0.7097\n",
            "Epoch 431/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9783\n",
            "Epoch 431: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0606 - accuracy: 0.9783 - val_loss: 1.5830 - val_accuracy: 0.6989\n",
            "Epoch 432/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9728\n",
            "Epoch 432: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0670 - accuracy: 0.9728 - val_loss: 1.5803 - val_accuracy: 0.6989\n",
            "Epoch 433/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9864\n",
            "Epoch 433: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0398 - accuracy: 0.9864 - val_loss: 1.6088 - val_accuracy: 0.7204\n",
            "Epoch 434/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9837\n",
            "Epoch 434: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0449 - accuracy: 0.9837 - val_loss: 1.6392 - val_accuracy: 0.7419\n",
            "Epoch 435/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9864\n",
            "Epoch 435: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0332 - accuracy: 0.9864 - val_loss: 1.6731 - val_accuracy: 0.7312\n",
            "Epoch 436/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9918\n",
            "Epoch 436: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0287 - accuracy: 0.9918 - val_loss: 1.6458 - val_accuracy: 0.7312\n",
            "Epoch 437/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9864\n",
            "Epoch 437: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.0323 - accuracy: 0.9864 - val_loss: 1.6404 - val_accuracy: 0.7312\n",
            "Epoch 438/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9837\n",
            "Epoch 438: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0392 - accuracy: 0.9837 - val_loss: 1.6401 - val_accuracy: 0.7312\n",
            "Epoch 439/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9810\n",
            "Epoch 439: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0290 - accuracy: 0.9810 - val_loss: 1.6389 - val_accuracy: 0.7312\n",
            "Epoch 440/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9891\n",
            "Epoch 440: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0283 - accuracy: 0.9891 - val_loss: 1.6189 - val_accuracy: 0.7312\n",
            "Epoch 441/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0353 - accuracy: 0.9883\n",
            "Epoch 441: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.0287 - accuracy: 0.9918 - val_loss: 1.6240 - val_accuracy: 0.7419\n",
            "Epoch 442/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9864\n",
            "Epoch 442: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0257 - accuracy: 0.9864 - val_loss: 1.6602 - val_accuracy: 0.7419\n",
            "Epoch 443/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9810\n",
            "Epoch 443: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.0253 - accuracy: 0.9810 - val_loss: 1.6719 - val_accuracy: 0.7312\n",
            "Epoch 444/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0340 - accuracy: 0.9883\n",
            "Epoch 444: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.0301 - accuracy: 0.9891 - val_loss: 1.6817 - val_accuracy: 0.7204\n",
            "Epoch 445/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9837\n",
            "Epoch 445: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0282 - accuracy: 0.9837 - val_loss: 1.7003 - val_accuracy: 0.7204\n",
            "Epoch 446/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9918\n",
            "Epoch 446: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0231 - accuracy: 0.9918 - val_loss: 1.7205 - val_accuracy: 0.7097\n",
            "Epoch 447/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9891\n",
            "Epoch 447: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0233 - accuracy: 0.9891 - val_loss: 1.7367 - val_accuracy: 0.7097\n",
            "Epoch 448/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9918\n",
            "Epoch 448: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 121ms/step - loss: 0.0245 - accuracy: 0.9918 - val_loss: 1.7453 - val_accuracy: 0.6989\n",
            "Epoch 449/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0466 - accuracy: 0.9766\n",
            "Epoch 449: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0372 - accuracy: 0.9810 - val_loss: 1.6847 - val_accuracy: 0.6989\n",
            "Epoch 450/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9891\n",
            "Epoch 450: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0241 - accuracy: 0.9891 - val_loss: 1.6776 - val_accuracy: 0.7312\n",
            "Epoch 451/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9864\n",
            "Epoch 451: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0349 - accuracy: 0.9864 - val_loss: 1.6871 - val_accuracy: 0.7419\n",
            "Epoch 452/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9864\n",
            "Epoch 452: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0329 - accuracy: 0.9864 - val_loss: 1.7029 - val_accuracy: 0.7097\n",
            "Epoch 453/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9810\n",
            "Epoch 453: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0330 - accuracy: 0.9810 - val_loss: 1.7709 - val_accuracy: 0.7097\n",
            "Epoch 454/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0246 - accuracy: 0.9844\n",
            "Epoch 454: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.0273 - accuracy: 0.9837 - val_loss: 1.8140 - val_accuracy: 0.7312\n",
            "Epoch 455/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9810\n",
            "Epoch 455: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0366 - accuracy: 0.9810 - val_loss: 1.7890 - val_accuracy: 0.7204\n",
            "Epoch 456/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0339 - accuracy: 0.9805\n",
            "Epoch 456: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.0452 - accuracy: 0.9783 - val_loss: 1.7794 - val_accuracy: 0.7204\n",
            "Epoch 457/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9891\n",
            "Epoch 457: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.0253 - accuracy: 0.9891 - val_loss: 1.7663 - val_accuracy: 0.7204\n",
            "Epoch 458/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9783\n",
            "Epoch 458: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0409 - accuracy: 0.9783 - val_loss: 1.7319 - val_accuracy: 0.7312\n",
            "Epoch 459/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9891\n",
            "Epoch 459: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0235 - accuracy: 0.9891 - val_loss: 1.7368 - val_accuracy: 0.7204\n",
            "Epoch 460/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9864\n",
            "Epoch 460: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0298 - accuracy: 0.9864 - val_loss: 1.7307 - val_accuracy: 0.7097\n",
            "Epoch 461/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9864\n",
            "Epoch 461: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0348 - accuracy: 0.9864 - val_loss: 1.6670 - val_accuracy: 0.7204\n",
            "Epoch 462/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9918\n",
            "Epoch 462: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0237 - accuracy: 0.9918 - val_loss: 1.6793 - val_accuracy: 0.7419\n",
            "Epoch 463/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9864\n",
            "Epoch 463: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0305 - accuracy: 0.9864 - val_loss: 1.7265 - val_accuracy: 0.7419\n",
            "Epoch 464/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9864\n",
            "Epoch 464: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0328 - accuracy: 0.9864 - val_loss: 1.8351 - val_accuracy: 0.7312\n",
            "Epoch 465/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9891\n",
            "Epoch 465: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0241 - accuracy: 0.9891 - val_loss: 1.8811 - val_accuracy: 0.7204\n",
            "Epoch 466/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9837\n",
            "Epoch 466: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0331 - accuracy: 0.9837 - val_loss: 1.9108 - val_accuracy: 0.7204\n",
            "Epoch 467/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9891\n",
            "Epoch 467: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0253 - accuracy: 0.9891 - val_loss: 1.9019 - val_accuracy: 0.7312\n",
            "Epoch 468/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9891\n",
            "Epoch 468: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0258 - accuracy: 0.9891 - val_loss: 1.8867 - val_accuracy: 0.7312\n",
            "Epoch 469/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9891\n",
            "Epoch 469: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0262 - accuracy: 0.9891 - val_loss: 1.8560 - val_accuracy: 0.7312\n",
            "Epoch 470/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9891\n",
            "Epoch 470: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0240 - accuracy: 0.9891 - val_loss: 1.8289 - val_accuracy: 0.7312\n",
            "Epoch 471/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9891\n",
            "Epoch 471: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0242 - accuracy: 0.9891 - val_loss: 1.8139 - val_accuracy: 0.7204\n",
            "Epoch 472/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9918\n",
            "Epoch 472: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0197 - accuracy: 0.9918 - val_loss: 1.8127 - val_accuracy: 0.7204\n",
            "Epoch 473/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9864\n",
            "Epoch 473: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0258 - accuracy: 0.9864 - val_loss: 1.8116 - val_accuracy: 0.7204\n",
            "Epoch 474/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9891\n",
            "Epoch 474: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0237 - accuracy: 0.9891 - val_loss: 1.8412 - val_accuracy: 0.7097\n",
            "Epoch 475/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9918\n",
            "Epoch 475: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0187 - accuracy: 0.9918 - val_loss: 1.8336 - val_accuracy: 0.7097\n",
            "Epoch 476/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9891\n",
            "Epoch 476: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0188 - accuracy: 0.9891 - val_loss: 1.8328 - val_accuracy: 0.7097\n",
            "Epoch 477/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9891\n",
            "Epoch 477: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0226 - accuracy: 0.9891 - val_loss: 1.8348 - val_accuracy: 0.7097\n",
            "Epoch 478/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9891\n",
            "Epoch 478: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0251 - accuracy: 0.9891 - val_loss: 1.8428 - val_accuracy: 0.7097\n",
            "Epoch 479/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9837\n",
            "Epoch 479: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0235 - accuracy: 0.9837 - val_loss: 1.8692 - val_accuracy: 0.7097\n",
            "Epoch 480/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9864\n",
            "Epoch 480: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0308 - accuracy: 0.9864 - val_loss: 1.8419 - val_accuracy: 0.7204\n",
            "Epoch 481/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9864\n",
            "Epoch 481: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0247 - accuracy: 0.9864 - val_loss: 1.8196 - val_accuracy: 0.7312\n",
            "Epoch 482/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9891\n",
            "Epoch 482: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0255 - accuracy: 0.9891 - val_loss: 1.7883 - val_accuracy: 0.7312\n",
            "Epoch 483/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9891\n",
            "Epoch 483: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0247 - accuracy: 0.9891 - val_loss: 1.7876 - val_accuracy: 0.7312\n",
            "Epoch 484/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9864\n",
            "Epoch 484: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0239 - accuracy: 0.9864 - val_loss: 1.8023 - val_accuracy: 0.7527\n",
            "Epoch 485/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9891\n",
            "Epoch 485: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0250 - accuracy: 0.9891 - val_loss: 1.8189 - val_accuracy: 0.7527\n",
            "Epoch 486/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9973\n",
            "Epoch 486: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0163 - accuracy: 0.9973 - val_loss: 1.8276 - val_accuracy: 0.7312\n",
            "Epoch 487/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9837\n",
            "Epoch 487: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0257 - accuracy: 0.9837 - val_loss: 1.8346 - val_accuracy: 0.7204\n",
            "Epoch 488/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9918\n",
            "Epoch 488: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0182 - accuracy: 0.9918 - val_loss: 1.8371 - val_accuracy: 0.7312\n",
            "Epoch 489/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9946\n",
            "Epoch 489: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0185 - accuracy: 0.9946 - val_loss: 1.8404 - val_accuracy: 0.7312\n",
            "Epoch 490/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9864\n",
            "Epoch 490: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0217 - accuracy: 0.9864 - val_loss: 1.8545 - val_accuracy: 0.7312\n",
            "Epoch 491/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9891\n",
            "Epoch 491: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0234 - accuracy: 0.9891 - val_loss: 1.8657 - val_accuracy: 0.7312\n",
            "Epoch 492/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9864\n",
            "Epoch 492: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0272 - accuracy: 0.9864 - val_loss: 1.8606 - val_accuracy: 0.7419\n",
            "Epoch 493/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9864\n",
            "Epoch 493: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0240 - accuracy: 0.9864 - val_loss: 1.8593 - val_accuracy: 0.7419\n",
            "Epoch 494/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9837\n",
            "Epoch 494: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0314 - accuracy: 0.9837 - val_loss: 1.8662 - val_accuracy: 0.7419\n",
            "Epoch 495/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9891\n",
            "Epoch 495: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0224 - accuracy: 0.9891 - val_loss: 1.8868 - val_accuracy: 0.7312\n",
            "Epoch 496/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9837\n",
            "Epoch 496: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0256 - accuracy: 0.9837 - val_loss: 1.8953 - val_accuracy: 0.7204\n",
            "Epoch 497/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9918\n",
            "Epoch 497: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0271 - accuracy: 0.9918 - val_loss: 1.8731 - val_accuracy: 0.7419\n",
            "Epoch 498/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9864\n",
            "Epoch 498: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0231 - accuracy: 0.9864 - val_loss: 1.8642 - val_accuracy: 0.7419\n",
            "Epoch 499/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9810\n",
            "Epoch 499: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0280 - accuracy: 0.9810 - val_loss: 1.8479 - val_accuracy: 0.7527\n",
            "Epoch 500/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9918\n",
            "Epoch 500: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0232 - accuracy: 0.9918 - val_loss: 1.8470 - val_accuracy: 0.7419\n",
            "Epoch 501/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9891\n",
            "Epoch 501: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0216 - accuracy: 0.9891 - val_loss: 1.8562 - val_accuracy: 0.7527\n",
            "Epoch 502/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9891\n",
            "Epoch 502: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0217 - accuracy: 0.9891 - val_loss: 1.8620 - val_accuracy: 0.7419\n",
            "Epoch 503/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9891\n",
            "Epoch 503: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0224 - accuracy: 0.9891 - val_loss: 1.8642 - val_accuracy: 0.7312\n",
            "Epoch 504/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9864\n",
            "Epoch 504: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0249 - accuracy: 0.9864 - val_loss: 1.8361 - val_accuracy: 0.7204\n",
            "Epoch 505/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9918\n",
            "Epoch 505: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0208 - accuracy: 0.9918 - val_loss: 1.8359 - val_accuracy: 0.7204\n",
            "Epoch 506/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9837\n",
            "Epoch 506: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0334 - accuracy: 0.9837 - val_loss: 1.8514 - val_accuracy: 0.7312\n",
            "Epoch 507/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9918\n",
            "Epoch 507: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0196 - accuracy: 0.9918 - val_loss: 1.9120 - val_accuracy: 0.7204\n",
            "Epoch 508/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9864\n",
            "Epoch 508: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0310 - accuracy: 0.9864 - val_loss: 1.9265 - val_accuracy: 0.7204\n",
            "Epoch 509/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9946\n",
            "Epoch 509: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0195 - accuracy: 0.9946 - val_loss: 1.9222 - val_accuracy: 0.7312\n",
            "Epoch 510/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9946\n",
            "Epoch 510: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0188 - accuracy: 0.9946 - val_loss: 1.9286 - val_accuracy: 0.7419\n",
            "Epoch 511/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0180 - accuracy: 0.9883\n",
            "Epoch 511: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0327 - accuracy: 0.9864 - val_loss: 1.9658 - val_accuracy: 0.7312\n",
            "Epoch 512/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9946\n",
            "Epoch 512: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0218 - accuracy: 0.9946 - val_loss: 2.0097 - val_accuracy: 0.7097\n",
            "Epoch 513/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9810\n",
            "Epoch 513: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0308 - accuracy: 0.9810 - val_loss: 2.0180 - val_accuracy: 0.7204\n",
            "Epoch 514/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9918\n",
            "Epoch 514: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0216 - accuracy: 0.9918 - val_loss: 2.0045 - val_accuracy: 0.7419\n",
            "Epoch 515/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9837\n",
            "Epoch 515: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0281 - accuracy: 0.9837 - val_loss: 1.9901 - val_accuracy: 0.7312\n",
            "Epoch 516/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9918\n",
            "Epoch 516: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0244 - accuracy: 0.9918 - val_loss: 1.9968 - val_accuracy: 0.7312\n",
            "Epoch 517/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9891\n",
            "Epoch 517: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0220 - accuracy: 0.9891 - val_loss: 2.0344 - val_accuracy: 0.7204\n",
            "Epoch 518/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9918\n",
            "Epoch 518: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0299 - accuracy: 0.9918 - val_loss: 2.0069 - val_accuracy: 0.7097\n",
            "Epoch 519/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9891\n",
            "Epoch 519: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0254 - accuracy: 0.9891 - val_loss: 1.9586 - val_accuracy: 0.7204\n",
            "Epoch 520/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9864\n",
            "Epoch 520: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0263 - accuracy: 0.9864 - val_loss: 1.9234 - val_accuracy: 0.7419\n",
            "Epoch 521/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9864\n",
            "Epoch 521: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.0269 - accuracy: 0.9864 - val_loss: 1.9543 - val_accuracy: 0.7419\n",
            "Epoch 522/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9891\n",
            "Epoch 522: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.0238 - accuracy: 0.9891 - val_loss: 1.9763 - val_accuracy: 0.7312\n",
            "Epoch 523/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9891\n",
            "Epoch 523: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.0289 - accuracy: 0.9891 - val_loss: 1.9864 - val_accuracy: 0.7312\n",
            "Epoch 524/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9864\n",
            "Epoch 524: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0232 - accuracy: 0.9864 - val_loss: 1.9675 - val_accuracy: 0.7419\n",
            "Epoch 525/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9864\n",
            "Epoch 525: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0251 - accuracy: 0.9864 - val_loss: 1.9532 - val_accuracy: 0.7419\n",
            "Epoch 526/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9810\n",
            "Epoch 526: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0244 - accuracy: 0.9810 - val_loss: 1.9590 - val_accuracy: 0.7312\n",
            "Epoch 527/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9864\n",
            "Epoch 527: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0224 - accuracy: 0.9864 - val_loss: 1.9962 - val_accuracy: 0.7204\n",
            "Epoch 528/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0181 - accuracy: 0.9883\n",
            "Epoch 528: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0274 - accuracy: 0.9837 - val_loss: 1.9928 - val_accuracy: 0.7419\n",
            "Epoch 529/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9918\n",
            "Epoch 529: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0223 - accuracy: 0.9918 - val_loss: 1.8975 - val_accuracy: 0.7419\n",
            "Epoch 530/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9891\n",
            "Epoch 530: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.0238 - accuracy: 0.9891 - val_loss: 1.8532 - val_accuracy: 0.7634\n",
            "Epoch 531/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9918\n",
            "Epoch 531: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.0242 - accuracy: 0.9918 - val_loss: 1.8935 - val_accuracy: 0.7419\n",
            "Epoch 532/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9891\n",
            "Epoch 532: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0193 - accuracy: 0.9891 - val_loss: 1.9273 - val_accuracy: 0.7312\n",
            "Epoch 533/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9837\n",
            "Epoch 533: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0294 - accuracy: 0.9837 - val_loss: 1.8826 - val_accuracy: 0.7527\n",
            "Epoch 534/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9891\n",
            "Epoch 534: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0192 - accuracy: 0.9891 - val_loss: 1.8911 - val_accuracy: 0.7527\n",
            "Epoch 535/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9864\n",
            "Epoch 535: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0238 - accuracy: 0.9864 - val_loss: 1.9080 - val_accuracy: 0.7527\n",
            "Epoch 536/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0246 - accuracy: 0.9883\n",
            "Epoch 536: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0249 - accuracy: 0.9891 - val_loss: 1.9285 - val_accuracy: 0.7419\n",
            "Epoch 537/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0244 - accuracy: 0.9883\n",
            "Epoch 537: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0215 - accuracy: 0.9891 - val_loss: 1.9502 - val_accuracy: 0.7419\n",
            "Epoch 538/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9891\n",
            "Epoch 538: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0223 - accuracy: 0.9891 - val_loss: 1.9501 - val_accuracy: 0.7312\n",
            "Epoch 539/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9837\n",
            "Epoch 539: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0228 - accuracy: 0.9837 - val_loss: 1.9324 - val_accuracy: 0.7312\n",
            "Epoch 540/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9864\n",
            "Epoch 540: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0226 - accuracy: 0.9864 - val_loss: 1.9173 - val_accuracy: 0.7312\n",
            "Epoch 541/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9864\n",
            "Epoch 541: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0236 - accuracy: 0.9864 - val_loss: 1.9178 - val_accuracy: 0.7312\n",
            "Epoch 542/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9891\n",
            "Epoch 542: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0198 - accuracy: 0.9891 - val_loss: 1.9110 - val_accuracy: 0.7312\n",
            "Epoch 543/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9891\n",
            "Epoch 543: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0228 - accuracy: 0.9891 - val_loss: 1.8980 - val_accuracy: 0.7312\n",
            "Epoch 544/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9946\n",
            "Epoch 544: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0193 - accuracy: 0.9946 - val_loss: 1.9099 - val_accuracy: 0.7312\n",
            "Epoch 545/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9918\n",
            "Epoch 545: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0201 - accuracy: 0.9918 - val_loss: 1.9547 - val_accuracy: 0.7204\n",
            "Epoch 546/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9918\n",
            "Epoch 546: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0207 - accuracy: 0.9918 - val_loss: 2.0030 - val_accuracy: 0.7204\n",
            "Epoch 547/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9918\n",
            "Epoch 547: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0212 - accuracy: 0.9918 - val_loss: 2.0086 - val_accuracy: 0.7312\n",
            "Epoch 548/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9918\n",
            "Epoch 548: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0178 - accuracy: 0.9918 - val_loss: 1.9807 - val_accuracy: 0.7312\n",
            "Epoch 549/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9918\n",
            "Epoch 549: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0200 - accuracy: 0.9918 - val_loss: 1.9615 - val_accuracy: 0.7419\n",
            "Epoch 550/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9918\n",
            "Epoch 550: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0200 - accuracy: 0.9918 - val_loss: 1.9419 - val_accuracy: 0.7527\n",
            "Epoch 551/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9864\n",
            "Epoch 551: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0204 - accuracy: 0.9864 - val_loss: 1.9337 - val_accuracy: 0.7419\n",
            "Epoch 552/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9864\n",
            "Epoch 552: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0230 - accuracy: 0.9864 - val_loss: 1.9673 - val_accuracy: 0.7312\n",
            "Epoch 553/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9918\n",
            "Epoch 553: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0228 - accuracy: 0.9918 - val_loss: 1.9837 - val_accuracy: 0.7312\n",
            "Epoch 554/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9864\n",
            "Epoch 554: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0222 - accuracy: 0.9864 - val_loss: 1.9929 - val_accuracy: 0.7312\n",
            "Epoch 555/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9837\n",
            "Epoch 555: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0250 - accuracy: 0.9837 - val_loss: 1.9895 - val_accuracy: 0.7312\n",
            "Epoch 556/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9918\n",
            "Epoch 556: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0190 - accuracy: 0.9918 - val_loss: 1.9940 - val_accuracy: 0.7204\n",
            "Epoch 557/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9837\n",
            "Epoch 557: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0260 - accuracy: 0.9837 - val_loss: 2.0040 - val_accuracy: 0.7312\n",
            "Epoch 558/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9918\n",
            "Epoch 558: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0186 - accuracy: 0.9918 - val_loss: 2.0158 - val_accuracy: 0.7312\n",
            "Epoch 559/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9891\n",
            "Epoch 559: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0215 - accuracy: 0.9891 - val_loss: 2.0140 - val_accuracy: 0.7312\n",
            "Epoch 560/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9891\n",
            "Epoch 560: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0225 - accuracy: 0.9891 - val_loss: 1.9985 - val_accuracy: 0.7312\n",
            "Epoch 561/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9918\n",
            "Epoch 561: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0192 - accuracy: 0.9918 - val_loss: 1.9957 - val_accuracy: 0.7312\n",
            "Epoch 562/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9918\n",
            "Epoch 562: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0190 - accuracy: 0.9918 - val_loss: 1.9989 - val_accuracy: 0.7312\n",
            "Epoch 563/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9891\n",
            "Epoch 563: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0190 - accuracy: 0.9891 - val_loss: 2.0000 - val_accuracy: 0.7312\n",
            "Epoch 564/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9837\n",
            "Epoch 564: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0209 - accuracy: 0.9837 - val_loss: 2.0048 - val_accuracy: 0.7312\n",
            "Epoch 565/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9864\n",
            "Epoch 565: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0232 - accuracy: 0.9864 - val_loss: 2.0047 - val_accuracy: 0.7312\n",
            "Epoch 566/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9891\n",
            "Epoch 566: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0244 - accuracy: 0.9891 - val_loss: 1.9994 - val_accuracy: 0.7419\n",
            "Epoch 567/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9810\n",
            "Epoch 567: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0281 - accuracy: 0.9810 - val_loss: 1.9728 - val_accuracy: 0.7419\n",
            "Epoch 568/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9864\n",
            "Epoch 568: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0205 - accuracy: 0.9864 - val_loss: 1.9253 - val_accuracy: 0.7527\n",
            "Epoch 569/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9891\n",
            "Epoch 569: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0235 - accuracy: 0.9891 - val_loss: 1.9018 - val_accuracy: 0.7527\n",
            "Epoch 570/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9891\n",
            "Epoch 570: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0214 - accuracy: 0.9891 - val_loss: 1.8970 - val_accuracy: 0.7527\n",
            "Epoch 571/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9918\n",
            "Epoch 571: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0188 - accuracy: 0.9918 - val_loss: 1.9007 - val_accuracy: 0.7527\n",
            "Epoch 572/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9837\n",
            "Epoch 572: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0279 - accuracy: 0.9837 - val_loss: 1.9177 - val_accuracy: 0.7419\n",
            "Epoch 573/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9891\n",
            "Epoch 573: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0225 - accuracy: 0.9891 - val_loss: 1.9333 - val_accuracy: 0.7312\n",
            "Epoch 574/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9891\n",
            "Epoch 574: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0209 - accuracy: 0.9891 - val_loss: 1.9465 - val_accuracy: 0.7097\n",
            "Epoch 575/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9891\n",
            "Epoch 575: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0228 - accuracy: 0.9891 - val_loss: 1.9587 - val_accuracy: 0.7204\n",
            "Epoch 576/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0184 - accuracy: 0.9883\n",
            "Epoch 576: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0196 - accuracy: 0.9864 - val_loss: 1.9743 - val_accuracy: 0.7312\n",
            "Epoch 577/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9864\n",
            "Epoch 577: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0258 - accuracy: 0.9864 - val_loss: 1.9831 - val_accuracy: 0.7312\n",
            "Epoch 578/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9864\n",
            "Epoch 578: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0231 - accuracy: 0.9864 - val_loss: 1.9939 - val_accuracy: 0.7312\n",
            "Epoch 579/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9918\n",
            "Epoch 579: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0186 - accuracy: 0.9918 - val_loss: 2.0160 - val_accuracy: 0.7312\n",
            "Epoch 580/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9918\n",
            "Epoch 580: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0201 - accuracy: 0.9918 - val_loss: 2.0360 - val_accuracy: 0.7312\n",
            "Epoch 581/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9837\n",
            "Epoch 581: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0283 - accuracy: 0.9837 - val_loss: 2.0671 - val_accuracy: 0.7204\n",
            "Epoch 582/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0247 - accuracy: 0.9844\n",
            "Epoch 582: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0243 - accuracy: 0.9864 - val_loss: 2.1376 - val_accuracy: 0.6989\n",
            "Epoch 583/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9891\n",
            "Epoch 583: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0269 - accuracy: 0.9891 - val_loss: 2.0703 - val_accuracy: 0.7204\n",
            "Epoch 584/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9918\n",
            "Epoch 584: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0213 - accuracy: 0.9918 - val_loss: 2.0429 - val_accuracy: 0.7419\n",
            "Epoch 585/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9783\n",
            "Epoch 585: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0357 - accuracy: 0.9783 - val_loss: 2.0493 - val_accuracy: 0.7204\n",
            "Epoch 586/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9837\n",
            "Epoch 586: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0290 - accuracy: 0.9837 - val_loss: 2.1540 - val_accuracy: 0.7204\n",
            "Epoch 587/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9783\n",
            "Epoch 587: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0534 - accuracy: 0.9783 - val_loss: 2.1858 - val_accuracy: 0.6989\n",
            "Epoch 588/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0614 - accuracy: 0.9766\n",
            "Epoch 588: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0490 - accuracy: 0.9810 - val_loss: 2.0495 - val_accuracy: 0.7204\n",
            "Epoch 589/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9891\n",
            "Epoch 589: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0252 - accuracy: 0.9891 - val_loss: 1.9913 - val_accuracy: 0.7419\n",
            "Epoch 590/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9810\n",
            "Epoch 590: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0480 - accuracy: 0.9810 - val_loss: 2.0170 - val_accuracy: 0.7312\n",
            "Epoch 591/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9864\n",
            "Epoch 591: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0321 - accuracy: 0.9864 - val_loss: 2.0722 - val_accuracy: 0.7312\n",
            "Epoch 592/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9755\n",
            "Epoch 592: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0565 - accuracy: 0.9755 - val_loss: 2.1877 - val_accuracy: 0.7097\n",
            "Epoch 593/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9864\n",
            "Epoch 593: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0323 - accuracy: 0.9864 - val_loss: 2.1852 - val_accuracy: 0.6989\n",
            "Epoch 594/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.9538\n",
            "Epoch 594: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.1432 - accuracy: 0.9538 - val_loss: 2.4301 - val_accuracy: 0.6344\n",
            "Epoch 595/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2118 - accuracy: 0.9321\n",
            "Epoch 595: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.2118 - accuracy: 0.9321 - val_loss: 1.9979 - val_accuracy: 0.6559\n",
            "Epoch 596/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9375\n",
            "Epoch 596: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.1865 - accuracy: 0.9375 - val_loss: 2.1666 - val_accuracy: 0.6452\n",
            "Epoch 597/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1136 - accuracy: 0.9609\n",
            "Epoch 597: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.1161 - accuracy: 0.9565 - val_loss: 1.9563 - val_accuracy: 0.7097\n",
            "Epoch 598/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 0.9348\n",
            "Epoch 598: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.2016 - accuracy: 0.9348 - val_loss: 1.9259 - val_accuracy: 0.6667\n",
            "Epoch 599/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2924 - accuracy: 0.9102\n",
            "Epoch 599: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.2858 - accuracy: 0.9049 - val_loss: 1.4914 - val_accuracy: 0.7097\n",
            "Epoch 600/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1872 - accuracy: 0.9258\n",
            "Epoch 600: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.2358 - accuracy: 0.9158 - val_loss: 1.4429 - val_accuracy: 0.6882\n",
            "Epoch 601/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9293\n",
            "Epoch 601: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 119ms/step - loss: 0.1608 - accuracy: 0.9293 - val_loss: 1.7264 - val_accuracy: 0.6882\n",
            "Epoch 602/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2150 - accuracy: 0.9158\n",
            "Epoch 602: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.2150 - accuracy: 0.9158 - val_loss: 1.5991 - val_accuracy: 0.7312\n",
            "Epoch 603/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9348\n",
            "Epoch 603: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1847 - accuracy: 0.9348 - val_loss: 1.4959 - val_accuracy: 0.6989\n",
            "Epoch 604/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1207 - accuracy: 0.9648\n",
            "Epoch 604: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1197 - accuracy: 0.9647 - val_loss: 1.6427 - val_accuracy: 0.6774\n",
            "Epoch 605/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1188 - accuracy: 0.9531\n",
            "Epoch 605: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1405 - accuracy: 0.9348 - val_loss: 1.4997 - val_accuracy: 0.7419\n",
            "Epoch 606/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9538\n",
            "Epoch 606: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1361 - accuracy: 0.9538 - val_loss: 1.5113 - val_accuracy: 0.7312\n",
            "Epoch 607/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9429\n",
            "Epoch 607: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.1569 - accuracy: 0.9429 - val_loss: 1.7316 - val_accuracy: 0.6989\n",
            "Epoch 608/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0863 - accuracy: 0.9648\n",
            "Epoch 608: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1535 - accuracy: 0.9484 - val_loss: 1.7590 - val_accuracy: 0.6989\n",
            "Epoch 609/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9620\n",
            "Epoch 609: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0863 - accuracy: 0.9620 - val_loss: 1.4739 - val_accuracy: 0.6882\n",
            "Epoch 610/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9674\n",
            "Epoch 610: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0834 - accuracy: 0.9674 - val_loss: 1.3458 - val_accuracy: 0.7419\n",
            "Epoch 611/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9647\n",
            "Epoch 611: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0815 - accuracy: 0.9647 - val_loss: 1.4125 - val_accuracy: 0.6989\n",
            "Epoch 612/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9755\n",
            "Epoch 612: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.0586 - accuracy: 0.9755 - val_loss: 1.6328 - val_accuracy: 0.6989\n",
            "Epoch 613/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9783\n",
            "Epoch 613: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.0700 - accuracy: 0.9783 - val_loss: 1.4928 - val_accuracy: 0.7097\n",
            "Epoch 614/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9837\n",
            "Epoch 614: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.0474 - accuracy: 0.9837 - val_loss: 1.4160 - val_accuracy: 0.7312\n",
            "Epoch 615/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9837\n",
            "Epoch 615: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0462 - accuracy: 0.9837 - val_loss: 1.3132 - val_accuracy: 0.7527\n",
            "Epoch 616/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9810\n",
            "Epoch 616: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0595 - accuracy: 0.9810 - val_loss: 1.3777 - val_accuracy: 0.7527\n",
            "Epoch 617/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9810\n",
            "Epoch 617: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.0444 - accuracy: 0.9810 - val_loss: 1.5182 - val_accuracy: 0.7419\n",
            "Epoch 618/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9755\n",
            "Epoch 618: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.0714 - accuracy: 0.9755 - val_loss: 1.5594 - val_accuracy: 0.7419\n",
            "Epoch 619/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9783\n",
            "Epoch 619: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.0512 - accuracy: 0.9783 - val_loss: 1.4785 - val_accuracy: 0.7204\n",
            "Epoch 620/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0609 - accuracy: 0.9766\n",
            "Epoch 620: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0443 - accuracy: 0.9837 - val_loss: 1.5787 - val_accuracy: 0.7204\n",
            "Epoch 621/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9783\n",
            "Epoch 621: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0411 - accuracy: 0.9783 - val_loss: 1.6056 - val_accuracy: 0.7312\n",
            "Epoch 622/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9864\n",
            "Epoch 622: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0343 - accuracy: 0.9864 - val_loss: 1.7355 - val_accuracy: 0.7204\n",
            "Epoch 623/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9783\n",
            "Epoch 623: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.0320 - accuracy: 0.9783 - val_loss: 1.7144 - val_accuracy: 0.7204\n",
            "Epoch 624/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9918\n",
            "Epoch 624: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0319 - accuracy: 0.9918 - val_loss: 1.6858 - val_accuracy: 0.7204\n",
            "Epoch 625/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9891\n",
            "Epoch 625: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0284 - accuracy: 0.9891 - val_loss: 1.7091 - val_accuracy: 0.7204\n",
            "Epoch 626/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9918\n",
            "Epoch 626: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0242 - accuracy: 0.9918 - val_loss: 1.7404 - val_accuracy: 0.7312\n",
            "Epoch 627/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9891\n",
            "Epoch 627: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0269 - accuracy: 0.9891 - val_loss: 1.7867 - val_accuracy: 0.7204\n",
            "Epoch 628/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9891\n",
            "Epoch 628: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.0237 - accuracy: 0.9891 - val_loss: 1.8282 - val_accuracy: 0.7204\n",
            "Epoch 629/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9891\n",
            "Epoch 629: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0243 - accuracy: 0.9891 - val_loss: 1.8453 - val_accuracy: 0.7204\n",
            "Epoch 630/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9891\n",
            "Epoch 630: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.0275 - accuracy: 0.9891 - val_loss: 1.8565 - val_accuracy: 0.7097\n",
            "Epoch 631/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9864\n",
            "Epoch 631: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.0260 - accuracy: 0.9864 - val_loss: 1.9223 - val_accuracy: 0.7204\n",
            "Epoch 632/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9864\n",
            "Epoch 632: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0259 - accuracy: 0.9864 - val_loss: 1.9646 - val_accuracy: 0.7204\n",
            "Epoch 633/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9918\n",
            "Epoch 633: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0267 - accuracy: 0.9918 - val_loss: 1.9573 - val_accuracy: 0.7204\n",
            "Epoch 634/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9891\n",
            "Epoch 634: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.0264 - accuracy: 0.9891 - val_loss: 1.9060 - val_accuracy: 0.7097\n",
            "Epoch 635/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9946\n",
            "Epoch 635: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.0255 - accuracy: 0.9946 - val_loss: 1.8643 - val_accuracy: 0.7097\n",
            "Epoch 636/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9864\n",
            "Epoch 636: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0233 - accuracy: 0.9864 - val_loss: 1.8690 - val_accuracy: 0.6989\n",
            "Epoch 637/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9864\n",
            "Epoch 637: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.0243 - accuracy: 0.9864 - val_loss: 1.8608 - val_accuracy: 0.7204\n",
            "Epoch 638/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0283 - accuracy: 0.9844\n",
            "Epoch 638: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0237 - accuracy: 0.9891 - val_loss: 1.8454 - val_accuracy: 0.7204\n",
            "Epoch 639/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9918\n",
            "Epoch 639: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0226 - accuracy: 0.9918 - val_loss: 1.8419 - val_accuracy: 0.7312\n",
            "Epoch 640/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9837\n",
            "Epoch 640: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0259 - accuracy: 0.9837 - val_loss: 1.8646 - val_accuracy: 0.7312\n",
            "Epoch 641/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9891\n",
            "Epoch 641: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0237 - accuracy: 0.9891 - val_loss: 1.9048 - val_accuracy: 0.7204\n",
            "Epoch 642/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9918\n",
            "Epoch 642: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0214 - accuracy: 0.9918 - val_loss: 1.9254 - val_accuracy: 0.7097\n",
            "Epoch 643/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9946\n",
            "Epoch 643: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0235 - accuracy: 0.9946 - val_loss: 1.9456 - val_accuracy: 0.7097\n",
            "Epoch 644/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9891\n",
            "Epoch 644: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0200 - accuracy: 0.9891 - val_loss: 1.9565 - val_accuracy: 0.7097\n",
            "Epoch 645/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9864\n",
            "Epoch 645: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0252 - accuracy: 0.9864 - val_loss: 1.9702 - val_accuracy: 0.7097\n",
            "Epoch 646/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9891\n",
            "Epoch 646: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0212 - accuracy: 0.9891 - val_loss: 1.9779 - val_accuracy: 0.7097\n",
            "Epoch 647/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9918\n",
            "Epoch 647: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0183 - accuracy: 0.9918 - val_loss: 1.9786 - val_accuracy: 0.7097\n",
            "Epoch 648/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9891\n",
            "Epoch 648: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0230 - accuracy: 0.9891 - val_loss: 1.9900 - val_accuracy: 0.7097\n",
            "Epoch 649/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9864\n",
            "Epoch 649: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0238 - accuracy: 0.9864 - val_loss: 2.0055 - val_accuracy: 0.7097\n",
            "Epoch 650/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9837\n",
            "Epoch 650: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0244 - accuracy: 0.9837 - val_loss: 2.0130 - val_accuracy: 0.7097\n",
            "Epoch 651/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9891\n",
            "Epoch 651: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0235 - accuracy: 0.9891 - val_loss: 2.0203 - val_accuracy: 0.7097\n",
            "Epoch 652/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9864\n",
            "Epoch 652: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0244 - accuracy: 0.9864 - val_loss: 2.0255 - val_accuracy: 0.7204\n",
            "Epoch 653/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9891\n",
            "Epoch 653: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0229 - accuracy: 0.9891 - val_loss: 2.0208 - val_accuracy: 0.7204\n",
            "Epoch 654/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9891\n",
            "Epoch 654: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0244 - accuracy: 0.9891 - val_loss: 2.0238 - val_accuracy: 0.7204\n",
            "Epoch 655/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9864\n",
            "Epoch 655: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0223 - accuracy: 0.9864 - val_loss: 2.0289 - val_accuracy: 0.7204\n",
            "Epoch 656/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9891\n",
            "Epoch 656: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0197 - accuracy: 0.9891 - val_loss: 2.0314 - val_accuracy: 0.7097\n",
            "Epoch 657/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9864\n",
            "Epoch 657: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0226 - accuracy: 0.9864 - val_loss: 2.0306 - val_accuracy: 0.7097\n",
            "Epoch 658/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9891\n",
            "Epoch 658: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0221 - accuracy: 0.9891 - val_loss: 2.0189 - val_accuracy: 0.7097\n",
            "Epoch 659/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9891\n",
            "Epoch 659: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0263 - accuracy: 0.9891 - val_loss: 2.0052 - val_accuracy: 0.7097\n",
            "Epoch 660/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9864\n",
            "Epoch 660: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0247 - accuracy: 0.9864 - val_loss: 1.9999 - val_accuracy: 0.7097\n",
            "Epoch 661/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9891\n",
            "Epoch 661: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0212 - accuracy: 0.9891 - val_loss: 2.0044 - val_accuracy: 0.7097\n",
            "Epoch 662/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9891\n",
            "Epoch 662: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0212 - accuracy: 0.9891 - val_loss: 2.0227 - val_accuracy: 0.7097\n",
            "Epoch 663/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9864\n",
            "Epoch 663: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0242 - accuracy: 0.9864 - val_loss: 2.0585 - val_accuracy: 0.7097\n",
            "Epoch 664/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9918\n",
            "Epoch 664: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0218 - accuracy: 0.9918 - val_loss: 2.0870 - val_accuracy: 0.7097\n",
            "Epoch 665/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9837\n",
            "Epoch 665: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0252 - accuracy: 0.9837 - val_loss: 2.1152 - val_accuracy: 0.7097\n",
            "Epoch 666/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9891\n",
            "Epoch 666: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0202 - accuracy: 0.9891 - val_loss: 2.1127 - val_accuracy: 0.7204\n",
            "Epoch 667/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9891\n",
            "Epoch 667: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0228 - accuracy: 0.9891 - val_loss: 2.0892 - val_accuracy: 0.7204\n",
            "Epoch 668/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9864\n",
            "Epoch 668: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0232 - accuracy: 0.9864 - val_loss: 2.0735 - val_accuracy: 0.7204\n",
            "Epoch 669/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9864\n",
            "Epoch 669: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0214 - accuracy: 0.9864 - val_loss: 2.0690 - val_accuracy: 0.7204\n",
            "Epoch 670/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9918\n",
            "Epoch 670: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0210 - accuracy: 0.9918 - val_loss: 2.0813 - val_accuracy: 0.7204\n",
            "Epoch 671/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9891\n",
            "Epoch 671: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0231 - accuracy: 0.9891 - val_loss: 2.0851 - val_accuracy: 0.7204\n",
            "Epoch 672/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9918\n",
            "Epoch 672: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0209 - accuracy: 0.9918 - val_loss: 2.0758 - val_accuracy: 0.7204\n",
            "Epoch 673/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9891\n",
            "Epoch 673: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.0201 - accuracy: 0.9891 - val_loss: 2.0692 - val_accuracy: 0.7204\n",
            "Epoch 674/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9837\n",
            "Epoch 674: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.0236 - accuracy: 0.9837 - val_loss: 2.0748 - val_accuracy: 0.7204\n",
            "Epoch 675/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9891\n",
            "Epoch 675: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0242 - accuracy: 0.9891 - val_loss: 2.0848 - val_accuracy: 0.7204\n",
            "Epoch 676/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9891\n",
            "Epoch 676: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0201 - accuracy: 0.9891 - val_loss: 2.0913 - val_accuracy: 0.7204\n",
            "Epoch 677/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0231 - accuracy: 0.9844\n",
            "Epoch 677: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 2.0957 - val_accuracy: 0.7097\n",
            "Epoch 678/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9891\n",
            "Epoch 678: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0249 - accuracy: 0.9891 - val_loss: 2.0945 - val_accuracy: 0.7097\n",
            "Epoch 679/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0245 - accuracy: 0.9883\n",
            "Epoch 679: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0215 - accuracy: 0.9891 - val_loss: 2.0916 - val_accuracy: 0.7097\n",
            "Epoch 680/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0147 - accuracy: 0.9961\n",
            "Epoch 680: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0218 - accuracy: 0.9918 - val_loss: 2.0895 - val_accuracy: 0.7097\n",
            "Epoch 681/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0359 - accuracy: 0.9727\n",
            "Epoch 681: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.0294 - accuracy: 0.9783 - val_loss: 2.1194 - val_accuracy: 0.7097\n",
            "Epoch 682/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9864\n",
            "Epoch 682: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0237 - accuracy: 0.9864 - val_loss: 2.0861 - val_accuracy: 0.7097\n",
            "Epoch 683/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9891\n",
            "Epoch 683: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0219 - accuracy: 0.9891 - val_loss: 2.0721 - val_accuracy: 0.7097\n",
            "Epoch 684/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9891\n",
            "Epoch 684: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0208 - accuracy: 0.9891 - val_loss: 2.0657 - val_accuracy: 0.7204\n",
            "Epoch 685/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9918\n",
            "Epoch 685: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0191 - accuracy: 0.9918 - val_loss: 2.0787 - val_accuracy: 0.7204\n",
            "Epoch 686/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9918\n",
            "Epoch 686: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0170 - accuracy: 0.9918 - val_loss: 2.1038 - val_accuracy: 0.7204\n",
            "Epoch 687/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9864\n",
            "Epoch 687: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0201 - accuracy: 0.9864 - val_loss: 2.1295 - val_accuracy: 0.7204\n",
            "Epoch 688/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9864\n",
            "Epoch 688: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.0222 - accuracy: 0.9864 - val_loss: 2.1395 - val_accuracy: 0.7204\n",
            "Epoch 689/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9891\n",
            "Epoch 689: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0194 - accuracy: 0.9891 - val_loss: 2.1420 - val_accuracy: 0.7204\n",
            "Epoch 690/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9864\n",
            "Epoch 690: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0219 - accuracy: 0.9864 - val_loss: 2.1519 - val_accuracy: 0.7204\n",
            "Epoch 691/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9864\n",
            "Epoch 691: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 2.1714 - val_accuracy: 0.7204\n",
            "Epoch 692/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9946\n",
            "Epoch 692: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0197 - accuracy: 0.9946 - val_loss: 2.1865 - val_accuracy: 0.7204\n",
            "Epoch 693/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9837\n",
            "Epoch 693: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.0197 - accuracy: 0.9837 - val_loss: 2.2054 - val_accuracy: 0.7097\n",
            "Epoch 694/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9864\n",
            "Epoch 694: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.0222 - accuracy: 0.9864 - val_loss: 2.2249 - val_accuracy: 0.7204\n",
            "Epoch 695/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0253 - accuracy: 0.9844\n",
            "Epoch 695: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0189 - accuracy: 0.9891 - val_loss: 2.2317 - val_accuracy: 0.7204\n",
            "Epoch 696/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9891\n",
            "Epoch 696: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0227 - accuracy: 0.9891 - val_loss: 2.2438 - val_accuracy: 0.7204\n",
            "Epoch 697/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9918\n",
            "Epoch 697: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0214 - accuracy: 0.9918 - val_loss: 2.2543 - val_accuracy: 0.7204\n",
            "Epoch 698/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9918\n",
            "Epoch 698: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0192 - accuracy: 0.9918 - val_loss: 2.2539 - val_accuracy: 0.7204\n",
            "Epoch 699/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9918\n",
            "Epoch 699: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0197 - accuracy: 0.9918 - val_loss: 2.2510 - val_accuracy: 0.7097\n",
            "Epoch 700/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9918\n",
            "Epoch 700: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0209 - accuracy: 0.9918 - val_loss: 2.2475 - val_accuracy: 0.7097\n",
            "Epoch 701/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9891\n",
            "Epoch 701: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0220 - accuracy: 0.9891 - val_loss: 2.2439 - val_accuracy: 0.7097\n",
            "Epoch 702/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9864\n",
            "Epoch 702: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0211 - accuracy: 0.9864 - val_loss: 2.2390 - val_accuracy: 0.7097\n",
            "Epoch 703/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9918\n",
            "Epoch 703: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0207 - accuracy: 0.9918 - val_loss: 2.2430 - val_accuracy: 0.7097\n",
            "Epoch 704/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9864\n",
            "Epoch 704: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0224 - accuracy: 0.9864 - val_loss: 2.2420 - val_accuracy: 0.7097\n",
            "Epoch 705/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9864\n",
            "Epoch 705: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0234 - accuracy: 0.9864 - val_loss: 2.2367 - val_accuracy: 0.7097\n",
            "Epoch 706/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9918\n",
            "Epoch 706: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0196 - accuracy: 0.9918 - val_loss: 2.2294 - val_accuracy: 0.7097\n",
            "Epoch 707/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9946\n",
            "Epoch 707: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0193 - accuracy: 0.9946 - val_loss: 2.2261 - val_accuracy: 0.7097\n",
            "Epoch 708/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9918\n",
            "Epoch 708: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0217 - accuracy: 0.9918 - val_loss: 2.2299 - val_accuracy: 0.7097\n",
            "Epoch 709/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9891\n",
            "Epoch 709: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0191 - accuracy: 0.9891 - val_loss: 2.2430 - val_accuracy: 0.7097\n",
            "Epoch 710/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9810\n",
            "Epoch 710: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0231 - accuracy: 0.9810 - val_loss: 2.2454 - val_accuracy: 0.7097\n",
            "Epoch 711/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9864\n",
            "Epoch 711: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0221 - accuracy: 0.9864 - val_loss: 2.2295 - val_accuracy: 0.7204\n",
            "Epoch 712/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9864\n",
            "Epoch 712: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0227 - accuracy: 0.9864 - val_loss: 2.2187 - val_accuracy: 0.7204\n",
            "Epoch 713/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9864\n",
            "Epoch 713: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0206 - accuracy: 0.9864 - val_loss: 2.2286 - val_accuracy: 0.7204\n",
            "Epoch 714/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9864\n",
            "Epoch 714: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0222 - accuracy: 0.9864 - val_loss: 2.2410 - val_accuracy: 0.7204\n",
            "Epoch 715/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9864\n",
            "Epoch 715: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0209 - accuracy: 0.9864 - val_loss: 2.2505 - val_accuracy: 0.7204\n",
            "Epoch 716/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9810\n",
            "Epoch 716: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0259 - accuracy: 0.9810 - val_loss: 2.2659 - val_accuracy: 0.7204\n",
            "Epoch 717/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9864\n",
            "Epoch 717: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0208 - accuracy: 0.9864 - val_loss: 2.2648 - val_accuracy: 0.7204\n",
            "Epoch 718/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9837\n",
            "Epoch 718: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0190 - accuracy: 0.9837 - val_loss: 2.2514 - val_accuracy: 0.7204\n",
            "Epoch 719/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9891\n",
            "Epoch 719: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0208 - accuracy: 0.9891 - val_loss: 2.2644 - val_accuracy: 0.7204\n",
            "Epoch 720/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9918\n",
            "Epoch 720: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0215 - accuracy: 0.9918 - val_loss: 2.2746 - val_accuracy: 0.7204\n",
            "Epoch 721/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9891\n",
            "Epoch 721: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0200 - accuracy: 0.9891 - val_loss: 2.2776 - val_accuracy: 0.7204\n",
            "Epoch 722/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9891\n",
            "Epoch 722: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0224 - accuracy: 0.9891 - val_loss: 2.2828 - val_accuracy: 0.7204\n",
            "Epoch 723/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9864\n",
            "Epoch 723: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0190 - accuracy: 0.9864 - val_loss: 2.2934 - val_accuracy: 0.7204\n",
            "Epoch 724/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9864\n",
            "Epoch 724: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0237 - accuracy: 0.9864 - val_loss: 2.3004 - val_accuracy: 0.7204\n",
            "Epoch 725/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9891\n",
            "Epoch 725: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0215 - accuracy: 0.9891 - val_loss: 2.3067 - val_accuracy: 0.7204\n",
            "Epoch 726/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9891\n",
            "Epoch 726: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0206 - accuracy: 0.9891 - val_loss: 2.3112 - val_accuracy: 0.7204\n",
            "Epoch 727/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9837\n",
            "Epoch 727: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0226 - accuracy: 0.9837 - val_loss: 2.3328 - val_accuracy: 0.7204\n",
            "Epoch 728/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9891\n",
            "Epoch 728: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0211 - accuracy: 0.9891 - val_loss: 2.3463 - val_accuracy: 0.7204\n",
            "Epoch 729/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9891\n",
            "Epoch 729: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0209 - accuracy: 0.9891 - val_loss: 2.3611 - val_accuracy: 0.7204\n",
            "Epoch 730/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9864\n",
            "Epoch 730: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0280 - accuracy: 0.9864 - val_loss: 2.3315 - val_accuracy: 0.7204\n",
            "Epoch 731/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9864\n",
            "Epoch 731: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0234 - accuracy: 0.9864 - val_loss: 2.3028 - val_accuracy: 0.7204\n",
            "Epoch 732/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9864\n",
            "Epoch 732: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0276 - accuracy: 0.9864 - val_loss: 2.4019 - val_accuracy: 0.7312\n",
            "Epoch 733/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9864\n",
            "Epoch 733: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0380 - accuracy: 0.9864 - val_loss: 2.4557 - val_accuracy: 0.7204\n",
            "Epoch 734/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9891\n",
            "Epoch 734: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0278 - accuracy: 0.9891 - val_loss: 2.2050 - val_accuracy: 0.6989\n",
            "Epoch 735/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9864\n",
            "Epoch 735: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0216 - accuracy: 0.9864 - val_loss: 2.1159 - val_accuracy: 0.6989\n",
            "Epoch 736/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9864\n",
            "Epoch 736: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0315 - accuracy: 0.9864 - val_loss: 2.1010 - val_accuracy: 0.6989\n",
            "Epoch 737/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9864\n",
            "Epoch 737: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0253 - accuracy: 0.9864 - val_loss: 2.2109 - val_accuracy: 0.6989\n",
            "Epoch 738/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9837\n",
            "Epoch 738: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0261 - accuracy: 0.9837 - val_loss: 2.2053 - val_accuracy: 0.7097\n",
            "Epoch 739/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9891\n",
            "Epoch 739: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0195 - accuracy: 0.9891 - val_loss: 2.1916 - val_accuracy: 0.7097\n",
            "Epoch 740/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9891\n",
            "Epoch 740: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0204 - accuracy: 0.9891 - val_loss: 2.2013 - val_accuracy: 0.7097\n",
            "Epoch 741/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9891\n",
            "Epoch 741: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0219 - accuracy: 0.9891 - val_loss: 2.2525 - val_accuracy: 0.7097\n",
            "Epoch 742/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9864\n",
            "Epoch 742: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0203 - accuracy: 0.9864 - val_loss: 2.2963 - val_accuracy: 0.7097\n",
            "Epoch 743/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9891\n",
            "Epoch 743: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0220 - accuracy: 0.9891 - val_loss: 2.3053 - val_accuracy: 0.7204\n",
            "Epoch 744/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9864\n",
            "Epoch 744: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0235 - accuracy: 0.9864 - val_loss: 2.3013 - val_accuracy: 0.7097\n",
            "Epoch 745/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9946\n",
            "Epoch 745: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0165 - accuracy: 0.9946 - val_loss: 2.2918 - val_accuracy: 0.6989\n",
            "Epoch 746/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9837\n",
            "Epoch 746: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0242 - accuracy: 0.9837 - val_loss: 2.2848 - val_accuracy: 0.6989\n",
            "Epoch 747/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9864\n",
            "Epoch 747: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0206 - accuracy: 0.9864 - val_loss: 2.2826 - val_accuracy: 0.7097\n",
            "Epoch 748/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9891\n",
            "Epoch 748: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0196 - accuracy: 0.9891 - val_loss: 2.2866 - val_accuracy: 0.7097\n",
            "Epoch 749/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9891\n",
            "Epoch 749: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0212 - accuracy: 0.9891 - val_loss: 2.2966 - val_accuracy: 0.7204\n",
            "Epoch 750/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9891\n",
            "Epoch 750: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0218 - accuracy: 0.9891 - val_loss: 2.3107 - val_accuracy: 0.7204\n",
            "Epoch 751/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9891\n",
            "Epoch 751: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0206 - accuracy: 0.9891 - val_loss: 2.3094 - val_accuracy: 0.7204\n",
            "Epoch 752/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9891\n",
            "Epoch 752: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0205 - accuracy: 0.9891 - val_loss: 2.3063 - val_accuracy: 0.7204\n",
            "Epoch 753/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9864\n",
            "Epoch 753: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0211 - accuracy: 0.9864 - val_loss: 2.3105 - val_accuracy: 0.7097\n",
            "Epoch 754/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9891\n",
            "Epoch 754: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0210 - accuracy: 0.9891 - val_loss: 2.3115 - val_accuracy: 0.7097\n",
            "Epoch 755/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9891\n",
            "Epoch 755: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.0193 - accuracy: 0.9891 - val_loss: 2.3216 - val_accuracy: 0.7097\n",
            "Epoch 756/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9891\n",
            "Epoch 756: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0214 - accuracy: 0.9891 - val_loss: 2.3315 - val_accuracy: 0.7204\n",
            "Epoch 757/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0265 - accuracy: 0.9805\n",
            "Epoch 757: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0225 - accuracy: 0.9837 - val_loss: 2.3349 - val_accuracy: 0.7204\n",
            "Epoch 758/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0251 - accuracy: 0.9844\n",
            "Epoch 758: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0208 - accuracy: 0.9891 - val_loss: 2.3293 - val_accuracy: 0.7204\n",
            "Epoch 759/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9864\n",
            "Epoch 759: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0216 - accuracy: 0.9864 - val_loss: 2.3318 - val_accuracy: 0.7204\n",
            "Epoch 760/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9918\n",
            "Epoch 760: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0191 - accuracy: 0.9918 - val_loss: 2.3294 - val_accuracy: 0.7097\n",
            "Epoch 761/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0158 - accuracy: 0.9922\n",
            "Epoch 761: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0194 - accuracy: 0.9918 - val_loss: 2.3209 - val_accuracy: 0.7097\n",
            "Epoch 762/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9918\n",
            "Epoch 762: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.0189 - accuracy: 0.9918 - val_loss: 2.3204 - val_accuracy: 0.7097\n",
            "Epoch 763/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9891\n",
            "Epoch 763: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.0204 - accuracy: 0.9891 - val_loss: 2.3143 - val_accuracy: 0.7097\n",
            "Epoch 764/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9864\n",
            "Epoch 764: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0221 - accuracy: 0.9864 - val_loss: 2.3254 - val_accuracy: 0.7097\n",
            "Epoch 765/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9864\n",
            "Epoch 765: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0232 - accuracy: 0.9864 - val_loss: 2.3460 - val_accuracy: 0.7204\n",
            "Epoch 766/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0232 - accuracy: 0.9844\n",
            "Epoch 766: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0190 - accuracy: 0.9891 - val_loss: 2.3604 - val_accuracy: 0.7097\n",
            "Epoch 767/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9891\n",
            "Epoch 767: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0207 - accuracy: 0.9891 - val_loss: 2.3404 - val_accuracy: 0.7097\n",
            "Epoch 768/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9891\n",
            "Epoch 768: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.0209 - accuracy: 0.9891 - val_loss: 2.3291 - val_accuracy: 0.7097\n",
            "Epoch 769/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9891\n",
            "Epoch 769: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0187 - accuracy: 0.9891 - val_loss: 2.3219 - val_accuracy: 0.6989\n",
            "Epoch 770/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9891\n",
            "Epoch 770: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.0197 - accuracy: 0.9891 - val_loss: 2.3212 - val_accuracy: 0.6989\n",
            "Epoch 771/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9891\n",
            "Epoch 771: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0205 - accuracy: 0.9891 - val_loss: 2.3212 - val_accuracy: 0.6989\n",
            "Epoch 772/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0180 - accuracy: 0.9961\n",
            "Epoch 772: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0193 - accuracy: 0.9918 - val_loss: 2.3254 - val_accuracy: 0.6989\n",
            "Epoch 773/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9837\n",
            "Epoch 773: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0226 - accuracy: 0.9837 - val_loss: 2.3351 - val_accuracy: 0.7097\n",
            "Epoch 774/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0225 - accuracy: 0.9883\n",
            "Epoch 774: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.0198 - accuracy: 0.9891 - val_loss: 2.3472 - val_accuracy: 0.7097\n",
            "Epoch 775/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9864\n",
            "Epoch 775: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.0213 - accuracy: 0.9864 - val_loss: 2.3665 - val_accuracy: 0.7097\n",
            "Epoch 776/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0204 - accuracy: 0.9883\n",
            "Epoch 776: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.0200 - accuracy: 0.9891 - val_loss: 2.3990 - val_accuracy: 0.7097\n",
            "Epoch 777/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9837\n",
            "Epoch 777: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0214 - accuracy: 0.9837 - val_loss: 2.4477 - val_accuracy: 0.7097\n",
            "Epoch 778/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9891\n",
            "Epoch 778: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0173 - accuracy: 0.9891 - val_loss: 2.4684 - val_accuracy: 0.7097\n",
            "Epoch 779/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9864\n",
            "Epoch 779: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0232 - accuracy: 0.9864 - val_loss: 2.4782 - val_accuracy: 0.7097\n",
            "Epoch 780/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9864\n",
            "Epoch 780: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0220 - accuracy: 0.9864 - val_loss: 2.4747 - val_accuracy: 0.7097\n",
            "Epoch 781/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9918\n",
            "Epoch 781: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0194 - accuracy: 0.9918 - val_loss: 2.4818 - val_accuracy: 0.7097\n",
            "Epoch 782/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9891\n",
            "Epoch 782: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0196 - accuracy: 0.9891 - val_loss: 2.4838 - val_accuracy: 0.7097\n",
            "Epoch 783/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9946\n",
            "Epoch 783: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0171 - accuracy: 0.9946 - val_loss: 2.4882 - val_accuracy: 0.6989\n",
            "Epoch 784/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9891\n",
            "Epoch 784: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0208 - accuracy: 0.9891 - val_loss: 2.4964 - val_accuracy: 0.6989\n",
            "Epoch 785/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9891\n",
            "Epoch 785: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0219 - accuracy: 0.9891 - val_loss: 2.5010 - val_accuracy: 0.6989\n",
            "Epoch 786/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9864\n",
            "Epoch 786: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0187 - accuracy: 0.9864 - val_loss: 2.5041 - val_accuracy: 0.6989\n",
            "Epoch 787/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9864\n",
            "Epoch 787: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0200 - accuracy: 0.9864 - val_loss: 2.4892 - val_accuracy: 0.6989\n",
            "Epoch 788/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9864\n",
            "Epoch 788: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0206 - accuracy: 0.9864 - val_loss: 2.4872 - val_accuracy: 0.6989\n",
            "Epoch 789/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9891\n",
            "Epoch 789: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0192 - accuracy: 0.9891 - val_loss: 2.4822 - val_accuracy: 0.6989\n",
            "Epoch 790/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9891\n",
            "Epoch 790: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0174 - accuracy: 0.9891 - val_loss: 2.4898 - val_accuracy: 0.6989\n",
            "Epoch 791/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9891\n",
            "Epoch 791: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0189 - accuracy: 0.9891 - val_loss: 2.5000 - val_accuracy: 0.6989\n",
            "Epoch 792/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9864\n",
            "Epoch 792: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0194 - accuracy: 0.9864 - val_loss: 2.5116 - val_accuracy: 0.6989\n",
            "Epoch 793/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9891\n",
            "Epoch 793: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0215 - accuracy: 0.9891 - val_loss: 2.5103 - val_accuracy: 0.7097\n",
            "Epoch 794/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9891\n",
            "Epoch 794: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0200 - accuracy: 0.9891 - val_loss: 2.5026 - val_accuracy: 0.7204\n",
            "Epoch 795/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9864\n",
            "Epoch 795: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0209 - accuracy: 0.9864 - val_loss: 2.4975 - val_accuracy: 0.7204\n",
            "Epoch 796/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9864\n",
            "Epoch 796: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0213 - accuracy: 0.9864 - val_loss: 2.4859 - val_accuracy: 0.7204\n",
            "Epoch 797/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9891\n",
            "Epoch 797: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0183 - accuracy: 0.9891 - val_loss: 2.4650 - val_accuracy: 0.7097\n",
            "Epoch 798/800\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0180 - accuracy: 0.9883\n",
            "Epoch 798: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0212 - accuracy: 0.9864 - val_loss: 2.4573 - val_accuracy: 0.7097\n",
            "Epoch 799/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9864\n",
            "Epoch 799: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0201 - accuracy: 0.9864 - val_loss: 2.4567 - val_accuracy: 0.7097\n",
            "Epoch 800/800\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9864\n",
            "Epoch 800: val_accuracy did not improve from 0.77419\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0206 - accuracy: 0.9864 - val_loss: 2.4513 - val_accuracy: 0.7097\n",
            "Training time: 144.10 seconds\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 150, 50)           100450    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 150, 100)          40400     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 150, 100)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 150, 100)          60400     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 150, 100)          0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 100)               60400     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 3)                 153       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 266853 (1.02 MB)\n",
            "Trainable params: 166403 (650.01 KB)\n",
            "Non-trainable params: 100450 (392.38 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation with the RNN\n",
        "y_valid_RNN = RNN_model.predict(feature_valid)\n",
        "valid_y_RNN = y_valid_RNN.copy()\n",
        "for i in range(len(y_valid_RNN)):\n",
        "    j = np.where(y_valid_RNN[i] == np.amax(y_valid_RNN[i]))\n",
        "    valid_y_RNN[i] = [0, 0, 0]\n",
        "    valid_y_RNN[i][j] = 1\n",
        "\n",
        "# print acc and report\n",
        "print(accuracy_score(label_valid_y,valid_y_RNN))\n",
        "print(classification_report(label_valid_y,valid_y_RNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_RNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUtVm7nWp9OC",
        "outputId": "c7b96c1e-6e7e-45b2-ddbd-5a2563a7f7a0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 2s 19ms/step\n",
            "0.7741935483870968\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.71      0.77        17\n",
            "           1       0.76      0.88      0.82        43\n",
            "           2       0.76      0.67      0.71        33\n",
            "\n",
            "   micro avg       0.77      0.77      0.77        93\n",
            "   macro avg       0.79      0.75      0.77        93\n",
            "weighted avg       0.78      0.77      0.77        93\n",
            " samples avg       0.77      0.77      0.77        93\n",
            "\n",
            "auc score:  0.8122145822833419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN+RNN"
      ],
      "metadata": {
        "id": "s8ohR024w44g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Bidirectional, GRU, LSTM, Attention, GlobalMaxPooling1D, Dense, Dropout, Input, Concatenate, Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "\n",
        "def create_CNN_RNN_model():\n",
        "    inputs = Input(shape=(maxlen,))\n",
        "    embeddings = Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False)(inputs)\n",
        "\n",
        "    conv1 = Conv1D(128, 3, activation='relu')(embeddings)\n",
        "    pool1 = MaxPooling1D(3)(conv1)\n",
        "    conv2 = Conv1D(128, 3, activation='relu')(pool1)\n",
        "    pool2 = MaxPooling1D(3)(conv2)\n",
        "\n",
        "    gru1 = Bidirectional(GRU(128, return_sequences=True))(pool2)\n",
        "    gru2 = Bidirectional(GRU(64, return_sequences=True))(gru1)\n",
        "\n",
        "    lstm1 = Bidirectional(LSTM(128, return_sequences=True))(pool2)\n",
        "    lstm2 = Bidirectional(LSTM(64, return_sequences=True))(lstm1)\n",
        "\n",
        "    concat = Concatenate(axis=-1)([gru2, lstm2])\n",
        "\n",
        "    attention = Attention()([concat, concat])\n",
        "\n",
        "    pool = GlobalMaxPooling1D()(attention)\n",
        "\n",
        "    dense1 = Dense(128, activation='relu')(pool)\n",
        "    dropout1 = Dropout(0.5)(dense1)\n",
        "\n",
        "    dense2 = Dense(64, activation='relu')(dropout1)\n",
        "    dropout2 = Dropout(0.5)(dense2)\n",
        "\n",
        "    outputs = Dense(3, activation='softmax')(dropout2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "D2_4zouxqAGF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_RNN_model = create_CNN_RNN_model()\n",
        "CNN_RNN_history = CNN_RNN_model.fit(feature_train, label_train_y, epochs=750, batch_size=128,validation_data=(feature_valid, label_valid_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W0MJctPqHdG",
        "outputId": "0e4dddc1-7d41-43bf-b268-4d6c9ed90de3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/750\n",
            "3/3 [==============================] - 17s 1s/step - loss: 1.0682 - accuracy: 0.4457 - val_loss: 1.0650 - val_accuracy: 0.4624\n",
            "Epoch 2/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 1.0430 - accuracy: 0.5190 - val_loss: 1.0531 - val_accuracy: 0.4624\n",
            "Epoch 3/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 1.0483 - accuracy: 0.5326 - val_loss: 1.0474 - val_accuracy: 0.4624\n",
            "Epoch 4/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 1.0370 - accuracy: 0.5217 - val_loss: 1.0448 - val_accuracy: 0.4624\n",
            "Epoch 5/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 1.0074 - accuracy: 0.5408 - val_loss: 1.0513 - val_accuracy: 0.4624\n",
            "Epoch 6/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 1.0207 - accuracy: 0.5625 - val_loss: 1.0425 - val_accuracy: 0.4624\n",
            "Epoch 7/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 1.0161 - accuracy: 0.5326 - val_loss: 1.0383 - val_accuracy: 0.4624\n",
            "Epoch 8/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.9932 - accuracy: 0.5462 - val_loss: 1.0468 - val_accuracy: 0.4624\n",
            "Epoch 9/750\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 1.0076 - accuracy: 0.5435 - val_loss: 1.0474 - val_accuracy: 0.4624\n",
            "Epoch 10/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.9901 - accuracy: 0.5571 - val_loss: 1.0348 - val_accuracy: 0.4624\n",
            "Epoch 11/750\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 1.0011 - accuracy: 0.5598 - val_loss: 1.0322 - val_accuracy: 0.4624\n",
            "Epoch 12/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.9854 - accuracy: 0.5543 - val_loss: 1.0355 - val_accuracy: 0.4624\n",
            "Epoch 13/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 1.0026 - accuracy: 0.5543 - val_loss: 1.0198 - val_accuracy: 0.4624\n",
            "Epoch 14/750\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.9814 - accuracy: 0.5543 - val_loss: 1.0136 - val_accuracy: 0.4624\n",
            "Epoch 15/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.9644 - accuracy: 0.5489 - val_loss: 1.0321 - val_accuracy: 0.4624\n",
            "Epoch 16/750\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.9541 - accuracy: 0.5543 - val_loss: 1.0103 - val_accuracy: 0.4624\n",
            "Epoch 17/750\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.9798 - accuracy: 0.5543 - val_loss: 0.9982 - val_accuracy: 0.4624\n",
            "Epoch 18/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.9554 - accuracy: 0.5543 - val_loss: 1.0334 - val_accuracy: 0.4624\n",
            "Epoch 19/750\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.9367 - accuracy: 0.5571 - val_loss: 1.0045 - val_accuracy: 0.4624\n",
            "Epoch 20/750\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.9536 - accuracy: 0.5598 - val_loss: 0.9793 - val_accuracy: 0.4624\n",
            "Epoch 21/750\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.9123 - accuracy: 0.5489 - val_loss: 0.9895 - val_accuracy: 0.4624\n",
            "Epoch 22/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.9301 - accuracy: 0.5707 - val_loss: 0.9733 - val_accuracy: 0.5376\n",
            "Epoch 23/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.8698 - accuracy: 0.6005 - val_loss: 1.0195 - val_accuracy: 0.4946\n",
            "Epoch 24/750\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.8516 - accuracy: 0.6005 - val_loss: 0.9396 - val_accuracy: 0.5269\n",
            "Epoch 25/750\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.8176 - accuracy: 0.6359 - val_loss: 1.0215 - val_accuracy: 0.5054\n",
            "Epoch 26/750\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.8319 - accuracy: 0.6060 - val_loss: 0.9603 - val_accuracy: 0.4946\n",
            "Epoch 27/750\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.8411 - accuracy: 0.6440 - val_loss: 0.9561 - val_accuracy: 0.5376\n",
            "Epoch 28/750\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.7994 - accuracy: 0.6304 - val_loss: 0.8654 - val_accuracy: 0.6452\n",
            "Epoch 29/750\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.7997 - accuracy: 0.6793 - val_loss: 0.8724 - val_accuracy: 0.5591\n",
            "Epoch 30/750\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.7322 - accuracy: 0.6739 - val_loss: 0.8465 - val_accuracy: 0.5591\n",
            "Epoch 31/750\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.7359 - accuracy: 0.6739 - val_loss: 0.8166 - val_accuracy: 0.6129\n",
            "Epoch 32/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.6753 - accuracy: 0.7011 - val_loss: 0.8124 - val_accuracy: 0.5806\n",
            "Epoch 33/750\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.6632 - accuracy: 0.6766 - val_loss: 0.9334 - val_accuracy: 0.5914\n",
            "Epoch 34/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6357 - accuracy: 0.7092 - val_loss: 0.7889 - val_accuracy: 0.6129\n",
            "Epoch 35/750\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.6248 - accuracy: 0.7201 - val_loss: 0.8662 - val_accuracy: 0.5699\n",
            "Epoch 36/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.7679 - accuracy: 0.6141 - val_loss: 0.8420 - val_accuracy: 0.6559\n",
            "Epoch 37/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6779 - accuracy: 0.7283 - val_loss: 0.9404 - val_accuracy: 0.5269\n",
            "Epoch 38/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7057 - accuracy: 0.6984 - val_loss: 0.8686 - val_accuracy: 0.5484\n",
            "Epoch 39/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6796 - accuracy: 0.7011 - val_loss: 0.7672 - val_accuracy: 0.6559\n",
            "Epoch 40/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6097 - accuracy: 0.7473 - val_loss: 0.8273 - val_accuracy: 0.6129\n",
            "Epoch 41/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5882 - accuracy: 0.7283 - val_loss: 0.8213 - val_accuracy: 0.6559\n",
            "Epoch 42/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5901 - accuracy: 0.7690 - val_loss: 0.7917 - val_accuracy: 0.6667\n",
            "Epoch 43/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.5092 - accuracy: 0.7908 - val_loss: 0.7683 - val_accuracy: 0.6989\n",
            "Epoch 44/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.5127 - accuracy: 0.8098 - val_loss: 0.8519 - val_accuracy: 0.6882\n",
            "Epoch 45/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4357 - accuracy: 0.8261 - val_loss: 0.7569 - val_accuracy: 0.6882\n",
            "Epoch 46/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4297 - accuracy: 0.7962 - val_loss: 0.8542 - val_accuracy: 0.7097\n",
            "Epoch 47/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4009 - accuracy: 0.8342 - val_loss: 1.0016 - val_accuracy: 0.6667\n",
            "Epoch 48/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4113 - accuracy: 0.8207 - val_loss: 0.8236 - val_accuracy: 0.7312\n",
            "Epoch 49/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3066 - accuracy: 0.8832 - val_loss: 0.9441 - val_accuracy: 0.7312\n",
            "Epoch 50/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3085 - accuracy: 0.9049 - val_loss: 0.7775 - val_accuracy: 0.7419\n",
            "Epoch 51/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4387 - accuracy: 0.8125 - val_loss: 0.7969 - val_accuracy: 0.7419\n",
            "Epoch 52/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4224 - accuracy: 0.8288 - val_loss: 0.7562 - val_accuracy: 0.6882\n",
            "Epoch 53/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3273 - accuracy: 0.8804 - val_loss: 1.1915 - val_accuracy: 0.6989\n",
            "Epoch 54/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3659 - accuracy: 0.8641 - val_loss: 0.7620 - val_accuracy: 0.7419\n",
            "Epoch 55/750\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.2458 - accuracy: 0.9103 - val_loss: 0.7218 - val_accuracy: 0.7204\n",
            "Epoch 56/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2436 - accuracy: 0.8940 - val_loss: 1.2309 - val_accuracy: 0.6989\n",
            "Epoch 57/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3670 - accuracy: 0.8533 - val_loss: 1.0242 - val_accuracy: 0.7419\n",
            "Epoch 58/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3708 - accuracy: 0.8342 - val_loss: 0.8045 - val_accuracy: 0.6989\n",
            "Epoch 59/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3071 - accuracy: 0.8995 - val_loss: 0.8457 - val_accuracy: 0.7097\n",
            "Epoch 60/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2566 - accuracy: 0.8967 - val_loss: 0.8119 - val_accuracy: 0.7312\n",
            "Epoch 61/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2546 - accuracy: 0.9049 - val_loss: 0.9046 - val_accuracy: 0.7419\n",
            "Epoch 62/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2521 - accuracy: 0.9049 - val_loss: 0.8648 - val_accuracy: 0.6989\n",
            "Epoch 63/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2225 - accuracy: 0.9321 - val_loss: 0.9150 - val_accuracy: 0.7097\n",
            "Epoch 64/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2068 - accuracy: 0.9429 - val_loss: 1.1846 - val_accuracy: 0.7097\n",
            "Epoch 65/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2235 - accuracy: 0.9293 - val_loss: 0.9095 - val_accuracy: 0.6989\n",
            "Epoch 66/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1812 - accuracy: 0.9402 - val_loss: 0.9647 - val_accuracy: 0.7097\n",
            "Epoch 67/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1226 - accuracy: 0.9701 - val_loss: 1.1968 - val_accuracy: 0.7204\n",
            "Epoch 68/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0960 - accuracy: 0.9755 - val_loss: 1.0434 - val_accuracy: 0.7312\n",
            "Epoch 69/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1386 - accuracy: 0.9457 - val_loss: 1.3493 - val_accuracy: 0.7312\n",
            "Epoch 70/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0899 - accuracy: 0.9701 - val_loss: 1.3356 - val_accuracy: 0.7312\n",
            "Epoch 71/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1216 - accuracy: 0.9647 - val_loss: 1.4188 - val_accuracy: 0.7527\n",
            "Epoch 72/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0680 - accuracy: 0.9755 - val_loss: 1.4779 - val_accuracy: 0.7634\n",
            "Epoch 73/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0747 - accuracy: 0.9755 - val_loss: 1.4916 - val_accuracy: 0.7204\n",
            "Epoch 74/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0751 - accuracy: 0.9783 - val_loss: 1.3445 - val_accuracy: 0.7312\n",
            "Epoch 75/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0937 - accuracy: 0.9701 - val_loss: 1.5878 - val_accuracy: 0.7527\n",
            "Epoch 76/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0762 - accuracy: 0.9755 - val_loss: 1.4613 - val_accuracy: 0.7634\n",
            "Epoch 77/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0898 - accuracy: 0.9755 - val_loss: 1.5435 - val_accuracy: 0.6667\n",
            "Epoch 78/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0914 - accuracy: 0.9674 - val_loss: 1.9886 - val_accuracy: 0.7097\n",
            "Epoch 79/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0824 - accuracy: 0.9728 - val_loss: 1.4823 - val_accuracy: 0.7527\n",
            "Epoch 80/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1078 - accuracy: 0.9728 - val_loss: 2.0840 - val_accuracy: 0.7312\n",
            "Epoch 81/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1625 - accuracy: 0.9375 - val_loss: 1.3458 - val_accuracy: 0.7312\n",
            "Epoch 82/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0911 - accuracy: 0.9620 - val_loss: 1.4050 - val_accuracy: 0.7097\n",
            "Epoch 83/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0775 - accuracy: 0.9728 - val_loss: 1.3845 - val_accuracy: 0.7312\n",
            "Epoch 84/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0622 - accuracy: 0.9728 - val_loss: 1.5374 - val_accuracy: 0.7312\n",
            "Epoch 85/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0485 - accuracy: 0.9810 - val_loss: 1.6457 - val_accuracy: 0.7312\n",
            "Epoch 86/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0460 - accuracy: 0.9837 - val_loss: 1.7600 - val_accuracy: 0.7527\n",
            "Epoch 87/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0465 - accuracy: 0.9810 - val_loss: 1.9009 - val_accuracy: 0.7204\n",
            "Epoch 88/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0351 - accuracy: 0.9891 - val_loss: 1.9198 - val_accuracy: 0.7527\n",
            "Epoch 89/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0475 - accuracy: 0.9810 - val_loss: 2.0640 - val_accuracy: 0.7204\n",
            "Epoch 90/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0688 - accuracy: 0.9810 - val_loss: 1.7058 - val_accuracy: 0.7742\n",
            "Epoch 91/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0678 - accuracy: 0.9755 - val_loss: 1.8210 - val_accuracy: 0.7419\n",
            "Epoch 92/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0729 - accuracy: 0.9755 - val_loss: 1.8261 - val_accuracy: 0.7312\n",
            "Epoch 93/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0476 - accuracy: 0.9837 - val_loss: 1.6391 - val_accuracy: 0.7312\n",
            "Epoch 94/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0624 - accuracy: 0.9755 - val_loss: 1.7440 - val_accuracy: 0.7097\n",
            "Epoch 95/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0400 - accuracy: 0.9837 - val_loss: 1.9072 - val_accuracy: 0.7204\n",
            "Epoch 96/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0588 - accuracy: 0.9755 - val_loss: 1.7578 - val_accuracy: 0.6989\n",
            "Epoch 97/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0439 - accuracy: 0.9755 - val_loss: 1.7658 - val_accuracy: 0.7097\n",
            "Epoch 98/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0391 - accuracy: 0.9810 - val_loss: 2.0168 - val_accuracy: 0.7419\n",
            "Epoch 99/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0388 - accuracy: 0.9891 - val_loss: 1.8095 - val_accuracy: 0.7419\n",
            "Epoch 100/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0430 - accuracy: 0.9864 - val_loss: 1.7694 - val_accuracy: 0.7419\n",
            "Epoch 101/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0318 - accuracy: 0.9810 - val_loss: 1.8853 - val_accuracy: 0.7419\n",
            "Epoch 102/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0325 - accuracy: 0.9783 - val_loss: 2.0060 - val_accuracy: 0.7419\n",
            "Epoch 103/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0360 - accuracy: 0.9783 - val_loss: 1.9333 - val_accuracy: 0.7634\n",
            "Epoch 104/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0437 - accuracy: 0.9810 - val_loss: 1.9787 - val_accuracy: 0.7527\n",
            "Epoch 105/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0301 - accuracy: 0.9864 - val_loss: 2.2424 - val_accuracy: 0.7419\n",
            "Epoch 106/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0757 - accuracy: 0.9783 - val_loss: 1.9757 - val_accuracy: 0.6989\n",
            "Epoch 107/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0797 - accuracy: 0.9674 - val_loss: 1.8932 - val_accuracy: 0.7527\n",
            "Epoch 108/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0692 - accuracy: 0.9701 - val_loss: 1.5585 - val_accuracy: 0.7312\n",
            "Epoch 109/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0631 - accuracy: 0.9810 - val_loss: 1.3871 - val_accuracy: 0.6989\n",
            "Epoch 110/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0841 - accuracy: 0.9620 - val_loss: 1.6473 - val_accuracy: 0.7312\n",
            "Epoch 111/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1026 - accuracy: 0.9674 - val_loss: 1.7935 - val_accuracy: 0.6989\n",
            "Epoch 112/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1689 - accuracy: 0.9565 - val_loss: 1.6278 - val_accuracy: 0.7097\n",
            "Epoch 113/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2059 - accuracy: 0.9266 - val_loss: 3.0688 - val_accuracy: 0.6452\n",
            "Epoch 114/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4790 - accuracy: 0.8533 - val_loss: 1.0388 - val_accuracy: 0.6989\n",
            "Epoch 115/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6430 - accuracy: 0.7527 - val_loss: 0.9889 - val_accuracy: 0.6129\n",
            "Epoch 116/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3985 - accuracy: 0.8342 - val_loss: 1.1466 - val_accuracy: 0.6989\n",
            "Epoch 117/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4324 - accuracy: 0.8832 - val_loss: 1.0364 - val_accuracy: 0.6989\n",
            "Epoch 118/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2685 - accuracy: 0.9130 - val_loss: 0.9638 - val_accuracy: 0.7204\n",
            "Epoch 119/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1835 - accuracy: 0.9266 - val_loss: 1.2609 - val_accuracy: 0.7204\n",
            "Epoch 120/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1420 - accuracy: 0.9402 - val_loss: 1.2624 - val_accuracy: 0.7097\n",
            "Epoch 121/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1405 - accuracy: 0.9647 - val_loss: 1.4327 - val_accuracy: 0.7312\n",
            "Epoch 122/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0915 - accuracy: 0.9755 - val_loss: 1.4606 - val_accuracy: 0.7742\n",
            "Epoch 123/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0759 - accuracy: 0.9701 - val_loss: 1.4787 - val_accuracy: 0.7634\n",
            "Epoch 124/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0599 - accuracy: 0.9783 - val_loss: 1.3444 - val_accuracy: 0.7312\n",
            "Epoch 125/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0451 - accuracy: 0.9783 - val_loss: 1.3735 - val_accuracy: 0.7312\n",
            "Epoch 126/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0519 - accuracy: 0.9755 - val_loss: 1.6348 - val_accuracy: 0.7527\n",
            "Epoch 127/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0490 - accuracy: 0.9837 - val_loss: 1.6666 - val_accuracy: 0.7634\n",
            "Epoch 128/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0386 - accuracy: 0.9810 - val_loss: 1.8520 - val_accuracy: 0.7312\n",
            "Epoch 129/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0442 - accuracy: 0.9783 - val_loss: 1.8530 - val_accuracy: 0.7419\n",
            "Epoch 130/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0252 - accuracy: 0.9837 - val_loss: 1.8227 - val_accuracy: 0.7849\n",
            "Epoch 131/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0474 - accuracy: 0.9810 - val_loss: 1.9176 - val_accuracy: 0.7419\n",
            "Epoch 132/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0572 - accuracy: 0.9755 - val_loss: 1.7169 - val_accuracy: 0.7312\n",
            "Epoch 133/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0876 - accuracy: 0.9620 - val_loss: 1.5209 - val_accuracy: 0.7527\n",
            "Epoch 134/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1104 - accuracy: 0.9592 - val_loss: 1.1260 - val_accuracy: 0.7527\n",
            "Epoch 135/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0782 - accuracy: 0.9783 - val_loss: 1.4695 - val_accuracy: 0.7419\n",
            "Epoch 136/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0636 - accuracy: 0.9755 - val_loss: 1.6150 - val_accuracy: 0.7527\n",
            "Epoch 137/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0586 - accuracy: 0.9755 - val_loss: 1.7699 - val_accuracy: 0.7527\n",
            "Epoch 138/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0561 - accuracy: 0.9783 - val_loss: 1.6301 - val_accuracy: 0.7634\n",
            "Epoch 139/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0413 - accuracy: 0.9864 - val_loss: 1.7650 - val_accuracy: 0.7742\n",
            "Epoch 140/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0638 - accuracy: 0.9674 - val_loss: 1.6991 - val_accuracy: 0.7419\n",
            "Epoch 141/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0448 - accuracy: 0.9783 - val_loss: 1.5880 - val_accuracy: 0.7204\n",
            "Epoch 142/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0507 - accuracy: 0.9864 - val_loss: 2.0211 - val_accuracy: 0.7204\n",
            "Epoch 143/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0411 - accuracy: 0.9728 - val_loss: 2.0543 - val_accuracy: 0.7419\n",
            "Epoch 144/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0417 - accuracy: 0.9755 - val_loss: 2.2488 - val_accuracy: 0.7204\n",
            "Epoch 145/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0423 - accuracy: 0.9837 - val_loss: 2.1105 - val_accuracy: 0.7527\n",
            "Epoch 146/750\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0284 - accuracy: 0.9810 - val_loss: 1.9828 - val_accuracy: 0.7204\n",
            "Epoch 147/750\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0314 - accuracy: 0.9837 - val_loss: 2.0061 - val_accuracy: 0.7204\n",
            "Epoch 148/750\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0271 - accuracy: 0.9918 - val_loss: 2.1122 - val_accuracy: 0.7419\n",
            "Epoch 149/750\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0363 - accuracy: 0.9864 - val_loss: 2.0707 - val_accuracy: 0.7527\n",
            "Epoch 150/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0316 - accuracy: 0.9864 - val_loss: 2.1052 - val_accuracy: 0.7634\n",
            "Epoch 151/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0280 - accuracy: 0.9864 - val_loss: 2.1712 - val_accuracy: 0.7849\n",
            "Epoch 152/750\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0379 - accuracy: 0.9755 - val_loss: 2.2388 - val_accuracy: 0.7634\n",
            "Epoch 153/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0370 - accuracy: 0.9783 - val_loss: 2.3018 - val_accuracy: 0.7527\n",
            "Epoch 154/750\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0206 - accuracy: 0.9946 - val_loss: 2.3087 - val_accuracy: 0.7527\n",
            "Epoch 155/750\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0297 - accuracy: 0.9837 - val_loss: 2.2789 - val_accuracy: 0.7742\n",
            "Epoch 156/750\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0338 - accuracy: 0.9891 - val_loss: 2.2895 - val_accuracy: 0.7527\n",
            "Epoch 157/750\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0324 - accuracy: 0.9837 - val_loss: 2.3897 - val_accuracy: 0.7527\n",
            "Epoch 158/750\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0304 - accuracy: 0.9837 - val_loss: 2.6148 - val_accuracy: 0.7312\n",
            "Epoch 159/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0419 - accuracy: 0.9783 - val_loss: 2.0209 - val_accuracy: 0.7204\n",
            "Epoch 160/750\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0334 - accuracy: 0.9837 - val_loss: 1.9120 - val_accuracy: 0.7204\n",
            "Epoch 161/750\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0367 - accuracy: 0.9783 - val_loss: 2.2012 - val_accuracy: 0.7527\n",
            "Epoch 162/750\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0311 - accuracy: 0.9810 - val_loss: 2.4979 - val_accuracy: 0.7419\n",
            "Epoch 163/750\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0336 - accuracy: 0.9837 - val_loss: 2.5092 - val_accuracy: 0.7527\n",
            "Epoch 164/750\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0389 - accuracy: 0.9810 - val_loss: 2.3847 - val_accuracy: 0.7527\n",
            "Epoch 165/750\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0295 - accuracy: 0.9810 - val_loss: 2.3510 - val_accuracy: 0.7419\n",
            "Epoch 166/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0336 - accuracy: 0.9864 - val_loss: 2.3869 - val_accuracy: 0.7527\n",
            "Epoch 167/750\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0353 - accuracy: 0.9810 - val_loss: 2.5203 - val_accuracy: 0.7419\n",
            "Epoch 168/750\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0286 - accuracy: 0.9864 - val_loss: 2.5563 - val_accuracy: 0.7419\n",
            "Epoch 169/750\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0281 - accuracy: 0.9837 - val_loss: 2.5484 - val_accuracy: 0.7634\n",
            "Epoch 170/750\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0254 - accuracy: 0.9864 - val_loss: 2.5676 - val_accuracy: 0.7634\n",
            "Epoch 171/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0319 - accuracy: 0.9837 - val_loss: 2.5660 - val_accuracy: 0.7634\n",
            "Epoch 172/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0226 - accuracy: 0.9891 - val_loss: 2.6166 - val_accuracy: 0.7419\n",
            "Epoch 173/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0332 - accuracy: 0.9810 - val_loss: 2.6425 - val_accuracy: 0.7419\n",
            "Epoch 174/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0327 - accuracy: 0.9783 - val_loss: 2.5575 - val_accuracy: 0.7419\n",
            "Epoch 175/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0254 - accuracy: 0.9810 - val_loss: 2.4507 - val_accuracy: 0.7527\n",
            "Epoch 176/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0251 - accuracy: 0.9837 - val_loss: 2.3958 - val_accuracy: 0.7634\n",
            "Epoch 177/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0322 - accuracy: 0.9837 - val_loss: 2.3817 - val_accuracy: 0.7634\n",
            "Epoch 178/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0344 - accuracy: 0.9783 - val_loss: 2.4265 - val_accuracy: 0.7419\n",
            "Epoch 179/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0312 - accuracy: 0.9891 - val_loss: 2.4513 - val_accuracy: 0.7419\n",
            "Epoch 180/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0296 - accuracy: 0.9837 - val_loss: 2.4347 - val_accuracy: 0.7312\n",
            "Epoch 181/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0291 - accuracy: 0.9864 - val_loss: 2.4492 - val_accuracy: 0.7312\n",
            "Epoch 182/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0252 - accuracy: 0.9891 - val_loss: 2.4595 - val_accuracy: 0.7312\n",
            "Epoch 183/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0284 - accuracy: 0.9810 - val_loss: 2.4784 - val_accuracy: 0.7312\n",
            "Epoch 184/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0274 - accuracy: 0.9864 - val_loss: 2.4977 - val_accuracy: 0.7312\n",
            "Epoch 185/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0278 - accuracy: 0.9810 - val_loss: 2.5142 - val_accuracy: 0.7312\n",
            "Epoch 186/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0318 - accuracy: 0.9864 - val_loss: 2.5495 - val_accuracy: 0.7419\n",
            "Epoch 187/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0284 - accuracy: 0.9810 - val_loss: 2.5867 - val_accuracy: 0.7419\n",
            "Epoch 188/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0282 - accuracy: 0.9837 - val_loss: 2.6195 - val_accuracy: 0.7419\n",
            "Epoch 189/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0247 - accuracy: 0.9864 - val_loss: 2.7084 - val_accuracy: 0.7419\n",
            "Epoch 190/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0344 - accuracy: 0.9783 - val_loss: 2.7216 - val_accuracy: 0.7312\n",
            "Epoch 191/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0257 - accuracy: 0.9837 - val_loss: 2.7039 - val_accuracy: 0.7634\n",
            "Epoch 192/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0275 - accuracy: 0.9891 - val_loss: 2.7453 - val_accuracy: 0.7419\n",
            "Epoch 193/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0265 - accuracy: 0.9891 - val_loss: 2.6218 - val_accuracy: 0.7634\n",
            "Epoch 194/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0397 - accuracy: 0.9810 - val_loss: 2.6019 - val_accuracy: 0.7527\n",
            "Epoch 195/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0282 - accuracy: 0.9837 - val_loss: 2.6027 - val_accuracy: 0.7312\n",
            "Epoch 196/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0342 - accuracy: 0.9783 - val_loss: 2.5703 - val_accuracy: 0.7204\n",
            "Epoch 197/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0307 - accuracy: 0.9810 - val_loss: 2.4818 - val_accuracy: 0.7204\n",
            "Epoch 198/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0326 - accuracy: 0.9837 - val_loss: 2.4385 - val_accuracy: 0.7419\n",
            "Epoch 199/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0318 - accuracy: 0.9837 - val_loss: 2.4217 - val_accuracy: 0.7527\n",
            "Epoch 200/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0292 - accuracy: 0.9810 - val_loss: 2.4233 - val_accuracy: 0.7527\n",
            "Epoch 201/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0255 - accuracy: 0.9891 - val_loss: 2.4772 - val_accuracy: 0.7634\n",
            "Epoch 202/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0337 - accuracy: 0.9837 - val_loss: 2.5414 - val_accuracy: 0.7634\n",
            "Epoch 203/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0294 - accuracy: 0.9837 - val_loss: 2.5996 - val_accuracy: 0.7634\n",
            "Epoch 204/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0263 - accuracy: 0.9810 - val_loss: 2.6324 - val_accuracy: 0.7634\n",
            "Epoch 205/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0297 - accuracy: 0.9837 - val_loss: 2.7806 - val_accuracy: 0.7742\n",
            "Epoch 206/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0232 - accuracy: 0.9891 - val_loss: 2.8780 - val_accuracy: 0.7527\n",
            "Epoch 207/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0310 - accuracy: 0.9864 - val_loss: 2.7017 - val_accuracy: 0.7742\n",
            "Epoch 208/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0341 - accuracy: 0.9837 - val_loss: 2.5247 - val_accuracy: 0.7634\n",
            "Epoch 209/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0324 - accuracy: 0.9837 - val_loss: 2.4673 - val_accuracy: 0.7634\n",
            "Epoch 210/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0279 - accuracy: 0.9864 - val_loss: 2.4596 - val_accuracy: 0.7527\n",
            "Epoch 211/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0309 - accuracy: 0.9837 - val_loss: 2.4264 - val_accuracy: 0.7527\n",
            "Epoch 212/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0331 - accuracy: 0.9837 - val_loss: 2.3489 - val_accuracy: 0.7527\n",
            "Epoch 213/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0338 - accuracy: 0.9810 - val_loss: 2.3027 - val_accuracy: 0.7419\n",
            "Epoch 214/750\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0298 - accuracy: 0.9810 - val_loss: 2.3064 - val_accuracy: 0.7634\n",
            "Epoch 215/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0263 - accuracy: 0.9864 - val_loss: 2.3589 - val_accuracy: 0.7634\n",
            "Epoch 216/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0320 - accuracy: 0.9837 - val_loss: 2.4099 - val_accuracy: 0.7634\n",
            "Epoch 217/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0289 - accuracy: 0.9810 - val_loss: 2.4517 - val_accuracy: 0.7527\n",
            "Epoch 218/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0293 - accuracy: 0.9891 - val_loss: 2.5011 - val_accuracy: 0.7527\n",
            "Epoch 219/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0320 - accuracy: 0.9755 - val_loss: 2.5514 - val_accuracy: 0.7527\n",
            "Epoch 220/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0290 - accuracy: 0.9864 - val_loss: 2.5985 - val_accuracy: 0.7419\n",
            "Epoch 221/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0276 - accuracy: 0.9891 - val_loss: 2.7040 - val_accuracy: 0.7527\n",
            "Epoch 222/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0248 - accuracy: 0.9837 - val_loss: 2.8095 - val_accuracy: 0.7527\n",
            "Epoch 223/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0254 - accuracy: 0.9837 - val_loss: 2.7895 - val_accuracy: 0.7527\n",
            "Epoch 224/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0266 - accuracy: 0.9891 - val_loss: 2.7090 - val_accuracy: 0.7527\n",
            "Epoch 225/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0281 - accuracy: 0.9783 - val_loss: 2.6800 - val_accuracy: 0.7419\n",
            "Epoch 226/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0308 - accuracy: 0.9864 - val_loss: 2.5872 - val_accuracy: 0.7634\n",
            "Epoch 227/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0262 - accuracy: 0.9891 - val_loss: 2.5688 - val_accuracy: 0.7849\n",
            "Epoch 228/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0254 - accuracy: 0.9864 - val_loss: 2.6319 - val_accuracy: 0.7849\n",
            "Epoch 229/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0244 - accuracy: 0.9864 - val_loss: 2.6788 - val_accuracy: 0.7527\n",
            "Epoch 230/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0293 - accuracy: 0.9864 - val_loss: 2.6718 - val_accuracy: 0.7634\n",
            "Epoch 231/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0348 - accuracy: 0.9810 - val_loss: 2.6447 - val_accuracy: 0.7849\n",
            "Epoch 232/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0257 - accuracy: 0.9837 - val_loss: 2.6102 - val_accuracy: 0.7849\n",
            "Epoch 233/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0271 - accuracy: 0.9837 - val_loss: 2.5367 - val_accuracy: 0.7742\n",
            "Epoch 234/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0300 - accuracy: 0.9783 - val_loss: 2.5631 - val_accuracy: 0.7634\n",
            "Epoch 235/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0274 - accuracy: 0.9810 - val_loss: 2.6042 - val_accuracy: 0.7527\n",
            "Epoch 236/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0279 - accuracy: 0.9864 - val_loss: 2.6507 - val_accuracy: 0.7527\n",
            "Epoch 237/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0298 - accuracy: 0.9783 - val_loss: 2.7166 - val_accuracy: 0.7742\n",
            "Epoch 238/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0235 - accuracy: 0.9918 - val_loss: 2.7733 - val_accuracy: 0.7634\n",
            "Epoch 239/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0228 - accuracy: 0.9837 - val_loss: 2.8469 - val_accuracy: 0.7634\n",
            "Epoch 240/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0265 - accuracy: 0.9918 - val_loss: 2.8627 - val_accuracy: 0.7634\n",
            "Epoch 241/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0258 - accuracy: 0.9918 - val_loss: 2.9158 - val_accuracy: 0.7419\n",
            "Epoch 242/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0278 - accuracy: 0.9783 - val_loss: 3.0248 - val_accuracy: 0.7419\n",
            "Epoch 243/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0354 - accuracy: 0.9837 - val_loss: 2.8498 - val_accuracy: 0.7527\n",
            "Epoch 244/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0354 - accuracy: 0.9837 - val_loss: 2.7578 - val_accuracy: 0.7849\n",
            "Epoch 245/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0292 - accuracy: 0.9755 - val_loss: 3.1704 - val_accuracy: 0.7527\n",
            "Epoch 246/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0348 - accuracy: 0.9810 - val_loss: 2.6616 - val_accuracy: 0.7312\n",
            "Epoch 247/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0274 - accuracy: 0.9864 - val_loss: 2.8543 - val_accuracy: 0.6667\n",
            "Epoch 248/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3170 - accuracy: 0.9429 - val_loss: 2.1173 - val_accuracy: 0.6022\n",
            "Epoch 249/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6968 - accuracy: 0.7038 - val_loss: 1.1659 - val_accuracy: 0.5269\n",
            "Epoch 250/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7750 - accuracy: 0.6440 - val_loss: 1.1765 - val_accuracy: 0.4516\n",
            "Epoch 251/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7620 - accuracy: 0.6522 - val_loss: 0.9892 - val_accuracy: 0.5269\n",
            "Epoch 252/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.7009 - accuracy: 0.6848 - val_loss: 1.1469 - val_accuracy: 0.4731\n",
            "Epoch 253/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6952 - accuracy: 0.6848 - val_loss: 0.9997 - val_accuracy: 0.5269\n",
            "Epoch 254/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6532 - accuracy: 0.6875 - val_loss: 0.8857 - val_accuracy: 0.5484\n",
            "Epoch 255/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.6181 - accuracy: 0.7201 - val_loss: 0.8336 - val_accuracy: 0.5914\n",
            "Epoch 256/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5155 - accuracy: 0.7554 - val_loss: 0.8798 - val_accuracy: 0.5699\n",
            "Epoch 257/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4798 - accuracy: 0.7609 - val_loss: 0.9027 - val_accuracy: 0.6237\n",
            "Epoch 258/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4370 - accuracy: 0.7799 - val_loss: 1.0545 - val_accuracy: 0.6022\n",
            "Epoch 259/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3892 - accuracy: 0.8016 - val_loss: 0.9520 - val_accuracy: 0.5914\n",
            "Epoch 260/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3738 - accuracy: 0.8397 - val_loss: 1.2843 - val_accuracy: 0.6129\n",
            "Epoch 261/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3682 - accuracy: 0.8478 - val_loss: 0.8704 - val_accuracy: 0.7097\n",
            "Epoch 262/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2486 - accuracy: 0.8995 - val_loss: 1.0505 - val_accuracy: 0.6882\n",
            "Epoch 263/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2936 - accuracy: 0.8995 - val_loss: 1.0634 - val_accuracy: 0.7097\n",
            "Epoch 264/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2084 - accuracy: 0.9293 - val_loss: 1.0863 - val_accuracy: 0.7312\n",
            "Epoch 265/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1587 - accuracy: 0.9484 - val_loss: 1.2585 - val_accuracy: 0.7204\n",
            "Epoch 266/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1553 - accuracy: 0.9538 - val_loss: 1.3405 - val_accuracy: 0.7204\n",
            "Epoch 267/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1271 - accuracy: 0.9674 - val_loss: 1.2107 - val_accuracy: 0.7419\n",
            "Epoch 268/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1310 - accuracy: 0.9647 - val_loss: 1.6033 - val_accuracy: 0.7419\n",
            "Epoch 269/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1155 - accuracy: 0.9592 - val_loss: 1.1965 - val_accuracy: 0.7527\n",
            "Epoch 270/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1206 - accuracy: 0.9674 - val_loss: 1.5108 - val_accuracy: 0.7097\n",
            "Epoch 271/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0885 - accuracy: 0.9728 - val_loss: 1.3724 - val_accuracy: 0.7634\n",
            "Epoch 272/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0983 - accuracy: 0.9674 - val_loss: 2.1856 - val_accuracy: 0.6882\n",
            "Epoch 273/750\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0944 - accuracy: 0.9674 - val_loss: 1.2011 - val_accuracy: 0.7742\n",
            "Epoch 274/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1007 - accuracy: 0.9647 - val_loss: 2.0777 - val_accuracy: 0.6989\n",
            "Epoch 275/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0894 - accuracy: 0.9701 - val_loss: 1.4885 - val_accuracy: 0.7527\n",
            "Epoch 276/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0758 - accuracy: 0.9837 - val_loss: 1.4532 - val_accuracy: 0.7742\n",
            "Epoch 277/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0449 - accuracy: 0.9810 - val_loss: 1.7419 - val_accuracy: 0.7204\n",
            "Epoch 278/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0702 - accuracy: 0.9728 - val_loss: 1.6330 - val_accuracy: 0.7419\n",
            "Epoch 279/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0445 - accuracy: 0.9810 - val_loss: 1.8542 - val_accuracy: 0.7527\n",
            "Epoch 280/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0513 - accuracy: 0.9837 - val_loss: 1.8934 - val_accuracy: 0.7204\n",
            "Epoch 281/750\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0301 - accuracy: 0.9864 - val_loss: 1.7010 - val_accuracy: 0.7419\n",
            "Epoch 282/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0462 - accuracy: 0.9810 - val_loss: 1.7208 - val_accuracy: 0.7312\n",
            "Epoch 283/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0654 - accuracy: 0.9837 - val_loss: 1.8447 - val_accuracy: 0.7312\n",
            "Epoch 284/750\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0284 - accuracy: 0.9891 - val_loss: 1.4936 - val_accuracy: 0.7849\n",
            "Epoch 285/750\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0479 - accuracy: 0.9755 - val_loss: 1.7796 - val_accuracy: 0.7312\n",
            "Epoch 286/750\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0413 - accuracy: 0.9810 - val_loss: 2.1313 - val_accuracy: 0.7204\n",
            "Epoch 287/750\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0317 - accuracy: 0.9837 - val_loss: 1.7875 - val_accuracy: 0.7527\n",
            "Epoch 288/750\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0443 - accuracy: 0.9810 - val_loss: 1.7757 - val_accuracy: 0.7419\n",
            "Epoch 289/750\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0411 - accuracy: 0.9837 - val_loss: 1.9348 - val_accuracy: 0.7097\n",
            "Epoch 290/750\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0376 - accuracy: 0.9864 - val_loss: 1.9752 - val_accuracy: 0.7097\n",
            "Epoch 291/750\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0253 - accuracy: 0.9837 - val_loss: 2.0402 - val_accuracy: 0.7527\n",
            "Epoch 292/750\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0359 - accuracy: 0.9783 - val_loss: 2.0907 - val_accuracy: 0.7312\n",
            "Epoch 293/750\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0352 - accuracy: 0.9755 - val_loss: 2.2073 - val_accuracy: 0.7312\n",
            "Epoch 294/750\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0394 - accuracy: 0.9755 - val_loss: 1.9687 - val_accuracy: 0.7634\n",
            "Epoch 295/750\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0369 - accuracy: 0.9810 - val_loss: 1.9873 - val_accuracy: 0.7527\n",
            "Epoch 296/750\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 0.0263 - accuracy: 0.9918 - val_loss: 2.3941 - val_accuracy: 0.7097\n",
            "Epoch 297/750\n",
            "3/3 [==============================] - 0s 189ms/step - loss: 0.0455 - accuracy: 0.9783 - val_loss: 1.8615 - val_accuracy: 0.7742\n",
            "Epoch 298/750\n",
            "3/3 [==============================] - 0s 167ms/step - loss: 0.0457 - accuracy: 0.9755 - val_loss: 1.7166 - val_accuracy: 0.7634\n",
            "Epoch 299/750\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 0.0262 - accuracy: 0.9918 - val_loss: 1.9929 - val_accuracy: 0.7312\n",
            "Epoch 300/750\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0299 - accuracy: 0.9837 - val_loss: 2.1749 - val_accuracy: 0.7634\n",
            "Epoch 301/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0469 - accuracy: 0.9810 - val_loss: 1.9454 - val_accuracy: 0.7419\n",
            "Epoch 302/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0397 - accuracy: 0.9837 - val_loss: 1.8452 - val_accuracy: 0.7634\n",
            "Epoch 303/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0389 - accuracy: 0.9810 - val_loss: 2.0315 - val_accuracy: 0.7419\n",
            "Epoch 304/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0344 - accuracy: 0.9837 - val_loss: 2.0511 - val_accuracy: 0.7419\n",
            "Epoch 305/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0327 - accuracy: 0.9837 - val_loss: 1.9193 - val_accuracy: 0.7419\n",
            "Epoch 306/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0247 - accuracy: 0.9891 - val_loss: 1.9463 - val_accuracy: 0.7419\n",
            "Epoch 307/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0271 - accuracy: 0.9864 - val_loss: 2.0814 - val_accuracy: 0.7419\n",
            "Epoch 308/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0307 - accuracy: 0.9837 - val_loss: 2.2341 - val_accuracy: 0.7419\n",
            "Epoch 309/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0274 - accuracy: 0.9783 - val_loss: 2.1851 - val_accuracy: 0.7419\n",
            "Epoch 310/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0293 - accuracy: 0.9783 - val_loss: 2.2025 - val_accuracy: 0.7419\n",
            "Epoch 311/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0293 - accuracy: 0.9837 - val_loss: 2.0864 - val_accuracy: 0.7742\n",
            "Epoch 312/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0298 - accuracy: 0.9837 - val_loss: 2.0668 - val_accuracy: 0.7634\n",
            "Epoch 313/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0230 - accuracy: 0.9864 - val_loss: 2.1231 - val_accuracy: 0.7527\n",
            "Epoch 314/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0328 - accuracy: 0.9864 - val_loss: 2.2337 - val_accuracy: 0.7527\n",
            "Epoch 315/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0199 - accuracy: 0.9918 - val_loss: 2.2643 - val_accuracy: 0.7527\n",
            "Epoch 316/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0224 - accuracy: 0.9918 - val_loss: 2.3650 - val_accuracy: 0.7527\n",
            "Epoch 317/750\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0411 - accuracy: 0.9837 - val_loss: 2.1886 - val_accuracy: 0.7742\n",
            "Epoch 318/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0268 - accuracy: 0.9891 - val_loss: 2.1796 - val_accuracy: 0.7527\n",
            "Epoch 319/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0240 - accuracy: 0.9918 - val_loss: 2.3154 - val_accuracy: 0.7419\n",
            "Epoch 320/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0354 - accuracy: 0.9810 - val_loss: 2.2913 - val_accuracy: 0.7419\n",
            "Epoch 321/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0248 - accuracy: 0.9891 - val_loss: 2.3369 - val_accuracy: 0.7419\n",
            "Epoch 322/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0299 - accuracy: 0.9783 - val_loss: 2.2804 - val_accuracy: 0.7419\n",
            "Epoch 323/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0360 - accuracy: 0.9837 - val_loss: 2.3102 - val_accuracy: 0.7419\n",
            "Epoch 324/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0389 - accuracy: 0.9810 - val_loss: 2.3752 - val_accuracy: 0.7419\n",
            "Epoch 325/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0278 - accuracy: 0.9864 - val_loss: 2.5409 - val_accuracy: 0.7312\n",
            "Epoch 326/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0300 - accuracy: 0.9783 - val_loss: 2.5009 - val_accuracy: 0.7419\n",
            "Epoch 327/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0295 - accuracy: 0.9837 - val_loss: 2.5283 - val_accuracy: 0.7419\n",
            "Epoch 328/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0299 - accuracy: 0.9837 - val_loss: 2.3650 - val_accuracy: 0.7742\n",
            "Epoch 329/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0296 - accuracy: 0.9973 - val_loss: 2.7658 - val_accuracy: 0.7312\n",
            "Epoch 330/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0355 - accuracy: 0.9783 - val_loss: 2.9133 - val_accuracy: 0.7097\n",
            "Epoch 331/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0348 - accuracy: 0.9783 - val_loss: 2.3796 - val_accuracy: 0.7419\n",
            "Epoch 332/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0253 - accuracy: 0.9864 - val_loss: 2.0679 - val_accuracy: 0.7634\n",
            "Epoch 333/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0347 - accuracy: 0.9783 - val_loss: 1.9822 - val_accuracy: 0.7527\n",
            "Epoch 334/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0311 - accuracy: 0.9783 - val_loss: 2.1510 - val_accuracy: 0.7312\n",
            "Epoch 335/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0336 - accuracy: 0.9810 - val_loss: 2.3517 - val_accuracy: 0.7419\n",
            "Epoch 336/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0384 - accuracy: 0.9810 - val_loss: 2.3168 - val_accuracy: 0.7419\n",
            "Epoch 337/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0323 - accuracy: 0.9810 - val_loss: 2.1038 - val_accuracy: 0.7419\n",
            "Epoch 338/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0364 - accuracy: 0.9755 - val_loss: 1.9185 - val_accuracy: 0.7527\n",
            "Epoch 339/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0355 - accuracy: 0.9783 - val_loss: 1.9481 - val_accuracy: 0.7419\n",
            "Epoch 340/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0237 - accuracy: 0.9864 - val_loss: 2.1345 - val_accuracy: 0.7634\n",
            "Epoch 341/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0211 - accuracy: 0.9918 - val_loss: 2.3151 - val_accuracy: 0.7312\n",
            "Epoch 342/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0307 - accuracy: 0.9864 - val_loss: 2.4958 - val_accuracy: 0.7312\n",
            "Epoch 343/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0354 - accuracy: 0.9810 - val_loss: 2.0776 - val_accuracy: 0.7742\n",
            "Epoch 344/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0313 - accuracy: 0.9837 - val_loss: 2.0201 - val_accuracy: 0.7634\n",
            "Epoch 345/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0349 - accuracy: 0.9783 - val_loss: 2.1775 - val_accuracy: 0.7527\n",
            "Epoch 346/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0295 - accuracy: 0.9783 - val_loss: 2.3107 - val_accuracy: 0.7419\n",
            "Epoch 347/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0286 - accuracy: 0.9837 - val_loss: 2.3708 - val_accuracy: 0.7419\n",
            "Epoch 348/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0283 - accuracy: 0.9810 - val_loss: 2.3770 - val_accuracy: 0.7419\n",
            "Epoch 349/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0283 - accuracy: 0.9864 - val_loss: 2.4377 - val_accuracy: 0.7419\n",
            "Epoch 350/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0277 - accuracy: 0.9918 - val_loss: 2.4351 - val_accuracy: 0.7634\n",
            "Epoch 351/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0302 - accuracy: 0.9783 - val_loss: 2.4622 - val_accuracy: 0.7742\n",
            "Epoch 352/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0313 - accuracy: 0.9810 - val_loss: 2.4393 - val_accuracy: 0.7634\n",
            "Epoch 353/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0281 - accuracy: 0.9837 - val_loss: 2.3448 - val_accuracy: 0.7634\n",
            "Epoch 354/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0269 - accuracy: 0.9837 - val_loss: 2.4136 - val_accuracy: 0.7634\n",
            "Epoch 355/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0299 - accuracy: 0.9837 - val_loss: 2.4468 - val_accuracy: 0.7634\n",
            "Epoch 356/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0370 - accuracy: 0.9837 - val_loss: 2.5386 - val_accuracy: 0.7312\n",
            "Epoch 357/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0242 - accuracy: 0.9891 - val_loss: 2.5317 - val_accuracy: 0.7312\n",
            "Epoch 358/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0274 - accuracy: 0.9864 - val_loss: 2.6018 - val_accuracy: 0.7312\n",
            "Epoch 359/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0336 - accuracy: 0.9837 - val_loss: 2.5657 - val_accuracy: 0.7419\n",
            "Epoch 360/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0263 - accuracy: 0.9783 - val_loss: 2.3276 - val_accuracy: 0.7527\n",
            "Epoch 361/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0289 - accuracy: 0.9891 - val_loss: 2.4377 - val_accuracy: 0.7419\n",
            "Epoch 362/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0285 - accuracy: 0.9837 - val_loss: 3.0957 - val_accuracy: 0.7312\n",
            "Epoch 363/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0354 - accuracy: 0.9755 - val_loss: 3.0837 - val_accuracy: 0.7204\n",
            "Epoch 364/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0245 - accuracy: 0.9891 - val_loss: 2.3464 - val_accuracy: 0.7527\n",
            "Epoch 365/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0299 - accuracy: 0.9864 - val_loss: 2.1630 - val_accuracy: 0.7312\n",
            "Epoch 366/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0226 - accuracy: 0.9918 - val_loss: 2.6902 - val_accuracy: 0.7419\n",
            "Epoch 367/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0343 - accuracy: 0.9755 - val_loss: 2.9458 - val_accuracy: 0.7634\n",
            "Epoch 368/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0346 - accuracy: 0.9810 - val_loss: 2.8075 - val_accuracy: 0.7527\n",
            "Epoch 369/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0310 - accuracy: 0.9837 - val_loss: 2.4129 - val_accuracy: 0.7742\n",
            "Epoch 370/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0252 - accuracy: 0.9864 - val_loss: 2.3020 - val_accuracy: 0.7634\n",
            "Epoch 371/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0273 - accuracy: 0.9837 - val_loss: 2.4309 - val_accuracy: 0.7634\n",
            "Epoch 372/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0317 - accuracy: 0.9837 - val_loss: 2.6706 - val_accuracy: 0.7419\n",
            "Epoch 373/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0276 - accuracy: 0.9864 - val_loss: 2.7743 - val_accuracy: 0.7419\n",
            "Epoch 374/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0325 - accuracy: 0.9728 - val_loss: 2.7548 - val_accuracy: 0.7312\n",
            "Epoch 375/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0294 - accuracy: 0.9810 - val_loss: 2.7048 - val_accuracy: 0.7312\n",
            "Epoch 376/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0289 - accuracy: 0.9864 - val_loss: 2.6396 - val_accuracy: 0.7419\n",
            "Epoch 377/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0332 - accuracy: 0.9837 - val_loss: 2.6836 - val_accuracy: 0.7312\n",
            "Epoch 378/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0276 - accuracy: 0.9864 - val_loss: 2.6064 - val_accuracy: 0.7634\n",
            "Epoch 379/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0270 - accuracy: 0.9837 - val_loss: 2.6395 - val_accuracy: 0.7419\n",
            "Epoch 380/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0219 - accuracy: 0.9918 - val_loss: 2.7488 - val_accuracy: 0.7312\n",
            "Epoch 381/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0249 - accuracy: 0.9837 - val_loss: 2.9054 - val_accuracy: 0.7312\n",
            "Epoch 382/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0272 - accuracy: 0.9810 - val_loss: 2.9156 - val_accuracy: 0.7312\n",
            "Epoch 383/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0345 - accuracy: 0.9837 - val_loss: 2.9672 - val_accuracy: 0.7312\n",
            "Epoch 384/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0299 - accuracy: 0.9810 - val_loss: 2.8678 - val_accuracy: 0.7312\n",
            "Epoch 385/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0232 - accuracy: 0.9946 - val_loss: 2.8078 - val_accuracy: 0.7419\n",
            "Epoch 386/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0246 - accuracy: 0.9810 - val_loss: 2.7670 - val_accuracy: 0.7527\n",
            "Epoch 387/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0316 - accuracy: 0.9837 - val_loss: 2.8760 - val_accuracy: 0.7204\n",
            "Epoch 388/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0257 - accuracy: 0.9864 - val_loss: 2.9015 - val_accuracy: 0.7204\n",
            "Epoch 389/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0245 - accuracy: 0.9837 - val_loss: 2.8617 - val_accuracy: 0.7204\n",
            "Epoch 390/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0263 - accuracy: 0.9837 - val_loss: 2.8256 - val_accuracy: 0.7204\n",
            "Epoch 391/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0253 - accuracy: 0.9891 - val_loss: 2.8469 - val_accuracy: 0.7204\n",
            "Epoch 392/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0274 - accuracy: 0.9837 - val_loss: 2.9676 - val_accuracy: 0.7204\n",
            "Epoch 393/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0357 - accuracy: 0.9783 - val_loss: 3.0702 - val_accuracy: 0.7204\n",
            "Epoch 394/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0300 - accuracy: 0.9810 - val_loss: 3.0483 - val_accuracy: 0.7204\n",
            "Epoch 395/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0221 - accuracy: 0.9864 - val_loss: 3.0855 - val_accuracy: 0.7204\n",
            "Epoch 396/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0357 - accuracy: 0.9837 - val_loss: 3.0858 - val_accuracy: 0.7312\n",
            "Epoch 397/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0249 - accuracy: 0.9918 - val_loss: 2.9362 - val_accuracy: 0.7312\n",
            "Epoch 398/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0214 - accuracy: 0.9891 - val_loss: 2.9839 - val_accuracy: 0.7312\n",
            "Epoch 399/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0275 - accuracy: 0.9810 - val_loss: 2.9963 - val_accuracy: 0.7312\n",
            "Epoch 400/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0259 - accuracy: 0.9864 - val_loss: 3.0338 - val_accuracy: 0.7312\n",
            "Epoch 401/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0319 - accuracy: 0.9837 - val_loss: 3.0223 - val_accuracy: 0.7312\n",
            "Epoch 402/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0252 - accuracy: 0.9891 - val_loss: 2.9988 - val_accuracy: 0.7312\n",
            "Epoch 403/750\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0222 - accuracy: 0.9891 - val_loss: 2.8725 - val_accuracy: 0.7312\n",
            "Epoch 404/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0284 - accuracy: 0.9837 - val_loss: 2.8252 - val_accuracy: 0.7634\n",
            "Epoch 405/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0275 - accuracy: 0.9837 - val_loss: 2.9568 - val_accuracy: 0.7204\n",
            "Epoch 406/750\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0238 - accuracy: 0.9837 - val_loss: 3.1778 - val_accuracy: 0.7204\n",
            "Epoch 407/750\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0284 - accuracy: 0.9864 - val_loss: 3.2210 - val_accuracy: 0.7204\n",
            "Epoch 408/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0237 - accuracy: 0.9918 - val_loss: 3.0339 - val_accuracy: 0.7312\n",
            "Epoch 409/750\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0334 - accuracy: 0.9837 - val_loss: 2.8895 - val_accuracy: 0.7527\n",
            "Epoch 410/750\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0210 - accuracy: 0.9946 - val_loss: 2.8183 - val_accuracy: 0.7634\n",
            "Epoch 411/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0327 - accuracy: 0.9810 - val_loss: 2.8116 - val_accuracy: 0.7527\n",
            "Epoch 412/750\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0296 - accuracy: 0.9783 - val_loss: 2.8361 - val_accuracy: 0.7527\n",
            "Epoch 413/750\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0288 - accuracy: 0.9864 - val_loss: 2.8530 - val_accuracy: 0.7527\n",
            "Epoch 414/750\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0272 - accuracy: 0.9755 - val_loss: 2.9130 - val_accuracy: 0.7312\n",
            "Epoch 415/750\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0254 - accuracy: 0.9864 - val_loss: 2.9266 - val_accuracy: 0.7312\n",
            "Epoch 416/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0272 - accuracy: 0.9755 - val_loss: 2.9320 - val_accuracy: 0.7312\n",
            "Epoch 417/750\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0261 - accuracy: 0.9810 - val_loss: 2.9855 - val_accuracy: 0.7312\n",
            "Epoch 418/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0229 - accuracy: 0.9918 - val_loss: 3.0499 - val_accuracy: 0.7312\n",
            "Epoch 419/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0242 - accuracy: 0.9864 - val_loss: 3.0431 - val_accuracy: 0.7312\n",
            "Epoch 420/750\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0311 - accuracy: 0.9810 - val_loss: 2.9818 - val_accuracy: 0.7312\n",
            "Epoch 421/750\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0322 - accuracy: 0.9864 - val_loss: 2.9360 - val_accuracy: 0.7312\n",
            "Epoch 422/750\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0236 - accuracy: 0.9837 - val_loss: 2.9609 - val_accuracy: 0.7312\n",
            "Epoch 423/750\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0295 - accuracy: 0.9810 - val_loss: 2.9469 - val_accuracy: 0.7312\n",
            "Epoch 424/750\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0284 - accuracy: 0.9864 - val_loss: 3.0241 - val_accuracy: 0.7312\n",
            "Epoch 425/750\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0295 - accuracy: 0.9864 - val_loss: 3.0207 - val_accuracy: 0.7204\n",
            "Epoch 426/750\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0289 - accuracy: 0.9837 - val_loss: 2.9747 - val_accuracy: 0.7204\n",
            "Epoch 427/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0240 - accuracy: 0.9891 - val_loss: 2.9827 - val_accuracy: 0.7204\n",
            "Epoch 428/750\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0308 - accuracy: 0.9864 - val_loss: 2.9801 - val_accuracy: 0.7204\n",
            "Epoch 429/750\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0233 - accuracy: 0.9864 - val_loss: 2.9362 - val_accuracy: 0.7204\n",
            "Epoch 430/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0282 - accuracy: 0.9837 - val_loss: 2.9199 - val_accuracy: 0.7204\n",
            "Epoch 431/750\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0272 - accuracy: 0.9837 - val_loss: 2.8841 - val_accuracy: 0.7204\n",
            "Epoch 432/750\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0316 - accuracy: 0.9810 - val_loss: 2.9034 - val_accuracy: 0.7312\n",
            "Epoch 433/750\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0309 - accuracy: 0.9837 - val_loss: 3.0637 - val_accuracy: 0.7204\n",
            "Epoch 434/750\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0267 - accuracy: 0.9864 - val_loss: 3.1563 - val_accuracy: 0.7204\n",
            "Epoch 435/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0308 - accuracy: 0.9783 - val_loss: 3.2199 - val_accuracy: 0.7204\n",
            "Epoch 436/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0262 - accuracy: 0.9837 - val_loss: 3.1336 - val_accuracy: 0.7312\n",
            "Epoch 437/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0263 - accuracy: 0.9864 - val_loss: 2.9272 - val_accuracy: 0.7312\n",
            "Epoch 438/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0246 - accuracy: 0.9864 - val_loss: 2.8299 - val_accuracy: 0.7419\n",
            "Epoch 439/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0270 - accuracy: 0.9837 - val_loss: 2.7941 - val_accuracy: 0.7634\n",
            "Epoch 440/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0302 - accuracy: 0.9864 - val_loss: 2.8923 - val_accuracy: 0.7312\n",
            "Epoch 441/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0277 - accuracy: 0.9810 - val_loss: 3.0106 - val_accuracy: 0.7312\n",
            "Epoch 442/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0265 - accuracy: 0.9810 - val_loss: 3.0801 - val_accuracy: 0.7312\n",
            "Epoch 443/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0259 - accuracy: 0.9864 - val_loss: 3.1516 - val_accuracy: 0.7312\n",
            "Epoch 444/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0255 - accuracy: 0.9891 - val_loss: 3.1228 - val_accuracy: 0.7312\n",
            "Epoch 445/750\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0272 - accuracy: 0.9864 - val_loss: 2.9765 - val_accuracy: 0.7312\n",
            "Epoch 446/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0252 - accuracy: 0.9837 - val_loss: 2.9149 - val_accuracy: 0.7312\n",
            "Epoch 447/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0262 - accuracy: 0.9810 - val_loss: 2.9233 - val_accuracy: 0.7312\n",
            "Epoch 448/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0233 - accuracy: 0.9864 - val_loss: 2.9433 - val_accuracy: 0.7312\n",
            "Epoch 449/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0292 - accuracy: 0.9837 - val_loss: 2.9985 - val_accuracy: 0.7312\n",
            "Epoch 450/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0309 - accuracy: 0.9810 - val_loss: 2.9889 - val_accuracy: 0.7312\n",
            "Epoch 451/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0265 - accuracy: 0.9918 - val_loss: 2.9313 - val_accuracy: 0.7312\n",
            "Epoch 452/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0262 - accuracy: 0.9864 - val_loss: 2.8674 - val_accuracy: 0.7312\n",
            "Epoch 453/750\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0232 - accuracy: 0.9918 - val_loss: 2.8561 - val_accuracy: 0.7312\n",
            "Epoch 454/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0289 - accuracy: 0.9837 - val_loss: 2.8637 - val_accuracy: 0.7312\n",
            "Epoch 455/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0277 - accuracy: 0.9891 - val_loss: 2.9007 - val_accuracy: 0.7312\n",
            "Epoch 456/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0249 - accuracy: 0.9837 - val_loss: 2.9161 - val_accuracy: 0.7312\n",
            "Epoch 457/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0261 - accuracy: 0.9864 - val_loss: 2.9350 - val_accuracy: 0.7312\n",
            "Epoch 458/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0245 - accuracy: 0.9864 - val_loss: 2.9300 - val_accuracy: 0.7312\n",
            "Epoch 459/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0291 - accuracy: 0.9837 - val_loss: 2.9166 - val_accuracy: 0.7312\n",
            "Epoch 460/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0273 - accuracy: 0.9864 - val_loss: 2.9287 - val_accuracy: 0.7312\n",
            "Epoch 461/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0273 - accuracy: 0.9864 - val_loss: 3.0038 - val_accuracy: 0.7312\n",
            "Epoch 462/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0323 - accuracy: 0.9837 - val_loss: 3.0972 - val_accuracy: 0.7312\n",
            "Epoch 463/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0258 - accuracy: 0.9864 - val_loss: 3.0448 - val_accuracy: 0.7312\n",
            "Epoch 464/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0269 - accuracy: 0.9810 - val_loss: 2.8247 - val_accuracy: 0.7419\n",
            "Epoch 465/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0259 - accuracy: 0.9891 - val_loss: 2.7723 - val_accuracy: 0.7312\n",
            "Epoch 466/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0253 - accuracy: 0.9891 - val_loss: 2.7465 - val_accuracy: 0.7312\n",
            "Epoch 467/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0299 - accuracy: 0.9810 - val_loss: 2.7750 - val_accuracy: 0.7312\n",
            "Epoch 468/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0235 - accuracy: 0.9891 - val_loss: 2.8413 - val_accuracy: 0.7312\n",
            "Epoch 469/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0281 - accuracy: 0.9783 - val_loss: 2.8535 - val_accuracy: 0.7204\n",
            "Epoch 470/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0239 - accuracy: 0.9891 - val_loss: 2.8309 - val_accuracy: 0.7312\n",
            "Epoch 471/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0252 - accuracy: 0.9810 - val_loss: 2.8271 - val_accuracy: 0.7312\n",
            "Epoch 472/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0289 - accuracy: 0.9864 - val_loss: 2.8647 - val_accuracy: 0.7312\n",
            "Epoch 473/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0267 - accuracy: 0.9810 - val_loss: 2.8444 - val_accuracy: 0.7312\n",
            "Epoch 474/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0253 - accuracy: 0.9891 - val_loss: 2.8133 - val_accuracy: 0.7419\n",
            "Epoch 475/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0269 - accuracy: 0.9783 - val_loss: 2.7868 - val_accuracy: 0.7634\n",
            "Epoch 476/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0316 - accuracy: 0.9810 - val_loss: 2.8085 - val_accuracy: 0.7634\n",
            "Epoch 477/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0293 - accuracy: 0.9810 - val_loss: 2.8528 - val_accuracy: 0.7634\n",
            "Epoch 478/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0235 - accuracy: 0.9864 - val_loss: 2.9742 - val_accuracy: 0.7312\n",
            "Epoch 479/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0265 - accuracy: 0.9837 - val_loss: 3.0743 - val_accuracy: 0.7312\n",
            "Epoch 480/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0242 - accuracy: 0.9837 - val_loss: 3.1937 - val_accuracy: 0.7419\n",
            "Epoch 481/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0234 - accuracy: 0.9891 - val_loss: 3.2993 - val_accuracy: 0.7312\n",
            "Epoch 482/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0289 - accuracy: 0.9837 - val_loss: 3.3597 - val_accuracy: 0.7312\n",
            "Epoch 483/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0288 - accuracy: 0.9864 - val_loss: 3.3083 - val_accuracy: 0.7204\n",
            "Epoch 484/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0251 - accuracy: 0.9864 - val_loss: 3.2498 - val_accuracy: 0.7204\n",
            "Epoch 485/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0256 - accuracy: 0.9810 - val_loss: 3.1521 - val_accuracy: 0.7204\n",
            "Epoch 486/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0234 - accuracy: 0.9864 - val_loss: 3.1588 - val_accuracy: 0.7204\n",
            "Epoch 487/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0281 - accuracy: 0.9810 - val_loss: 3.2690 - val_accuracy: 0.7204\n",
            "Epoch 488/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0243 - accuracy: 0.9891 - val_loss: 3.3176 - val_accuracy: 0.7204\n",
            "Epoch 489/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0281 - accuracy: 0.9810 - val_loss: 3.3476 - val_accuracy: 0.7097\n",
            "Epoch 490/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0320 - accuracy: 0.9837 - val_loss: 3.3985 - val_accuracy: 0.7097\n",
            "Epoch 491/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0277 - accuracy: 0.9864 - val_loss: 3.3131 - val_accuracy: 0.7097\n",
            "Epoch 492/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0259 - accuracy: 0.9891 - val_loss: 3.2306 - val_accuracy: 0.7204\n",
            "Epoch 493/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0239 - accuracy: 0.9891 - val_loss: 3.1774 - val_accuracy: 0.7204\n",
            "Epoch 494/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0242 - accuracy: 0.9864 - val_loss: 3.1902 - val_accuracy: 0.7204\n",
            "Epoch 495/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0286 - accuracy: 0.9864 - val_loss: 3.2036 - val_accuracy: 0.7204\n",
            "Epoch 496/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0289 - accuracy: 0.9837 - val_loss: 3.2527 - val_accuracy: 0.7204\n",
            "Epoch 497/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0262 - accuracy: 0.9891 - val_loss: 3.2495 - val_accuracy: 0.7204\n",
            "Epoch 498/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0278 - accuracy: 0.9864 - val_loss: 3.1509 - val_accuracy: 0.7312\n",
            "Epoch 499/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0249 - accuracy: 0.9810 - val_loss: 3.1154 - val_accuracy: 0.7527\n",
            "Epoch 500/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0295 - accuracy: 0.9864 - val_loss: 3.1555 - val_accuracy: 0.7312\n",
            "Epoch 501/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0286 - accuracy: 0.9783 - val_loss: 3.1797 - val_accuracy: 0.7312\n",
            "Epoch 502/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0195 - accuracy: 0.9891 - val_loss: 3.2273 - val_accuracy: 0.7204\n",
            "Epoch 503/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0292 - accuracy: 0.9837 - val_loss: 3.2899 - val_accuracy: 0.7204\n",
            "Epoch 504/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0250 - accuracy: 0.9891 - val_loss: 3.3306 - val_accuracy: 0.7204\n",
            "Epoch 505/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0238 - accuracy: 0.9864 - val_loss: 3.2004 - val_accuracy: 0.7204\n",
            "Epoch 506/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0277 - accuracy: 0.9810 - val_loss: 3.1011 - val_accuracy: 0.7204\n",
            "Epoch 507/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0243 - accuracy: 0.9810 - val_loss: 2.9953 - val_accuracy: 0.7312\n",
            "Epoch 508/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0221 - accuracy: 0.9918 - val_loss: 2.9506 - val_accuracy: 0.7312\n",
            "Epoch 509/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0229 - accuracy: 0.9918 - val_loss: 2.9435 - val_accuracy: 0.7312\n",
            "Epoch 510/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0264 - accuracy: 0.9837 - val_loss: 2.9849 - val_accuracy: 0.7312\n",
            "Epoch 511/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0254 - accuracy: 0.9864 - val_loss: 3.0196 - val_accuracy: 0.7312\n",
            "Epoch 512/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0250 - accuracy: 0.9918 - val_loss: 3.1042 - val_accuracy: 0.7312\n",
            "Epoch 513/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0261 - accuracy: 0.9864 - val_loss: 3.1757 - val_accuracy: 0.7312\n",
            "Epoch 514/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0235 - accuracy: 0.9864 - val_loss: 3.1038 - val_accuracy: 0.7312\n",
            "Epoch 515/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0278 - accuracy: 0.9864 - val_loss: 3.0653 - val_accuracy: 0.7312\n",
            "Epoch 516/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0232 - accuracy: 0.9891 - val_loss: 3.0971 - val_accuracy: 0.7312\n",
            "Epoch 517/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0253 - accuracy: 0.9864 - val_loss: 3.1005 - val_accuracy: 0.7312\n",
            "Epoch 518/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0276 - accuracy: 0.9837 - val_loss: 3.0701 - val_accuracy: 0.7312\n",
            "Epoch 519/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0272 - accuracy: 0.9837 - val_loss: 3.0458 - val_accuracy: 0.7312\n",
            "Epoch 520/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0260 - accuracy: 0.9864 - val_loss: 3.0664 - val_accuracy: 0.7312\n",
            "Epoch 521/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0213 - accuracy: 0.9864 - val_loss: 3.0973 - val_accuracy: 0.7312\n",
            "Epoch 522/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0225 - accuracy: 0.9864 - val_loss: 3.1472 - val_accuracy: 0.7312\n",
            "Epoch 523/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0295 - accuracy: 0.9837 - val_loss: 3.2025 - val_accuracy: 0.7312\n",
            "Epoch 524/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0224 - accuracy: 0.9891 - val_loss: 3.2727 - val_accuracy: 0.7097\n",
            "Epoch 525/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0307 - accuracy: 0.9864 - val_loss: 3.3029 - val_accuracy: 0.7204\n",
            "Epoch 526/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0248 - accuracy: 0.9864 - val_loss: 3.5680 - val_accuracy: 0.7312\n",
            "Epoch 527/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0316 - accuracy: 0.9810 - val_loss: 3.6977 - val_accuracy: 0.7419\n",
            "Epoch 528/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0285 - accuracy: 0.9810 - val_loss: 3.5169 - val_accuracy: 0.7527\n",
            "Epoch 529/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0380 - accuracy: 0.9783 - val_loss: 2.9732 - val_accuracy: 0.7527\n",
            "Epoch 530/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.5045 - accuracy: 0.9103 - val_loss: 1.5514 - val_accuracy: 0.5484\n",
            "Epoch 531/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4503 - accuracy: 0.7527 - val_loss: 2.5084 - val_accuracy: 0.5269\n",
            "Epoch 532/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.0347 - accuracy: 0.7418 - val_loss: 1.0091 - val_accuracy: 0.6022\n",
            "Epoch 533/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6693 - accuracy: 0.7065 - val_loss: 0.8580 - val_accuracy: 0.5484\n",
            "Epoch 534/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6050 - accuracy: 0.6984 - val_loss: 0.8102 - val_accuracy: 0.6022\n",
            "Epoch 535/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.5350 - accuracy: 0.7554 - val_loss: 1.1543 - val_accuracy: 0.5699\n",
            "Epoch 536/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.5523 - accuracy: 0.7799 - val_loss: 0.8163 - val_accuracy: 0.6452\n",
            "Epoch 537/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4092 - accuracy: 0.8342 - val_loss: 0.8075 - val_accuracy: 0.6882\n",
            "Epoch 538/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3768 - accuracy: 0.8777 - val_loss: 0.9764 - val_accuracy: 0.7097\n",
            "Epoch 539/750\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.3259 - accuracy: 0.8859 - val_loss: 0.8459 - val_accuracy: 0.6989\n",
            "Epoch 540/750\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.2769 - accuracy: 0.8995 - val_loss: 1.4157 - val_accuracy: 0.6559\n",
            "Epoch 541/750\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.3331 - accuracy: 0.8832 - val_loss: 1.2124 - val_accuracy: 0.7419\n",
            "Epoch 542/750\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.2246 - accuracy: 0.9429 - val_loss: 0.9588 - val_accuracy: 0.7312\n",
            "Epoch 543/750\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1580 - accuracy: 0.9620 - val_loss: 0.9809 - val_accuracy: 0.7312\n",
            "Epoch 544/750\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.1738 - accuracy: 0.9375 - val_loss: 0.8981 - val_accuracy: 0.7312\n",
            "Epoch 545/750\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.1797 - accuracy: 0.9511 - val_loss: 1.1341 - val_accuracy: 0.7312\n",
            "Epoch 546/750\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.1112 - accuracy: 0.9701 - val_loss: 1.1265 - val_accuracy: 0.7527\n",
            "Epoch 547/750\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.1082 - accuracy: 0.9674 - val_loss: 1.3224 - val_accuracy: 0.7204\n",
            "Epoch 548/750\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0921 - accuracy: 0.9701 - val_loss: 1.3339 - val_accuracy: 0.7312\n",
            "Epoch 549/750\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0707 - accuracy: 0.9728 - val_loss: 1.3834 - val_accuracy: 0.7097\n",
            "Epoch 550/750\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0575 - accuracy: 0.9810 - val_loss: 1.5019 - val_accuracy: 0.7204\n",
            "Epoch 551/750\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0598 - accuracy: 0.9891 - val_loss: 1.5854 - val_accuracy: 0.7419\n",
            "Epoch 552/750\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0512 - accuracy: 0.9783 - val_loss: 1.7363 - val_accuracy: 0.7312\n",
            "Epoch 553/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0612 - accuracy: 0.9783 - val_loss: 1.8263 - val_accuracy: 0.7097\n",
            "Epoch 554/750\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0459 - accuracy: 0.9810 - val_loss: 1.8295 - val_accuracy: 0.6989\n",
            "Epoch 555/750\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0570 - accuracy: 0.9728 - val_loss: 1.8276 - val_accuracy: 0.7204\n",
            "Epoch 556/750\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0389 - accuracy: 0.9837 - val_loss: 1.8898 - val_accuracy: 0.7527\n",
            "Epoch 557/750\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0645 - accuracy: 0.9864 - val_loss: 1.9596 - val_accuracy: 0.7312\n",
            "Epoch 558/750\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.0711 - accuracy: 0.9755 - val_loss: 1.8419 - val_accuracy: 0.7312\n",
            "Epoch 559/750\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0436 - accuracy: 0.9755 - val_loss: 1.6703 - val_accuracy: 0.7527\n",
            "Epoch 560/750\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0391 - accuracy: 0.9783 - val_loss: 1.7050 - val_accuracy: 0.7312\n",
            "Epoch 561/750\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0347 - accuracy: 0.9891 - val_loss: 1.8013 - val_accuracy: 0.7097\n",
            "Epoch 562/750\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0423 - accuracy: 0.9864 - val_loss: 1.8689 - val_accuracy: 0.7312\n",
            "Epoch 563/750\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0439 - accuracy: 0.9810 - val_loss: 1.9431 - val_accuracy: 0.7204\n",
            "Epoch 564/750\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0453 - accuracy: 0.9810 - val_loss: 2.1229 - val_accuracy: 0.7204\n",
            "Epoch 565/750\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0409 - accuracy: 0.9837 - val_loss: 2.1810 - val_accuracy: 0.7204\n",
            "Epoch 566/750\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0390 - accuracy: 0.9810 - val_loss: 2.1399 - val_accuracy: 0.7312\n",
            "Epoch 567/750\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0378 - accuracy: 0.9864 - val_loss: 2.1933 - val_accuracy: 0.7419\n",
            "Epoch 568/750\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0420 - accuracy: 0.9837 - val_loss: 2.2298 - val_accuracy: 0.7097\n",
            "Epoch 569/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0292 - accuracy: 0.9891 - val_loss: 2.2648 - val_accuracy: 0.6989\n",
            "Epoch 570/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0324 - accuracy: 0.9864 - val_loss: 2.2700 - val_accuracy: 0.7097\n",
            "Epoch 571/750\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0328 - accuracy: 0.9837 - val_loss: 2.1700 - val_accuracy: 0.7097\n",
            "Epoch 572/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0279 - accuracy: 0.9810 - val_loss: 2.1389 - val_accuracy: 0.6989\n",
            "Epoch 573/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0404 - accuracy: 0.9783 - val_loss: 2.0802 - val_accuracy: 0.7204\n",
            "Epoch 574/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0266 - accuracy: 0.9864 - val_loss: 2.0614 - val_accuracy: 0.7097\n",
            "Epoch 575/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0325 - accuracy: 0.9837 - val_loss: 2.1007 - val_accuracy: 0.7204\n",
            "Epoch 576/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0288 - accuracy: 0.9837 - val_loss: 2.1504 - val_accuracy: 0.7204\n",
            "Epoch 577/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0393 - accuracy: 0.9810 - val_loss: 2.1231 - val_accuracy: 0.7097\n",
            "Epoch 578/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0295 - accuracy: 0.9810 - val_loss: 2.0846 - val_accuracy: 0.7204\n",
            "Epoch 579/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0357 - accuracy: 0.9783 - val_loss: 2.0405 - val_accuracy: 0.7419\n",
            "Epoch 580/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0521 - accuracy: 0.9728 - val_loss: 2.0205 - val_accuracy: 0.7312\n",
            "Epoch 581/750\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0307 - accuracy: 0.9837 - val_loss: 2.2224 - val_accuracy: 0.7204\n",
            "Epoch 582/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0439 - accuracy: 0.9810 - val_loss: 2.1414 - val_accuracy: 0.7097\n",
            "Epoch 583/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0295 - accuracy: 0.9837 - val_loss: 2.4378 - val_accuracy: 0.7204\n",
            "Epoch 584/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0398 - accuracy: 0.9837 - val_loss: 2.4567 - val_accuracy: 0.7312\n",
            "Epoch 585/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0393 - accuracy: 0.9783 - val_loss: 2.1954 - val_accuracy: 0.7419\n",
            "Epoch 586/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0268 - accuracy: 0.9810 - val_loss: 2.3131 - val_accuracy: 0.7204\n",
            "Epoch 587/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0286 - accuracy: 0.9946 - val_loss: 2.3740 - val_accuracy: 0.7097\n",
            "Epoch 588/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0299 - accuracy: 0.9891 - val_loss: 2.3369 - val_accuracy: 0.7097\n",
            "Epoch 589/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0379 - accuracy: 0.9755 - val_loss: 2.4126 - val_accuracy: 0.7204\n",
            "Epoch 590/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0248 - accuracy: 0.9918 - val_loss: 2.4756 - val_accuracy: 0.7204\n",
            "Epoch 591/750\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.0280 - accuracy: 0.9891 - val_loss: 2.5181 - val_accuracy: 0.7204\n",
            "Epoch 592/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0284 - accuracy: 0.9891 - val_loss: 2.5608 - val_accuracy: 0.7204\n",
            "Epoch 593/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0285 - accuracy: 0.9783 - val_loss: 2.5758 - val_accuracy: 0.7204\n",
            "Epoch 594/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0250 - accuracy: 0.9837 - val_loss: 2.6104 - val_accuracy: 0.7204\n",
            "Epoch 595/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0248 - accuracy: 0.9864 - val_loss: 2.6704 - val_accuracy: 0.7204\n",
            "Epoch 596/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0302 - accuracy: 0.9783 - val_loss: 2.7104 - val_accuracy: 0.7419\n",
            "Epoch 597/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0345 - accuracy: 0.9891 - val_loss: 2.6407 - val_accuracy: 0.7204\n",
            "Epoch 598/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0268 - accuracy: 0.9837 - val_loss: 2.6267 - val_accuracy: 0.7204\n",
            "Epoch 599/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0381 - accuracy: 0.9783 - val_loss: 2.5923 - val_accuracy: 0.7204\n",
            "Epoch 600/750\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.0276 - accuracy: 0.9864 - val_loss: 2.5456 - val_accuracy: 0.7312\n",
            "Epoch 601/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0249 - accuracy: 0.9891 - val_loss: 2.5523 - val_accuracy: 0.7204\n",
            "Epoch 602/750\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0244 - accuracy: 0.9891 - val_loss: 2.6116 - val_accuracy: 0.7312\n",
            "Epoch 603/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0297 - accuracy: 0.9810 - val_loss: 2.6442 - val_accuracy: 0.7312\n",
            "Epoch 604/750\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0300 - accuracy: 0.9783 - val_loss: 2.6384 - val_accuracy: 0.7312\n",
            "Epoch 605/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0254 - accuracy: 0.9946 - val_loss: 2.5974 - val_accuracy: 0.7097\n",
            "Epoch 606/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0259 - accuracy: 0.9864 - val_loss: 2.5945 - val_accuracy: 0.6989\n",
            "Epoch 607/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0273 - accuracy: 0.9837 - val_loss: 2.5913 - val_accuracy: 0.7097\n",
            "Epoch 608/750\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0277 - accuracy: 0.9837 - val_loss: 2.5925 - val_accuracy: 0.7204\n",
            "Epoch 609/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0270 - accuracy: 0.9918 - val_loss: 2.5983 - val_accuracy: 0.7312\n",
            "Epoch 610/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0299 - accuracy: 0.9837 - val_loss: 2.6252 - val_accuracy: 0.7312\n",
            "Epoch 611/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0250 - accuracy: 0.9837 - val_loss: 2.6367 - val_accuracy: 0.7312\n",
            "Epoch 612/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0283 - accuracy: 0.9810 - val_loss: 2.6477 - val_accuracy: 0.7312\n",
            "Epoch 613/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0287 - accuracy: 0.9810 - val_loss: 2.6593 - val_accuracy: 0.7312\n",
            "Epoch 614/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0290 - accuracy: 0.9783 - val_loss: 2.6786 - val_accuracy: 0.7312\n",
            "Epoch 615/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0293 - accuracy: 0.9918 - val_loss: 2.6878 - val_accuracy: 0.7312\n",
            "Epoch 616/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0263 - accuracy: 0.9864 - val_loss: 2.7057 - val_accuracy: 0.7312\n",
            "Epoch 617/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0264 - accuracy: 0.9810 - val_loss: 2.7412 - val_accuracy: 0.7312\n",
            "Epoch 618/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0263 - accuracy: 0.9918 - val_loss: 2.7418 - val_accuracy: 0.7419\n",
            "Epoch 619/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0269 - accuracy: 0.9783 - val_loss: 2.7497 - val_accuracy: 0.7312\n",
            "Epoch 620/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0285 - accuracy: 0.9810 - val_loss: 2.7578 - val_accuracy: 0.7312\n",
            "Epoch 621/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0228 - accuracy: 0.9891 - val_loss: 2.7121 - val_accuracy: 0.7204\n",
            "Epoch 622/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0221 - accuracy: 0.9891 - val_loss: 2.6522 - val_accuracy: 0.7204\n",
            "Epoch 623/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0239 - accuracy: 0.9891 - val_loss: 2.6380 - val_accuracy: 0.7204\n",
            "Epoch 624/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0253 - accuracy: 0.9864 - val_loss: 2.6264 - val_accuracy: 0.7204\n",
            "Epoch 625/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0324 - accuracy: 0.9864 - val_loss: 2.6188 - val_accuracy: 0.7527\n",
            "Epoch 626/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0259 - accuracy: 0.9864 - val_loss: 2.6700 - val_accuracy: 0.7527\n",
            "Epoch 627/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0273 - accuracy: 0.9864 - val_loss: 2.7069 - val_accuracy: 0.7527\n",
            "Epoch 628/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0243 - accuracy: 0.9918 - val_loss: 2.6418 - val_accuracy: 0.7312\n",
            "Epoch 629/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0278 - accuracy: 0.9810 - val_loss: 2.6140 - val_accuracy: 0.7312\n",
            "Epoch 630/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0301 - accuracy: 0.9810 - val_loss: 2.6154 - val_accuracy: 0.7312\n",
            "Epoch 631/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0256 - accuracy: 0.9891 - val_loss: 2.6218 - val_accuracy: 0.7312\n",
            "Epoch 632/750\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0245 - accuracy: 0.9891 - val_loss: 2.6285 - val_accuracy: 0.7312\n",
            "Epoch 633/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0280 - accuracy: 0.9837 - val_loss: 2.6365 - val_accuracy: 0.7312\n",
            "Epoch 634/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0251 - accuracy: 0.9864 - val_loss: 2.6534 - val_accuracy: 0.7419\n",
            "Epoch 635/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0278 - accuracy: 0.9837 - val_loss: 2.6731 - val_accuracy: 0.7312\n",
            "Epoch 636/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0233 - accuracy: 0.9918 - val_loss: 2.7059 - val_accuracy: 0.7527\n",
            "Epoch 637/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0348 - accuracy: 0.9837 - val_loss: 2.7002 - val_accuracy: 0.7527\n",
            "Epoch 638/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0355 - accuracy: 0.9810 - val_loss: 2.6694 - val_accuracy: 0.7527\n",
            "Epoch 639/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0304 - accuracy: 0.9837 - val_loss: 2.6782 - val_accuracy: 0.7312\n",
            "Epoch 640/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0260 - accuracy: 0.9810 - val_loss: 2.6667 - val_accuracy: 0.7312\n",
            "Epoch 641/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0209 - accuracy: 0.9946 - val_loss: 2.6712 - val_accuracy: 0.7204\n",
            "Epoch 642/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0250 - accuracy: 0.9864 - val_loss: 2.6870 - val_accuracy: 0.7312\n",
            "Epoch 643/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0300 - accuracy: 0.9755 - val_loss: 2.6982 - val_accuracy: 0.7312\n",
            "Epoch 644/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0334 - accuracy: 0.9783 - val_loss: 2.7173 - val_accuracy: 0.7312\n",
            "Epoch 645/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0216 - accuracy: 0.9918 - val_loss: 2.7196 - val_accuracy: 0.7312\n",
            "Epoch 646/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0256 - accuracy: 0.9864 - val_loss: 2.7360 - val_accuracy: 0.7312\n",
            "Epoch 647/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0247 - accuracy: 0.9918 - val_loss: 2.7585 - val_accuracy: 0.7312\n",
            "Epoch 648/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0308 - accuracy: 0.9864 - val_loss: 2.7810 - val_accuracy: 0.7419\n",
            "Epoch 649/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0270 - accuracy: 0.9810 - val_loss: 2.8105 - val_accuracy: 0.7312\n",
            "Epoch 650/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0266 - accuracy: 0.9864 - val_loss: 2.8359 - val_accuracy: 0.7312\n",
            "Epoch 651/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0249 - accuracy: 0.9918 - val_loss: 2.8411 - val_accuracy: 0.7312\n",
            "Epoch 652/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0320 - accuracy: 0.9810 - val_loss: 2.8462 - val_accuracy: 0.7312\n",
            "Epoch 653/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0242 - accuracy: 0.9864 - val_loss: 2.8415 - val_accuracy: 0.7312\n",
            "Epoch 654/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0253 - accuracy: 0.9837 - val_loss: 2.8553 - val_accuracy: 0.7312\n",
            "Epoch 655/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0241 - accuracy: 0.9864 - val_loss: 2.8590 - val_accuracy: 0.7312\n",
            "Epoch 656/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0300 - accuracy: 0.9864 - val_loss: 2.8571 - val_accuracy: 0.7312\n",
            "Epoch 657/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0299 - accuracy: 0.9837 - val_loss: 2.8565 - val_accuracy: 0.7312\n",
            "Epoch 658/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0235 - accuracy: 0.9918 - val_loss: 2.8503 - val_accuracy: 0.7312\n",
            "Epoch 659/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0213 - accuracy: 0.9891 - val_loss: 2.8522 - val_accuracy: 0.7419\n",
            "Epoch 660/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0259 - accuracy: 0.9864 - val_loss: 2.8498 - val_accuracy: 0.7419\n",
            "Epoch 661/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0284 - accuracy: 0.9810 - val_loss: 2.8521 - val_accuracy: 0.7419\n",
            "Epoch 662/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0177 - accuracy: 0.9918 - val_loss: 2.8598 - val_accuracy: 0.7419\n",
            "Epoch 663/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0291 - accuracy: 0.9783 - val_loss: 2.8719 - val_accuracy: 0.7419\n",
            "Epoch 664/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0306 - accuracy: 0.9837 - val_loss: 2.8564 - val_accuracy: 0.7204\n",
            "Epoch 665/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0227 - accuracy: 0.9891 - val_loss: 2.8947 - val_accuracy: 0.7312\n",
            "Epoch 666/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0256 - accuracy: 0.9837 - val_loss: 2.9329 - val_accuracy: 0.7204\n",
            "Epoch 667/750\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0293 - accuracy: 0.9837 - val_loss: 2.9136 - val_accuracy: 0.7312\n",
            "Epoch 668/750\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0240 - accuracy: 0.9864 - val_loss: 2.8811 - val_accuracy: 0.7419\n",
            "Epoch 669/750\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0303 - accuracy: 0.9864 - val_loss: 2.9012 - val_accuracy: 0.7312\n",
            "Epoch 670/750\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0260 - accuracy: 0.9810 - val_loss: 2.9238 - val_accuracy: 0.7312\n",
            "Epoch 671/750\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0273 - accuracy: 0.9837 - val_loss: 2.9414 - val_accuracy: 0.7312\n",
            "Epoch 672/750\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0223 - accuracy: 0.9864 - val_loss: 2.9704 - val_accuracy: 0.7312\n",
            "Epoch 673/750\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0365 - accuracy: 0.9864 - val_loss: 2.9419 - val_accuracy: 0.7097\n",
            "Epoch 674/750\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0287 - accuracy: 0.9837 - val_loss: 2.9269 - val_accuracy: 0.7204\n",
            "Epoch 675/750\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0290 - accuracy: 0.9864 - val_loss: 2.9420 - val_accuracy: 0.7097\n",
            "Epoch 676/750\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0256 - accuracy: 0.9891 - val_loss: 2.9594 - val_accuracy: 0.7097\n",
            "Epoch 677/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0274 - accuracy: 0.9810 - val_loss: 2.9626 - val_accuracy: 0.7097\n",
            "Epoch 678/750\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0243 - accuracy: 0.9810 - val_loss: 2.9563 - val_accuracy: 0.7097\n",
            "Epoch 679/750\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0244 - accuracy: 0.9864 - val_loss: 2.9442 - val_accuracy: 0.7204\n",
            "Epoch 680/750\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0244 - accuracy: 0.9864 - val_loss: 2.9431 - val_accuracy: 0.7204\n",
            "Epoch 681/750\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.0285 - accuracy: 0.9810 - val_loss: 2.9346 - val_accuracy: 0.7204\n",
            "Epoch 682/750\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0192 - accuracy: 0.9946 - val_loss: 2.9375 - val_accuracy: 0.7312\n",
            "Epoch 683/750\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0279 - accuracy: 0.9810 - val_loss: 2.9570 - val_accuracy: 0.7419\n",
            "Epoch 684/750\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0236 - accuracy: 0.9891 - val_loss: 2.9793 - val_accuracy: 0.7312\n",
            "Epoch 685/750\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0304 - accuracy: 0.9864 - val_loss: 3.0086 - val_accuracy: 0.7312\n",
            "Epoch 686/750\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0241 - accuracy: 0.9837 - val_loss: 3.0429 - val_accuracy: 0.7204\n",
            "Epoch 687/750\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0310 - accuracy: 0.9864 - val_loss: 2.8872 - val_accuracy: 0.7312\n",
            "Epoch 688/750\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0298 - accuracy: 0.9864 - val_loss: 2.7853 - val_accuracy: 0.7312\n",
            "Epoch 689/750\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0274 - accuracy: 0.9837 - val_loss: 2.7427 - val_accuracy: 0.7312\n",
            "Epoch 690/750\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0293 - accuracy: 0.9891 - val_loss: 2.7346 - val_accuracy: 0.7312\n",
            "Epoch 691/750\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0308 - accuracy: 0.9891 - val_loss: 2.7314 - val_accuracy: 0.7312\n",
            "Epoch 692/750\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0297 - accuracy: 0.9783 - val_loss: 2.7018 - val_accuracy: 0.7312\n",
            "Epoch 693/750\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0313 - accuracy: 0.9783 - val_loss: 2.6918 - val_accuracy: 0.7312\n",
            "Epoch 694/750\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0271 - accuracy: 0.9810 - val_loss: 2.7089 - val_accuracy: 0.7312\n",
            "Epoch 695/750\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0272 - accuracy: 0.9837 - val_loss: 2.7246 - val_accuracy: 0.7204\n",
            "Epoch 696/750\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0321 - accuracy: 0.9810 - val_loss: 2.7247 - val_accuracy: 0.7097\n",
            "Epoch 697/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0294 - accuracy: 0.9837 - val_loss: 2.7237 - val_accuracy: 0.7097\n",
            "Epoch 698/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0252 - accuracy: 0.9918 - val_loss: 2.7372 - val_accuracy: 0.7097\n",
            "Epoch 699/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0271 - accuracy: 0.9783 - val_loss: 2.7573 - val_accuracy: 0.6989\n",
            "Epoch 700/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0268 - accuracy: 0.9837 - val_loss: 2.7753 - val_accuracy: 0.6989\n",
            "Epoch 701/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0272 - accuracy: 0.9891 - val_loss: 2.7845 - val_accuracy: 0.7097\n",
            "Epoch 702/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0262 - accuracy: 0.9864 - val_loss: 2.8001 - val_accuracy: 0.7097\n",
            "Epoch 703/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0254 - accuracy: 0.9810 - val_loss: 2.8221 - val_accuracy: 0.7097\n",
            "Epoch 704/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0226 - accuracy: 0.9891 - val_loss: 2.8383 - val_accuracy: 0.7204\n",
            "Epoch 705/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0347 - accuracy: 0.9810 - val_loss: 2.8840 - val_accuracy: 0.7097\n",
            "Epoch 706/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0265 - accuracy: 0.9891 - val_loss: 2.9200 - val_accuracy: 0.6989\n",
            "Epoch 707/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0276 - accuracy: 0.9837 - val_loss: 2.9378 - val_accuracy: 0.7097\n",
            "Epoch 708/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0225 - accuracy: 0.9891 - val_loss: 2.9649 - val_accuracy: 0.7204\n",
            "Epoch 709/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0261 - accuracy: 0.9864 - val_loss: 2.9767 - val_accuracy: 0.7204\n",
            "Epoch 710/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0238 - accuracy: 0.9891 - val_loss: 2.9877 - val_accuracy: 0.7204\n",
            "Epoch 711/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0244 - accuracy: 0.9891 - val_loss: 3.0093 - val_accuracy: 0.7204\n",
            "Epoch 712/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0289 - accuracy: 0.9864 - val_loss: 3.0232 - val_accuracy: 0.7204\n",
            "Epoch 713/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0244 - accuracy: 0.9864 - val_loss: 3.0340 - val_accuracy: 0.7204\n",
            "Epoch 714/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0258 - accuracy: 0.9810 - val_loss: 3.0552 - val_accuracy: 0.7097\n",
            "Epoch 715/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0232 - accuracy: 0.9891 - val_loss: 3.0442 - val_accuracy: 0.7204\n",
            "Epoch 716/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0264 - accuracy: 0.9864 - val_loss: 3.0432 - val_accuracy: 0.7204\n",
            "Epoch 717/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0307 - accuracy: 0.9864 - val_loss: 3.0294 - val_accuracy: 0.7204\n",
            "Epoch 718/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0251 - accuracy: 0.9891 - val_loss: 3.0273 - val_accuracy: 0.7097\n",
            "Epoch 719/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0236 - accuracy: 0.9891 - val_loss: 3.0388 - val_accuracy: 0.7097\n",
            "Epoch 720/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0257 - accuracy: 0.9837 - val_loss: 3.0212 - val_accuracy: 0.7097\n",
            "Epoch 721/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0242 - accuracy: 0.9864 - val_loss: 3.0211 - val_accuracy: 0.7097\n",
            "Epoch 722/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0279 - accuracy: 0.9810 - val_loss: 3.0190 - val_accuracy: 0.7097\n",
            "Epoch 723/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0205 - accuracy: 0.9891 - val_loss: 3.0227 - val_accuracy: 0.7097\n",
            "Epoch 724/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0317 - accuracy: 0.9783 - val_loss: 3.0288 - val_accuracy: 0.7204\n",
            "Epoch 725/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0263 - accuracy: 0.9837 - val_loss: 3.0478 - val_accuracy: 0.7097\n",
            "Epoch 726/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0256 - accuracy: 0.9864 - val_loss: 3.0611 - val_accuracy: 0.7204\n",
            "Epoch 727/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0250 - accuracy: 0.9918 - val_loss: 3.0824 - val_accuracy: 0.7204\n",
            "Epoch 728/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0297 - accuracy: 0.9810 - val_loss: 3.1115 - val_accuracy: 0.7312\n",
            "Epoch 729/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0245 - accuracy: 0.9864 - val_loss: 3.1098 - val_accuracy: 0.7204\n",
            "Epoch 730/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0236 - accuracy: 0.9864 - val_loss: 3.1270 - val_accuracy: 0.7204\n",
            "Epoch 731/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0296 - accuracy: 0.9783 - val_loss: 3.1483 - val_accuracy: 0.7204\n",
            "Epoch 732/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0264 - accuracy: 0.9891 - val_loss: 3.1728 - val_accuracy: 0.7097\n",
            "Epoch 733/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0218 - accuracy: 0.9918 - val_loss: 3.1908 - val_accuracy: 0.7097\n",
            "Epoch 734/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0282 - accuracy: 0.9864 - val_loss: 3.2052 - val_accuracy: 0.7097\n",
            "Epoch 735/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0235 - accuracy: 0.9837 - val_loss: 3.1908 - val_accuracy: 0.7097\n",
            "Epoch 736/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0276 - accuracy: 0.9837 - val_loss: 3.2123 - val_accuracy: 0.7312\n",
            "Epoch 737/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0244 - accuracy: 0.9810 - val_loss: 3.2270 - val_accuracy: 0.7312\n",
            "Epoch 738/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0205 - accuracy: 0.9946 - val_loss: 3.2364 - val_accuracy: 0.7204\n",
            "Epoch 739/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0276 - accuracy: 0.9918 - val_loss: 3.2294 - val_accuracy: 0.7204\n",
            "Epoch 740/750\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0269 - accuracy: 0.9864 - val_loss: 3.2149 - val_accuracy: 0.7419\n",
            "Epoch 741/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0208 - accuracy: 0.9864 - val_loss: 3.1793 - val_accuracy: 0.7204\n",
            "Epoch 742/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0265 - accuracy: 0.9810 - val_loss: 3.1965 - val_accuracy: 0.7204\n",
            "Epoch 743/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0266 - accuracy: 0.9837 - val_loss: 3.2032 - val_accuracy: 0.7097\n",
            "Epoch 744/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0260 - accuracy: 0.9864 - val_loss: 3.1291 - val_accuracy: 0.7204\n",
            "Epoch 745/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0253 - accuracy: 0.9891 - val_loss: 3.1563 - val_accuracy: 0.7634\n",
            "Epoch 746/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0238 - accuracy: 0.9864 - val_loss: 3.2117 - val_accuracy: 0.7634\n",
            "Epoch 747/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0251 - accuracy: 0.9837 - val_loss: 3.1967 - val_accuracy: 0.7527\n",
            "Epoch 748/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0285 - accuracy: 0.9810 - val_loss: 3.0913 - val_accuracy: 0.7204\n",
            "Epoch 749/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0259 - accuracy: 0.9864 - val_loss: 3.0819 - val_accuracy: 0.7204\n",
            "Epoch 750/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0259 - accuracy: 0.9810 - val_loss: 3.1162 - val_accuracy: 0.7204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation with the RNN\n",
        "y_valid_CNN_RNN = CNN_RNN_model.predict(feature_valid)\n",
        "valid_y_CNN_RNN = y_valid_CNN_RNN.copy()\n",
        "for i in range(len(y_valid_CNN_RNN)):\n",
        "    j = np.where(y_valid_CNN_RNN[i] == np.amax(y_valid_CNN_RNN[i]))\n",
        "    valid_y_CNN_RNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN_RNN[i][j] = 1\n",
        "\n",
        "# print acc and report\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN_RNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN_RNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN_RNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHHgMxjqqJ45",
        "outputId": "2d4d5765-f155-41a0-bc5a-44c61aacf8e2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 3009 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7bdfaaae8160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 2s 53ms/step\n",
            "0.7204301075268817\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.71      0.71        17\n",
            "           1       0.73      0.86      0.79        43\n",
            "           2       0.72      0.55      0.62        33\n",
            "\n",
            "   micro avg       0.72      0.72      0.72        93\n",
            "   macro avg       0.72      0.70      0.70        93\n",
            "weighted avg       0.72      0.72      0.71        93\n",
            " samples avg       0.72      0.72      0.72        93\n",
            "\n",
            "auc score:  0.7748909790539859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 尝试下不用knowledgebase"
      ],
      "metadata": {
        "id": "jVcHHnXTsohD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN1"
      ],
      "metadata": {
        "id": "BLmVSvmnwuNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UudoQCT4qO64",
        "outputId": "09141379-2847-4a43-9288-55a97fa816c1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "410    Fund Summary\\n\\nFund/Class:\\nFidelity® Emergin...\n",
              "265    Ivy Core Equity Fund\\n\\nObjective\\n\\nTo seek t...\n",
              "57     Fund Summary\\n\\nInvestment Objective\\nThe fund...\n",
              "199    SUMMARY OF COLUMBIA VP – LARGE CAP GROWTH FUND...\n",
              "175    Janus Henderson International Opportunities Fu...\n",
              "                             ...                        \n",
              "106    Eaton Vance Global Small-Cap Fund\\n\\nIn connec...\n",
              "270    Ivy Global Growth Fund\\n\\nObjective\\n\\nTo seek...\n",
              "348    Franklin FTSE China ETF\\n\\nInvestment Goal\\n\\n...\n",
              "435    INVESTMENT OBJECTIVE\\nThe USAA Growth and Tax ...\n",
              "102    Fund Summary\\n\\nInvestment Objective\\n\\nThe Fu...\n",
              "Name: summary, Length: 368, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MVjJup3sxlU",
        "outputId": "0c0a336d-8180-48c1-f831-a5fa27652be8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 44., 168.,  35., ..., 434., 237.,  87.],\n",
              "       [378., 224.,  10., ..., 546.,  29., 109.],\n",
              "       [228.,   2.,  10., ...,   8.,   8.,   1.],\n",
              "       ...,\n",
              "       [  2.,  43., 425., ..., 141.,  60., 176.],\n",
              "       [101.,  83.,   9., ..., 236., 567.,  63.],\n",
              "       [101.,  83.,   9., ...,   2., 129.,   3.]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "# 加载预训练的BERT模型和分词器\n",
        "bert_model_name = 'distilbert-base-uncased'  # 可以根据需要选择其他预训练的BERT模型\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = TFBertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "# 定义函数将文本转换为BERT输入格式\n",
        "def convert_text_to_bert_input(text):\n",
        "    # 对文本进行分词\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # 截断或补齐tokens到指定长度\n",
        "    max_length = 512  # BERT的最大输入长度\n",
        "    if len(tokens) > max_length - 2:\n",
        "        tokens = tokens[:max_length - 2]\n",
        "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "\n",
        "    # 将tokens转换为对应的ID\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # 创建attention mask\n",
        "    attention_mask = [1] * len(token_ids)\n",
        "\n",
        "    # 补齐token_ids和attention_mask到指定长度\n",
        "    padding_length = max_length - len(token_ids)\n",
        "    token_ids += [0] * padding_length\n",
        "    attention_mask += [0] * padding_length\n",
        "\n",
        "    return token_ids, attention_mask\n",
        "\n",
        "# 将X_train中的每个文本转换为BERT输入格式\n",
        "bert_input = []\n",
        "for text in X_train:\n",
        "    token_ids, attention_mask = convert_text_to_bert_input(text)\n",
        "    bert_input.append((token_ids, attention_mask))\n",
        "\n",
        "# 将BERT输入转换为tensorflow Dataset\n",
        "bert_input_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: bert_input,\n",
        "    output_types=(tf.int32, tf.int32),\n",
        "    output_shapes=((None,), (None,))\n",
        ")\n",
        "\n",
        "# 使用BERT模型提取特征\n",
        "feature_train = []\n",
        "for token_ids, attention_mask in bert_input_dataset:\n",
        "    token_ids = tf.reshape(token_ids, (1, -1))\n",
        "    attention_mask = tf.reshape(attention_mask, (1, -1))\n",
        "    outputs = bert_model(token_ids, attention_mask=attention_mask)\n",
        "    pooled_output = outputs.pooler_output\n",
        "    feature_train.append(pooled_output)\n",
        "\n",
        "feature_train = tf.concat(feature_train, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "Yx3DV21qs94g",
        "outputId": "97c0a47f-4ac1-4dbf-bedf-fc01e1d31078"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-8241c2e3c7a7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 加载预训练的BERT模型和分词器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbert_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'distilbert-base-uncased'\u001b[0m  \u001b[0;31m# 可以根据需要选择其他预训练的BERT模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0;31m from .generic import (\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_torch_pytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_model_output_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_pytree.Context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1854\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_decomp/decompositions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_prims/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msym_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_get_default_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_prims\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_debug_prims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng_prims\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_rng_prims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m from torch._prims_common import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_prims/debug_prims.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_custom_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_store\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContentStoreReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_custom_op/impl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionSchema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOperatorName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSchemaKind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mListType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseTy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchgen/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2086\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mArguments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2087\u001b[0m     \u001b[0;31m# pre_self_positional is usually empty, but is notably non-empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2088\u001b[0m     \u001b[0;31m# for where.self, where the condition argument comes before the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/dataclasses.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m         return _process_class(cls, init, repr, eq, order, unsafe_hash,\n\u001b[0m\u001b[1;32m   1176\u001b[0m                               frozen, match_args, kw_only, slots)\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/dataclasses.py\u001b[0m in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Create a class doc-string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         cls.__doc__ = (cls.__name__ +\n\u001b[0;32m-> 1093\u001b[0;31m                        str(inspect.signature(cls)).replace(' -> None', ''))\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmatch_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3209\u001b[0m         \u001b[0mrender_kw_only_separator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m             \u001b[0mformatted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m             \u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m             formatted = '{}: {}'.format(formatted,\n\u001b[0;32m-> 2728\u001b[0;31m                                        formatannotation(self._annotation))\n\u001b[0m\u001b[1;32m   2729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mformatannotation\u001b[0;34m(annotation, base_module)\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveprefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'typing.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[\\w\\.]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenericAlias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_train"
      ],
      "metadata": {
        "id": "AlV3Yv-Jt6SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 处理X_valid\n",
        "bert_input_valid = []\n",
        "for text in X_valid:\n",
        "    token_ids, attention_mask = convert_text_to_bert_input(text)\n",
        "    bert_input_valid.append((token_ids, attention_mask))\n",
        "\n",
        "bert_input_valid_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: bert_input_valid,\n",
        "    output_types=(tf.int32, tf.int32),\n",
        "    output_shapes=((None,), (None,))\n",
        ")\n",
        "\n",
        "feature_valid = []\n",
        "for token_ids, attention_mask in bert_input_valid_dataset:\n",
        "    token_ids = tf.reshape(token_ids, (1, -1))\n",
        "    attention_mask = tf.reshape(attention_mask, (1, -1))\n",
        "    outputs = bert_model(token_ids, attention_mask=attention_mask)\n",
        "    pooled_output = outputs.pooler_output\n",
        "    feature_valid.append(pooled_output)\n",
        "\n",
        "feature_valid = tf.concat(feature_valid, axis=0)\n",
        "\n",
        "# 处理X_test\n",
        "bert_input_test = []\n",
        "for text in X_test:\n",
        "    token_ids, attention_mask = convert_text_to_bert_input(text)\n",
        "    bert_input_test.append((token_ids, attention_mask))\n",
        "\n",
        "bert_input_test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: bert_input_test,\n",
        "    output_types=(tf.int32, tf.int32),\n",
        "    output_shapes=((None,), (None,))\n",
        ")\n",
        "\n",
        "feature_test = []\n",
        "for token_ids, attention_mask in bert_input_test_dataset:\n",
        "    token_ids = tf.reshape(token_ids, (1, -1))\n",
        "    attention_mask = tf.reshape(attention_mask, (1, -1))\n",
        "    outputs = bert_model(token_ids, attention_mask=attention_mask)\n",
        "    pooled_output = outputs.pooler_output\n",
        "    feature_test.append(pooled_output)\n",
        "\n",
        "feature_test = tf.concat(feature_test, axis=0)"
      ],
      "metadata": {
        "id": "wliWXrrCt9gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Reshape\n",
        "\n",
        "def create_CNN_model(bert_output_dim):\n",
        "    CNN = Sequential()\n",
        "    CNN.add(Reshape((bert_output_dim, 1), input_shape=(bert_output_dim,)))\n",
        "    CNN.add(Convolution1D(64, 5, activation='relu'))\n",
        "    CNN.add(MaxPooling1D(pool_size=5))\n",
        "    CNN.add(Convolution1D(32, 5, activation='relu'))\n",
        "    CNN.add(MaxPooling1D(pool_size=5))\n",
        "    CNN.add(Flatten())\n",
        "    CNN.add(Dense(units=128, activation='relu'))\n",
        "    CNN.add(Dropout(0.5))\n",
        "    CNN.add(Dense(units=3, activation='softmax'))\n",
        "    CNN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return CNN\n",
        "\n",
        "bert_output_dim = feature_train.shape[1]  # 获取BERT特征的维度\n",
        "CNN_model = create_CNN_model(bert_output_dim)\n",
        "CNN_history = CNN_model.fit(feature_train, label_train_y, epochs=800, batch_size=100,\n",
        "                            validation_data=(feature_valid, label_valid_y))"
      ],
      "metadata": {
        "id": "ZQr_4lP4u4r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_CNN = CNN_model.predict(feature_valid)\n",
        "\n",
        "# convert the validation vector\n",
        "valid_y_CNN = y_valid_CNN.copy()\n",
        "for i in range(len(y_valid_CNN)):\n",
        "    j = np.where(y_valid_CNN[i] == np.amax(y_valid_CNN[i]))\n",
        "    valid_y_CNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN))"
      ],
      "metadata": {
        "id": "4ynH01tfvyjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN2"
      ],
      "metadata": {
        "id": "VdJdIIRUwxZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import BatchNormalization, Reshape\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def create_CNN_model(bert_output_dim):\n",
        "    CNN = Sequential()\n",
        "    CNN.add(Reshape((bert_output_dim, 1), input_shape=(bert_output_dim,)))\n",
        "    CNN.add(Convolution1D(256, 3, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size=3))\n",
        "    CNN.add(Convolution1D(128, 3, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size=3))\n",
        "    CNN.add(Convolution1D(64, 3, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size=3))\n",
        "    CNN.add(Flatten())\n",
        "    CNN.add(Dense(units=512, activation='relu', kernel_regularizer=l2(0.02)))\n",
        "    CNN.add(Dropout(0.4))\n",
        "    CNN.add(Dense(units=256, activation='relu', kernel_regularizer=l2(0.02)))\n",
        "    CNN.add(Dropout(0.4))\n",
        "    CNN.add(Dense(units=3, activation='softmax'))\n",
        "\n",
        "    opt = Adam(learning_rate=0.001)\n",
        "    CNN.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return CNN\n",
        "\n",
        "bert_output_dim = feature_train.shape[1]  # 获取BERT特征的维度\n",
        "CNN_model = create_CNN_model(bert_output_dim)\n",
        "\n",
        "# 创建ModelCheckpoint回调函数\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "class_weights = {0: 1.0, 1: 0.5, 2: 2.0}\n",
        "\n",
        "CNN_history = CNN_model.fit(feature_train, label_train_y, epochs=800, batch_size=128,\n",
        "                            validation_data=(feature_valid, label_valid_y),\n",
        "                            class_weight=class_weights, callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "Az0H6y6cwKf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_CNN = CNN_model.predict(feature_valid)\n",
        "\n",
        "# convert the validation vector\n",
        "valid_y_CNN = y_valid_CNN.copy()\n",
        "for i in range(len(y_valid_CNN)):\n",
        "    j = np.where(y_valid_CNN[i] == np.amax(y_valid_CNN[i]))\n",
        "    valid_y_CNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN))"
      ],
      "metadata": {
        "id": "YLniu_o0w0KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 直接用bert在下游训练一下然后做预测"
      ],
      "metadata": {
        "id": "SGdCTr-6z1No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# 加载微调的FinBERT模型和分词器\n",
        "model_path = \"mrm8488/finetuned-finbert-finetuned-financial-news-sentiment-analysis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# 将文本转换为词汇表索引\n",
        "def convert_examples_to_features(examples, max_seq_length):\n",
        "    features = []\n",
        "    for example in examples:\n",
        "        tokens = tokenizer.tokenize(example)\n",
        "        tokens = ['[CLS]'] + tokens[:max_seq_length - 2] + ['[SEP]']\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        features.append(input_ids)\n",
        "    return tf.convert_to_tensor(features)\n",
        "\n",
        "# 转换训练集和验证集\n",
        "max_seq_length = 128\n",
        "train_features = convert_examples_to_features(X_train, max_seq_length)\n",
        "valid_features = convert_examples_to_features(X_valid, max_seq_length)\n",
        "\n",
        "# 定义模型\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
        "\n",
        "# 编译模型\n",
        "optimizer = 'adam'\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "# 计算类别权重\n",
        "classes = np.unique(label_train_y)\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=classes, y=label_train_y)\n",
        "class_weights = dict(zip(classes, class_weights))\n",
        "\n",
        "# 训练模型\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "history = model.fit(\n",
        "    train_features,\n",
        "    label_train_y,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(valid_features, label_valid_y),\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "# 在验证集上进行预测\n",
        "valid_predictions = model.predict(valid_features)\n",
        "valid_predictions = tf.argmax(valid_predictions.logits, axis=1)\n",
        "\n",
        "# 评估验证集上的性能\n",
        "valid_acc = accuracy_score(label_valid_y, valid_predictions)\n",
        "valid_f1 = f1_score(label_valid_y, valid_predictions, average='weighted')\n",
        "print(f'Validation Accuracy: {valid_acc:.4f}')\n",
        "print(f'Validation F1 (weighted): {valid_f1:.4f}')\n",
        "\n",
        "# 打印分类报告\n",
        "print('Classification Report:')\n",
        "print(classification_report(label_valid_y, valid_predictions))"
      ],
      "metadata": {
        "id": "_HfMGlyL5Fhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_valid_y"
      ],
      "metadata": {
        "id": "eaqabDmL5_GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TCl-i7BH6nrE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}