{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b07b4199b8d64178acef2910f3908e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ebf2a9a641a4a6e925119c2eb511f20",
              "IPY_MODEL_f69191df65464684b4d09327385b975e",
              "IPY_MODEL_0f8c80f5bfb54f35960e8e9a6e97a22e"
            ],
            "layout": "IPY_MODEL_4b5dbab8171a45a8b43a230ca870321d"
          }
        },
        "9ebf2a9a641a4a6e925119c2eb511f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d41668d3603484a93d34767bf98442a",
            "placeholder": "​",
            "style": "IPY_MODEL_77e9ddbec4194df6ab0641582b040034",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "f69191df65464684b4d09327385b975e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d720b2d313a44df384d35ced93e94c3c",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbabbbde5c8c481ea29b5f6b5a8db4b3",
            "value": 48
          }
        },
        "0f8c80f5bfb54f35960e8e9a6e97a22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_783e0bf2f66c4180aa4d1b6446ba3205",
            "placeholder": "​",
            "style": "IPY_MODEL_6b5afafcfafc4669a3cc5fd7eaf0fe5d",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.63kB/s]"
          }
        },
        "4b5dbab8171a45a8b43a230ca870321d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d41668d3603484a93d34767bf98442a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77e9ddbec4194df6ab0641582b040034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d720b2d313a44df384d35ced93e94c3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbabbbde5c8c481ea29b5f6b5a8db4b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "783e0bf2f66c4180aa4d1b6446ba3205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5afafcfafc4669a3cc5fd7eaf0fe5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "910e630657194385b1b260f15cade8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12c8c500bf66422e95081607e85d0626",
              "IPY_MODEL_85a732ce94c44d91b690a3330d85db7f",
              "IPY_MODEL_2659c008781d4c9b99b2f2c488c51541"
            ],
            "layout": "IPY_MODEL_b960f17aedd34de9ac1811e7d409d5b0"
          }
        },
        "12c8c500bf66422e95081607e85d0626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21704c4e2926479baf6e311739e01bd9",
            "placeholder": "​",
            "style": "IPY_MODEL_e27a9cbb9d3643b382fa8c3f8f81a174",
            "value": "vocab.txt: 100%"
          }
        },
        "85a732ce94c44d91b690a3330d85db7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c158a2221ca14ed8bfff558e59ca9a41",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8eb3cace2fd14a179d13df1794ab273a",
            "value": 231508
          }
        },
        "2659c008781d4c9b99b2f2c488c51541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de6650abbaea4f4d9b7d25ccc413f518",
            "placeholder": "​",
            "style": "IPY_MODEL_8e38aab262b64ee593fd6b797c704f58",
            "value": " 232k/232k [00:00&lt;00:00, 4.89MB/s]"
          }
        },
        "b960f17aedd34de9ac1811e7d409d5b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21704c4e2926479baf6e311739e01bd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e27a9cbb9d3643b382fa8c3f8f81a174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c158a2221ca14ed8bfff558e59ca9a41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eb3cace2fd14a179d13df1794ab273a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de6650abbaea4f4d9b7d25ccc413f518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e38aab262b64ee593fd6b797c704f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4cb53a4042843c69bad09739d30ce79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84221b8dbfdb4252b4d1d334f7543ce9",
              "IPY_MODEL_3733a8d2476c461e8fb8e8d355db03d8",
              "IPY_MODEL_b320d6cfdddb4286a616ee9f05996e1d"
            ],
            "layout": "IPY_MODEL_eb72a27acc3d404baf8ab82f55369fe5"
          }
        },
        "84221b8dbfdb4252b4d1d334f7543ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb81dd1666ec4f8f852571f34e7ee965",
            "placeholder": "​",
            "style": "IPY_MODEL_fc3384d778e842e9a299b06101368e46",
            "value": "tokenizer.json: 100%"
          }
        },
        "3733a8d2476c461e8fb8e8d355db03d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64f4f3263271499291952060d8ea89f8",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccb65119fc874a83b5c5d57a2897234c",
            "value": 466062
          }
        },
        "b320d6cfdddb4286a616ee9f05996e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4f85c98a6b941489fbe02860f220ef9",
            "placeholder": "​",
            "style": "IPY_MODEL_ac65e73df9234067899f33affa669219",
            "value": " 466k/466k [00:00&lt;00:00, 18.0MB/s]"
          }
        },
        "eb72a27acc3d404baf8ab82f55369fe5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb81dd1666ec4f8f852571f34e7ee965": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc3384d778e842e9a299b06101368e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64f4f3263271499291952060d8ea89f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccb65119fc874a83b5c5d57a2897234c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4f85c98a6b941489fbe02860f220ef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac65e73df9234067899f33affa669219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c4a9e16a2384bc79b4721031cd2d103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b0af796114d4a4ab737047acc663eac",
              "IPY_MODEL_796d55f88cc246dfbe936a83e16e5bb6",
              "IPY_MODEL_f786a5fa7f614d51803b32093d5a47b7"
            ],
            "layout": "IPY_MODEL_bb98b04dd5104614853b89dbad25e292"
          }
        },
        "4b0af796114d4a4ab737047acc663eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cde2c17c43984dacb09e6232a2c6a69d",
            "placeholder": "​",
            "style": "IPY_MODEL_43588aa756e44402aaf237c6178fcbd0",
            "value": "config.json: 100%"
          }
        },
        "796d55f88cc246dfbe936a83e16e5bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7c8448d91564699af670d8871cec092",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc99cf52329e47e0abfb657860023835",
            "value": 570
          }
        },
        "f786a5fa7f614d51803b32093d5a47b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c29c8c0dde248f6a92fdd0bf21d5adf",
            "placeholder": "​",
            "style": "IPY_MODEL_5c0dffb3bf86461d8c1fb8c19d7aefb1",
            "value": " 570/570 [00:00&lt;00:00, 29.5kB/s]"
          }
        },
        "bb98b04dd5104614853b89dbad25e292": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cde2c17c43984dacb09e6232a2c6a69d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43588aa756e44402aaf237c6178fcbd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7c8448d91564699af670d8871cec092": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc99cf52329e47e0abfb657860023835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c29c8c0dde248f6a92fdd0bf21d5adf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c0dffb3bf86461d8c1fb8c19d7aefb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51f24b6462d4402289d72c102858f052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a49328cb681c4b58b06e228ff15bf1ae",
              "IPY_MODEL_4a94b8c1747f45db942846c4665e3887",
              "IPY_MODEL_88cd5cbb2efd4495abd04d62adb72f6e"
            ],
            "layout": "IPY_MODEL_8b38c2da61ea4004a0524a2f9a4d3cbd"
          }
        },
        "a49328cb681c4b58b06e228ff15bf1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45512c6ec39048e183bb88d7fbd547ab",
            "placeholder": "​",
            "style": "IPY_MODEL_6820f0ff66894ef08549ac7326e1482b",
            "value": "model.safetensors: 100%"
          }
        },
        "4a94b8c1747f45db942846c4665e3887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc306a92790541a3acccdd20c1006043",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53097f73bb4544aca158b80fa791eab4",
            "value": 440449768
          }
        },
        "88cd5cbb2efd4495abd04d62adb72f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033ec1d612384b8b8dffe1c5f1ef1742",
            "placeholder": "​",
            "style": "IPY_MODEL_11dcad8f588a4a5eae89795a18411157",
            "value": " 440M/440M [00:04&lt;00:00, 149MB/s]"
          }
        },
        "8b38c2da61ea4004a0524a2f9a4d3cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45512c6ec39048e183bb88d7fbd547ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6820f0ff66894ef08549ac7326e1482b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc306a92790541a3acccdd20c1006043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53097f73bb4544aca158b80fa791eab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "033ec1d612384b8b8dffe1c5f1ef1742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11dcad8f588a4a5eae89795a18411157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsWC7cdyZIaA",
        "outputId": "f3c7595f-bc2c-475a-bfc7-6adb7eb8675a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow>=2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.15.0) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (3.20.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.15.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.2.2)\n",
            "Mounted at /content/gdrive\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive'\n",
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install \"tensorflow>=2.15.0\"\n",
        "!pip install --upgrade tensorflow-hub\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/drive/MyDrive\n",
        "# Import the libraries\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import ceil\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import collections\n",
        "import random\n",
        "import time\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, Dropout, LSTM, Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up"
      ],
      "metadata": {
        "id": "dzt2flDgZjNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We now set the directory to access the data\n",
        "def find(name, path):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        if name in files:\n",
        "            return os.path.join(root, name)\n",
        "\n",
        "# The folder with the data and this script should be saved in your drive.\n",
        "script_name = 'CourseWork_v2.ipynb'\n",
        "script_path = find(script_name, '/content/gdrive/My Drive')\n",
        "#DIRECTORY = '.'#os.path.dirname(script_path)\n",
        "# If your Drive is too large and the \"find\" function takes to much time, you can set the directory manually :\n",
        "\n",
        "#SUMMARY_PATH = '/content/drive/MyDrive/MutualFundSummary'\n",
        "#SUMMARY_LABELS_PATH = '/content/drive/MyDrive/MutualFundLabels.csv'\n",
        "\n",
        "DIRECTORY = '/content/gdrive/MyDrive/Colab Notebooks/NLP_app'\n",
        "\n",
        "SUMMARY_PATH = '/content/gdrive/MyDrive/Colab Notebooks/NLP_app/MutualFundSummary'\n",
        "SUMMARY_LABELS_PATH = '/content/gdrive/MyDrive/Colab Notebooks/MF815/NLP/NLP_app/MutualFundLabels.csv'\n",
        "\n",
        "glove_word2vec = 'glove.6B.50d.txt'\n",
        "our_word2vec = 'word2vec_perso.txt'\n",
        "\n",
        "# Progress bar\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "# Save a word2vec dictionary.\n",
        "def save_word2vec(filename):\n",
        "    with open(os.path.join('/content/drive/MyDrive', filename),'a' , encoding='utf-8') as f :\n",
        "        for k, v in word2vec.items():\n",
        "            line = k+' '+str(list(v)).strip('[]').replace(',','')+'\\n'\n",
        "            f.write(line)\n",
        "\n",
        "# Load a word2vec dictionary.\n",
        "def load_word2vec(filename):\n",
        "    word2vec = {}\n",
        "    with open(os.path.join('/content/drive/MyDrive', filename), encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            try :\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vec = np.asarray(values[1:], dtype='float32')\n",
        "                word2vec[word] = vec\n",
        "            except :\n",
        "                None\n",
        "    return word2vec\n",
        "\n",
        "# read the repo in PATH and append the texts in a list\n",
        "def get_data(PATH):\n",
        "    list_dir = os.listdir(PATH)\n",
        "    texts = []\n",
        "    fund_names = []\n",
        "    out = display(progress(0, len(list_dir)-1), display_id=True)\n",
        "    for ii, filename in enumerate(list_dir) :\n",
        "        with open(PATH+'/'+filename, 'r', encoding=\"utf8\") as f :\n",
        "            txt = f.read()\n",
        "            try :\n",
        "                txt_split = txt.split('<head_breaker>')\n",
        "                summary = txt_split[1].strip()\n",
        "                fund_name = txt_split[0].strip()\n",
        "            except :\n",
        "                summary = txt\n",
        "                fund_name = ''\n",
        "        texts.append(summary)\n",
        "        fund_names.append(fund_name)\n",
        "        out.update(progress(ii, len(list_dir)-1))\n",
        "    return fund_names, texts"
      ],
      "metadata": {
        "id": "gM6VrA9iZpgV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "0n98w1vUZ1ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "#labels = pd.read_csv('/content/MutualFundLabels.csv')\n",
        "labels = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/MutualFundLabels.csv')\n",
        "labels = labels.drop('Performance fee?', axis=1)\n",
        "removes = labels.loc[(labels['Ivestment Strategy']=='Long Short Funds (High Risk)')| (labels['Ivestment Strategy']=='Commodities Fund (Low Risk)')]\n",
        "labels_clean = labels.drop(removes.index)\n",
        "\n",
        "fund_names, summaries = get_data(SUMMARY_PATH)\n",
        "cleaned_fund_names = labels_clean['fund_name'].tolist()\n",
        "fund_name_counts = {name: 0 for name in fund_names}\n",
        "\n",
        "for name in fund_names:\n",
        "    if name in cleaned_fund_names:\n",
        "        fund_name_counts[name] += 1\n",
        "\n",
        "single_occurrences = {name: count for name, count in fund_name_counts.items() if count == 1}\n",
        "\n",
        "print(f\"Number of matching fund names that appear exactly once: {len(single_occurrences)}\")\n",
        "\n",
        "multiple_occurrences = {name: count for name, count in fund_name_counts.items() if count > 1}\n",
        "\n",
        "if multiple_occurrences:\n",
        "    print(f\"There are fund names that appear more than once:\")\n",
        "    for name, count in multiple_occurrences.items():\n",
        "        print(f\"{name}: {count} times\")\n",
        "else:\n",
        "    print(\"No fund names appear more than once.\")\n",
        "\n",
        "labels_clean_filtered = labels_clean[labels_clean['fund_name'].isin(fund_names)]\n",
        "\n",
        "df_summaries = pd.DataFrame(data={'fund_name':fund_names, 'summary':summaries})\n",
        "\n",
        "merge = labels_clean_filtered.merge(df_summaries, on=['fund_name'], how='left')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "MBP4j7DwZ2nM",
        "outputId": "42bcf921-9b9f-4cce-c7b4-b98a1051eaa7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='544'\n",
              "            max='544',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            544\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of matching fund names that appear exactly once: 461\n",
            "No fund names appear more than once.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, X_test, y, y_test = train_test_split(merge['summary'], merge['Ivestment Strategy'], test_size=0.2, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state=42)"
      ],
      "metadata": {
        "id": "sjvq7XU-adEu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\")+list(string.punctuation)+['``',\"''\"]+[\"]\",\"[\",\"*\"]+['doe', 'ha', 'wa'])\n",
        "\n",
        "# clean and tokenize without lemmatizing\n",
        "def tokenizer(txt):\n",
        "    txt = txt.replace('\\n', ' ').replace('\\t', ' ').lower()\n",
        "    word_tokens = word_tokenize(txt)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_sentence = [w for w in filtered_sentence if re.sub(\"[^A-Za-z ]+\",'',w) != '']\n",
        "    return filtered_sentence\n",
        "\n",
        "train_text_words = np.concatenate([tokenizer(summary) for summary in X_train])\n",
        "\n",
        "train_text_words[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60UUVD-Kayaz",
        "outputId": "e7db8a93-f3b5-481d-d071-fac7c6189c40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['mainstay', 'vp', 'epoch', 'u.s.', 'small', 'cap', 'portfolio',\n",
              "       'investment', 'objective', 'portfolio', 'seeks', 'long-term',\n",
              "       'capital', 'appreciation', 'investing', 'primarily', 'securities',\n",
              "       'small-cap', 'companies', 'fees'], dtype='<U44')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-gram Model"
      ],
      "metadata": {
        "id": "GWV983HjbASj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "batch_size = 128\n",
        "num_epochs = 2\n",
        "\n",
        "# Word2vec parameters\n",
        "embedding_size = 50\n",
        "max_vocabulary_size = 5000\n",
        "min_occurrence = 10\n",
        "skip_window = 3\n",
        "num_skips = 4\n",
        "\n",
        "count = [('UNK', -1)]\n",
        "count.extend(collections.Counter(train_text_words).most_common(max_vocabulary_size - 1))\n",
        "# Remove samples with less than 'min_occurrence' occurrences\n",
        "for i in range(len(count) - 1, -1, -1):\n",
        "    if count[i][1] < min_occurrence:\n",
        "        count.pop(i)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "word2id = dict()\n",
        "for i, (word, _)in enumerate(count):\n",
        "    word2id[word] = i\n",
        "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
        "vocab_size = len(id2word)\n",
        "\n",
        "data = list()\n",
        "unk_count = 0\n",
        "for word in train_text_words:\n",
        "    # Retrieve a word id, or assign it index 0 ('UNK') if not in dictionary\n",
        "    index = word2id.get(word, 0)\n",
        "    if index == 0:\n",
        "        unk_count += 1\n",
        "    data.append(index)\n",
        "count[0] = ('UNK', unk_count)\n",
        "\n",
        "# build OneHot vector from index\n",
        "def to_one_hot(data_point_index, vocab_size):\n",
        "    temp = np.zeros(vocab_size)\n",
        "    temp[data_point_index] = 1\n",
        "    return temp\n",
        "\n",
        "# Generate training batch for the skip-gram model\n",
        "def batch_generator(batch_size, num_skips, skip_window, vocab_size):\n",
        "    data_index = 0\n",
        "    while True :\n",
        "        assert batch_size % num_skips == 0\n",
        "        assert num_skips <= 2 * skip_window\n",
        "        # batch is filled with 128 inputs\n",
        "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "        # labels is filled with 128 outputs\n",
        "        labels = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "        span = 2 * skip_window + 1\n",
        "        # buffer keep track of the visited indexes visited\n",
        "        buffer = collections.deque(maxlen=span)\n",
        "        if data_index + span > len(data):\n",
        "            data_index = 0\n",
        "            # We stop the loop when we went through all the corpus\n",
        "            break\n",
        "        buffer.extend(data[data_index:data_index + span])\n",
        "        data_index += span\n",
        "        for i in range(batch_size // num_skips):\n",
        "            # Take the context current word\n",
        "            context_words = [w for w in range(span) if w != skip_window]\n",
        "            # Randomly select num_skips words in the context\n",
        "            words_to_use = random.sample(context_words, num_skips)\n",
        "            for j, context_word in enumerate(words_to_use):\n",
        "                # Creates one raw data\n",
        "                batch[i * num_skips + j] = buffer[skip_window]\n",
        "                labels[i * num_skips + j] = buffer[context_word]\n",
        "            if data_index == len(data):\n",
        "                buffer.extend(data[0:span])\n",
        "                data_index = span\n",
        "            else:\n",
        "                buffer.append(data[data_index])\n",
        "                data_index += 1\n",
        "        # Backtrack a little bit to avoid skipping words in the end of a batch\n",
        "        data_index = (data_index + len(data) - span) % len(data)\n",
        "\n",
        "        # translate word index to on-hot representation\n",
        "        batch_one_hot = np.array([to_one_hot(b, vocab_size) for b in batch])\n",
        "        labels_one_hot = np.array([to_one_hot(l, vocab_size) for l in labels])\n",
        "\n",
        "        # output one batch\n",
        "        yield batch_one_hot, labels_one_hot"
      ],
      "metadata": {
        "id": "V2LJo_H2bDIY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create en compile the Autoencoder\n",
        "def creat_word2vec_model():\n",
        "    input_word = Input(shape=(vocab_size,))\n",
        "\n",
        "    encoded = Dense(embedding_size, activation='linear')(input_word)\n",
        "    decoded = Dense(vocab_size, activation='softmax')(encoded)\n",
        "\n",
        "    autoencoder = Model(input_word, decoded)\n",
        "    encoder = Model(input_word, encoded)\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return encoder, autoencoder\n",
        "\n",
        "encoder, autoencoder = creat_word2vec_model()\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuIv1F86bDLm",
        "outputId": "6cd21613-d24d-480b-e307-dca10f43932a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 2821)]            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 50)                141100    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2821)              143871    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 284971 (1.09 MB)\n",
            "Trainable params: 284971 (1.09 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit_generator(batch_generator(batch_size, num_skips, skip_window, vocab_size), steps_per_epoch=ceil(len(data) / batch_size), epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaFAcoJYbDQM",
        "outputId": "17462401-3960-472a-ecc9-3a31ff8dd0ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-a9705c45700e>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  autoencoder.fit_generator(batch_generator(batch_size, num_skips, skip_window, vocab_size), steps_per_epoch=ceil(len(data) / batch_size), epochs=num_epochs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3990/3990 [==============================] - 45s 11ms/step - loss: 0.0221\n",
            "Epoch 2/2\n",
            "3990/3990 [==============================] - 43s 11ms/step - loss: 0.0027\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7db0e1c34d30>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vecotrize(word):\n",
        "    word_one_hot = to_one_hot(word2id[word], vocab_size)\n",
        "    return encoder.predict(np.array([word_one_hot]))[0]\n",
        "\n",
        "word2vec = {w : vecotrize(w) for w in word2id.keys()}\n",
        "save_word2vec('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/train_word2vec')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veIJ897obDSx",
        "outputId": "051f4420-d5a3-4e9d-9451-77ddb1bd7437"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# 将word2vec存储到文件\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/word2vec.pkl', 'wb') as f:\n",
        "    pickle.dump(word2vec, f)"
      ],
      "metadata": {
        "id": "4bhsJCGip4OK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTNdbtOQs0b_",
        "outputId": "90210fd1-091b-4b2e-d4e0-631e5947cd8c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'UNK': array([ 0.48072565,  0.51377416, -0.54403675, -0.51044124, -0.45459208,\n",
              "         0.5521796 ,  0.4633712 , -0.5046222 , -0.49841246, -0.52438706,\n",
              "        -0.4290832 ,  0.43921882,  0.50274366,  0.45586252, -0.49299294,\n",
              "        -0.42885375,  0.47066087,  0.50352675, -0.44458264, -0.47604772,\n",
              "        -0.42738396,  0.45919538,  0.43910316,  0.4691752 , -0.4864342 ,\n",
              "        -0.44266894,  0.48551178,  0.5560428 ,  0.5303056 ,  0.44054934,\n",
              "        -0.49877766,  0.50908875, -0.5336134 , -0.4452104 , -0.45649168,\n",
              "        -0.5082537 ,  0.46866453, -0.48271877,  0.48063073, -0.52256614,\n",
              "         0.44421977, -0.5295721 , -0.4661731 , -0.49586105,  0.44448003,\n",
              "         0.52480435,  0.51730585,  0.45067438, -0.49341455,  0.5057015 ],\n",
              "       dtype=float32),\n",
              " 'fund': array([ 0.5494262 ,  0.5116825 , -0.5306753 , -0.4978309 , -0.5128819 ,\n",
              "         0.4811934 ,  0.50425524, -0.4421385 , -0.5277038 , -0.44910365,\n",
              "        -0.52465904,  0.45798868,  0.47875848,  0.4368152 , -0.4838019 ,\n",
              "        -0.5206828 ,  0.49439883,  0.46216694, -0.51883227, -0.4510743 ,\n",
              "        -0.44402564,  0.5065334 ,  0.46985227,  0.5279121 , -0.5061857 ,\n",
              "        -0.53039044,  0.52065974,  0.49249303,  0.4606119 ,  0.44928348,\n",
              "        -0.49423146,  0.46113792, -0.506473  , -0.46356905, -0.45345348,\n",
              "        -0.48154217,  0.47939658, -0.49287874,  0.48805857, -0.5168996 ,\n",
              "         0.48057196, -0.4745591 , -0.44518837, -0.43848634,  0.47343618,\n",
              "         0.52930677,  0.48288542,  0.46385774, -0.5180455 ,  0.5105699 ],\n",
              "       dtype=float32),\n",
              " 'may': array([ 0.5180172 ,  0.47529003, -0.4948643 , -0.48061353, -0.46070692,\n",
              "         0.507089  ,  0.4643397 , -0.48356572, -0.5031372 , -0.49610937,\n",
              "        -0.4836012 ,  0.4406737 ,  0.44681162,  0.47048712, -0.48219272,\n",
              "        -0.4897042 ,  0.52098125,  0.45679098, -0.5204068 , -0.5181525 ,\n",
              "        -0.49968594,  0.5106672 ,  0.48303565,  0.43043178, -0.44300753,\n",
              "        -0.45968068,  0.5199207 ,  0.4755045 ,  0.4486191 ,  0.5231633 ,\n",
              "        -0.45638248,  0.50198984, -0.4706858 , -0.44583803, -0.49740455,\n",
              "        -0.50533473,  0.4797983 , -0.45667708,  0.5118642 , -0.4493836 ,\n",
              "         0.53808486, -0.49854258, -0.5168395 , -0.48824096,  0.4998577 ,\n",
              "         0.52646405,  0.49877465,  0.519749  , -0.46193847,  0.5069678 ],\n",
              "       dtype=float32),\n",
              " 'investment': array([ 0.50630563,  0.5105561 , -0.47073427, -0.4329241 , -0.47784027,\n",
              "         0.46503562,  0.49681234, -0.4876376 , -0.48565277, -0.47456616,\n",
              "        -0.49224073,  0.4921947 ,  0.46148926,  0.48505148, -0.5225205 ,\n",
              "        -0.4788695 ,  0.43895105,  0.4567115 , -0.48352355, -0.48668736,\n",
              "        -0.4702241 ,  0.51071084,  0.47819018,  0.43699443, -0.51605767,\n",
              "        -0.50101304,  0.50618386,  0.52214116,  0.45882213,  0.49949342,\n",
              "        -0.49821848,  0.5001719 , -0.5281954 , -0.4906566 , -0.4706209 ,\n",
              "        -0.47714728,  0.53381115, -0.4985694 ,  0.50250745, -0.49323815,\n",
              "         0.46991614, -0.5063585 , -0.5171248 , -0.5150074 ,  0.44201624,\n",
              "         0.46556932,  0.48520023,  0.4800681 , -0.48079437,  0.47303963],\n",
              "       dtype=float32),\n",
              " 'shares': array([ 0.4222421 ,  0.467205  , -0.476623  , -0.4264494 , -0.48724127,\n",
              "         0.5502428 ,  0.41571665, -0.47166252, -0.4669664 , -0.46916002,\n",
              "        -0.58169806,  0.47243991,  0.53364223,  0.54515094, -0.482823  ,\n",
              "        -0.5188775 ,  0.570211  ,  0.45569846, -0.46368963, -0.486911  ,\n",
              "        -0.5715221 ,  0.46156514,  0.5334    ,  0.53989786, -0.5110938 ,\n",
              "        -0.49136585,  0.46061146,  0.43081447,  0.52445847,  0.5684682 ,\n",
              "        -0.49349266,  0.4946501 , -0.46312937, -0.5153492 , -0.44886875,\n",
              "        -0.5372971 ,  0.4846109 , -0.57330495,  0.5415701 , -0.45051706,\n",
              "         0.5658578 , -0.60058177, -0.49129647, -0.44821143,  0.5638583 ,\n",
              "         0.54583544,  0.45056438,  0.4922889 , -0.52654505,  0.5262014 ],\n",
              "       dtype=float32),\n",
              " 'securities': array([ 0.46904823,  0.49739832, -0.48924884, -0.4706229 , -0.46776485,\n",
              "         0.52938354,  0.5012419 , -0.46150124, -0.5257821 , -0.44816646,\n",
              "        -0.46067023,  0.4495787 ,  0.52471185,  0.5262809 , -0.5378621 ,\n",
              "        -0.5155866 ,  0.4823692 ,  0.45763734, -0.5099095 , -0.4732816 ,\n",
              "        -0.47722334,  0.51654804,  0.5146393 ,  0.46448123, -0.42058158,\n",
              "        -0.49390563,  0.49591994,  0.48890465,  0.4916898 ,  0.42765465,\n",
              "        -0.45941597,  0.5174983 , -0.4935481 , -0.52060544, -0.43615073,\n",
              "        -0.52918166,  0.5041953 , -0.4871892 ,  0.4704265 , -0.50180686,\n",
              "         0.47200385, -0.46826088, -0.49983197, -0.45443043,  0.43579552,\n",
              "         0.48109698,  0.51527363,  0.5001258 , -0.5225646 ,  0.43537602],\n",
              "       dtype=float32),\n",
              " 'class': array([ 0.4251918 ,  0.4421374 , -0.52698696, -0.51002735, -0.51112455,\n",
              "         0.49613094,  0.50510514, -0.4161622 , -0.4652835 , -0.48142725,\n",
              "        -0.58986115,  0.48236692,  0.476242  ,  0.4507147 , -0.5266179 ,\n",
              "        -0.5518067 ,  0.584621  ,  0.42360106, -0.4748741 , -0.45463982,\n",
              "        -0.5696913 ,  0.49991637,  0.475355  ,  0.52115905, -0.51465946,\n",
              "        -0.43277907,  0.5022904 ,  0.42600426,  0.5150541 ,  0.60682017,\n",
              "        -0.44335157,  0.42058462, -0.52603656, -0.5220636 , -0.40701878,\n",
              "        -0.50668395,  0.54386103, -0.587748  ,  0.45950145, -0.49459544,\n",
              "         0.53894305, -0.61191994, -0.49113423, -0.5061426 ,  0.53881425,\n",
              "         0.5940997 ,  0.50621593,  0.4613548 , -0.59469795,  0.46486303],\n",
              "       dtype=float32),\n",
              " 'portfolio': array([ 0.5025819 ,  0.46500513, -0.52411115, -0.50614285, -0.49694774,\n",
              "         0.5278387 ,  0.5073649 , -0.4990843 , -0.52351874, -0.4624309 ,\n",
              "        -0.48875952,  0.483939  ,  0.4782073 ,  0.47211558, -0.53317195,\n",
              "        -0.42957592,  0.49093795,  0.4941699 , -0.49871394, -0.50782466,\n",
              "        -0.5127944 ,  0.4782136 ,  0.50625443,  0.52605224, -0.5164007 ,\n",
              "        -0.51967037,  0.42942026,  0.45137766,  0.499619  ,  0.43345815,\n",
              "        -0.46214104,  0.521258  , -0.48122156, -0.4993733 , -0.50298405,\n",
              "        -0.43734506,  0.4615149 , -0.4967871 ,  0.49170363, -0.519461  ,\n",
              "         0.4684507 , -0.45595804, -0.48317975, -0.4803135 ,  0.44241405,\n",
              "         0.4435089 ,  0.47924882,  0.5138056 , -0.5052753 ,  0.49543056],\n",
              "       dtype=float32),\n",
              " 'risk': array([ 0.5471449 ,  0.5205069 , -0.48763454, -0.44966334, -0.46517268,\n",
              "         0.45932636,  0.47265464, -0.4672904 , -0.513247  , -0.4777124 ,\n",
              "        -0.48655668,  0.46303487,  0.43743527,  0.4789795 , -0.507003  ,\n",
              "        -0.45968202,  0.44940302,  0.5270116 , -0.52677083, -0.46124166,\n",
              "        -0.44383097,  0.5263639 ,  0.5399851 ,  0.5105074 , -0.51860356,\n",
              "        -0.4376276 ,  0.49105525,  0.5308684 ,  0.45607463,  0.43224886,\n",
              "        -0.5253522 ,  0.52582604, -0.47395062, -0.5285339 , -0.4629032 ,\n",
              "        -0.5347372 ,  0.50660557, -0.47135347,  0.43757617, -0.54015356,\n",
              "         0.49892926, -0.4554178 , -0.43502572, -0.51674604,  0.5019382 ,\n",
              "         0.5136428 ,  0.45495605,  0.5243977 , -0.50930744,  0.45631263],\n",
              "       dtype=float32),\n",
              " 'expenses': array([ 0.5416196 ,  0.43119067, -0.5072055 , -0.512571  , -0.49094668,\n",
              "         0.45390457,  0.46449882, -0.50448996, -0.47131598, -0.49722847,\n",
              "        -0.5160322 ,  0.53154093,  0.5341762 ,  0.5278438 , -0.50243145,\n",
              "        -0.53973424,  0.47805858,  0.52535444, -0.49007663, -0.46205527,\n",
              "        -0.5277432 ,  0.48537695,  0.44221503,  0.5208116 , -0.52135223,\n",
              "        -0.459414  ,  0.4934525 ,  0.5467672 ,  0.5067513 ,  0.48933053,\n",
              "        -0.4943598 ,  0.5146309 , -0.49953753, -0.51455706, -0.49702412,\n",
              "        -0.4794373 ,  0.53584886, -0.43488994,  0.4962671 , -0.4457252 ,\n",
              "         0.51890683, -0.49952114, -0.50760704, -0.50034064,  0.4866829 ,\n",
              "         0.5305925 ,  0.5024535 ,  0.44571537, -0.4620024 ,  0.49395913],\n",
              "       dtype=float32),\n",
              " 'market': array([ 0.4672032 ,  0.46050736, -0.4582265 , -0.5034111 , -0.4927946 ,\n",
              "         0.4957952 ,  0.5268398 , -0.45773223, -0.48742557, -0.5244305 ,\n",
              "        -0.5117077 ,  0.45392638,  0.53378135,  0.51347375, -0.5046135 ,\n",
              "        -0.46371785,  0.4675616 ,  0.4790446 , -0.48686194, -0.48854333,\n",
              "        -0.4568408 ,  0.47739956,  0.46131516,  0.47179878, -0.46770892,\n",
              "        -0.4499835 ,  0.44743562,  0.4753001 ,  0.52976483,  0.4737279 ,\n",
              "        -0.46066892,  0.49593586, -0.534126  , -0.48692012, -0.5141677 ,\n",
              "        -0.5169076 ,  0.47585878, -0.51107824,  0.4786719 , -0.49268904,\n",
              "         0.4668117 , -0.4602909 , -0.47072285, -0.4810081 ,  0.4562672 ,\n",
              "         0.4441569 ,  0.5263478 ,  0.5111577 , -0.46529683,  0.49129018],\n",
              "       dtype=float32),\n",
              " 'value': array([ 0.54060924,  0.46229935, -0.5136368 , -0.5047396 , -0.5052207 ,\n",
              "         0.45763612,  0.46144673, -0.48138812, -0.5067287 , -0.43916127,\n",
              "        -0.43449655,  0.5060672 ,  0.46908936,  0.47456196, -0.51970696,\n",
              "        -0.52395356,  0.49548417,  0.4980117 , -0.5196466 , -0.4638236 ,\n",
              "        -0.4861966 ,  0.495145  ,  0.5340792 ,  0.5149835 , -0.4617574 ,\n",
              "        -0.4525619 ,  0.51611906,  0.54368013,  0.48890358,  0.5122805 ,\n",
              "        -0.45583743,  0.45974264, -0.47362798, -0.50516194, -0.44429994,\n",
              "        -0.5105138 ,  0.4899778 , -0.48874903,  0.5238999 , -0.48585153,\n",
              "         0.5044017 , -0.4707836 , -0.47618598, -0.48435423,  0.4923543 ,\n",
              "         0.53142715,  0.49575704,  0.4597087 , -0.4415707 ,  0.51026726],\n",
              "       dtype=float32),\n",
              " 'performance': array([ 0.54812264,  0.47792226, -0.47528493, -0.47015268, -0.53817976,\n",
              "         0.48853478,  0.42809838, -0.49013758, -0.49918395, -0.4478757 ,\n",
              "        -0.49434534,  0.4818914 ,  0.46485597,  0.5036632 , -0.5491817 ,\n",
              "        -0.47001365,  0.4860861 ,  0.523816  , -0.4917315 , -0.49207717,\n",
              "        -0.48695278,  0.49585503,  0.49903965,  0.43215916, -0.46740738,\n",
              "        -0.5128735 ,  0.49611393,  0.4724733 ,  0.4773394 ,  0.50708616,\n",
              "        -0.49138117,  0.49448213, -0.5184295 , -0.45517457, -0.5072644 ,\n",
              "        -0.51257735,  0.47994873, -0.49749327,  0.5061713 , -0.50814384,\n",
              "         0.504265  , -0.5123398 , -0.51762956, -0.5034782 ,  0.50600094,\n",
              "         0.50743544,  0.43769315,  0.46172753, -0.4799528 ,  0.4286258 ],\n",
              "       dtype=float32),\n",
              " 'fees': array([ 0.49823824,  0.45644185, -0.491636  , -0.48016384, -0.48823908,\n",
              "         0.44619176,  0.44124812, -0.46743768, -0.46763206, -0.50851035,\n",
              "        -0.5109452 ,  0.45816457,  0.5273131 ,  0.4523201 , -0.52174306,\n",
              "        -0.5344763 ,  0.53330594,  0.51755553, -0.52787113, -0.5250929 ,\n",
              "        -0.50297993,  0.46481133,  0.51392114,  0.4558613 , -0.4969946 ,\n",
              "        -0.4464521 ,  0.5203438 ,  0.514362  ,  0.5318493 ,  0.48127797,\n",
              "        -0.5220595 ,  0.5265731 , -0.5120155 , -0.46137744, -0.49808067,\n",
              "        -0.53947026,  0.51188177, -0.5004843 ,  0.44474247, -0.4526513 ,\n",
              "         0.4985261 , -0.48871094, -0.4733963 , -0.5045647 ,  0.5401838 ,\n",
              "         0.5222428 ,  0.4814304 ,  0.48672917, -0.4528011 ,  0.48912537],\n",
              "       dtype=float32),\n",
              " 'funds': array([ 0.52576286,  0.46198246, -0.4950104 , -0.44706655, -0.49415258,\n",
              "         0.46588558,  0.45780796, -0.45372   , -0.44345504, -0.47414163,\n",
              "        -0.5246269 ,  0.50622976,  0.5005831 ,  0.5206105 , -0.5159655 ,\n",
              "        -0.5298224 ,  0.504338  ,  0.49534383, -0.51745087, -0.45598853,\n",
              "        -0.4400141 ,  0.493105  ,  0.47637153,  0.46134135, -0.4982561 ,\n",
              "        -0.44725856,  0.43959597,  0.48339966,  0.46815592,  0.47723922,\n",
              "        -0.5075874 ,  0.4781082 , -0.47926953, -0.53979796, -0.49575445,\n",
              "        -0.49446172,  0.45684692, -0.45507932,  0.46277034, -0.526973  ,\n",
              "         0.51809907, -0.48325562, -0.49593535, -0.5247448 ,  0.491159  ,\n",
              "         0.5102326 ,  0.44344786,  0.51440066, -0.5081162 ,  0.4524371 ],\n",
              "       dtype=float32),\n",
              " 'index': array([ 0.5278048 ,  0.47901055, -0.44372222, -0.5051517 , -0.5261322 ,\n",
              "         0.5288436 ,  0.46617743, -0.52045417, -0.43856734, -0.47436744,\n",
              "        -0.4804555 ,  0.47471863,  0.5107688 ,  0.45708454, -0.5299985 ,\n",
              "        -0.49534935,  0.46480462,  0.5112846 , -0.4486164 , -0.4606754 ,\n",
              "        -0.4945094 ,  0.48803955,  0.509442  ,  0.44949612, -0.47782418,\n",
              "        -0.5182557 ,  0.42897758,  0.4791756 ,  0.51050913,  0.45436704,\n",
              "        -0.5115863 ,  0.42361563, -0.47337416, -0.4902103 , -0.42994425,\n",
              "        -0.50641423,  0.52348286, -0.49242938,  0.50844014, -0.47397578,\n",
              "         0.48522794, -0.48176074, -0.43244863, -0.48761848,  0.46176422,\n",
              "         0.50137484,  0.4700499 ,  0.4821453 , -0.42767778,  0.4794482 ],\n",
              "       dtype=float32),\n",
              " 'taxes': array([ 0.4716308 ,  0.52015364, -0.4642501 , -0.46073237, -0.48962495,\n",
              "         0.48575604,  0.43385515, -0.5350306 , -0.47345847, -0.47940296,\n",
              "        -0.54551923,  0.5018526 ,  0.5011541 ,  0.5321427 , -0.5112654 ,\n",
              "        -0.55011487,  0.4820385 ,  0.44835895, -0.44466662, -0.46661794,\n",
              "        -0.4784642 ,  0.44756475,  0.5302785 ,  0.45190766, -0.52949715,\n",
              "        -0.5242631 ,  0.5205459 ,  0.53976494,  0.47490346,  0.5309753 ,\n",
              "        -0.51716447,  0.45205078, -0.53213817, -0.4608779 , -0.47394243,\n",
              "        -0.5373625 ,  0.49166995, -0.5419165 ,  0.47458702, -0.50752646,\n",
              "         0.4557373 , -0.48256534, -0.51307464, -0.5160556 ,  0.5036323 ,\n",
              "         0.543197  ,  0.43975383,  0.4502959 , -0.5239697 ,  0.4661685 ],\n",
              "       dtype=float32),\n",
              " 'risks': array([ 0.48160547,  0.5087007 , -0.46666744, -0.451442  , -0.5427524 ,\n",
              "         0.4571184 ,  0.45548975, -0.5042868 , -0.46314862, -0.51003325,\n",
              "        -0.47944817,  0.47737277,  0.53147155,  0.47568846, -0.57716054,\n",
              "        -0.44488323,  0.45138207,  0.5223889 , -0.4514875 , -0.52573293,\n",
              "        -0.5160416 ,  0.4548832 ,  0.5150857 ,  0.45837358, -0.45279148,\n",
              "        -0.4849748 ,  0.48386294,  0.51106966,  0.4988696 ,  0.51223224,\n",
              "        -0.5029442 ,  0.4588464 , -0.47788438, -0.52598053, -0.456103  ,\n",
              "        -0.49247572,  0.46589422, -0.4918863 ,  0.46172023, -0.45782107,\n",
              "         0.52478063, -0.516459  , -0.48180288, -0.4972557 ,  0.49775773,\n",
              "         0.46642065,  0.5111269 ,  0.49232084, -0.5071734 ,  0.4509481 ],\n",
              "       dtype=float32),\n",
              " 'investments': array([ 0.49473137,  0.5022262 , -0.4761761 , -0.5161088 , -0.47168738,\n",
              "         0.50774026,  0.5043537 , -0.47202727, -0.5066366 , -0.4932445 ,\n",
              "        -0.5051548 ,  0.47562867,  0.45242375,  0.48529634, -0.5477454 ,\n",
              "        -0.4415379 ,  0.44806546,  0.47439533, -0.45160067, -0.46530482,\n",
              "        -0.48456132,  0.5100769 ,  0.45946956,  0.46911538, -0.51848686,\n",
              "        -0.47983924,  0.46965122,  0.5421413 ,  0.47879308,  0.46418366,\n",
              "        -0.50617903,  0.45318693, -0.45976827, -0.48739943, -0.52103263,\n",
              "        -0.4950658 ,  0.4967964 , -0.46536738,  0.50331485, -0.45819604,\n",
              "         0.53450215, -0.4999579 , -0.46643856, -0.48731357,  0.5245366 ,\n",
              "         0.47030514,  0.52653605,  0.47909188, -0.50764525,  0.52068824],\n",
              "       dtype=float32),\n",
              " 'returns': array([ 0.5193349 ,  0.48003206, -0.45325738, -0.44763362, -0.47855636,\n",
              "         0.51588356,  0.46152842, -0.4687596 , -0.44034562, -0.46098015,\n",
              "        -0.5214731 ,  0.5366646 ,  0.5325984 ,  0.4577843 , -0.5622507 ,\n",
              "        -0.4521061 ,  0.5300454 ,  0.49358094, -0.48805526, -0.50662893,\n",
              "        -0.50607765,  0.49767905,  0.51924384,  0.50484866, -0.4470064 ,\n",
              "        -0.46745616,  0.50292397,  0.4854181 ,  0.50819343,  0.53107435,\n",
              "        -0.5076645 ,  0.4394147 , -0.48251292, -0.4926889 , -0.44647226,\n",
              "        -0.5104965 ,  0.54136   , -0.48383752,  0.49064153, -0.51192373,\n",
              "         0.45957962, -0.48900098, -0.47110665, -0.5230593 ,  0.5324486 ,\n",
              "         0.50880253,  0.44310844,  0.5122722 , -0.46620932,  0.49761716],\n",
              "       dtype=float32),\n",
              " 'year': array([ 0.50597197,  0.45337364, -0.5369713 , -0.44217065, -0.4579716 ,\n",
              "         0.5233759 ,  0.5033    , -0.45741355, -0.52431893, -0.5171538 ,\n",
              "        -0.52217853,  0.46157673,  0.48257354,  0.51401615, -0.5516769 ,\n",
              "        -0.5003741 ,  0.54857206,  0.46727124, -0.4937437 , -0.50905675,\n",
              "        -0.52522075,  0.5108345 ,  0.5125024 ,  0.44728696, -0.5267549 ,\n",
              "        -0.5208123 ,  0.48713207,  0.5251918 ,  0.5145645 ,  0.49940038,\n",
              "        -0.49543175,  0.48478016, -0.505121  , -0.49307975, -0.5110305 ,\n",
              "        -0.48028994,  0.48852614, -0.49496236,  0.46001017, -0.53387725,\n",
              "         0.48052964, -0.5451645 , -0.47065315, -0.47905916,  0.5021347 ,\n",
              "         0.4533219 ,  0.47599033,  0.47693795, -0.49067324,  0.4798012 ],\n",
              "       dtype=float32),\n",
              " 'interest': array([ 0.51130587,  0.44789964, -0.49709758, -0.45015594, -0.49544147,\n",
              "         0.51576155,  0.46235117, -0.52104616, -0.47237208, -0.508806  ,\n",
              "        -0.47096062,  0.5080676 ,  0.47798625,  0.52045095, -0.5417895 ,\n",
              "        -0.52257377,  0.47654566,  0.4992897 , -0.44417182, -0.46531507,\n",
              "        -0.5196642 ,  0.5011743 ,  0.51553637,  0.5086067 , -0.47126088,\n",
              "        -0.51994133,  0.47810805,  0.46652347,  0.5131736 ,  0.51341736,\n",
              "        -0.4928478 ,  0.4897072 , -0.51288766, -0.46681514, -0.43414795,\n",
              "        -0.4931901 ,  0.52543676, -0.46239504,  0.4469638 , -0.4970865 ,\n",
              "         0.4579193 , -0.45757285, -0.5121953 , -0.43881753,  0.51230544,\n",
              "         0.44850472,  0.500025  ,  0.5063673 , -0.45404786,  0.45806137],\n",
              "       dtype=float32),\n",
              " 'investing': array([ 0.52394027,  0.48407853, -0.48954794, -0.4875713 , -0.53580064,\n",
              "         0.5192895 ,  0.52480364, -0.49836123, -0.4748444 , -0.49352515,\n",
              "        -0.46572697,  0.50521266,  0.44728595,  0.5181408 , -0.5215841 ,\n",
              "        -0.5274996 ,  0.45584124,  0.49541083, -0.4506101 , -0.4900339 ,\n",
              "        -0.52420795,  0.47018966,  0.5149511 ,  0.47612065, -0.5269213 ,\n",
              "        -0.46719426,  0.50535345,  0.48629686,  0.4823143 ,  0.51807714,\n",
              "        -0.46036267,  0.4406578 , -0.50793076, -0.5024708 , -0.493255  ,\n",
              "        -0.51544863,  0.47030866, -0.50519913,  0.44404504, -0.4711787 ,\n",
              "         0.50097096, -0.4963231 , -0.46930534, -0.5034486 ,  0.5220747 ,\n",
              "         0.48482585,  0.46824816,  0.4499476 , -0.45907792,  0.47695297],\n",
              "       dtype=float32),\n",
              " 'companies': array([ 0.46208712,  0.45705262, -0.45230535, -0.5308168 , -0.52519894,\n",
              "         0.46194968,  0.52401555, -0.4807586 , -0.52576494, -0.4416969 ,\n",
              "        -0.4289355 ,  0.5286191 ,  0.48520413,  0.46687058, -0.5199526 ,\n",
              "        -0.4707935 ,  0.4790507 ,  0.4681337 , -0.43812147, -0.51249146,\n",
              "        -0.46915153,  0.5177057 ,  0.45233953,  0.47959286, -0.46870047,\n",
              "        -0.52366483,  0.44300604,  0.5168375 ,  0.51702935,  0.49717015,\n",
              "        -0.5105592 ,  0.4390477 , -0.5083709 , -0.48252648, -0.4457902 ,\n",
              "        -0.4727285 ,  0.49148548, -0.46532562,  0.44635934, -0.5026814 ,\n",
              "         0.5055028 , -0.45984215, -0.47570932, -0.50516576,  0.4383457 ,\n",
              "         0.49003565,  0.5131916 ,  0.48477632, -0.5155145 ,  0.5196256 ],\n",
              "       dtype=float32),\n",
              " 'financial': array([ 0.46785274,  0.4663992 , -0.44534925, -0.5218285 , -0.51891816,\n",
              "         0.47981685,  0.4906345 , -0.47992456, -0.51308787, -0.4530776 ,\n",
              "        -0.52142906,  0.48608238,  0.5062285 ,  0.5236954 , -0.51466435,\n",
              "        -0.50333524,  0.4814363 ,  0.48514485, -0.48758805, -0.51952744,\n",
              "        -0.52711064,  0.44282785,  0.47267953,  0.46106035, -0.5270128 ,\n",
              "        -0.4480398 ,  0.51531726,  0.5017099 ,  0.4603228 ,  0.44248158,\n",
              "        -0.5208115 ,  0.46276048, -0.5136994 , -0.45705414, -0.44758332,\n",
              "        -0.46869847,  0.4577929 , -0.47519025,  0.48144335, -0.4857397 ,\n",
              "         0.46967503, -0.46462756, -0.46733645, -0.5265249 ,  0.53333175,\n",
              "         0.51573336,  0.4851431 ,  0.49198592, -0.46526137,  0.46208745],\n",
              "       dtype=float32),\n",
              " 'underlying': array([ 0.52869064,  0.47925586, -0.44839165, -0.4747556 , -0.53060037,\n",
              "         0.5180532 ,  0.5186972 , -0.45839503, -0.47036284, -0.5121455 ,\n",
              "        -0.5131352 ,  0.50854975,  0.4525283 ,  0.53620994, -0.543207  ,\n",
              "        -0.44269758,  0.4853965 ,  0.46314067, -0.47258905, -0.48615932,\n",
              "        -0.51516855,  0.43995348,  0.49566334,  0.46495304, -0.5236287 ,\n",
              "        -0.47785336,  0.48558164,  0.5357878 ,  0.5132655 ,  0.45326412,\n",
              "        -0.46498457,  0.43798545, -0.5348979 , -0.47468835, -0.46540546,\n",
              "        -0.49337244,  0.45477566, -0.4966067 ,  0.46213126, -0.47190645,\n",
              "         0.49361524, -0.49962083, -0.4457348 , -0.4455714 ,  0.5007337 ,\n",
              "         0.49928328,  0.5100116 ,  0.52823174, -0.47503918,  0.43761507],\n",
              "       dtype=float32),\n",
              " 'return': array([ 0.53235674,  0.5106114 , -0.53115046, -0.5111666 , -0.51312244,\n",
              "         0.5381222 ,  0.471387  , -0.45165202, -0.4436429 , -0.4835321 ,\n",
              "        -0.49833846,  0.50840133,  0.50813097,  0.48073345, -0.4847267 ,\n",
              "        -0.46034312,  0.5170586 ,  0.46337515, -0.49560142, -0.46762693,\n",
              "        -0.46549094,  0.51102567,  0.54700303,  0.49384248, -0.4678582 ,\n",
              "        -0.5347382 ,  0.49702942,  0.47083434,  0.4694942 ,  0.49331373,\n",
              "        -0.4732564 ,  0.53695977, -0.5155021 , -0.511862  , -0.47112665,\n",
              "        -0.5466419 ,  0.4710847 , -0.49346521,  0.4997229 , -0.47651133,\n",
              "         0.45520198, -0.50642455, -0.5339928 , -0.45029148,  0.4952277 ,\n",
              "         0.4860614 ,  0.5267271 ,  0.4450437 , -0.53882986,  0.5136998 ],\n",
              "       dtype=float32),\n",
              " 'income': array([ 0.54416823,  0.5024657 , -0.45564234, -0.50962764, -0.48753878,\n",
              "         0.45592746,  0.47405374, -0.44419813, -0.5132036 , -0.51720136,\n",
              "        -0.4889749 ,  0.51830643,  0.43709484,  0.4469057 , -0.49347705,\n",
              "        -0.49457562,  0.49661386,  0.52679455, -0.4803266 , -0.45042723,\n",
              "        -0.5113634 ,  0.5190083 ,  0.4592487 ,  0.47380066, -0.51294076,\n",
              "        -0.47682428,  0.5043168 ,  0.4918143 ,  0.4390624 ,  0.48440695,\n",
              "        -0.46177486,  0.45572323, -0.47129592, -0.54133385, -0.46424174,\n",
              "        -0.46503183,  0.5087042 , -0.486394  ,  0.49258703, -0.53385967,\n",
              "         0.5012613 , -0.44827965, -0.4616863 , -0.4473575 ,  0.50396377,\n",
              "         0.4414306 ,  0.44243696,  0.44577637, -0.5324594 ,  0.4581768 ],\n",
              "       dtype=float32),\n",
              " \"'s\": array([ 0.522891  ,  0.475061  , -0.46516776, -0.50697273, -0.46733245,\n",
              "         0.5114795 ,  0.46511263, -0.46914622, -0.5117172 , -0.5267879 ,\n",
              "        -0.5016106 ,  0.48626465,  0.5102781 ,  0.4572007 , -0.51243174,\n",
              "        -0.543351  ,  0.5103512 ,  0.4559065 , -0.4654619 , -0.4544247 ,\n",
              "        -0.5024685 ,  0.5161386 ,  0.54649204,  0.482919  , -0.5014552 ,\n",
              "        -0.5066457 ,  0.4692242 ,  0.5157018 ,  0.48183358,  0.45676276,\n",
              "        -0.481927  ,  0.46313533, -0.49190956, -0.46699446, -0.47518477,\n",
              "        -0.54466516,  0.4609202 , -0.5373941 ,  0.5248441 , -0.5239854 ,\n",
              "         0.5068829 , -0.53560245, -0.5154587 , -0.47396588,  0.5194541 ,\n",
              "         0.46417716,  0.4690887 ,  0.4516834 , -0.46066764,  0.47002488],\n",
              "       dtype=float32),\n",
              " 'none': array([ 0.49826112,  0.44513398, -0.45384848, -0.4731842 , -0.44467482,\n",
              "         0.49184495,  0.52916306, -0.45559707, -0.434875  , -0.52167565,\n",
              "        -0.53506225,  0.45833635,  0.5707413 ,  0.55009687, -0.49137065,\n",
              "        -0.52070695,  0.4327679 ,  0.49662012, -0.5221586 , -0.47565177,\n",
              "        -0.46085966,  0.48776168,  0.48789486,  0.5026045 , -0.45680752,\n",
              "        -0.45251155,  0.5468116 ,  0.45470992,  0.48078683,  0.5435295 ,\n",
              "        -0.46561623,  0.47362408, -0.4773315 , -0.5220069 , -0.5377284 ,\n",
              "        -0.5394702 ,  0.4933092 , -0.5514431 ,  0.51850915, -0.52197844,\n",
              "         0.46733963, -0.48711124, -0.52072376, -0.49414095,  0.5556699 ,\n",
              "         0.53574395,  0.4520331 ,  0.48435694, -0.49469253,  0.5470444 ],\n",
              "       dtype=float32),\n",
              " 'annual': array([ 0.48763132,  0.46986657, -0.5093507 , -0.5087262 , -0.5016785 ,\n",
              "         0.49876767,  0.4704681 , -0.49601448, -0.52580947, -0.50159323,\n",
              "        -0.48707545,  0.52459896,  0.54215133,  0.48862836, -0.49304882,\n",
              "        -0.49721947,  0.55530995,  0.45991853, -0.5130518 , -0.51011366,\n",
              "        -0.4522041 ,  0.5149606 ,  0.49728006,  0.51031196, -0.5253463 ,\n",
              "        -0.5013852 ,  0.527547  ,  0.48453683,  0.54524875,  0.5183144 ,\n",
              "        -0.46070275,  0.4846381 , -0.48267126, -0.45125479, -0.4937275 ,\n",
              "        -0.5044815 ,  0.4959324 , -0.52458256,  0.53205067, -0.49152896,\n",
              "         0.49399534, -0.5306092 , -0.535642  , -0.50270146,  0.5347381 ,\n",
              "         0.5010395 ,  0.5165249 ,  0.44796285, -0.46853626,  0.48184696],\n",
              "       dtype=float32),\n",
              " 'information': array([ 0.54755855,  0.50037086, -0.48963004, -0.4709112 , -0.46861973,\n",
              "         0.47304803,  0.47610638, -0.4552389 , -0.4431504 , -0.5103839 ,\n",
              "        -0.5286318 ,  0.51915663,  0.456448  ,  0.4976653 , -0.50519913,\n",
              "        -0.46203518,  0.5325527 ,  0.46925864, -0.51069003, -0.4403893 ,\n",
              "        -0.49269843,  0.512861  ,  0.5191887 ,  0.43025073, -0.48468652,\n",
              "        -0.511112  ,  0.43196315,  0.52724713,  0.4765284 ,  0.4812997 ,\n",
              "        -0.45325848,  0.5098625 , -0.47667298, -0.505329  , -0.47296205,\n",
              "        -0.4662021 ,  0.48864824, -0.5171555 ,  0.4844125 , -0.5262628 ,\n",
              "         0.477885  , -0.4492258 , -0.47038946, -0.44600365,  0.48010665,\n",
              "         0.47036418,  0.44991466,  0.5120382 , -0.5128811 ,  0.52147186],\n",
              "       dtype=float32),\n",
              " 'years': array([ 0.5150971 ,  0.49691513, -0.55158603, -0.50445026, -0.5116863 ,\n",
              "         0.54666233,  0.44238245, -0.42737645, -0.4340589 , -0.53371865,\n",
              "        -0.5313734 ,  0.45691645,  0.4646797 ,  0.53147435, -0.54708654,\n",
              "        -0.47119045,  0.5073125 ,  0.50213397, -0.508263  , -0.4732144 ,\n",
              "        -0.5192828 ,  0.48927918,  0.4902588 ,  0.49679017, -0.46648878,\n",
              "        -0.43705836,  0.45641074,  0.47082704,  0.49435744,  0.5554151 ,\n",
              "        -0.5268709 ,  0.4787862 , -0.45444754, -0.4592162 , -0.45348305,\n",
              "        -0.54381216,  0.55814457, -0.48958635,  0.47888237, -0.5038754 ,\n",
              "         0.47746876, -0.5524671 , -0.5038918 , -0.52464217,  0.5532668 ,\n",
              "         0.510222  ,  0.4742185 ,  0.49815083, -0.53873885,  0.48953152],\n",
              "       dtype=float32),\n",
              " 'markets': array([ 0.53999805,  0.46037585, -0.44892347, -0.46070606, -0.4687025 ,\n",
              "         0.45763963,  0.45604742, -0.49281418, -0.4502714 , -0.47216013,\n",
              "        -0.50032777,  0.50155616,  0.5245783 ,  0.45072708, -0.55826163,\n",
              "        -0.46897793,  0.52560824,  0.50448173, -0.47860014, -0.46958053,\n",
              "        -0.51949364,  0.45194805,  0.52224016,  0.44875425, -0.43707335,\n",
              "        -0.52509385,  0.46293694,  0.4736789 ,  0.5090762 ,  0.47388098,\n",
              "        -0.5094723 ,  0.5192104 , -0.4868449 , -0.5448738 , -0.5067896 ,\n",
              "        -0.50650907,  0.46426013, -0.46826655,  0.5060565 , -0.5340299 ,\n",
              "         0.47952977, -0.51035273, -0.4711621 , -0.48377648,  0.46971563,\n",
              "         0.4612282 ,  0.51524127,  0.46336108, -0.48059684,  0.4499547 ],\n",
              "       dtype=float32),\n",
              " 'price': array([ 0.5490214 ,  0.46421763, -0.51944774, -0.44372192, -0.48044607,\n",
              "         0.4530634 ,  0.49008664, -0.5038575 , -0.44527528, -0.47178057,\n",
              "        -0.48055995,  0.4529299 ,  0.4845617 ,  0.4572728 , -0.55518454,\n",
              "        -0.5067088 ,  0.50668555,  0.45639595, -0.50368977, -0.482228  ,\n",
              "        -0.50435466,  0.5186776 ,  0.44835845,  0.4913742 , -0.50490546,\n",
              "        -0.45318007,  0.47919655,  0.53524446,  0.48733518,  0.44768023,\n",
              "        -0.44331902,  0.45239237, -0.49345452, -0.5145343 , -0.4324408 ,\n",
              "        -0.49345866,  0.5192869 , -0.5000258 ,  0.47908908, -0.44516575,\n",
              "         0.50460166, -0.49153754, -0.43948734, -0.506643  ,  0.48303115,\n",
              "         0.49370638,  0.5179342 ,  0.5099085 , -0.5160842 ,  0.48679852],\n",
              "       dtype=float32),\n",
              " 'invest': array([ 0.5285386 ,  0.47198442, -0.4460071 , -0.4431014 , -0.5062871 ,\n",
              "         0.51360506,  0.48932663, -0.4793509 , -0.44794294, -0.522541  ,\n",
              "        -0.5325645 ,  0.5241696 ,  0.5231706 ,  0.4604986 , -0.55958474,\n",
              "        -0.5037785 ,  0.50289106,  0.5039363 , -0.5248582 , -0.46995765,\n",
              "        -0.4980359 ,  0.48119864,  0.50978714,  0.49469054, -0.5170268 ,\n",
              "        -0.4708715 ,  0.5066715 ,  0.5234093 ,  0.4886065 ,  0.44906402,\n",
              "        -0.5246557 ,  0.5090159 , -0.5322813 , -0.45910367, -0.4334115 ,\n",
              "        -0.44780138,  0.47694352, -0.4789021 ,  0.5345359 , -0.4718373 ,\n",
              "         0.46987677, -0.46464723, -0.49204862, -0.4623615 ,  0.43707776,\n",
              "         0.502539  ,  0.5276228 ,  0.5368213 , -0.4516841 ,  0.47213647],\n",
              "       dtype=float32),\n",
              " 'foreign': array([ 0.5531569 ,  0.4988562 , -0.4704507 , -0.49438816, -0.515705  ,\n",
              "         0.5354711 ,  0.5253917 , -0.49914092, -0.4683417 , -0.48458627,\n",
              "        -0.47438204,  0.5071103 ,  0.49870038,  0.4743288 , -0.5206245 ,\n",
              "        -0.4678104 ,  0.48749882,  0.48792264, -0.48861217, -0.4391071 ,\n",
              "        -0.48418045,  0.44731933,  0.5197924 ,  0.5147265 , -0.5103868 ,\n",
              "        -0.46898323,  0.5081823 ,  0.5502887 ,  0.49186897,  0.48806652,\n",
              "        -0.5201609 ,  0.46903846, -0.4720896 , -0.49530256, -0.47854385,\n",
              "        -0.48498258,  0.47006106, -0.53363425,  0.4555344 , -0.5055336 ,\n",
              "         0.52134156, -0.5290722 , -0.4517372 , -0.45258558,  0.47320876,\n",
              "         0.48088568,  0.47913277,  0.5232968 , -0.47883555,  0.48142856],\n",
              "       dtype=float32),\n",
              " 'operating': array([ 0.5235851 ,  0.5080072 , -0.54577136, -0.45116338, -0.4613806 ,\n",
              "         0.47675198,  0.49917147, -0.5184646 , -0.48588035, -0.48032257,\n",
              "        -0.5568188 ,  0.5508362 ,  0.5523283 ,  0.5279216 , -0.5182425 ,\n",
              "        -0.48483652,  0.48429787,  0.4754602 , -0.48325095, -0.547971  ,\n",
              "        -0.5263486 ,  0.48564607,  0.46908325,  0.49945626, -0.49194664,\n",
              "        -0.51570994,  0.48904502,  0.5213236 ,  0.5136145 ,  0.5338169 ,\n",
              "        -0.55388707,  0.54187006, -0.5411695 , -0.45599705, -0.5106701 ,\n",
              "        -0.4758032 ,  0.5450409 , -0.46366158,  0.5112413 , -0.4703477 ,\n",
              "         0.47510496, -0.5240447 , -0.49034333, -0.53115153,  0.48945978,\n",
              "         0.46651655,  0.5259581 ,  0.44227803, -0.54615706,  0.4825333 ],\n",
              "       dtype=float32),\n",
              " 'management': array([ 0.49335948,  0.43417463, -0.5163593 , -0.48083043, -0.4720622 ,\n",
              "         0.5168662 ,  0.4670491 , -0.48008692, -0.47220865, -0.5158892 ,\n",
              "        -0.49824286,  0.4760197 ,  0.5002064 ,  0.52484965, -0.53146714,\n",
              "        -0.531282  ,  0.49913347,  0.44054288, -0.43190312, -0.49060348,\n",
              "        -0.46351263,  0.5160795 ,  0.5203312 ,  0.47140706, -0.5134376 ,\n",
              "        -0.46492782,  0.5215344 ,  0.5410637 ,  0.44139645,  0.44136712,\n",
              "        -0.4990207 ,  0.5156794 , -0.5364623 , -0.4580318 , -0.47461033,\n",
              "        -0.5112275 ,  0.4518537 , -0.52280223,  0.49793893, -0.4572509 ,\n",
              "         0.46432105, -0.44670054, -0.46517187, -0.43444687,  0.5254875 ,\n",
              "         0.44972277,  0.4930271 ,  0.51217854, -0.5040925 ,  0.5268354 ],\n",
              "       dtype=float32),\n",
              " 'total': array([ 0.47493774,  0.43534604, -0.47381547, -0.525853  , -0.5509299 ,\n",
              "         0.52024823,  0.52332586, -0.5135341 , -0.53639334, -0.4576915 ,\n",
              "        -0.51567626,  0.49798286,  0.5028873 ,  0.5005498 , -0.5636798 ,\n",
              "        -0.5144597 ,  0.5291232 ,  0.47458836, -0.528222  , -0.53086364,\n",
              "        -0.4899243 ,  0.48292965,  0.47421074,  0.5022268 , -0.44179785,\n",
              "        -0.4647277 ,  0.46528628,  0.47958648,  0.49908012,  0.53306556,\n",
              "        -0.47958854,  0.45945057, -0.47803822, -0.44968602, -0.5341041 ,\n",
              "        -0.53639054,  0.48609444, -0.5069229 ,  0.47695327, -0.5290144 ,\n",
              "         0.47285926, -0.47784466, -0.48899567, -0.527085  ,  0.5253271 ,\n",
              "         0.45043293,  0.48098895,  0.5136438 , -0.52913976,  0.4519134 ],\n",
              "       dtype=float32),\n",
              " 'higher': array([ 0.48774344,  0.47376293, -0.5084207 , -0.5082373 , -0.4678372 ,\n",
              "         0.54183125,  0.4601702 , -0.4866629 , -0.5392507 , -0.47908327,\n",
              "        -0.46770057,  0.47619283,  0.4479072 ,  0.51813596, -0.5658723 ,\n",
              "        -0.4975049 ,  0.50636154,  0.45853585, -0.4975722 , -0.50305974,\n",
              "        -0.4802521 ,  0.5097999 ,  0.5013483 ,  0.5142289 , -0.49805495,\n",
              "        -0.50183797,  0.495997  ,  0.47841075,  0.50228786,  0.5340001 ,\n",
              "        -0.49342787,  0.5265219 , -0.49052486, -0.466867  , -0.5142684 ,\n",
              "        -0.45753342,  0.49987975, -0.52938306,  0.4601134 , -0.5130919 ,\n",
              "         0.5059469 , -0.47864306, -0.4923494 , -0.48703378,  0.54439676,\n",
              "         0.46832854,  0.48510206,  0.5421116 , -0.4908601 ,  0.4472429 ],\n",
              "       dtype=float32),\n",
              " 'costs': array([ 0.5187298 ,  0.5169765 , -0.4497702 , -0.5154188 , -0.45186666,\n",
              "         0.5354279 ,  0.5080747 , -0.5010115 , -0.49274904, -0.5269562 ,\n",
              "        -0.5208109 ,  0.5304114 ,  0.4726903 ,  0.44428572, -0.53502667,\n",
              "        -0.52967656,  0.5100513 ,  0.5224381 , -0.4499872 , -0.44740802,\n",
              "        -0.48941904,  0.46133515,  0.49012813,  0.45822605, -0.45974502,\n",
              "        -0.52552354,  0.4501399 ,  0.507196  ,  0.44325545,  0.5046776 ,\n",
              "        -0.51154125,  0.5167431 , -0.47970605, -0.50641423, -0.5279969 ,\n",
              "        -0.47327468,  0.48606077, -0.48269948,  0.49211252, -0.43317863,\n",
              "         0.48559335, -0.5384016 , -0.48949897, -0.49831662,  0.44487193,\n",
              "         0.4983281 ,  0.50417393,  0.53888416, -0.4895588 ,  0.48086616],\n",
              "       dtype=float32),\n",
              " 'also': array([ 0.49762917,  0.52023125, -0.47386003, -0.4778198 , -0.45644394,\n",
              "         0.4528557 ,  0.49670318, -0.52046716, -0.46684834, -0.4805424 ,\n",
              "        -0.48713553,  0.5143006 ,  0.4699412 ,  0.5011742 , -0.496663  ,\n",
              "        -0.50649506,  0.47545707,  0.45742095, -0.5016125 , -0.4871478 ,\n",
              "        -0.48497367,  0.5200459 ,  0.5445643 ,  0.46594924, -0.45542878,\n",
              "        -0.49042264,  0.54046905,  0.51721406,  0.4451716 ,  0.51203436,\n",
              "        -0.49412838,  0.530347  , -0.47529674, -0.50088155, -0.47646245,\n",
              "        -0.463557  ,  0.48584157, -0.49859646,  0.49992195, -0.45506608,\n",
              "         0.5183431 , -0.48004255, -0.46755242, -0.46287057,  0.50020677,\n",
              "         0.48393708,  0.52418965,  0.50146323, -0.46467167,  0.48116967],\n",
              "       dtype=float32),\n",
              " 'periods': array([ 0.5151044 ,  0.48068768, -0.4713866 , -0.50628   , -0.49895948,\n",
              "         0.5116576 ,  0.50128025, -0.49011186, -0.47875476, -0.43895563,\n",
              "        -0.54367113,  0.5372436 ,  0.49313417,  0.52702963, -0.5051991 ,\n",
              "        -0.5333005 ,  0.46789512,  0.49755046, -0.44916055, -0.5263603 ,\n",
              "        -0.44812617,  0.5293738 ,  0.5361363 ,  0.5028225 , -0.4633236 ,\n",
              "        -0.48125988,  0.4557714 ,  0.47829497,  0.45793933,  0.5096447 ,\n",
              "        -0.53181505,  0.45659673, -0.49524945, -0.51282793, -0.47230747,\n",
              "        -0.48203963,  0.51084095, -0.4839003 ,  0.47744665, -0.5336597 ,\n",
              "         0.46625835, -0.5209762 , -0.47254163, -0.5001389 ,  0.49422646,\n",
              "         0.45292628,  0.4725316 ,  0.5059926 , -0.5103167 ,  0.44063532],\n",
              "       dtype=float32),\n",
              " 'purchase': array([ 0.51692617,  0.5223695 , -0.48134968, -0.45615685, -0.54499096,\n",
              "         0.5148479 ,  0.49188155, -0.48696625, -0.4969989 , -0.5113368 ,\n",
              "        -0.53819376,  0.4558714 ,  0.45591205,  0.4593649 , -0.49715772,\n",
              "        -0.51209307,  0.46790186,  0.4502657 , -0.46444923, -0.49184826,\n",
              "        -0.49425107,  0.53581136,  0.519675  ,  0.4943727 , -0.5375549 ,\n",
              "        -0.46614057,  0.508079  ,  0.5013673 ,  0.45005468,  0.46490133,\n",
              "        -0.47564143,  0.47539678, -0.53693724, -0.51626664, -0.51779646,\n",
              "        -0.475777  ,  0.47815484, -0.5435393 ,  0.46594054, -0.52305865,\n",
              "         0.53996384, -0.480163  , -0.5198822 , -0.4890616 ,  0.52777904,\n",
              "         0.48631912,  0.45926246,  0.46338683, -0.4665618 ,  0.44861886],\n",
              "       dtype=float32),\n",
              " 'u.s.': array([ 0.5188333 ,  0.43295982, -0.50561726, -0.46681315, -0.5216072 ,\n",
              "         0.46059436,  0.50852513, -0.48334634, -0.44265723, -0.50054604,\n",
              "        -0.48355612,  0.46361616,  0.5135564 ,  0.44038022, -0.55423605,\n",
              "        -0.43080506,  0.45288855,  0.44732293, -0.44321212, -0.45354572,\n",
              "        -0.50548744,  0.47401896,  0.46650723,  0.4533127 , -0.50564474,\n",
              "        -0.5179354 ,  0.45015165,  0.5311943 ,  0.47734782,  0.5179475 ,\n",
              "        -0.44478688,  0.49242347, -0.50317395, -0.5142372 , -0.500658  ,\n",
              "        -0.49480465,  0.46514022, -0.51911354,  0.43862745, -0.5036804 ,\n",
              "         0.45826158, -0.45896822, -0.42794815, -0.47106722,  0.52094144,\n",
              "         0.48417374,  0.51703346,  0.5254085 , -0.517606  ,  0.4716022 ],\n",
              "       dtype=float32),\n",
              " 'example': array([ 0.52838814,  0.50537384, -0.4592803 , -0.50606143, -0.5047147 ,\n",
              "         0.5385856 ,  0.5120027 , -0.45087096, -0.44832668, -0.4806288 ,\n",
              "        -0.46909177,  0.54524493,  0.45151237,  0.54385334, -0.5423395 ,\n",
              "        -0.5020873 ,  0.49412936,  0.49683487, -0.48484805, -0.439485  ,\n",
              "        -0.51103836,  0.49176115,  0.48862353,  0.5087659 , -0.4481856 ,\n",
              "        -0.5354907 ,  0.49519065,  0.5373579 ,  0.4565598 ,  0.47596166,\n",
              "        -0.4855165 ,  0.47414017, -0.5422679 , -0.5143724 , -0.49716896,\n",
              "        -0.44237792,  0.50227857, -0.5346057 ,  0.46039405, -0.45746255,\n",
              "         0.50269157, -0.5443702 , -0.50456166, -0.52190226,  0.48083982,\n",
              "         0.48825514,  0.5254563 ,  0.46329618, -0.49245256,  0.4716337 ],\n",
              "       dtype=float32),\n",
              " 'sales': array([ 0.46166205,  0.5043392 , -0.49836868, -0.4643493 , -0.46322522,\n",
              "         0.5119189 ,  0.5413877 , -0.5382111 , -0.46357563, -0.49815118,\n",
              "        -0.46883547,  0.4841587 ,  0.44062543,  0.4542274 , -0.5673115 ,\n",
              "        -0.4488595 ,  0.5480754 ,  0.46613213, -0.45113844, -0.52013683,\n",
              "        -0.5081792 ,  0.4422245 ,  0.46721   ,  0.47236687, -0.5165904 ,\n",
              "        -0.47833544,  0.5061432 ,  0.48103026,  0.5060862 ,  0.47045043,\n",
              "        -0.4575823 ,  0.4188702 , -0.50081456, -0.4560407 , -0.5052385 ,\n",
              "        -0.4437    ,  0.53765535, -0.4450413 ,  0.5309222 , -0.48382849,\n",
              "         0.47154713, -0.46073472, -0.47885662, -0.51778984,  0.49424666,\n",
              "         0.51043534,  0.45074174,  0.4637183 , -0.48848742,  0.47349536],\n",
              "       dtype=float32),\n",
              " 'table': array([ 0.47199684,  0.50219053, -0.52492136, -0.47323447, -0.50893897,\n",
              "         0.5251394 ,  0.45128328, -0.5010723 , -0.45538154, -0.4502527 ,\n",
              "        -0.4898495 ,  0.5193894 ,  0.5248177 ,  0.5187714 , -0.53771627,\n",
              "        -0.48411185,  0.49308157,  0.53743345, -0.48290053, -0.50283414,\n",
              "        -0.5237342 ,  0.4770717 ,  0.50030804,  0.48953843, -0.46407145,\n",
              "        -0.52024597,  0.49622488,  0.5048207 ,  0.49787152,  0.45649317,\n",
              "        -0.5355264 ,  0.48124802, -0.46225622, -0.5305454 , -0.44185033,\n",
              "        -0.5268924 ,  0.5102634 , -0.4506975 ,  0.5176017 , -0.47713998,\n",
              "         0.5344323 , -0.48397887, -0.50401425, -0.5041029 ,  0.4557754 ,\n",
              "         0.46133363,  0.47899196,  0.50897133, -0.4745865 ,  0.4470367 ],\n",
              "       dtype=float32),\n",
              " 'rate': array([ 0.48448604,  0.5334236 , -0.4746017 , -0.5062741 , -0.545077  ,\n",
              "         0.46755934,  0.48848775, -0.45588887, -0.4754995 , -0.50565064,\n",
              "        -0.46112677,  0.5370453 ,  0.54411566,  0.4835895 , -0.54382455,\n",
              "        -0.5122433 ,  0.46469703,  0.46523118, -0.47672427, -0.5063857 ,\n",
              "        -0.54281086,  0.45777276,  0.4965038 ,  0.48501825, -0.47892487,\n",
              "        -0.45898825,  0.46146443,  0.48737192,  0.4494916 ,  0.45881328,\n",
              "        -0.4944694 ,  0.46847358, -0.5247789 , -0.47010124, -0.5186203 ,\n",
              "        -0.53387654,  0.53544587, -0.5284807 ,  0.48373643, -0.45676416,\n",
              "         0.4872191 , -0.46715426, -0.5003224 , -0.5395703 ,  0.49340346,\n",
              "         0.5135957 ,  0.52257735,  0.54621136, -0.46913934,  0.46150666],\n",
              "       dtype=float32),\n",
              " 'rates': array([ 0.5036814 ,  0.4357044 , -0.4783904 , -0.5259934 , -0.46553305,\n",
              "         0.45656365,  0.45166606, -0.50130475, -0.46573743, -0.51608336,\n",
              "        -0.47603893,  0.49938178,  0.45012677,  0.52081805, -0.50297445,\n",
              "        -0.46952447,  0.5045993 ,  0.5096719 , -0.5037129 , -0.47905588,\n",
              "        -0.46514675,  0.4366929 ,  0.5179341 ,  0.48860228, -0.5065318 ,\n",
              "        -0.48095694,  0.4888935 ,  0.5466328 ,  0.46636128,  0.44233304,\n",
              "        -0.5356451 ,  0.5234975 , -0.50991803, -0.50263494, -0.47666064,\n",
              "        -0.5165721 ,  0.4752396 , -0.50763726,  0.43654203, -0.5040908 ,\n",
              "         0.51302016, -0.457011  , -0.5017183 , -0.45119497,  0.5214502 ,\n",
              "         0.5200812 ,  0.50922203,  0.47238088, -0.51138365,  0.4348993 ],\n",
              "       dtype=float32),\n",
              " 'assets': array([ 0.46718675,  0.48652846, -0.49380803, -0.49966282, -0.5038582 ,\n",
              "         0.47963414,  0.47523946, -0.5328596 , -0.50115913, -0.47195825,\n",
              "        -0.4846908 ,  0.5261227 ,  0.45001337,  0.4642882 , -0.5375302 ,\n",
              "        -0.45037848,  0.50013566,  0.49226913, -0.50000495, -0.4379754 ,\n",
              "        -0.48048276,  0.48256674,  0.5056695 ,  0.5245159 , -0.49715137,\n",
              "        -0.5307211 ,  0.51138556,  0.5391933 ,  0.4507164 ,  0.47079656,\n",
              "        -0.469773  ,  0.4947224 , -0.4788812 , -0.49757332, -0.4373355 ,\n",
              "        -0.4911089 ,  0.46728376, -0.4524621 ,  0.45879033, -0.45705515,\n",
              "         0.48046836, -0.46153545, -0.47618207, -0.48597783,  0.5248072 ,\n",
              "         0.4737623 ,  0.5166868 ,  0.5251408 , -0.4625145 ,  0.4395308 ],\n",
              "       dtype=float32),\n",
              " 'account': array([ 0.47428963,  0.49237683, -0.4790554 , -0.53058875, -0.4854971 ,\n",
              "         0.51295   ,  0.44066262, -0.4971252 , -0.48573816, -0.48738238,\n",
              "        -0.45243287,  0.52857256,  0.44483522,  0.46380854, -0.48459667,\n",
              "        -0.52262   ,  0.4784973 ,  0.52975184, -0.4501072 , -0.5063682 ,\n",
              "        -0.51318264,  0.4970421 ,  0.4653523 ,  0.46790868, -0.459079  ,\n",
              "        -0.49542573,  0.44818887,  0.4615183 ,  0.45431867,  0.5089234 ,\n",
              "        -0.46908724,  0.4648121 , -0.53906643, -0.5412404 , -0.5149944 ,\n",
              "        -0.53382593,  0.48916286, -0.4670475 ,  0.5033116 , -0.47205412,\n",
              "         0.4668169 , -0.4677419 , -0.46928853, -0.47073874,  0.53144455,\n",
              "         0.5209243 ,  0.48130295,  0.49270362, -0.50961584,  0.53326   ],\n",
              "       dtype=float32),\n",
              " 'subject': array([ 0.51785624,  0.5171864 , -0.4932508 , -0.52649623, -0.5036834 ,\n",
              "         0.50529873,  0.48658273, -0.4982607 , -0.46922782, -0.51235855,\n",
              "        -0.44646084,  0.48733062,  0.5169777 ,  0.4970565 , -0.56320983,\n",
              "        -0.4680499 ,  0.4484472 ,  0.4485986 , -0.52226084, -0.44554505,\n",
              "        -0.4384642 ,  0.4330689 ,  0.5292899 ,  0.47623354, -0.46084288,\n",
              "        -0.48055685,  0.45613238,  0.5505039 ,  0.47656187,  0.49879628,\n",
              "        -0.5067014 ,  0.47424635, -0.544951  , -0.47008425, -0.5144038 ,\n",
              "        -0.5009454 ,  0.5264539 , -0.44191208,  0.47394562, -0.5356699 ,\n",
              "         0.5032157 , -0.44501093, -0.45784473, -0.5066165 ,  0.47239336,\n",
              "         0.5294182 ,  0.49049222,  0.46105343, -0.48420858,  0.5042943 ],\n",
              "       dtype=float32),\n",
              " 'time': array([ 0.49650213,  0.44735634, -0.47895306, -0.45453137, -0.51926434,\n",
              "         0.47279027,  0.4788843 , -0.46362382, -0.51915234, -0.49898875,\n",
              "        -0.5245478 ,  0.44767693,  0.5187674 ,  0.45541143, -0.49807447,\n",
              "        -0.48693767,  0.47770306,  0.49300462, -0.5211574 , -0.51135707,\n",
              "        -0.45470858,  0.49532092,  0.52906084,  0.50002545, -0.4388042 ,\n",
              "        -0.5217911 ,  0.47087747,  0.54099333,  0.5026081 ,  0.48271206,\n",
              "        -0.46179217,  0.50999665, -0.5219628 , -0.4714496 , -0.4926651 ,\n",
              "        -0.4519217 ,  0.48939025, -0.47326455,  0.5127082 , -0.45913708,\n",
              "         0.48079318, -0.5175983 , -0.5127083 , -0.4880407 ,  0.47219294,\n",
              "         0.5333512 ,  0.51278496,  0.49774057, -0.47528726,  0.46289665],\n",
              "       dtype=float32),\n",
              " 'changes': array([ 0.52260447,  0.5060945 , -0.5115872 , -0.46772453, -0.50005114,\n",
              "         0.46557716,  0.5078282 , -0.52481556, -0.51195693, -0.44779822,\n",
              "        -0.5150097 ,  0.5002903 ,  0.48961666,  0.45873323, -0.52810204,\n",
              "        -0.48399153,  0.49072292,  0.5206493 , -0.5192476 , -0.46189943,\n",
              "        -0.4941427 ,  0.4849472 ,  0.5223927 ,  0.49149606, -0.48045033,\n",
              "        -0.4735335 ,  0.4382228 ,  0.49585637,  0.4375828 ,  0.4714092 ,\n",
              "        -0.48021352,  0.45165694, -0.54997396, -0.48871687, -0.43176112,\n",
              "        -0.46758276,  0.5347281 , -0.45411736,  0.44381666, -0.46848857,\n",
              "         0.5072832 , -0.47237754, -0.5130762 , -0.47226322,  0.44668186,\n",
              "         0.43520266,  0.48527938,  0.49047062, -0.48179752,  0.5193973 ],\n",
              "       dtype=float32),\n",
              " 'tax': array([ 0.47086877,  0.50534254, -0.43678495, -0.44222116, -0.5042013 ,\n",
              "         0.4712677 ,  0.43529868, -0.47619945, -0.490331  , -0.47967398,\n",
              "        -0.51378673,  0.5013321 ,  0.4558423 ,  0.5082781 , -0.5574865 ,\n",
              "        -0.5086707 ,  0.46799293,  0.51089704, -0.4538728 , -0.5145247 ,\n",
              "        -0.42774752,  0.5064166 ,  0.47935617,  0.44949237, -0.47705308,\n",
              "        -0.47260407,  0.44921276,  0.5528782 ,  0.43301645,  0.47816968,\n",
              "        -0.44436544,  0.52004606, -0.52525914, -0.50504816, -0.4973182 ,\n",
              "        -0.53244287,  0.52276766, -0.5154484 ,  0.49949998, -0.45124307,\n",
              "         0.4460242 , -0.51524204, -0.49606505, -0.50263107,  0.52224666,\n",
              "         0.4986015 ,  0.5011041 ,  0.44538525, -0.49538594,  0.5122108 ],\n",
              "       dtype=float32),\n",
              " 'certain': array([ 0.5182178 ,  0.4579903 , -0.4491307 , -0.49348634, -0.49796152,\n",
              "         0.46582767,  0.51207453, -0.51971656, -0.50667816, -0.5038204 ,\n",
              "        -0.42960757,  0.5251559 ,  0.4674748 ,  0.4736108 , -0.5563711 ,\n",
              "        -0.46400356,  0.4472631 ,  0.4459295 , -0.5123378 , -0.44432116,\n",
              "        -0.4730026 ,  0.52004594,  0.4543755 ,  0.43024164, -0.5146965 ,\n",
              "        -0.49514222,  0.45368028,  0.50931174,  0.45812556,  0.5133604 ,\n",
              "        -0.45824116,  0.4784113 , -0.4860885 , -0.45959213, -0.47541398,\n",
              "        -0.51072776,  0.48897463, -0.49533746,  0.5059295 , -0.5227594 ,\n",
              "         0.4645271 , -0.4684443 , -0.43359885, -0.476039  ,  0.49779463,\n",
              "         0.49606663,  0.5147691 ,  0.5205035 , -0.4566273 ,  0.46245256],\n",
              "       dtype=float32),\n",
              " 'and/or': array([ 0.48976374,  0.43332732, -0.5398924 , -0.4625366 , -0.47249377,\n",
              "         0.46550262,  0.4607196 , -0.46113944, -0.4540573 , -0.4589985 ,\n",
              "        -0.49892074,  0.4863528 ,  0.48516902,  0.47731897, -0.51704735,\n",
              "        -0.45199996,  0.48183763,  0.49630636, -0.51766276, -0.52079797,\n",
              "        -0.52335167,  0.49847457,  0.48203498,  0.5166404 , -0.48422042,\n",
              "        -0.47810593,  0.4836657 ,  0.47464582,  0.49625325,  0.49578136,\n",
              "        -0.4731011 ,  0.5281926 , -0.4708144 , -0.4680995 , -0.5041052 ,\n",
              "        -0.49298126,  0.5115897 , -0.5158557 ,  0.45851827, -0.46724737,\n",
              "         0.525881  , -0.49695584, -0.46803075, -0.458902  ,  0.4998685 ,\n",
              "         0.5359248 ,  0.4847551 ,  0.52796733, -0.4538146 ,  0.47195736],\n",
              "       dtype=float32),\n",
              " 'could': array([ 0.48547167,  0.4705981 , -0.49311417, -0.5190736 , -0.49061286,\n",
              "         0.53281   ,  0.4516653 , -0.5282341 , -0.47638234, -0.4928426 ,\n",
              "        -0.4892347 ,  0.4907171 ,  0.5111097 ,  0.45211163, -0.5248813 ,\n",
              "        -0.5227104 ,  0.54402065,  0.48038527, -0.46875244, -0.5125874 ,\n",
              "        -0.45899403,  0.52137923,  0.48613834,  0.50193286, -0.46126702,\n",
              "        -0.47185406,  0.4478093 ,  0.47751158,  0.5140836 ,  0.5177417 ,\n",
              "        -0.50092584,  0.4899219 , -0.4635881 , -0.47963893, -0.48486185,\n",
              "        -0.50546175,  0.45233473, -0.44846675,  0.4519831 , -0.46815476,\n",
              "         0.48708284, -0.47234964, -0.46846575, -0.51405156,  0.50588465,\n",
              "         0.45099354,  0.5215366 ,  0.50457776, -0.51317525,  0.47478327],\n",
              "       dtype=float32),\n",
              " 'economic': array([ 0.4789417 ,  0.4355834 , -0.47244713, -0.5270525 , -0.5242545 ,\n",
              "         0.5234561 ,  0.5351942 , -0.52504516, -0.51161003, -0.49242634,\n",
              "        -0.5198728 ,  0.4529737 ,  0.4626851 ,  0.49536556, -0.5009515 ,\n",
              "        -0.43325144,  0.5162869 ,  0.47633588, -0.4390085 , -0.5201362 ,\n",
              "        -0.43765667,  0.44822875,  0.46548456,  0.49160382, -0.438257  ,\n",
              "        -0.46494627,  0.4257818 ,  0.5003994 ,  0.44226488,  0.4866339 ,\n",
              "        -0.4806045 ,  0.4803171 , -0.52609885, -0.514332  , -0.4885015 ,\n",
              "        -0.504471  ,  0.5152368 , -0.49338222,  0.50985134, -0.49734724,\n",
              "         0.45108512, -0.45430446, -0.49526912, -0.48093787,  0.4794665 ,\n",
              "         0.45635605,  0.45147258,  0.48772067, -0.47179633,  0.43375063],\n",
              "       dtype=float32),\n",
              " 'including': array([ 0.49111515,  0.44800615, -0.4552611 , -0.46016663, -0.5177272 ,\n",
              "         0.5064546 ,  0.5338149 , -0.4575246 , -0.52089685, -0.49667522,\n",
              "        -0.518441  ,  0.44899422,  0.4927748 ,  0.49466178, -0.51714575,\n",
              "        -0.52054965,  0.48216113,  0.47020864, -0.5020627 , -0.5123635 ,\n",
              "        -0.45830715,  0.46651164,  0.44844663,  0.49515113, -0.50324476,\n",
              "        -0.4746519 ,  0.44333342,  0.46732894,  0.4945426 ,  0.47231874,\n",
              "        -0.5281491 ,  0.4777853 , -0.48298594, -0.5250691 , -0.48966554,\n",
              "        -0.47097275,  0.53792   , -0.47371164,  0.4665531 , -0.4962821 ,\n",
              "         0.45434374, -0.4617526 , -0.4485657 , -0.52110296,  0.50242454,\n",
              "         0.5122553 ,  0.49813086,  0.5034961 , -0.46629024,  0.5239718 ],\n",
              "       dtype=float32),\n",
              " 'debt': array([ 0.46899018,  0.4988103 , -0.49201664, -0.46794772, -0.4851656 ,\n",
              "         0.53173494,  0.5244732 , -0.5159311 , -0.47036844, -0.50675976,\n",
              "        -0.49281228,  0.49679223,  0.4668318 ,  0.47736597, -0.54316473,\n",
              "        -0.5227742 ,  0.4675687 ,  0.5251532 , -0.5179186 , -0.5125329 ,\n",
              "        -0.47498497,  0.47346598,  0.51325905,  0.47723642, -0.431908  ,\n",
              "        -0.5034627 ,  0.4375831 ,  0.5301476 ,  0.4438803 ,  0.4763014 ,\n",
              "        -0.47740173,  0.489968  , -0.45603776, -0.48158804, -0.5134678 ,\n",
              "        -0.49792203,  0.47846574, -0.46963197,  0.49517485, -0.5192806 ,\n",
              "         0.44968748, -0.49140725, -0.43527144, -0.48040804,  0.47238767,\n",
              "         0.49488688,  0.48522723,  0.44297162, -0.5157078 ,  0.47893658],\n",
              "       dtype=float32),\n",
              " 'intermediary': array([ 0.46110612,  0.45346776, -0.44162208, -0.5181307 , -0.5307068 ,\n",
              "         0.5064373 ,  0.52302676, -0.44429708, -0.4742434 , -0.44662246,\n",
              "        -0.49041513,  0.48542458,  0.5115153 ,  0.48114672, -0.5312983 ,\n",
              "        -0.51582944,  0.54385084,  0.5117319 , -0.45883286, -0.4660554 ,\n",
              "        -0.5301147 ,  0.5049741 ,  0.5355058 ,  0.44273296, -0.51958185,\n",
              "        -0.45777717,  0.50446635,  0.4754291 ,  0.47390625,  0.46161088,\n",
              "        -0.5257388 ,  0.51258266, -0.50037444, -0.44641173, -0.44829088,\n",
              "        -0.506414  ,  0.48975724, -0.5367424 ,  0.53450525, -0.46744987,\n",
              "         0.48001197, -0.53365105, -0.49666384, -0.5164329 ,  0.46533713,\n",
              "         0.45372838,  0.4385778 ,  0.47784463, -0.45206216,  0.48564643],\n",
              "       dtype=float32),\n",
              " 'pay': array([ 0.50986224,  0.50733334, -0.51198924, -0.45415485, -0.48315087,\n",
              "         0.44974837,  0.478988  , -0.5343362 , -0.47766683, -0.440188  ,\n",
              "        -0.5159955 ,  0.4995789 ,  0.50044966,  0.5324736 , -0.54151803,\n",
              "        -0.47833657,  0.5028031 ,  0.49909002, -0.5186266 , -0.48997954,\n",
              "        -0.52823186,  0.47416827,  0.5166046 ,  0.4820619 , -0.5147344 ,\n",
              "        -0.46082774,  0.5167354 ,  0.5184321 ,  0.51216847,  0.46020496,\n",
              "        -0.53270936,  0.47798777, -0.4735776 , -0.52427936, -0.505505  ,\n",
              "        -0.46148828,  0.49262616, -0.5178408 ,  0.5260293 , -0.5080853 ,\n",
              "         0.53960794, -0.49701723, -0.49322218, -0.44899285,  0.522017  ,\n",
              "         0.50835204,  0.4754045 ,  0.46079636, -0.5082216 ,  0.48857784],\n",
              "       dtype=float32),\n",
              " 'less': array([ 0.5319859 ,  0.45601827, -0.50458497, -0.44319734, -0.50183016,\n",
              "         0.47283486,  0.5065069 , -0.49778578, -0.5050981 , -0.45836926,\n",
              "        -0.4760161 ,  0.4935118 ,  0.5068425 ,  0.44287407, -0.5712106 ,\n",
              "        -0.46623883,  0.46295038,  0.46028757, -0.45541635, -0.5241951 ,\n",
              "        -0.44104278,  0.52007437,  0.47150466,  0.47296152, -0.49235767,\n",
              "        -0.5191982 ,  0.4564408 ,  0.54238665,  0.46674454,  0.502146  ,\n",
              "        -0.45473507,  0.4854778 , -0.48717776, -0.4678924 , -0.46945435,\n",
              "        -0.4781423 ,  0.5295038 , -0.5350693 ,  0.5127264 , -0.48006767,\n",
              "         0.52215385, -0.49889195, -0.4816314 , -0.49103147,  0.49037194,\n",
              "         0.454894  ,  0.47817832,  0.47943544, -0.45443603,  0.4898575 ],\n",
              "       dtype=float32),\n",
              " 'shown': array([ 0.5493097 ,  0.43689045, -0.5329705 , -0.49702978, -0.5205863 ,\n",
              "         0.52979445,  0.48738787, -0.46739528, -0.5211914 , -0.5130195 ,\n",
              "        -0.49402475,  0.51820445,  0.4848684 ,  0.48113188, -0.5694469 ,\n",
              "        -0.5055062 ,  0.48988038,  0.45830968, -0.47691852, -0.47789335,\n",
              "        -0.5138037 ,  0.5273973 ,  0.523688  ,  0.45579043, -0.5008159 ,\n",
              "        -0.53101254,  0.52938545,  0.48838577,  0.4826976 ,  0.49713212,\n",
              "        -0.51365685,  0.4666322 , -0.46282193, -0.47749424, -0.43712232,\n",
              "        -0.53466845,  0.4945801 , -0.536245  ,  0.5116691 , -0.45550004,\n",
              "         0.45017606, -0.47439268, -0.52151257, -0.46920776,  0.4553104 ,\n",
              "         0.476513  ,  0.5045933 ,  0.5207554 , -0.49987212,  0.5052377 ],\n",
              "       dtype=float32),\n",
              " 'would': array([ 0.50901777,  0.47221485, -0.5277134 , -0.49342853, -0.4954853 ,\n",
              "         0.49279293,  0.49938247, -0.4704846 , -0.46317998, -0.5133956 ,\n",
              "        -0.49124083,  0.4482236 ,  0.48316002,  0.50233614, -0.5201444 ,\n",
              "        -0.44927004,  0.5370853 ,  0.5078387 , -0.44840384, -0.47368276,\n",
              "        -0.44624165,  0.4667128 ,  0.5236342 ,  0.49129558, -0.4517859 ,\n",
              "        -0.51575327,  0.4956473 ,  0.5378543 ,  0.45239887,  0.5160357 ,\n",
              "        -0.53811264,  0.45509312, -0.5223282 , -0.45758006, -0.4615549 ,\n",
              "        -0.54055214,  0.4768585 , -0.4990583 ,  0.4786109 , -0.51562655,\n",
              "         0.49846566, -0.52752584, -0.4574367 , -0.47821292,  0.48093867,\n",
              "         0.4998774 ,  0.48697764,  0.5306708 , -0.44660097,  0.5215723 ],\n",
              "       dtype=float32),\n",
              " 'security': array([ 0.5039277 ,  0.488459  , -0.5331912 , -0.52379274, -0.52220434,\n",
              "         0.45854533,  0.48971152, -0.5351143 , -0.52932554, -0.5148635 ,\n",
              "        -0.45100743,  0.48193806,  0.5105585 ,  0.43950242, -0.5408947 ,\n",
              "        -0.45733857,  0.45041543,  0.50730675, -0.5016689 , -0.43501252,\n",
              "        -0.46158814,  0.52159554,  0.4778261 ,  0.5180196 , -0.4671037 ,\n",
              "        -0.5156861 ,  0.49173665,  0.54791963,  0.43269452,  0.46678758,\n",
              "        -0.49722505,  0.5010523 , -0.45828754, -0.45192996, -0.4769229 ,\n",
              "        -0.462308  ,  0.47319126, -0.47604093,  0.4391603 , -0.44518948,\n",
              "         0.4807483 , -0.53316224, -0.46432093, -0.48869568,  0.47097456,\n",
              "         0.45017177,  0.47296137,  0.5041077 , -0.5136177 ,  0.48277363],\n",
              "       dtype=float32),\n",
              " 'capital': array([ 0.5128819 ,  0.48039675, -0.5152836 , -0.4411786 , -0.5108893 ,\n",
              "         0.52436405,  0.480653  , -0.47904775, -0.52139956, -0.45144132,\n",
              "        -0.47247094,  0.5097463 ,  0.46438327,  0.48863474, -0.5076952 ,\n",
              "        -0.4614603 ,  0.5066677 ,  0.4526025 , -0.44688532, -0.49400306,\n",
              "        -0.46605843,  0.47577092,  0.5223032 ,  0.46405566, -0.48489916,\n",
              "        -0.4996126 ,  0.47534695,  0.5259388 ,  0.45664188,  0.47346687,\n",
              "        -0.46411473,  0.48162776, -0.4592209 , -0.52223194, -0.44347697,\n",
              "        -0.47720823,  0.53548354, -0.49955678,  0.48734277, -0.5245963 ,\n",
              "         0.52980775, -0.46808374, -0.48950267, -0.46958154,  0.51203066,\n",
              "         0.45339885,  0.44956404,  0.51000345, -0.50906235,  0.47814894],\n",
              "       dtype=float32),\n",
              " 'adviser': array([ 0.51790506,  0.5025385 , -0.5285317 , -0.48483264, -0.4481045 ,\n",
              "         0.5259493 ,  0.5009968 , -0.4485802 , -0.4609038 , -0.49580726,\n",
              "        -0.5139859 ,  0.46812525,  0.5120187 ,  0.5289872 , -0.5162945 ,\n",
              "        -0.50480247,  0.5147933 ,  0.4581072 , -0.51662797, -0.51635677,\n",
              "        -0.45158696,  0.5119234 ,  0.46351337,  0.48647228, -0.51847553,\n",
              "        -0.5088524 ,  0.52013355,  0.47031793,  0.44969803,  0.4863525 ,\n",
              "        -0.49963164,  0.44626763, -0.5059329 , -0.5214455 , -0.47417417,\n",
              "        -0.44020936,  0.465472  , -0.5025609 ,  0.5005314 , -0.5208825 ,\n",
              "         0.46399048, -0.48518708, -0.49741036, -0.46860912,  0.51284647,\n",
              "         0.5014121 ,  0.44480655,  0.5070493 , -0.513915  ,  0.4485724 ],\n",
              "       dtype=float32),\n",
              " 'issuer': array([ 0.52178425,  0.49797314, -0.46385512, -0.44031927, -0.5072745 ,\n",
              "         0.5070712 ,  0.47182858, -0.47119233, -0.4521405 , -0.4341881 ,\n",
              "        -0.4846592 ,  0.51869   ,  0.47939467,  0.46679553, -0.523873  ,\n",
              "        -0.44002607,  0.48019505,  0.51917624, -0.49857548, -0.4721292 ,\n",
              "        -0.44591692,  0.47364447,  0.52529573,  0.52440923, -0.44944796,\n",
              "        -0.46025857,  0.51184255,  0.508404  ,  0.4558561 ,  0.43719476,\n",
              "        -0.46176332,  0.4970019 , -0.5360365 , -0.5421184 , -0.4429468 ,\n",
              "        -0.5249621 ,  0.45779046, -0.4591405 ,  0.4367385 , -0.5305063 ,\n",
              "         0.5050112 , -0.44338277, -0.4564191 , -0.48824823,  0.4956078 ,\n",
              "         0.47053748,  0.5219376 ,  0.44963253, -0.5218271 ,  0.49433962],\n",
              "       dtype=float32),\n",
              " 'generally': array([ 0.46236724,  0.45310006, -0.49985477, -0.519929  , -0.4764563 ,\n",
              "         0.4930194 ,  0.52619493, -0.491115  , -0.46727484, -0.47890782,\n",
              "        -0.45366788,  0.456603  ,  0.49294275,  0.44368467, -0.558529  ,\n",
              "        -0.46545255,  0.4948291 ,  0.52865934, -0.4881743 , -0.46362644,\n",
              "        -0.4759963 ,  0.4834296 ,  0.45151466,  0.51222885, -0.45604682,\n",
              "        -0.5157686 ,  0.45700723,  0.46807212,  0.52021587,  0.5194117 ,\n",
              "        -0.46785295,  0.47744697, -0.47034988, -0.47129002, -0.50538146,\n",
              "        -0.44968978,  0.49259055, -0.522403  ,  0.4739962 , -0.5171624 ,\n",
              "         0.49564427, -0.4926145 , -0.5101594 , -0.48723388,  0.5188717 ,\n",
              "         0.51346433,  0.48135495,  0.4949727 , -0.4519733 ,  0.4721096 ],\n",
              "       dtype=float32),\n",
              " 'principal': array([ 0.47303614,  0.43754014, -0.51845944, -0.485624  , -0.54476047,\n",
              "         0.45521843,  0.5372641 , -0.5345376 , -0.49662513, -0.4594998 ,\n",
              "        -0.47088635,  0.5421183 ,  0.5341053 ,  0.47518656, -0.54005635,\n",
              "        -0.46495944,  0.48455346,  0.48040795, -0.4947337 , -0.5091227 ,\n",
              "        -0.45632794,  0.4712917 ,  0.46226698,  0.5097571 , -0.5118686 ,\n",
              "        -0.51290375,  0.47926757,  0.5016666 ,  0.5279517 ,  0.47454095,\n",
              "        -0.46978426,  0.5081269 , -0.48886892, -0.5081536 , -0.5364382 ,\n",
              "        -0.48912853,  0.5263997 , -0.488053  ,  0.46090147, -0.4735243 ,\n",
              "         0.5427916 , -0.5029    , -0.49693716, -0.46899706,  0.4770713 ,\n",
              "         0.47844687,  0.46244028,  0.51230305, -0.46635428,  0.47779238],\n",
              "       dtype=float32),\n",
              " 'prospectus': array([ 0.52482516,  0.4653451 , -0.45743898, -0.47674882, -0.4620292 ,\n",
              "         0.4880896 ,  0.47386158, -0.499936  , -0.48257014, -0.44536877,\n",
              "        -0.5264476 ,  0.5149764 ,  0.46784517,  0.49661422, -0.5547012 ,\n",
              "        -0.5276646 ,  0.53206646,  0.5294544 , -0.49933395, -0.44380233,\n",
              "        -0.44095367,  0.4648559 ,  0.47474518,  0.49174336, -0.45589477,\n",
              "        -0.4966156 ,  0.50136244,  0.5467265 ,  0.4546668 ,  0.49299395,\n",
              "        -0.47558895,  0.4426324 , -0.47747588, -0.50922775, -0.4406578 ,\n",
              "        -0.452212  ,  0.47743785, -0.48104545,  0.48154634, -0.49556267,\n",
              "         0.46506205, -0.53851455, -0.5128535 , -0.47503284,  0.5260916 ,\n",
              "         0.49472383,  0.48688513,  0.48979992, -0.5164257 ,  0.50235033],\n",
              "       dtype=float32),\n",
              " 'average': array([ 0.47863865,  0.4830952 , -0.5443368 , -0.4864847 , -0.50371855,\n",
              "         0.51827013,  0.5020766 , -0.47008944, -0.47535262, -0.4852556 ,\n",
              "        -0.48144266,  0.48245564,  0.5217367 ,  0.5029187 , -0.5232512 ,\n",
              "        -0.48672712,  0.5309902 ,  0.48923397, -0.44895396, -0.5036364 ,\n",
              "        -0.4774574 ,  0.45764267,  0.52842206,  0.53290117, -0.46665487,\n",
              "        -0.49640146,  0.4503515 ,  0.5371132 ,  0.528265  ,  0.48616698,\n",
              "        -0.50422275,  0.48015785, -0.46384168, -0.50823265, -0.49231675,\n",
              "        -0.5340086 ,  0.5443871 , -0.4595484 ,  0.45796603, -0.4723975 ,\n",
              "         0.5223978 , -0.5440225 , -0.50825834, -0.4543606 ,  0.50714856,\n",
              "         0.4432456 ,  0.51022696,  0.45658642, -0.49073926,  0.50402164],\n",
              "       dtype=float32),\n",
              " 'stocks': array([ 0.5468752 ,  0.4741459 , -0.4679582 , -0.452398  , -0.4813485 ,\n",
              "         0.4997449 ,  0.50400823, -0.5079776 , -0.5066252 , -0.49874467,\n",
              "        -0.45418072,  0.44591388,  0.48363903,  0.50192225, -0.503616  ,\n",
              "        -0.50636244,  0.49902254,  0.51882577, -0.48660156, -0.49717698,\n",
              "        -0.45804676,  0.45088217,  0.48680633,  0.4796204 , -0.50668675,\n",
              "        -0.47073352,  0.46840262,  0.48464635,  0.460738  ,  0.4713952 ,\n",
              "        -0.5232002 ,  0.502402  , -0.48482442, -0.5315617 , -0.49965993,\n",
              "        -0.52277416,  0.5019857 , -0.47286394,  0.4810068 , -0.46701288,\n",
              "         0.47119924, -0.45525372, -0.47581443, -0.4499355 ,  0.45342702,\n",
              "         0.5089293 ,  0.47260484,  0.4846014 , -0.5244024 ,  0.47527456],\n",
              "       dtype=float32),\n",
              " 'federal': array([ 0.5037949 ,  0.48486036, -0.49881446, -0.5128262 , -0.47505793,\n",
              "         0.44793847,  0.50208277, -0.43764532, -0.4788928 , -0.44946122,\n",
              "        -0.496023  ,  0.52862597,  0.5199109 ,  0.46944773, -0.50970083,\n",
              "        -0.49225658,  0.49753165,  0.45404914, -0.51214147, -0.45496112,\n",
              "        -0.47462487,  0.52557296,  0.4940809 ,  0.49084985, -0.5014757 ,\n",
              "        -0.4516692 ,  0.4332619 ,  0.47895893,  0.4420642 ,  0.5145964 ,\n",
              "        -0.44777435,  0.45868027, -0.46167317, -0.48867726, -0.4846731 ,\n",
              "        -0.50994235,  0.5243534 , -0.48621023,  0.44647673, -0.5023666 ,\n",
              "         0.49739924, -0.5038076 , -0.45578283, -0.48077255,  0.53334373,\n",
              "         0.4848897 ,  0.5021721 ,  0.4939458 , -0.47197434,  0.5065035 ],\n",
              "       dtype=float32),\n",
              " 'manager': array([ 0.48561424,  0.49998987, -0.49666965, -0.5286113 , -0.49809986,\n",
              "         0.52433467,  0.46331125, -0.4948231 , -0.5168818 , -0.49506435,\n",
              "        -0.50478995,  0.51982915,  0.49004063,  0.4505643 , -0.5474446 ,\n",
              "        -0.45188668,  0.5346766 ,  0.48836237, -0.4636911 , -0.47987813,\n",
              "        -0.45439827,  0.51360625,  0.49614462,  0.49767122, -0.48585004,\n",
              "        -0.50052404,  0.50191283,  0.5601702 ,  0.44592628,  0.45078462,\n",
              "        -0.4886647 ,  0.50795263, -0.49657717, -0.51068115, -0.45664707,\n",
              "        -0.46217185,  0.5292801 , -0.50115335,  0.49699974, -0.5425192 ,\n",
              "         0.47803652, -0.5211652 , -0.4443084 , -0.47562772,  0.48638254,\n",
              "         0.5178226 ,  0.47989994,  0.4543514 , -0.44915277,  0.44536072],\n",
              "       dtype=float32),\n",
              " 'share': array([ 0.5095743 ,  0.44372663, -0.47430342, -0.493024  , -0.4555454 ,\n",
              "         0.45572558,  0.45438337, -0.47716   , -0.5013889 , -0.5110913 ,\n",
              "        -0.54382986,  0.47574416,  0.45285156,  0.47260618, -0.5448398 ,\n",
              "        -0.47576275,  0.45910063,  0.5226394 , -0.50288147, -0.45528045,\n",
              "        -0.5124898 ,  0.48313215,  0.5203259 ,  0.48908105, -0.493672  ,\n",
              "        -0.4936068 ,  0.51024914,  0.4831143 ,  0.473189  ,  0.52646196,\n",
              "        -0.4758892 ,  0.5024729 , -0.45898393, -0.48415282, -0.5107233 ,\n",
              "        -0.540255  ,  0.5038617 , -0.5071719 ,  0.5072721 , -0.5243166 ,\n",
              "         0.53180015, -0.53454685, -0.46438974, -0.47380272,  0.45700213,\n",
              "         0.52104974,  0.4420826 ,  0.47932532, -0.53086495,  0.50273615],\n",
              "       dtype=float32),\n",
              " 'asset': array([ 0.5238519 ,  0.46880072, -0.5306573 , -0.4471652 , -0.4687683 ,\n",
              "         0.49547014,  0.48876965, -0.46236834, -0.5244533 , -0.50712097,\n",
              "        -0.46687418,  0.48774996,  0.5055298 ,  0.521367  , -0.5685062 ,\n",
              "        -0.47835764,  0.4920049 ,  0.53096396, -0.46863234, -0.47482723,\n",
              "        -0.48886254,  0.47092113,  0.4915343 ,  0.49367878, -0.4646145 ,\n",
              "        -0.4820252 ,  0.5260797 ,  0.5437407 ,  0.46102574,  0.44160822,\n",
              "        -0.4575113 ,  0.5278036 , -0.4826349 , -0.50904167, -0.45647615,\n",
              "        -0.48388183,  0.45385385, -0.49609426,  0.49734804, -0.5063473 ,\n",
              "         0.48435372, -0.52444434, -0.4370838 , -0.4448492 ,  0.53602123,\n",
              "         0.4606187 ,  0.504696  ,  0.4457247 , -0.4511185 ,  0.47061265],\n",
              "       dtype=float32),\n",
              " 'sale': array([ 0.49901032,  0.5304642 , -0.50843495, -0.48334756, -0.4683236 ,\n",
              "         0.5347209 ,  0.49271372, -0.48482242, -0.52973187, -0.5232761 ,\n",
              "        -0.5360657 ,  0.5313578 ,  0.48351714,  0.5165837 , -0.48971903,\n",
              "        -0.5312717 ,  0.51551986,  0.5061503 , -0.5266519 , -0.46138874,\n",
              "        -0.49453565,  0.51352143,  0.5411562 ,  0.50634605, -0.47577038,\n",
              "        -0.48700345,  0.5106894 ,  0.48757482,  0.5127566 ,  0.46655187,\n",
              "        -0.4763287 ,  0.48147923, -0.48336723, -0.5346006 , -0.45137838,\n",
              "        -0.53013027,  0.46144912, -0.5206054 ,  0.52460706, -0.51757324,\n",
              "         0.5493179 , -0.523227  , -0.5291971 , -0.5291828 ,  0.5000218 ,\n",
              "         0.49974445,  0.46594018,  0.48912543, -0.45505303,  0.45952508],\n",
              "       dtype=float32),\n",
              " 'growth': array([ 0.52378416,  0.46398512, -0.48240536, -0.5319106 , -0.46628517,\n",
              "         0.51290524,  0.5276692 , -0.45081893, -0.50971186, -0.4853311 ,\n",
              "        -0.48746136,  0.4873327 ,  0.44809505,  0.4507702 , -0.5545855 ,\n",
              "        -0.5092527 ,  0.47822502,  0.52064675, -0.44554275, -0.487684  ,\n",
              "        -0.45209718,  0.47858804,  0.47767642,  0.48725954, -0.50648355,\n",
              "        -0.45206052,  0.5159886 ,  0.55141145,  0.4630062 ,  0.48578045,\n",
              "        -0.4946894 ,  0.4451361 , -0.54363614, -0.44903964, -0.501314  ,\n",
              "        -0.46038157,  0.5286453 , -0.45974144,  0.4466266 , -0.45223504,\n",
              "         0.5298251 , -0.49783188, -0.43088225, -0.48145992,  0.45492837,\n",
              "         0.49090523,  0.5248154 ,  0.45722988, -0.49626967,  0.5250792 ],\n",
              "       dtype=float32),\n",
              " 'credit': array([ 0.52486384,  0.45885006, -0.492241  , -0.50331867, -0.47069848,\n",
              "         0.47658348,  0.52018213, -0.4428808 , -0.51612914, -0.4837054 ,\n",
              "        -0.44103694,  0.46002743,  0.51389635,  0.50569725, -0.5276437 ,\n",
              "        -0.48188114,  0.5039551 ,  0.44106477, -0.4929161 , -0.45798331,\n",
              "        -0.46088156,  0.51207376,  0.46418867,  0.51129586, -0.44880858,\n",
              "        -0.48320225,  0.43510088,  0.49028108,  0.518155  ,  0.5155381 ,\n",
              "        -0.48316926,  0.48455662, -0.49233863, -0.48355058, -0.47892907,\n",
              "        -0.47322413,  0.51958096, -0.51278055,  0.44937292, -0.52051383,\n",
              "         0.51503646, -0.45285106, -0.45944056, -0.45913744,  0.48305297,\n",
              "         0.4618873 ,  0.47823548,  0.50586563, -0.46483302,  0.48428103],\n",
              "       dtype=float32),\n",
              " 'greater': array([ 0.5211676 ,  0.50775266, -0.53095526, -0.49437904, -0.531124  ,\n",
              "         0.47559583,  0.4880576 , -0.46316764, -0.44398817, -0.47760883,\n",
              "        -0.5174051 ,  0.45910278,  0.46258846,  0.50964487, -0.5865822 ,\n",
              "        -0.45608062,  0.4903334 ,  0.51575136, -0.5171874 , -0.46356797,\n",
              "        -0.47683734,  0.46095294,  0.48786417,  0.5150287 , -0.48544925,\n",
              "        -0.4408397 ,  0.52158463,  0.53300023,  0.5007134 ,  0.5066862 ,\n",
              "        -0.4651074 ,  0.44564205, -0.4870084 , -0.50569856, -0.45385018,\n",
              "        -0.51197606,  0.4791574 , -0.4745764 ,  0.51877236, -0.4658109 ,\n",
              "         0.45928714, -0.5141956 , -0.5004177 , -0.45393077,  0.46983108,\n",
              "         0.48478997,  0.5035265 ,  0.4561068 , -0.51668745,  0.5086387 ],\n",
              "       dtype=float32),\n",
              " 'result': array([ 0.5152629 ,  0.5157516 , -0.4725974 , -0.4527678 , -0.46297264,\n",
              "         0.4793087 ,  0.48531005, -0.5375269 , -0.50858   , -0.5294473 ,\n",
              "        -0.50304353,  0.50644976,  0.5098491 ,  0.5242267 , -0.5185043 ,\n",
              "        -0.5177223 ,  0.46421987,  0.44939893, -0.530176  , -0.43925107,\n",
              "        -0.49536166,  0.47041124,  0.4594869 ,  0.49664348, -0.47625044,\n",
              "        -0.49937966,  0.4643085 ,  0.51401365,  0.46182245,  0.46226934,\n",
              "        -0.5187888 ,  0.51524365, -0.50981367, -0.48675603, -0.46803316,\n",
              "        -0.5257592 ,  0.54155964, -0.5250968 ,  0.48297724, -0.47915372,\n",
              "         0.5184644 , -0.51687086, -0.48036706, -0.46296152,  0.5244531 ,\n",
              "         0.50627065,  0.4916506 ,  0.50142294, -0.4527084 ,  0.4529085 ],\n",
              "       dtype=float32),\n",
              " 'issuers': array([ 0.46431458,  0.4916204 , -0.5017291 , -0.45111078, -0.4562213 ,\n",
              "         0.51923424,  0.47550362, -0.46455863, -0.44080082, -0.51617897,\n",
              "        -0.46366155,  0.4326669 ,  0.4911086 ,  0.4935791 , -0.5380601 ,\n",
              "        -0.46992594,  0.5041824 ,  0.46721113, -0.50751567, -0.4822585 ,\n",
              "        -0.457947  ,  0.4558528 ,  0.45211223,  0.50222266, -0.5008843 ,\n",
              "        -0.52517104,  0.5001398 ,  0.4938009 ,  0.49092156,  0.42820254,\n",
              "        -0.4585338 ,  0.5072608 , -0.5078311 , -0.4577259 , -0.49680856,\n",
              "        -0.5031532 ,  0.46760166, -0.51654136,  0.48043922, -0.47994447,\n",
              "         0.5063623 , -0.47326943, -0.45478398, -0.46327   ,  0.5232437 ,\n",
              "         0.45568192,  0.50863606,  0.4879095 , -0.44253877,  0.5177183 ],\n",
              "       dtype=float32),\n",
              " 'charge': array([ 0.5009924 ,  0.50497365, -0.49045032, -0.45072997, -0.46651584,\n",
              "         0.47413966,  0.5328405 , -0.53088963, -0.5119053 , -0.43767884,\n",
              "        -0.53243977,  0.48096848,  0.4319036 ,  0.5099269 , -0.5475217 ,\n",
              "        -0.4408908 ,  0.5425362 ,  0.5194886 , -0.44978896, -0.52164614,\n",
              "        -0.48601168,  0.49301556,  0.49192035,  0.45836338, -0.47569445,\n",
              "        -0.45785928,  0.5245513 ,  0.4628109 ,  0.50182384,  0.4844522 ,\n",
              "        -0.52075607,  0.42498916, -0.53370476, -0.44304895, -0.46427923,\n",
              "        -0.47827902,  0.4542749 , -0.48031437,  0.4941903 , -0.47625545,\n",
              "         0.50498426, -0.44565323, -0.4591198 , -0.44702616,  0.5188666 ,\n",
              "         0.5307814 ,  0.4526927 ,  0.46385053, -0.47432533,  0.48074886],\n",
              "       dtype=float32),\n",
              " 'affect': array([ 0.54523134,  0.44953912, -0.49592188, -0.48185197, -0.473925  ,\n",
              "         0.48955533,  0.48773283, -0.5067797 , -0.49536267, -0.4805975 ,\n",
              "        -0.46697572,  0.4920575 ,  0.5105424 ,  0.5402963 , -0.5416124 ,\n",
              "        -0.4694751 ,  0.51714885,  0.45388788, -0.5380605 , -0.5169998 ,\n",
              "        -0.4549091 ,  0.48579425,  0.53961724,  0.49372303, -0.50584346,\n",
              "        -0.47593406,  0.50850266,  0.5002711 ,  0.45162097,  0.49375457,\n",
              "        -0.47455296,  0.5015635 , -0.47777525, -0.5042916 , -0.5323528 ,\n",
              "        -0.4907231 ,  0.5273651 , -0.4426271 ,  0.5000088 , -0.49547696,\n",
              "         0.51997894, -0.4771326 , -0.5285869 , -0.44813833,  0.5385981 ,\n",
              "         0.48637432,  0.5196059 ,  0.53260803, -0.51953435,  0.49424177],\n",
              "       dtype=float32),\n",
              " 'equity': array([ 0.4685905 ,  0.473141  , -0.50405   , -0.5176277 , -0.48081905,\n",
              "         0.50893795,  0.48307297, -0.5312885 , -0.47497752, -0.4312182 ,\n",
              "        -0.46291047,  0.4843586 ,  0.5159817 ,  0.51138437, -0.5435455 ,\n",
              "        -0.48151904,  0.4459162 ,  0.4576498 , -0.4591389 , -0.5136113 ,\n",
              "        -0.49208492,  0.5022396 ,  0.47550333,  0.50495845, -0.43865097,\n",
              "        -0.5218386 ,  0.44359857,  0.49096537,  0.45173565,  0.4880688 ,\n",
              "        -0.4636179 ,  0.48303705, -0.5334543 , -0.5013374 , -0.49783844,\n",
              "        -0.4734365 ,  0.4532539 , -0.52623314,  0.5187227 , -0.45352215,\n",
              "         0.46844625, -0.48351339, -0.4858806 , -0.5208541 ,  0.5293112 ,\n",
              "         0.53492963,  0.5180527 ,  0.5234584 , -0.46382087,  0.46282753],\n",
              "       dtype=float32),\n",
              " 'conditions': array([ 0.47345313,  0.466353  , -0.4923417 , -0.5156625 , -0.50473475,\n",
              "         0.5315895 ,  0.52323854, -0.4651014 , -0.48628145, -0.51576567,\n",
              "        -0.48693344,  0.5190341 ,  0.43389672,  0.505     , -0.535228  ,\n",
              "        -0.44390133,  0.50232524,  0.52587557, -0.48075175, -0.5073012 ,\n",
              "        -0.47191003,  0.4981282 ,  0.4729246 ,  0.4977615 , -0.42903042,\n",
              "        -0.47637263,  0.44072503,  0.49150026,  0.44319102,  0.41729107,\n",
              "        -0.4512782 ,  0.45232832, -0.54476124, -0.50853217, -0.43684152,\n",
              "        -0.52182454,  0.45456365, -0.46736676,  0.4636062 , -0.44808108,\n",
              "         0.5139959 , -0.50825536, -0.5041628 , -0.5163453 ,  0.5077218 ,\n",
              "         0.48441228,  0.47634277,  0.5293397 , -0.46252146,  0.46489486],\n",
              "       dtype=float32),\n",
              " 'distributions': array([ 0.56381184,  0.49276593, -0.47908163, -0.4866392 , -0.51982725,\n",
              "         0.4872898 ,  0.46706977, -0.5105603 , -0.525039  , -0.49108762,\n",
              "        -0.49203962,  0.47035167,  0.52511346,  0.48079145, -0.4993138 ,\n",
              "        -0.5413654 ,  0.50692   ,  0.48542857, -0.51325   , -0.456139  ,\n",
              "        -0.5198792 ,  0.4830111 ,  0.51882523,  0.48528233, -0.49304923,\n",
              "        -0.4856211 ,  0.44956017,  0.48331138,  0.467184  ,  0.5267187 ,\n",
              "        -0.47190067,  0.46693283, -0.5124918 , -0.49288502, -0.497086  ,\n",
              "        -0.5280875 ,  0.46367627, -0.51735175,  0.5344195 , -0.4785317 ,\n",
              "         0.53867495, -0.5292058 , -0.48076028, -0.50250745,  0.51349854,\n",
              "         0.5157885 ,  0.5030304 ,  0.52479994, -0.4700996 ,  0.5210685 ],\n",
              "       dtype=float32),\n",
              " 'lower': array([ 0.53839844,  0.49342394, -0.45370388, -0.46093258, -0.4983031 ,\n",
              "         0.49651715,  0.4471756 , -0.4509906 , -0.52953494, -0.5263171 ,\n",
              "        -0.5252785 ,  0.48338038,  0.48315647,  0.5109948 , -0.5643537 ,\n",
              "        -0.50437   ,  0.5397322 ,  0.52854496, -0.5095834 , -0.43179172,\n",
              "        -0.49857208,  0.49136043,  0.48873973,  0.48107508, -0.5101175 ,\n",
              "        -0.44649443,  0.51983714,  0.50899845,  0.46516466,  0.457472  ,\n",
              "        -0.47406906,  0.43887317, -0.4876192 , -0.462652  , -0.44509387,\n",
              "        -0.48696408,  0.5181158 , -0.49384713,  0.5123405 , -0.50052196,\n",
              "         0.4748681 , -0.5360452 , -0.46585387, -0.48339087,  0.44903356,\n",
              "         0.4993448 ,  0.49170327,  0.48516902, -0.4707703 ,  0.46896258],\n",
              "       dtype=float32),\n",
              " 'company': array([ 0.50630957,  0.47306016, -0.4700413 , -0.49470305, -0.48890397,\n",
              "         0.52394915,  0.456994  , -0.50493145, -0.49131143, -0.42671585,\n",
              "        -0.4459172 ,  0.5207742 ,  0.51948977,  0.44762424, -0.5482768 ,\n",
              "        -0.47621405,  0.50282997,  0.5105414 , -0.4808928 , -0.43525735,\n",
              "        -0.5027869 ,  0.45804653,  0.47191727,  0.48593587, -0.48184228,\n",
              "        -0.50948477,  0.49738297,  0.5451002 ,  0.46914855,  0.4973335 ,\n",
              "        -0.51958936,  0.4836836 , -0.46494827, -0.49526793, -0.45427504,\n",
              "        -0.5131802 ,  0.47184741, -0.4973796 ,  0.42978564, -0.5041142 ,\n",
              "         0.52145165, -0.4565028 , -0.5035334 , -0.48035732,  0.48330173,\n",
              "         0.46379828,  0.44736654,  0.46333   , -0.4335119 ,  0.47115374],\n",
              "       dtype=float32),\n",
              " 'sell': array([ 0.47035652,  0.4712404 , -0.5371306 , -0.49610895, -0.5118096 ,\n",
              "         0.47110093,  0.510956  , -0.5087828 , -0.50120205, -0.47607818,\n",
              "        -0.50350946,  0.52040255,  0.46736068,  0.48730725, -0.542927  ,\n",
              "        -0.4572265 ,  0.4726771 ,  0.52302575, -0.472836  , -0.5206486 ,\n",
              "        -0.44823048,  0.4492885 ,  0.5108211 ,  0.50183606, -0.47892565,\n",
              "        -0.48596698,  0.48363203,  0.47730726,  0.4990206 ,  0.4484234 ,\n",
              "        -0.54560226,  0.45978263, -0.4979608 , -0.47809234, -0.5192297 ,\n",
              "        -0.45163342,  0.48891392, -0.48670027,  0.5013709 , -0.5360392 ,\n",
              "         0.509844  , -0.54170465, -0.49785113, -0.5002479 ,  0.50991654,\n",
              "         0.4958866 ,  0.47160226,  0.44635314, -0.5210357 ,  0.52966183],\n",
              "       dtype=float32),\n",
              " 'investors': array([ 0.48088267,  0.50587606, -0.44329378, -0.4779374 , -0.48920926,\n",
              "         0.51161146,  0.44472674, -0.46768162, -0.44034258, -0.44882214,\n",
              "        -0.48328543,  0.51609707,  0.47234643,  0.47378018, -0.5235678 ,\n",
              "        -0.5289388 ,  0.52707255,  0.49130347, -0.48886883, -0.48235187,\n",
              "        -0.4841727 ,  0.47896078,  0.45456073,  0.4971382 , -0.50709015,\n",
              "        -0.48338628,  0.4785011 ,  0.4909846 ,  0.4890373 ,  0.47226262,\n",
              "        -0.5304879 ,  0.5008181 , -0.5183369 , -0.4672264 , -0.4608213 ,\n",
              "        -0.50483406,  0.46804932, -0.5166263 ,  0.48031682, -0.45294648,\n",
              "         0.48338556, -0.49342284, -0.5109212 , -0.5121398 ,  0.50134504,\n",
              "         0.5143453 ,  0.523659  ,  0.44707012, -0.5161066 ,  0.51462585],\n",
              "       dtype=float32),\n",
              " 'net': array([ 0.509023  ,  0.50174606, -0.53489304, -0.4859527 , -0.5156847 ,\n",
              "         0.46983021,  0.48145038, -0.5077703 , -0.52833354, -0.4734085 ,\n",
              "        -0.49822035,  0.48854804,  0.5096818 ,  0.5253404 , -0.56294364,\n",
              "        -0.48638177,  0.45607644,  0.48565337, -0.5078162 , -0.47379702,\n",
              "        -0.46323407,  0.49693373,  0.45321986,  0.5205812 , -0.4392895 ,\n",
              "        -0.52693856,  0.5020404 ,  0.5256044 ,  0.4440308 ,  0.47859895,\n",
              "        -0.5141879 ,  0.47740498, -0.46289468, -0.48404166, -0.45740923,\n",
              "        -0.45699376,  0.49680015, -0.45736402,  0.52468985, -0.4715749 ,\n",
              "         0.46714446, -0.45150033, -0.4872835 , -0.5078774 ,  0.51111215,\n",
              "         0.51542145,  0.46733877,  0.48111778, -0.51562643,  0.5112094 ],\n",
              "       dtype=float32),\n",
              " 'retirement': array([ 0.4902958 ,  0.44333264, -0.52086616, -0.4657635 , -0.47039726,\n",
              "         0.5328934 ,  0.48101038, -0.5162784 , -0.44651297, -0.4493098 ,\n",
              "        -0.4695908 ,  0.44824076,  0.4692096 ,  0.46178353, -0.5155089 ,\n",
              "        -0.4621933 ,  0.47115496,  0.5306221 , -0.51832974, -0.49513966,\n",
              "        -0.4434773 ,  0.46128464,  0.49232602,  0.50666773, -0.46987012,\n",
              "        -0.46182585,  0.48530996,  0.5410966 ,  0.48454952,  0.5237069 ,\n",
              "        -0.5438387 ,  0.50972605, -0.5400864 , -0.48842642, -0.43991584,\n",
              "        -0.48770738,  0.45253217, -0.4666833 ,  0.49458143, -0.507415  ,\n",
              "         0.5191623 , -0.48581183, -0.4493108 , -0.5029109 ,  0.48389572,\n",
              "         0.4961372 ,  0.47739238,  0.50696576, -0.4983173 ,  0.46531788],\n",
              "       dtype=float32),\n",
              " 'turnover': array([ 0.5116079 ,  0.47308797, -0.48329914, -0.53640676, -0.5534379 ,\n",
              "         0.51816815,  0.53818715, -0.5384074 , -0.5456562 , -0.52115494,\n",
              "        -0.5003525 ,  0.5377676 ,  0.51359737,  0.47860077, -0.5008521 ,\n",
              "        -0.48896214,  0.50103843,  0.49594203, -0.53066415, -0.5357941 ,\n",
              "        -0.49575847,  0.45827633,  0.5053851 ,  0.5526658 , -0.473278  ,\n",
              "        -0.47605807,  0.5198343 ,  0.5021907 ,  0.46645114,  0.51182806,\n",
              "        -0.4732332 ,  0.47555482, -0.50485337, -0.49539822, -0.47188428,\n",
              "        -0.5189889 ,  0.50281525, -0.50286806,  0.55265886, -0.471731  ,\n",
              "         0.5053433 , -0.498025  , -0.51400334, -0.4781939 ,  0.553035  ,\n",
              "         0.49503756,  0.4689419 ,  0.5444863 , -0.4795683 ,  0.45418447],\n",
              "       dtype=float32),\n",
              " 'instruments': array([ 0.5282122 ,  0.49298012, -0.46981925, -0.48175898, -0.44979602,\n",
              "         0.44475797,  0.4637826 , -0.4631544 , -0.5217224 , -0.44739202,\n",
              "        -0.45731914,  0.48479253,  0.48686454,  0.50736666, -0.54325026,\n",
              "        -0.5186669 ,  0.52232397,  0.45720947, -0.48735517, -0.44822508,\n",
              "        -0.43966147,  0.47636738,  0.5275763 ,  0.4666276 , -0.47896868,\n",
              "        -0.44194442,  0.4596    ,  0.4923058 ,  0.5092151 ,  0.48034695,\n",
              "        -0.5215422 ,  0.47040504, -0.51961493, -0.5280372 , -0.4419196 ,\n",
              "        -0.530776  ,  0.5196762 , -0.4362377 ,  0.47834206, -0.49819392,\n",
              "         0.4727891 , -0.48883474, -0.45792684, -0.46434712,  0.48625243,\n",
              "         0.5126057 ,  0.4978048 ,  0.50452346, -0.44952154,  0.5138905 ],\n",
              "       dtype=float32),\n",
              " 'after-tax': array([ 0.4828995 ,  0.4780137 , -0.53453803, -0.4847736 , -0.47706166,\n",
              "         0.54048365,  0.46981943, -0.51264375, -0.45370162, -0.49436527,\n",
              "        -0.52857816,  0.48734212,  0.5217654 ,  0.49602848, -0.51650846,\n",
              "        -0.5332253 ,  0.5302884 ,  0.5199693 , -0.45929426, -0.48361966,\n",
              "        -0.44640362,  0.439829  ,  0.46802014,  0.50714296, -0.52623725,\n",
              "        -0.45308435,  0.5030371 ,  0.5410744 ,  0.47820964,  0.48643598,\n",
              "        -0.47632307,  0.45946163, -0.54139787, -0.47822526, -0.45224887,\n",
              "        -0.48661977,  0.46105042, -0.46197155,  0.50551456, -0.49491674,\n",
              "         0.4461986 , -0.53900534, -0.46356666, -0.47456428,  0.4851846 ,\n",
              "         0.5154554 ,  0.52873546,  0.5061428 , -0.48101407,  0.48681888],\n",
              "       dtype=float32),\n",
              " 'derivatives': array([ 0.5094498 ,  0.43929806, -0.4606774 , -0.49766237, -0.4995175 ,\n",
              "         0.4878462 ,  0.52897334, -0.46612528, -0.4439993 , -0.47329533,\n",
              "        -0.51291114,  0.5094665 ,  0.4760402 ,  0.49268895, -0.5290806 ,\n",
              "        -0.48512286,  0.4880345 ,  0.466664  , -0.4510771 , -0.44063702,\n",
              "        -0.49869207,  0.4894917 ,  0.46809   ,  0.47770035, -0.45932835,\n",
              "        -0.45311233,  0.52417827,  0.48182777,  0.5158009 ,  0.44997633,\n",
              "        -0.49760398,  0.5137723 , -0.47737333, -0.5408796 , -0.4569299 ,\n",
              "        -0.51842594,  0.45371556, -0.5209774 ,  0.5076531 , -0.4596306 ,\n",
              "         0.51096797, -0.53748685, -0.49439216, -0.4587506 ,  0.48487362,\n",
              "         0.4880778 ,  0.52288246,  0.48861063, -0.51987654,  0.47270644],\n",
              "       dtype=float32),\n",
              " 'transaction': array([ 0.5317593 ,  0.48050362, -0.47412005, -0.50959533, -0.5001032 ,\n",
              "         0.4988923 ,  0.48130396, -0.5251398 , -0.5333972 , -0.480956  ,\n",
              "        -0.4553001 ,  0.49346042,  0.4852601 ,  0.4871099 , -0.50797975,\n",
              "        -0.45733446,  0.5375117 ,  0.5083367 , -0.4923402 , -0.48150054,\n",
              "        -0.4771011 ,  0.43888   ,  0.45083535,  0.528898  , -0.5165983 ,\n",
              "        -0.52871454,  0.5377236 ,  0.46328685,  0.45042273,  0.4694245 ,\n",
              "        -0.4848563 ,  0.5058324 , -0.46807852, -0.5126705 , -0.50881165,\n",
              "        -0.47475544,  0.49902475, -0.4878226 ,  0.4910848 , -0.44340068,\n",
              "         0.48430443, -0.4845413 , -0.5108068 , -0.52832806,  0.51200753,\n",
              "         0.51999265,  0.4741732 ,  0.5385011 , -0.48669523,  0.48119387],\n",
              "       dtype=float32),\n",
              " 'payments': array([ 0.5226453 ,  0.43548137, -0.52746797, -0.47557208, -0.4910711 ,\n",
              "         0.5267192 ,  0.44736385, -0.50709444, -0.46784517, -0.5040048 ,\n",
              "        -0.51212984,  0.45204705,  0.5026709 ,  0.491952  , -0.5657179 ,\n",
              "        -0.49669638,  0.5135775 ,  0.52082473, -0.5082907 , -0.48959434,\n",
              "        -0.4831627 ,  0.48138937,  0.46359575,  0.4876584 , -0.46469453,\n",
              "        -0.51345265,  0.46848097,  0.49123406,  0.43559566,  0.49664003,\n",
              "        -0.4512443 ,  0.50133324, -0.5382138 , -0.49856544, -0.47927663,\n",
              "        -0.51833725,  0.44880852, -0.53181005,  0.51658845, -0.48999697,\n",
              "         0.5145907 , -0.45464653, -0.49171078, -0.44839233,  0.4388863 ,\n",
              "         0.49654698,  0.47697145,  0.52583236, -0.49355224,  0.45768523],\n",
              "       dtype=float32),\n",
              " 'volatile': array([ 0.49326137,  0.51721865, -0.4723238 , -0.45430994, -0.4855242 ,\n",
              "         0.5374645 ,  0.49349263, -0.46603286, -0.51252943, -0.46566853,\n",
              "        -0.45786244,  0.48857674,  0.44424167,  0.45286146, -0.50520325,\n",
              "        -0.5160107 ,  0.53572816,  0.5152519 , -0.51784647, -0.528686  ,\n",
              "        -0.47840005,  0.4506599 ,  0.5054776 ,  0.5239521 , -0.44395193,\n",
              "        -0.47136173,  0.44904464,  0.47686115,  0.4660559 ,  0.5168998 ,\n",
              "        -0.5189638 ,  0.51822346, -0.5176273 , -0.52914125, -0.51774573,\n",
              "        -0.47524065,  0.5080628 , -0.5376171 ,  0.47129598, -0.47595835,\n",
              "         0.5388427 , -0.49965984, -0.49854085, -0.52323556,  0.48383027,\n",
              "         0.4632189 ,  0.4714015 ,  0.49418733, -0.48604313,  0.45113552],\n",
              "       dtype=float32),\n",
              " 'c': array([ 0.46100342,  0.4881909 , -0.4499731 , -0.486123  , -0.50467837,\n",
              "         0.5645874 ,  0.46254987, -0.43086886, -0.44420716, -0.5148197 ,\n",
              "        -0.54315656,  0.4485875 ,  0.53523564,  0.46325678, -0.581053  ,\n",
              "        -0.5236221 ,  0.5253889 ,  0.43957177, -0.47484922, -0.54697096,\n",
              "        -0.5151524 ,  0.508114  ,  0.51380414,  0.55029666, -0.47475982,\n",
              "        -0.5156212 ,  0.5167372 ,  0.4585919 ,  0.54832536,  0.5969671 ,\n",
              "        -0.47057903,  0.472913  , -0.5544407 , -0.53437936, -0.43883026,\n",
              "        -0.48606962,  0.5440886 , -0.6076535 ,  0.5471184 , -0.50843287,\n",
              "         0.517479  , -0.5954671 , -0.45884156, -0.5208154 ,  0.5532329 ,\n",
              "         0.52854824,  0.53136337,  0.50764257, -0.5668833 ,  0.5336991 ],\n",
              "       dtype=float32),\n",
              " 'invests': array([ 0.4926324 ,  0.47417027, -0.4567734 , -0.5271774 , -0.45600376,\n",
              "         0.4728683 ,  0.52786374, -0.5260724 , -0.47944736, -0.44640535,\n",
              "        -0.52315295,  0.46612063,  0.4866799 ,  0.4644172 , -0.54506326,\n",
              "        -0.48135993,  0.50834167,  0.5001965 , -0.49369517, -0.45898658,\n",
              "        -0.52929956,  0.49530005,  0.52445877,  0.49154067, -0.5259993 ,\n",
              "        -0.49483526,  0.47993413,  0.5249498 ,  0.49209833,  0.4501833 ,\n",
              "        -0.53455627,  0.44659567, -0.48916635, -0.45441523, -0.5261137 ,\n",
              "        -0.5320555 ,  0.54273605, -0.50553626,  0.47285402, -0.51026   ,\n",
              "         0.51467264, -0.47587365, -0.5145141 , -0.4923097 ,  0.49319714,\n",
              "         0.46743923,  0.48140216,  0.5208269 , -0.4596538 ,  0.46177143],\n",
              "       dtype=float32),\n",
              " 'plan': array([ 0.50595605,  0.44406727, -0.43919837, -0.5092831 , -0.45328608,\n",
              "         0.5117062 ,  0.51122427, -0.51900256, -0.46875238, -0.47914726,\n",
              "        -0.4858948 ,  0.43497348,  0.47793463,  0.44199166, -0.54947054,\n",
              "        -0.4873826 ,  0.4920616 ,  0.5225038 , -0.43841538, -0.46551594,\n",
              "        -0.4609367 ,  0.4440418 ,  0.4813005 ,  0.48559797, -0.4609165 ,\n",
              "        -0.52241045,  0.4375074 ,  0.5384217 ,  0.44120812,  0.50638884,\n",
              "        -0.5191709 ,  0.45908138, -0.4757769 , -0.45714593, -0.49572834,\n",
              "        -0.5169649 ,  0.5155495 , -0.49281964,  0.51466274, -0.49997422,\n",
              "         0.50673246, -0.5112821 , -0.4724509 , -0.4780547 ,  0.51601607,\n",
              "         0.52710956,  0.46713054,  0.46150896, -0.44095236,  0.44454828],\n",
              "       dtype=float32),\n",
              " 'held': array([ 0.5232474 ,  0.47171026, -0.46471828, -0.49649465, -0.5198506 ,\n",
              "         0.47467288,  0.44836947, -0.47300813, -0.51390016, -0.5339014 ,\n",
              "        -0.47592056,  0.45386454,  0.50898796,  0.47752118, -0.50718194,\n",
              "        -0.5369178 ,  0.47934857,  0.54035425, -0.5291714 , -0.5310892 ,\n",
              "        -0.52318573,  0.5215133 ,  0.49606073,  0.45982355, -0.47487983,\n",
              "        -0.44955298,  0.53414685,  0.5206192 ,  0.4988098 ,  0.46935004,\n",
              "        -0.4760303 ,  0.4719507 , -0.51784956, -0.49953938, -0.45311254,\n",
              "        -0.5428923 ,  0.53184474, -0.46634513,  0.52237713, -0.45569703,\n",
              "         0.49907184, -0.4688949 , -0.48227388, -0.47316164,  0.4703129 ,\n",
              "         0.52388585,  0.52517295,  0.45394227, -0.54017407,  0.515152  ],\n",
              "       dtype=float32),\n",
              " 'addition': array([ 0.49143693,  0.5002773 , -0.51214933, -0.4576594 , -0.4631751 ,\n",
              "         0.525035  ,  0.47646207, -0.52321756, -0.52161306, -0.4779433 ,\n",
              "        -0.51184714,  0.47118056,  0.4526261 ,  0.48876435, -0.5701189 ,\n",
              "        -0.486863  ,  0.5023423 ,  0.44798708, -0.43829724, -0.47801283,\n",
              "        -0.48246887,  0.46660963,  0.45550022,  0.5077404 , -0.4883704 ,\n",
              "        -0.52651477,  0.47309607,  0.4676121 ,  0.47654638,  0.51731557,\n",
              "        -0.5059986 ,  0.50747055, -0.45930043, -0.5198923 , -0.51502484,\n",
              "        -0.47314724,  0.48681414, -0.48408213,  0.46825954, -0.44916174,\n",
              "         0.4873917 , -0.4851719 , -0.49512097, -0.47087622,  0.44770426,\n",
              "         0.5316092 ,  0.50663793,  0.49581695, -0.5106706 ,  0.5177607 ],\n",
              "       dtype=float32),\n",
              " 'fee': array([ 0.52982616,  0.49429646, -0.49377027, -0.44001448, -0.48488575,\n",
              "         0.5149618 ,  0.48133844, -0.52864105, -0.45053035, -0.500939  ,\n",
              "        -0.51837337,  0.46402973,  0.46316215,  0.50657934, -0.5109939 ,\n",
              "        -0.49606887,  0.49225545,  0.51495516, -0.5239522 , -0.43874222,\n",
              "        -0.48887217,  0.51159257,  0.4685224 ,  0.44739795, -0.5271698 ,\n",
              "        -0.499412  ,  0.4416886 ,  0.50655925,  0.49079034,  0.52754253,\n",
              "        -0.45509148,  0.43978965, -0.5258267 , -0.5308163 , -0.4977001 ,\n",
              "        -0.5080386 ,  0.5254161 , -0.48783052,  0.44630858, -0.5153835 ,\n",
              "         0.47583687, -0.53826773, -0.48033965, -0.5238774 ,  0.48631048,\n",
              "         0.46238363,  0.514861  ,  0.5166869 , -0.48748434,  0.4493244 ],\n",
              "       dtype=float32),\n",
              " 'insurance': array([ 0.5299372 ,  0.46292824, -0.47875768, -0.4993509 , -0.47741723,\n",
              "         0.52543193,  0.46760827, -0.52155405, -0.4754044 , -0.4356968 ,\n",
              "        -0.4984149 ,  0.5160896 ,  0.44892284,  0.46692497, -0.5393909 ,\n",
              "        -0.5161263 ,  0.53270286,  0.4841573 , -0.45094797, -0.5193347 ,\n",
              "        -0.4304369 ,  0.48422632,  0.46784243,  0.43305123, -0.5034255 ,\n",
              "        -0.5121372 ,  0.49785984,  0.5151128 ,  0.5099806 ,  0.46244743,\n",
              "        -0.522311  ,  0.43964112, -0.47497404, -0.4437883 , -0.49534023,\n",
              "        -0.4453011 ,  0.5016804 , -0.4509474 ,  0.46610504, -0.43279046,\n",
              "         0.49806562, -0.4728137 , -0.4710974 , -0.4421507 ,  0.50503397,\n",
              "         0.512228  ,  0.49969172,  0.50328493, -0.51295674,  0.51722956],\n",
              "       dtype=float32),\n",
              " 'individual': array([ 0.473115  ,  0.43315986, -0.53561145, -0.53270304, -0.5394903 ,\n",
              "         0.47726673,  0.44659746, -0.4786324 , -0.44099602, -0.48280594,\n",
              "        -0.4849998 ,  0.5275762 ,  0.4867175 ,  0.5126248 , -0.54149705,\n",
              "        -0.4413959 ,  0.47081184,  0.4590582 , -0.46757963, -0.49278393,\n",
              "        -0.5044838 ,  0.4953245 ,  0.5105465 ,  0.48738784, -0.4640989 ,\n",
              "        -0.4422671 ,  0.4717927 ,  0.497797  ,  0.4599078 ,  0.5085011 ,\n",
              "        -0.49437597,  0.47234523, -0.54420197, -0.47345766, -0.5195921 ,\n",
              "        -0.48742077,  0.4572248 , -0.46692106,  0.5214098 , -0.5076373 ,\n",
              "         0.4651512 , -0.4530067 , -0.47513413, -0.44054344,  0.4533577 ,\n",
              "         0.49580467,  0.47040212,  0.49117494, -0.51146483,  0.49406374],\n",
              "       dtype=float32),\n",
              " 'stock': array([ 0.5233936 ,  0.48917884, -0.49882948, -0.5094377 , -0.46973085,\n",
              "         0.47103086,  0.4875008 , -0.52364254, -0.46381396, -0.5031146 ,\n",
              "        -0.4896597 ,  0.5120315 ,  0.44463846,  0.46591684, -0.5426838 ,\n",
              "        -0.5012618 ,  0.44997314,  0.4817372 , -0.43458402, -0.49403182,\n",
              "        -0.43801507,  0.4294832 ,  0.5205383 ,  0.46117008, -0.48389232,\n",
              "        -0.45591012,  0.51648384,  0.53131485,  0.46949577,  0.45447195,\n",
              "        -0.51224166,  0.45261633, -0.51182485, -0.51124704, -0.461883  ,\n",
              "        -0.5260909 ,  0.52486116, -0.5242877 ,  0.48297554, -0.48423085,\n",
              "         0.4969341 , -0.4638312 , -0.4414627 , -0.49848628,  0.5066713 ,\n",
              "         0.5013619 ,  0.4714604 ,  0.50819916, -0.50634295,  0.45058793],\n",
              "       dtype=float32),\n",
              " 'municipal': array([ 0.4910707 ,  0.47883224, -0.5007795 , -0.47733656, -0.5250044 ,\n",
              "         0.5273284 ,  0.48319978, -0.47333205, -0.47598907, -0.47339946,\n",
              "        -0.47241688,  0.48092797,  0.5116801 ,  0.45349646, -0.5518928 ,\n",
              "        -0.5160787 ,  0.499866  ,  0.527211  , -0.50003266, -0.46845746,\n",
              "        -0.5006952 ,  0.4728653 ,  0.4627946 ,  0.45480177, -0.46376142,\n",
              "        -0.4734848 ,  0.44730186,  0.56571937,  0.46307302,  0.4888585 ,\n",
              "        -0.49547014,  0.4843717 , -0.50814825, -0.48538738, -0.5163699 ,\n",
              "        -0.4650551 ,  0.4544396 , -0.48147696,  0.5170524 , -0.48523223,\n",
              "         0.45157966, -0.46648327, -0.45496505, -0.5170591 ,  0.48359084,\n",
              "         0.48786303,  0.46943036,  0.44307443, -0.44401664,  0.5299557 ],\n",
              "       dtype=float32),\n",
              " 'political': array([ 0.5346012 ,  0.43049887, -0.45649523, -0.4822066 , -0.45539683,\n",
              "         0.5280144 ,  0.46219623, -0.4706974 , -0.43966347, -0.4870288 ,\n",
              "        -0.4596903 ,  0.5151868 ,  0.50228006,  0.49466488, -0.46516836,\n",
              "        -0.44869918,  0.47744477,  0.5191445 , -0.4369413 , -0.44947925,\n",
              "        -0.4892986 ,  0.4496257 ,  0.51497364,  0.5130602 , -0.48013106,\n",
              "        -0.46223396,  0.43810755,  0.49027154,  0.4611253 ,  0.44403505,\n",
              "        -0.53011286,  0.44332346, -0.5337989 , -0.52436775, -0.51828796,\n",
              "        -0.529445  ,  0.474733  , -0.501096  ,  0.49678552, -0.45868355,\n",
              "         0.51861   , -0.50085217, -0.4638125 , -0.4628811 ,  0.49354064,\n",
              "         0.48647916,  0.49093813,  0.5131985 , -0.46654108,  0.4559394 ],\n",
              "       dtype=float32),\n",
              " 'countries': array([ 0.46706837,  0.48339155, -0.5013339 , -0.51874053, -0.5333437 ,\n",
              "         0.5334865 ,  0.4585337 , -0.5367536 , -0.5171831 , -0.50772625,\n",
              "        -0.47893035,  0.45883757,  0.4447621 ,  0.46595913, -0.51927376,\n",
              "        -0.45074996,  0.46017772,  0.5319161 , -0.4441572 , -0.507142  ,\n",
              "        -0.49325842,  0.4985816 ,  0.49884966,  0.46585923, -0.46588928,\n",
              "        -0.47261545,  0.45004916,  0.51839924,  0.48464018,  0.5102557 ,\n",
              "        -0.5033962 ,  0.46333295, -0.49662492, -0.45366943, -0.5185883 ,\n",
              "        -0.5243535 ,  0.49971724, -0.45021564,  0.51365113, -0.5360156 ,\n",
              "         0.49701643, -0.47453704, -0.49184155, -0.45006797,  0.5123083 ,\n",
              "         0.4762032 ,  0.47186092,  0.45435455, -0.44881117,  0.503146  ],\n",
              "       dtype=float32),\n",
              " 'reflects': array([ 0.53124267,  0.4885188 , -0.47775638, -0.4791758 , -0.49362093,\n",
              "         0.512556  ,  0.47991556, -0.47968245, -0.44677472, -0.47109315,\n",
              "        -0.48037001,  0.5036585 ,  0.49864587,  0.50248945, -0.4858705 ,\n",
              "        -0.5273418 ,  0.4859552 ,  0.46726727, -0.4709561 , -0.472399  ,\n",
              "        -0.49982512,  0.50354177,  0.47921726,  0.52194047, -0.49177867,\n",
              "        -0.4781437 ,  0.49335903,  0.4727371 ,  0.53138393,  0.50318176,\n",
              "        -0.49957174,  0.48588613, -0.48248008, -0.48868123, -0.47748327,\n",
              "        -0.51604223,  0.49751514, -0.45413524,  0.44510758, -0.5305968 ,\n",
              "         0.4784402 , -0.5204279 , -0.45695782, -0.50802565,  0.53344196,\n",
              "         0.5088073 ,  0.47771963,  0.48242074, -0.52347624,  0.53183985],\n",
              "       dtype=float32),\n",
              " 'emerging': array([ 0.4823725 ,  0.44269148, -0.4793547 , -0.5043829 , -0.5448356 ,\n",
              "         0.5154811 ,  0.49160475, -0.45755452, -0.45504707, -0.510831  ,\n",
              "        -0.45947883,  0.47896275,  0.4996106 ,  0.49605042, -0.5199762 ,\n",
              "        -0.44454056,  0.5280308 ,  0.5463046 , -0.50293744, -0.47414345,\n",
              "        -0.46098554,  0.48822513,  0.53665304,  0.48473758, -0.47114754,\n",
              "        -0.5058382 ,  0.48082808,  0.5388969 ,  0.4684563 ,  0.45649102,\n",
              "        -0.5267267 ,  0.49981824, -0.5157687 , -0.53949463, -0.48971504,\n",
              "        -0.47310144,  0.49674082, -0.47601613,  0.464747  , -0.5200683 ,\n",
              "         0.53489095, -0.4684436 , -0.45529544, -0.5124438 ,  0.53824806,\n",
              "         0.501194  ,  0.52678555,  0.49100852, -0.47823277,  0.45630866],\n",
              "       dtype=float32),\n",
              " 'objective': array([ 0.5142187 ,  0.48982227, -0.53732747, -0.46919495, -0.501936  ,\n",
              "         0.468312  ,  0.50718635, -0.5162518 , -0.46493465, -0.5065817 ,\n",
              "        -0.50783366,  0.4868719 ,  0.52163976,  0.48197043, -0.5170774 ,\n",
              "        -0.45326915,  0.49866122,  0.486803  , -0.47188306, -0.46931654,\n",
              "        -0.52472025,  0.49284035,  0.5276456 ,  0.50994444, -0.52225053,\n",
              "        -0.46892628,  0.47543442,  0.4807549 ,  0.5140345 ,  0.47659785,\n",
              "        -0.52257156,  0.46370816, -0.507263  , -0.4997095 , -0.5071066 ,\n",
              "        -0.5210652 ,  0.5208499 , -0.48607948,  0.4991321 , -0.4760374 ,\n",
              "         0.5444078 , -0.50200427, -0.5192953 , -0.48903012,  0.47434324,\n",
              "         0.4537693 ,  0.4808028 ,  0.474055  , -0.48198718,  0.46819687],\n",
              "       dtype=float32),\n",
              " 'date': array([ 0.5302737 ,  0.49528316, -0.5346575 , -0.526969  , -0.4966696 ,\n",
              "         0.47838157,  0.5255949 , -0.45667037, -0.48078474, -0.43563893,\n",
              "        -0.48429   ,  0.4939755 ,  0.46291044,  0.45889008, -0.56733304,\n",
              "        -0.52264124,  0.497078  ,  0.5303271 , -0.44226873, -0.49390432,\n",
              "        -0.463239  ,  0.5175837 ,  0.4776687 ,  0.5121543 , -0.4399764 ,\n",
              "        -0.45635492,  0.48235422,  0.49154365,  0.4564932 ,  0.46899644,\n",
              "        -0.5179671 ,  0.525234  , -0.5098339 , -0.53390247, -0.5047631 ,\n",
              "        -0.48310265,  0.52665174, -0.45049956,  0.49640292, -0.4993068 ,\n",
              "         0.51516587, -0.45072484, -0.44106457, -0.49056295,  0.5044734 ,\n",
              "         0.46315348,  0.48643804,  0.5244087 , -0.49797893,  0.49210948],\n",
              "       dtype=float32),\n",
              " 'chart': array([ 0.47174576,  0.44523162, -0.5236284 , -0.5047864 , -0.46237877,\n",
              "         0.49100375,  0.45900044, -0.50067985, -0.43324843, -0.5020127 ,\n",
              "        -0.53338194,  0.5068437 ,  0.4467776 ,  0.5012079 , -0.5677873 ,\n",
              "        -0.53128576,  0.48036632,  0.44709888, -0.44003797, -0.49073464,\n",
              "        -0.4568182 ,  0.5124292 ,  0.51044935,  0.52082264, -0.4760467 ,\n",
              "        -0.49730948,  0.48022774,  0.48490852,  0.45498538,  0.49174783,\n",
              "        -0.5050761 ,  0.5080546 , -0.47758946, -0.49070734, -0.47900677,\n",
              "        -0.47006395,  0.5288465 , -0.48997483,  0.5256001 , -0.50113267,\n",
              "         0.5366589 , -0.5263638 , -0.48181704, -0.4532165 ,  0.48681194,\n",
              "         0.47917622,  0.5212863 ,  0.49362838, -0.45616978,  0.5020853 ],\n",
              "       dtype=float32),\n",
              " 'based': array([ 0.5381813 ,  0.48116815, -0.50007296, -0.45858636, -0.5236408 ,\n",
              "         0.46899334,  0.4419118 , -0.489334  , -0.49430066, -0.47829992,\n",
              "        -0.50758433,  0.4732838 ,  0.45807686,  0.47621372, -0.5247886 ,\n",
              "        -0.5148397 ,  0.46801433,  0.47889024, -0.4431494 , -0.4481346 ,\n",
              "        -0.47133896,  0.45418066,  0.45300746,  0.45769113, -0.489974  ,\n",
              "        -0.5344431 ,  0.45057368,  0.5088035 ,  0.48430353,  0.5152688 ,\n",
              "        -0.48206046,  0.47564226, -0.46468082, -0.54396605, -0.49968988,\n",
              "        -0.5178624 ,  0.5460244 , -0.5445733 ,  0.5057632 , -0.48647222,\n",
              "         0.468543  , -0.53676796, -0.51316524, -0.5238032 ,  0.4664483 ,\n",
              "         0.48298952,  0.4650231 ,  0.5313317 , -0.4849937 ,  0.5242673 ],\n",
              "       dtype=float32),\n",
              " 'directly': array([ 0.48809254,  0.48824078, -0.4889777 , -0.49313098, -0.498724  ,\n",
              "         0.46911734,  0.5147497 , -0.47998995, -0.54291344, -0.47758454,\n",
              "        -0.51973444,  0.49309626,  0.5175558 ,  0.4964261 , -0.5471044 ,\n",
              "        -0.5154036 ,  0.46615726,  0.4668986 , -0.5182749 , -0.51426834,\n",
              "        -0.5280559 ,  0.50830597,  0.5083937 ,  0.51426643, -0.45645475,\n",
              "        -0.46168172,  0.46268523,  0.48203003,  0.4746345 ,  0.4712745 ,\n",
              "        -0.4830872 ,  0.45978698, -0.47604162, -0.493135  , -0.5128872 ,\n",
              "        -0.5251494 ,  0.5181519 , -0.5142424 ,  0.4574383 , -0.4633585 ,\n",
              "         0.4930697 , -0.54441315, -0.48462045, -0.4658172 ,  0.46781883,\n",
              "         0.55231535,  0.45375827,  0.533682  , -0.52158046,  0.49179083],\n",
              "       dtype=float32),\n",
              " 'liquidity': array([ 0.5180923 ,  0.46001765, -0.5101247 , -0.46194834, -0.46119747,\n",
              "         0.49463242,  0.492399  , -0.46869355, -0.49628556, -0.44303453,\n",
              "        -0.52078146,  0.491458  ,  0.48252976,  0.4957325 , -0.5109696 ,\n",
              "        -0.52852714,  0.4515883 ,  0.4814173 , -0.4473609 , -0.4958375 ,\n",
              "        -0.50454634,  0.46964118,  0.48022646,  0.5064839 , -0.43284765,\n",
              "        -0.5165541 ,  0.46555108,  0.5008863 ,  0.5132977 ,  0.4929953 ,\n",
              "        -0.48109218,  0.45829946, -0.5248617 , -0.522441  , -0.48049003,\n",
              "        -0.49655247,  0.4563557 , -0.5096157 ,  0.48118225, -0.4634278 ,\n",
              "         0.4670761 , -0.5145729 , -0.50415355, -0.4473368 ,  0.47942132,\n",
              "         0.5151634 ,  0.45796648,  0.46160743, -0.52748764,  0.5144872 ],\n",
              "       dtype=float32),\n",
              " 'decline': array([ 0.4877636 ,  0.49327356, -0.5201424 , -0.45019737, -0.53965354,\n",
              "         0.4908749 ,  0.53679174, -0.5297476 , -0.49719578, -0.48514104,\n",
              "        -0.48637766,  0.45341858,  0.52789146,  0.48142725, -0.5125773 ,\n",
              "        -0.46229807,  0.45315772,  0.4642379 , -0.45690584, -0.52088183,\n",
              "        -0.4872672 ,  0.4444183 ,  0.52720255,  0.46700686, -0.5027148 ,\n",
              "        -0.4546149 ,  0.5189723 ,  0.4840337 ,  0.46666998,  0.52415305,\n",
              "        -0.47351515,  0.4621603 , -0.504568  , -0.49605566, -0.50164247,\n",
              "        -0.53610855,  0.47786385, -0.4606365 ,  0.44877535, -0.5462161 ,\n",
              "         0.5151515 , -0.49147084, -0.44912025, -0.5270248 ,  0.45111132,\n",
              "         0.49418983,  0.52649015,  0.50723803, -0.48600805,  0.50670356],\n",
              "       dtype=float32),\n",
              " 'related': array([ 0.5285664 ,  0.5220318 , -0.51859844, -0.5203469 , -0.5419169 ,\n",
              "         0.48789677,  0.49924758, -0.49502072, -0.51069635, -0.45306677,\n",
              "        -0.5239932 ,  0.5306915 ,  0.4558248 ,  0.5091263 , -0.5047707 ,\n",
              "        -0.5274842 ,  0.46615875,  0.4496379 , -0.45078695, -0.5110037 ,\n",
              "        -0.46961987,  0.5100973 ,  0.53277594,  0.5348352 , -0.50294363,\n",
              "        -0.480753  ,  0.48333418,  0.4882738 ,  0.535551  ,  0.47884572,\n",
              "        -0.5360017 ,  0.47503543, -0.5312567 , -0.49878597, -0.48269352,\n",
              "        -0.46222907,  0.4904419 , -0.46316603,  0.5083747 , -0.52055347,\n",
              "         0.5036967 , -0.50903183, -0.49282384, -0.46967736,  0.50786257,\n",
              "         0.53242636,  0.49416274,  0.4669836 , -0.44603008,  0.5106285 ],\n",
              "       dtype=float32),\n",
              " 'services': array([ 0.52239585,  0.46526933, -0.50089353, -0.44188955, -0.5053522 ,\n",
              "         0.50968015,  0.44936812, -0.5136344 , -0.53117716, -0.43663043,\n",
              "        -0.5246008 ,  0.5064434 ,  0.5085043 ,  0.4995518 , -0.5520503 ,\n",
              "        -0.51537144,  0.5308958 ,  0.50772226, -0.473675  , -0.46881837,\n",
              "        -0.53112847,  0.53076214,  0.47050518,  0.51309085, -0.44645754,\n",
              "        -0.50210065,  0.5001487 ,  0.4725911 ,  0.44183913,  0.5151836 ,\n",
              "        -0.48007393,  0.44777092, -0.4556512 , -0.4691956 , -0.4991612 ,\n",
              "        -0.5334653 ,  0.51175493, -0.5185534 ,  0.47487974, -0.47165778,\n",
              "         0.5259597 , -0.51513356, -0.4722851 , -0.5254603 ,  0.47827837,\n",
              "         0.46679688,  0.5096853 ,  0.4913236 , -0.51122415,  0.4489987 ],\n",
              "       dtype=float32),\n",
              " 'government': array([ 0.47017735,  0.49122426, -0.47283024, -0.49222746, -0.47609335,\n",
              "         0.50454766,  0.4524285 , -0.5056262 , -0.45448583, -0.43504515,\n",
              "        -0.52042633,  0.5090569 ,  0.48419774,  0.48621225, -0.55977434,\n",
              "        -0.5070888 ,  0.516521  ,  0.46484017, -0.44682288, -0.45428944,\n",
              "        -0.43554458,  0.4858747 ,  0.4994288 ,  0.5107922 , -0.4394195 ,\n",
              "        -0.45706308,  0.48018637,  0.49258754,  0.5089968 ,  0.50184536,\n",
              "        -0.4782985 ,  0.4763189 , -0.48809373, -0.52984154, -0.48587227,\n",
              "        -0.49129024,  0.45825   , -0.51827425,  0.5245923 , -0.532871  ,\n",
              "         0.5399208 , -0.46517086, -0.4328423 , -0.45592996,  0.45212156,\n",
              "         0.51964736,  0.53126955,  0.5271037 , -0.46350268,  0.50530744],\n",
              "       dtype=float32),\n",
              " 'volatility': array([ 0.5464356 ,  0.46034956, -0.5015168 , -0.4519884 , -0.5132128 ,\n",
              "         0.460805  ,  0.48539716, -0.5000743 , -0.44580814, -0.5071666 ,\n",
              "        -0.4919562 ,  0.50531614,  0.47357595,  0.5022181 , -0.5288395 ,\n",
              "        -0.46130252,  0.5326993 ,  0.46930388, -0.4915893 , -0.5108459 ,\n",
              "        -0.45294073,  0.5276009 ,  0.44997567,  0.48131704, -0.512074  ,\n",
              "        -0.5172685 ,  0.50986576,  0.49080047,  0.5029468 ,  0.43872544,\n",
              "        -0.48738906,  0.44383875, -0.52091527, -0.53428394, -0.46823496,\n",
              "        -0.46295327,  0.48787275, -0.46288562,  0.46667287, -0.5032607 ,\n",
              "         0.4690695 , -0.48428163, -0.47997192, -0.51147157,  0.53818727,\n",
              "         0.4470397 ,  0.48373353,  0.52508307, -0.5217346 ,  0.4749124 ],\n",
              "       dtype=float32),\n",
              " 'since': array([ 0.49994814,  0.50503224, -0.50156784, -0.45856306, -0.5382133 ,\n",
              "         0.4804643 ,  0.5150084 , -0.4506509 , -0.48079616, -0.47951967,\n",
              "        -0.48068357,  0.5220862 ,  0.5322977 ,  0.51180416, -0.5325837 ,\n",
              "        -0.5291022 ,  0.5270169 ,  0.4538641 , -0.50005263, -0.44208384,\n",
              "        -0.46449384,  0.45236757,  0.45415032,  0.49738896, -0.51134866,\n",
              "        -0.49485478,  0.5162767 ,  0.49190888,  0.47793344,  0.50105697,\n",
              "        -0.5130878 ,  0.5307454 , -0.54804486, -0.44894102, -0.5024738 ,\n",
              "        -0.5441341 ,  0.45929998, -0.48344544,  0.4789282 , -0.49335045,\n",
              "         0.4561419 , -0.48050034, -0.51951534, -0.49553233,  0.4582094 ,\n",
              "         0.5066186 ,  0.48288772,  0.45415068, -0.46316037,  0.49299982],\n",
              "       dtype=float32),\n",
              " 'exposure': array([ 0.53761846,  0.47652978, -0.47426042, -0.52063894, -0.48301616,\n",
              "         0.47730607,  0.45092458, -0.5078422 , -0.45047992, -0.52306074,\n",
              "        -0.46340296,  0.47990113,  0.5300146 ,  0.47246015, -0.52140313,\n",
              "        -0.46850315,  0.44676775,  0.46331632, -0.48320788, -0.47008404,\n",
              "        -0.48562175,  0.52311075,  0.49411285,  0.47210938, -0.5296756 ,\n",
              "        -0.507682  ,  0.5088165 ,  0.51083463,  0.4879664 ,  0.4576811 ,\n",
              "        -0.5294685 ,  0.48536015, -0.49813047, -0.47284475, -0.53329885,\n",
              "        -0.45992312,  0.46413878, -0.49381965,  0.52170324, -0.48241517,\n",
              "         0.47960466, -0.48023757, -0.47665226, -0.44946957,  0.50069857,\n",
              "         0.46908146,  0.53632045,  0.5233711 , -0.51519644,  0.44552836],\n",
              "       dtype=float32),\n",
              " 'prices': array([ 0.5419222 ,  0.4291323 , -0.4899798 , -0.47154582, -0.47315526,\n",
              "         0.5239038 ,  0.51423204, -0.5195622 , -0.44191557, -0.44912648,\n",
              "        -0.48001558,  0.5285345 ,  0.520047  ,  0.44857362, -0.5219966 ,\n",
              "        -0.47928363,  0.46187437,  0.48952624, -0.45154712, -0.51531065,\n",
              "        -0.4595553 ,  0.45169526,  0.46494624,  0.4480001 , -0.44947574,\n",
              "        -0.47671893,  0.5006823 ,  0.4762063 ,  0.4913192 ,  0.47224623,\n",
              "        -0.45397374,  0.49205655, -0.52606905, -0.50805897, -0.46042684,\n",
              "        -0.4981417 ,  0.5239741 , -0.503094  ,  0.49708506, -0.47028345,\n",
              "         0.4701531 , -0.52285284, -0.43371686, -0.5229826 ,  0.52990425,\n",
              "         0.49236158,  0.48377836,  0.48722482, -0.47429368,  0.52187914],\n",
              "       dtype=float32),\n",
              " 'accounts': array([ 0.50793517,  0.42877415, -0.47634298, -0.5066037 , -0.44710267,\n",
              "         0.49904847,  0.47145644, -0.45719856, -0.5126804 , -0.46886817,\n",
              "        -0.48878497,  0.4951978 ,  0.4728223 ,  0.5012485 , -0.52328014,\n",
              "        -0.47798058,  0.47210035,  0.48965687, -0.4941957 , -0.4997415 ,\n",
              "        -0.43875653,  0.4693554 ,  0.49677947,  0.5175568 , -0.49232   ,\n",
              "        -0.5119518 ,  0.46767753,  0.50417876,  0.49474865,  0.47103643,\n",
              "        -0.4683005 ,  0.4264504 , -0.5194829 , -0.4845651 , -0.43164766,\n",
              "        -0.51848274,  0.5105123 , -0.46998328,  0.49004456, -0.5142595 ,\n",
              "         0.47576952, -0.4579141 , -0.50512457, -0.4671212 ,  0.52134365,\n",
              "         0.5260527 ,  0.5081161 ,  0.481619  , -0.46710104,  0.5026592 ],\n",
              "       dtype=float32),\n",
              " 'expense': array([ 0.52956414,  0.44058326, -0.5319002 , -0.48030943, -0.461143  ,\n",
              "         0.52831125,  0.44618326, -0.5305757 , -0.47539684, -0.45305544,\n",
              "        -0.5168096 ,  0.46297055,  0.45297715,  0.4576243 , -0.49375886,\n",
              "        -0.49606794,  0.5311204 ,  0.49035507, -0.4736545 , -0.45711365,\n",
              "        -0.48217526,  0.4758491 ,  0.4973607 ,  0.4585261 , -0.49206367,\n",
              "        -0.4477082 ,  0.52658725,  0.507522  ,  0.5088866 ,  0.48677167,\n",
              "        -0.49017537,  0.49151662, -0.5156275 , -0.48603654, -0.5061355 ,\n",
              "        -0.5165528 ,  0.4876644 , -0.48433995,  0.44618645, -0.4876926 ,\n",
              "         0.47462642, -0.4902704 , -0.49957737, -0.500561  ,  0.47248563,\n",
              "         0.51279384,  0.48777586,  0.43967578, -0.46148953,  0.48034313],\n",
              "       dtype=float32),\n",
              " 'money': array([ 0.48781058,  0.5098746 , -0.5253473 , -0.49468395, -0.5319259 ,\n",
              "         0.48309147,  0.51462513, -0.4491289 , -0.47051775, -0.4852897 ,\n",
              "        -0.46587884,  0.44683993,  0.49491867,  0.45091662, -0.50470215,\n",
              "        -0.5246534 ,  0.526796  ,  0.4903803 , -0.48316368, -0.4874797 ,\n",
              "        -0.49937075,  0.510304  ,  0.52357227,  0.50599205, -0.45021543,\n",
              "        -0.52454525,  0.4634552 ,  0.5024393 ,  0.4887851 ,  0.45064914,\n",
              "        -0.53414625,  0.4591802 , -0.53888845, -0.5249678 , -0.49120986,\n",
              "        -0.47351223,  0.5188778 , -0.4815746 ,  0.49890715, -0.50281996,\n",
              "         0.51393825, -0.5314732 , -0.4702596 , -0.5194863 ,  0.48763153,\n",
              "         0.50530005,  0.52324617,  0.44755018, -0.5024157 ,  0.45257792],\n",
              "       dtype=float32),\n",
              " 'cost': array([ 0.5265387 ,  0.5078043 , -0.5079344 , -0.4623326 , -0.50898063,\n",
              "         0.5308888 ,  0.51850235, -0.4712259 , -0.4511521 , -0.50435543,\n",
              "        -0.45983356,  0.49364567,  0.5094388 ,  0.47314045, -0.5307333 ,\n",
              "        -0.49237218,  0.47171032,  0.48544693, -0.4410925 , -0.47318175,\n",
              "        -0.44553986,  0.48322183,  0.5102957 ,  0.51152265, -0.4645022 ,\n",
              "        -0.49649054,  0.5131558 ,  0.4740443 ,  0.5274414 ,  0.43993813,\n",
              "        -0.5241943 ,  0.46884292, -0.46337283, -0.495543  , -0.46095654,\n",
              "        -0.46938783,  0.48182595, -0.4821121 ,  0.48865566, -0.50030977,\n",
              "         0.5356924 , -0.5166512 , -0.45556065, -0.5174062 ,  0.5276017 ,\n",
              "         0.45243233,  0.4819286 ,  0.5106003 , -0.52835906,  0.5275447 ],\n",
              "       dtype=float32),\n",
              " 'minimum': array([ 0.49045348,  0.5041959 , -0.50326484, -0.46670827, -0.45933348,\n",
              "         0.46627474,  0.49781716, -0.47015446, -0.51591456, -0.52673906,\n",
              "        -0.5388213 ,  0.5066595 ,  0.47359425,  0.49384138, -0.5072762 ,\n",
              "        -0.5124753 ,  0.5360109 ,  0.4819258 , -0.46595863, -0.44539297,\n",
              "        -0.4709798 ,  0.5087993 ,  0.48876074,  0.44016874, -0.47031048,\n",
              "        -0.49715483,  0.49359012,  0.49287674,  0.49653366,  0.52812666,\n",
              "        -0.5221973 ,  0.44255918, -0.4940448 , -0.54607075, -0.49253643,\n",
              "        -0.45917448,  0.47903496, -0.51012176,  0.48655653, -0.46836773,\n",
              "         0.51717466, -0.5102898 , -0.4409876 , -0.516446  ,  0.47305307,\n",
              "         0.4550907 ,  0.46477348,  0.4762097 , -0.4677529 ,  0.48166478],\n",
              "       dtype=float32),\n",
              " 'bonds': array([ 0.5428308 ,  0.47851014, -0.52522326, -0.4952628 , -0.48766643,\n",
              "         0.4727412 ,  0.48782662, -0.507805  , -0.48488873, -0.46545118,\n",
              "        -0.45610526,  0.45522404,  0.46445876,  0.4911746 , -0.52406967,\n",
              "        -0.4805846 ,  0.45933956,  0.5120436 , -0.49277228, -0.48219934,\n",
              "        -0.477715  ,  0.44430792,  0.4967637 ,  0.4334698 , -0.44435516,\n",
              "        -0.5131413 ,  0.4589955 ,  0.53053325,  0.46193385,  0.45985737,\n",
              "        -0.48725826,  0.49846947, -0.46504152, -0.4990925 , -0.49073124,\n",
              "        -0.515118  ,  0.5205266 , -0.4714545 ,  0.5212516 , -0.4527705 ,\n",
              "         0.4631724 , -0.48124206, -0.45605293, -0.45225778,  0.47504216,\n",
              "         0.45284104,  0.50129014,  0.5132779 , -0.4397132 ,  0.45978454],\n",
              "       dtype=float32),\n",
              " 'classes': array([ 0.49096638,  0.48341292, -0.51600677, -0.5032644 , -0.485078  ,\n",
              "         0.4800256 ,  0.4396596 , -0.46112388, -0.51166415, -0.45737463,\n",
              "        -0.5102892 ,  0.48457333,  0.5027943 ,  0.45264575, -0.48079324,\n",
              "        -0.48919135,  0.52367306,  0.48328733, -0.49069053, -0.5049227 ,\n",
              "        -0.5307061 ,  0.5101918 ,  0.4714431 ,  0.459671  , -0.46269566,\n",
              "        -0.5110146 ,  0.4678911 ,  0.54064167,  0.468319  ,  0.5105701 ,\n",
              "        -0.46767747,  0.5065907 , -0.52319515, -0.52016866, -0.4518465 ,\n",
              "        -0.52331614,  0.5276714 , -0.5153692 ,  0.48324975, -0.44244766,\n",
              "         0.4559061 , -0.51640564, -0.46793854, -0.45179662,  0.51401746,\n",
              "         0.53091794,  0.4886391 ,  0.5033851 , -0.47198838,  0.4568929 ],\n",
              "       dtype=float32),\n",
              " 'different': array([ 0.5398966 ,  0.51535356, -0.44921628, -0.48654702, -0.53228873,\n",
              "         0.49168384,  0.44535047, -0.4601313 , -0.48687056, -0.52672994,\n",
              "        -0.49843448,  0.5189869 ,  0.5034526 ,  0.44940504, -0.51131624,\n",
              "        -0.48938054,  0.5152861 ,  0.50945544, -0.49223062, -0.5088551 ,\n",
              "        -0.4495646 ,  0.49118912,  0.4769786 ,  0.47327062, -0.45774344,\n",
              "        -0.5073063 ,  0.5248649 ,  0.51080936,  0.48282245,  0.47043765,\n",
              "        -0.51571095,  0.44981706, -0.51120347, -0.4611381 , -0.530142  ,\n",
              "        -0.4662609 ,  0.5199902 , -0.4971405 ,  0.5013399 , -0.53321797,\n",
              "         0.48216677, -0.45894137, -0.5192025 , -0.46460176,  0.44731614,\n",
              "         0.46315464,  0.44559067,  0.49829984, -0.46094784,  0.4574023 ],\n",
              "       dtype=float32),\n",
              " 'reflect': array([ 0.50458556,  0.47459608, -0.45229706, -0.52650774, -0.5385767 ,\n",
              "         0.5299641 ,  0.49815333, -0.45439062, -0.48878688, -0.48724255,\n",
              "        -0.46219003,  0.4910655 ,  0.4934216 ,  0.5015459 , -0.5378048 ,\n",
              "        -0.47914678,  0.5235141 ,  0.5301189 , -0.4812955 , -0.45994318,\n",
              "        -0.47262397,  0.44581333,  0.48949033,  0.45066583, -0.5304589 ,\n",
              "        -0.52274334,  0.45144603,  0.5331465 ,  0.5201738 ,  0.47102684,\n",
              "        -0.48710132,  0.5237749 , -0.5345619 , -0.5284275 , -0.44549477,\n",
              "        -0.4829312 ,  0.5330969 , -0.4764753 ,  0.46631593, -0.50828236,\n",
              "         0.49273625, -0.45556772, -0.4979158 , -0.50583166,  0.46397752,\n",
              "         0.48632264,  0.4594086 ,  0.49943587, -0.5098778 ,  0.49378994],\n",
              "       dtype=float32),\n",
              " 'nav': array([ 0.46312338,  0.46966082, -0.47827917, -0.4379256 , -0.51781285,\n",
              "         0.5295582 ,  0.44603196, -0.5250875 , -0.5147932 , -0.4992645 ,\n",
              "        -0.45096046,  0.46347356,  0.45077372,  0.5074011 , -0.5352617 ,\n",
              "        -0.5064266 ,  0.44852102,  0.48919153, -0.5208848 , -0.52086586,\n",
              "        -0.44342643,  0.5286168 ,  0.48476046,  0.47777647, -0.44940457,\n",
              "        -0.44899616,  0.45855832,  0.47374675,  0.5184206 ,  0.49532956,\n",
              "        -0.5020962 ,  0.44131464, -0.511372  , -0.520481  , -0.51613593,\n",
              "        -0.47702664,  0.5244433 , -0.47211188,  0.5319377 , -0.5278766 ,\n",
              "         0.5341041 , -0.46009898, -0.43544242, -0.46369302,  0.45333397,\n",
              "         0.48699868,  0.49263662,  0.47313395, -0.4996758 ,  0.4880943 ],\n",
              "       dtype=float32),\n",
              " 'actual': array([ 0.49093673,  0.49215442, -0.46214983, -0.5182979 , -0.46620107,\n",
              "         0.4847177 ,  0.44032788, -0.5321839 , -0.4601256 , -0.5026978 ,\n",
              "        -0.54049575,  0.5296926 ,  0.46760526,  0.47388005, -0.49849468,\n",
              "        -0.49070457,  0.47210184,  0.5007342 , -0.51020247, -0.47366613,\n",
              "        -0.502564  ,  0.44322842,  0.48491207,  0.5252728 , -0.51197475,\n",
              "        -0.48509416,  0.51300305,  0.5264408 ,  0.49484298,  0.48840255,\n",
              "        -0.54025567,  0.52278894, -0.5051255 , -0.49291784, -0.45105505,\n",
              "        -0.53925025,  0.46026322, -0.5357194 ,  0.4866697 , -0.47817972,\n",
              "         0.4536295 , -0.47121292, -0.52257776, -0.47377384,  0.46076494,\n",
              "         0.45290473,  0.48971316,  0.4764314 , -0.48300746,  0.500298  ],\n",
              "       dtype=float32),\n",
              " 'include': array([ 0.5388146 ,  0.42896265, -0.5128793 , -0.50840646, -0.5260165 ,\n",
              "         0.47037426,  0.5169298 , -0.48384565, -0.51009953, -0.44775268,\n",
              "        -0.517008  ,  0.46510035,  0.5239274 ,  0.47168684, -0.51847106,\n",
              "        -0.4815094 ,  0.5304891 ,  0.5003007 , -0.5012418 , -0.47228873,\n",
              "        -0.52683127,  0.51779795,  0.50669044,  0.44833443, -0.456701  ,\n",
              "        -0.47771928,  0.49179423,  0.5401272 ,  0.43959525,  0.49265555,\n",
              "        -0.45437583,  0.48310858, -0.46728265, -0.46297547, -0.49237263,\n",
              "        -0.46808606,  0.46485764, -0.52445096,  0.48571637, -0.5228043 ,\n",
              "         0.5089138 , -0.47243142, -0.44108924, -0.45513272,  0.45677745,\n",
              "         0.47476864,  0.49827242,  0.4909749 , -0.51809037,  0.47994938],\n",
              "       dtype=float32),\n",
              " 'exchange': array([ 0.55246633,  0.47629273, -0.48299912, -0.4524951 , -0.51466435,\n",
              "         0.46387547,  0.513759  , -0.4750978 , -0.5179175 , -0.5102339 ,\n",
              "        -0.49477676,  0.46320954,  0.5021933 ,  0.526783  , -0.5664499 ,\n",
              "        -0.4517878 ,  0.4527422 ,  0.4874139 , -0.44940436, -0.4777533 ,\n",
              "        -0.45288846,  0.4857842 ,  0.45616686,  0.513609  , -0.44163656,\n",
              "        -0.5130242 ,  0.44326603,  0.46925843,  0.4523475 ,  0.5189443 ,\n",
              "        -0.5330138 ,  0.46917653, -0.49342644, -0.53888917, -0.4980766 ,\n",
              "        -0.47427553,  0.4986801 , -0.4966567 ,  0.45246664, -0.5329068 ,\n",
              "         0.5226311 , -0.46866354, -0.44676182, -0.47074544,  0.44530055,\n",
              "         0.46432307,  0.48177338,  0.48018414, -0.44658878,  0.47924122],\n",
              "       dtype=float32),\n",
              " 'currency': array([ 0.46970746,  0.46003366, -0.5183713 , -0.47669294, -0.48356813,\n",
              "         0.5239004 ,  0.48039007, -0.47985947, -0.43897656, -0.44658095,\n",
              "        -0.47268775,  0.49796265,  0.48052025,  0.50641996, -0.50040936,\n",
              "        -0.46644533,  0.47285408,  0.53493845, -0.48168072, -0.4689166 ,\n",
              "        -0.48241448,  0.51687074,  0.51896805,  0.4478444 , -0.50320137,\n",
              "        -0.4653136 ,  0.5087722 ,  0.5141523 ,  0.51163054,  0.46180305,\n",
              "        -0.5337682 ,  0.5297893 , -0.48741356, -0.51427215, -0.5182626 ,\n",
              "        -0.49486643,  0.48148862, -0.44666615,  0.4830648 , -0.48868173,\n",
              "         0.45419988, -0.44994882, -0.48181623, -0.46347442,  0.47419968,\n",
              "         0.45846292,  0.4988182 ,  0.44756857, -0.45647398,  0.5112077 ],\n",
              "       dtype=float32),\n",
              " 'results': array([ 0.4914142 ,  0.48761946, -0.4792805 , -0.46757522, -0.5007175 ,\n",
              "         0.45671746,  0.50270987, -0.50929767, -0.44355533, -0.5163438 ,\n",
              "        -0.46772292,  0.47234765,  0.4879374 ,  0.46677777, -0.5051966 ,\n",
              "        -0.4990064 ,  0.5153458 ,  0.5029662 , -0.51271266, -0.522184  ,\n",
              "        -0.5169743 ,  0.45123136,  0.46889374,  0.44767097, -0.5217539 ,\n",
              "        -0.5157199 ,  0.5260837 ,  0.4661211 ,  0.49109942,  0.4416663 ,\n",
              "        -0.5206785 ,  0.5246213 , -0.5463187 , -0.4709726 , -0.48685005,\n",
              "        -0.5319094 ,  0.45067495, -0.45891517,  0.5114367 , -0.5051518 ,\n",
              "         0.4510939 , -0.535177  , -0.43639237, -0.44869614,  0.48625097,\n",
              "         0.5066153 ,  0.47747174,  0.44040617, -0.5253609 ,  0.47351483],\n",
              "       dtype=float32),\n",
              " 'allocation': array([ 0.51322246,  0.42920586, -0.461312  , -0.47103858, -0.4513695 ,\n",
              "         0.4923698 ,  0.50868124, -0.4687878 , -0.52113897, -0.51354414,\n",
              "        -0.5172084 ,  0.51978403,  0.5025325 ,  0.4887979 , -0.49736312,\n",
              "        -0.50949764,  0.52932394,  0.46532995, -0.5150933 , -0.47203296,\n",
              "        -0.44879943,  0.47848135,  0.4814527 ,  0.48815352, -0.5009912 ,\n",
              "        -0.48010036,  0.44170213,  0.46519625,  0.46187142,  0.44468868,\n",
              "        -0.527652  ,  0.48558706, -0.50887835, -0.48180985, -0.4349549 ,\n",
              "        -0.4608652 ,  0.47767654, -0.4970748 ,  0.5039421 , -0.44793567,\n",
              "         0.5365625 , -0.5113107 , -0.49888828, -0.49590555,  0.5319764 ,\n",
              "         0.44906044,  0.49215144,  0.4983778 , -0.44644433,  0.50145173],\n",
              "       dtype=float32),\n",
              " 'strategies': array([ 0.53855455,  0.45420846, -0.54995525, -0.5029263 , -0.5193913 ,\n",
              "         0.4904142 ,  0.4944491 , -0.47355878, -0.4754267 , -0.46047187,\n",
              "        -0.47706464,  0.47875804,  0.5213595 ,  0.48088035, -0.49385586,\n",
              "        -0.45417836,  0.51233506,  0.50406694, -0.52722824, -0.5306468 ,\n",
              "        -0.46940434,  0.5258038 ,  0.52081513,  0.50852215, -0.53370833,\n",
              "        -0.4665858 ,  0.4906814 ,  0.49415362,  0.4836223 ,  0.5267729 ,\n",
              "        -0.45761126,  0.5225041 , -0.5326519 , -0.46931297, -0.49206638,\n",
              "        -0.48235115,  0.5185849 , -0.49729708,  0.49224222, -0.5143176 ,\n",
              "         0.522406  , -0.4557627 , -0.4652336 , -0.5099554 ,  0.4698612 ,\n",
              "         0.48026603,  0.46360892,  0.46696165, -0.52201337,  0.46350965],\n",
              "       dtype=float32),\n",
              " 'period': array([ 0.5246851 ,  0.49344504, -0.4681498 , -0.50138855, -0.49412787,\n",
              "         0.4539776 ,  0.5266521 , -0.4962037 , -0.49191007, -0.4479319 ,\n",
              "        -0.493935  ,  0.4845753 ,  0.49746066,  0.48622668, -0.5579149 ,\n",
              "        -0.5260046 ,  0.5053892 ,  0.46632266, -0.48282272, -0.4316082 ,\n",
              "        -0.5080851 ,  0.47930032,  0.49096468,  0.43917012, -0.47364247,\n",
              "        -0.4458166 ,  0.478422  ,  0.4813739 ,  0.47285065,  0.47729185,\n",
              "        -0.5382308 ,  0.48447996, -0.49971202, -0.4992444 , -0.49939743,\n",
              "        -0.50812906,  0.53632665, -0.5027799 ,  0.44279355, -0.47993836,\n",
              "         0.49927115, -0.5378533 , -0.45210856, -0.48401126,  0.47128943,\n",
              "         0.4941971 ,  0.46770987,  0.5141684 , -0.47116247,  0.4900027 ],\n",
              "       dtype=float32),\n",
              " 'obligations': array([ 0.5225492 ,  0.4484867 , -0.5011048 , -0.4396417 , -0.46671665,\n",
              "         0.4899798 ,  0.5087535 , -0.4511098 , -0.51462156, -0.46948832,\n",
              "        -0.4333395 ,  0.4888234 ,  0.47777712,  0.44497252, -0.5479935 ,\n",
              "        -0.5226501 ,  0.51316947,  0.48556718, -0.47249323, -0.4528986 ,\n",
              "        -0.50751346,  0.4359568 ,  0.5002494 ,  0.4712962 , -0.48577738,\n",
              "        -0.47118112,  0.4904124 ,  0.53682494,  0.4524497 ,  0.48110753,\n",
              "        -0.46834487,  0.46217698, -0.46934596, -0.45219728, -0.5018097 ,\n",
              "        -0.46168053,  0.5152671 , -0.520517  ,  0.46471375, -0.51208377,\n",
              "         0.48450577, -0.4698884 , -0.42981562, -0.50437135,  0.5095107 ,\n",
              "         0.46510506,  0.524639  ,  0.45567232, -0.4720118 ,  0.46995285],\n",
              "       dtype=float32),\n",
              " 'redeem': array([ 0.525408  ,  0.46476474, -0.49607438, -0.49933377, -0.48475447,\n",
              "         0.46161973,  0.47918585, -0.52591807, -0.5072077 , -0.46887338,\n",
              "        -0.473683  ,  0.4583876 ,  0.48012906,  0.46688727, -0.51719433,\n",
              "        -0.5334041 ,  0.497641  ,  0.50007874, -0.52021337, -0.46986833,\n",
              "        -0.54299396,  0.5279057 ,  0.4569765 ,  0.46762508, -0.50833803,\n",
              "        -0.5385524 ,  0.48643574,  0.5150018 ,  0.48252055,  0.4815113 ,\n",
              "        -0.46058744,  0.5262322 , -0.47658372, -0.5207572 , -0.5068231 ,\n",
              "        -0.48283303,  0.50619984, -0.51463485,  0.51403785, -0.45409632,\n",
              "         0.4694671 , -0.52738047, -0.4684292 , -0.5035961 ,  0.5369903 ,\n",
              "         0.47511676,  0.5433103 ,  0.4951986 , -0.46012512,  0.47155428],\n",
              "       dtype=float32),\n",
              " 'factors': array([ 0.55220205,  0.44893667, -0.4480601 , -0.4810841 , -0.49143443,\n",
              "         0.5058652 ,  0.50791895, -0.50505495, -0.517066  , -0.52469957,\n",
              "        -0.49006805,  0.5036314 ,  0.51668805,  0.5163572 , -0.4956145 ,\n",
              "        -0.461232  ,  0.44728717,  0.44584122, -0.4954924 , -0.44394523,\n",
              "        -0.50402886,  0.48720783,  0.5091765 ,  0.5006614 , -0.43471244,\n",
              "        -0.50739956,  0.50628656,  0.46957242,  0.4532047 ,  0.49025136,\n",
              "        -0.47819087,  0.4989479 , -0.5328901 , -0.4923985 , -0.46180925,\n",
              "        -0.5361634 ,  0.49401477, -0.50747377,  0.45380726, -0.472263  ,\n",
              "         0.46015188, -0.52288187, -0.4400581 , -0.44785348,  0.51292217,\n",
              "         0.4648553 ,  0.50269514,  0.4682232 , -0.4521126 ,  0.4514317 ],\n",
              "       dtype=float32),\n",
              " 'adverse': array([ 0.4691069 ,  0.49201405, -0.4751327 , -0.48431376, -0.4886067 ,\n",
              "         0.46647155,  0.46510932, -0.4992687 , -0.46520567, -0.47181147,\n",
              "        -0.4522042 ,  0.50521266,  0.5217807 ,  0.48324165, -0.4844595 ,\n",
              "        -0.5228474 ,  0.44738147,  0.5134909 , -0.43329084, -0.4546289 ,\n",
              "        -0.513466  ,  0.44454062,  0.5126765 ,  0.45162356, -0.48342195,\n",
              "        -0.4951083 ,  0.46599537,  0.5217484 ,  0.50304174,  0.48499367,\n",
              "        -0.47517487,  0.43886572, -0.4949665 , -0.48578158, -0.5216759 ,\n",
              "        -0.45482895,  0.51848185, -0.51857096,  0.5118315 , -0.53418016,\n",
              "         0.47649905, -0.45320773, -0.48682374, -0.45969877,  0.51948225,\n",
              "         0.4940057 ,  0.48157674,  0.45497745, -0.49101424,  0.46037078],\n",
              "       dtype=float32),\n",
              " 'redemption': array([ 0.52903295,  0.4524399 , -0.4505792 , -0.46731684, -0.4700519 ,\n",
              "         0.4581097 ,  0.49400383, -0.46465975, -0.445912  , -0.45822206,\n",
              "        -0.4662119 ,  0.50627446,  0.4343177 ,  0.46457666, -0.51751804,\n",
              "        -0.45893583,  0.5105796 ,  0.46347743, -0.4542162 , -0.48694772,\n",
              "        -0.48347867,  0.47757652,  0.52049655,  0.46721905, -0.45059115,\n",
              "        -0.52176636,  0.5210582 ,  0.52017385,  0.45356485,  0.48498082,\n",
              "        -0.50233734,  0.48268598, -0.52283865, -0.5368963 , -0.5002482 ,\n",
              "        -0.45067158,  0.52871543, -0.5154977 ,  0.4978345 , -0.45365736,\n",
              "         0.4587345 , -0.48655874, -0.46309742, -0.51753753,  0.4760361 ,\n",
              "         0.5319159 ,  0.51508164,  0.50702244, -0.49482995,  0.5262981 ],\n",
              "       dtype=float32),\n",
              " 'associated': array([ 0.49512002,  0.50458515, -0.49669242, -0.4459721 , -0.4870946 ,\n",
              "         0.45879143,  0.47200137, -0.4714594 , -0.45792907, -0.46741605,\n",
              "        -0.47205824,  0.45360035,  0.5185738 ,  0.46499544, -0.5311435 ,\n",
              "        -0.45804742,  0.51754206,  0.53349686, -0.51178724, -0.4814907 ,\n",
              "        -0.47490716,  0.44854   ,  0.53059995,  0.526175  , -0.46207002,\n",
              "        -0.5355734 ,  0.50195295,  0.54750955,  0.5088248 ,  0.48629233,\n",
              "        -0.48642716,  0.46761885, -0.48723724, -0.5073775 , -0.52368647,\n",
              "        -0.47203472,  0.5117459 , -0.47110903,  0.46591207, -0.5232567 ,\n",
              "         0.49337092, -0.5430996 , -0.45997423, -0.52298075,  0.52042997,\n",
              "         0.48546827,  0.5026547 ,  0.53014797, -0.4522031 ,  0.49950755],\n",
              "       dtype=float32),\n",
              " 'bond': array([ 0.47256172,  0.42470422, -0.4418133 , -0.44815633, -0.5028365 ,\n",
              "         0.45840976,  0.51475835, -0.46666458, -0.5121977 , -0.42438468,\n",
              "        -0.4343332 ,  0.5267358 ,  0.50009555,  0.4588916 , -0.48465964,\n",
              "        -0.49271256,  0.4973277 ,  0.49297118, -0.43609214, -0.50675756,\n",
              "        -0.49213225,  0.48723924,  0.4983703 ,  0.44416544, -0.51121753,\n",
              "        -0.49514437,  0.50690395,  0.50411075,  0.51304585,  0.49297544,\n",
              "        -0.5234869 ,  0.50686276, -0.48997724, -0.4811321 , -0.4901756 ,\n",
              "        -0.5135609 ,  0.4500634 , -0.50464076,  0.4706273 , -0.50497454,\n",
              "         0.45964718, -0.5258887 , -0.481071  , -0.4496041 ,  0.5161101 ,\n",
              "         0.47584808,  0.49591488,  0.4990129 , -0.46609342,  0.48658785],\n",
              "       dtype=float32),\n",
              " 'shareholder': array([ 0.49802044,  0.51700324, -0.4548801 , -0.5006509 , -0.5042012 ,\n",
              "         0.4655847 ,  0.5302714 , -0.49282217, -0.51984966, -0.5190307 ,\n",
              "        -0.4597744 ,  0.5256674 ,  0.4597122 ,  0.46782613, -0.5583201 ,\n",
              "        -0.5122163 ,  0.5024023 ,  0.4861677 , -0.48589414, -0.49187124,\n",
              "        -0.5077009 ,  0.5251678 ,  0.5351871 ,  0.51901436, -0.47477964,\n",
              "        -0.44574693,  0.48409757,  0.5388467 ,  0.47329247,  0.5097078 ,\n",
              "        -0.5229548 ,  0.46776265, -0.5068085 , -0.47860283, -0.479019  ,\n",
              "        -0.52233225,  0.48471165, -0.4459333 ,  0.5155874 , -0.5261765 ,\n",
              "         0.45432207, -0.5137465 , -0.46955562, -0.4706323 ,  0.45514172,\n",
              "         0.46409324,  0.44699463,  0.5106229 , -0.4988982 ,  0.45221043],\n",
              "       dtype=float32),\n",
              " 'available': array([ 0.4766995 ,  0.49380028, -0.4802516 , -0.5078887 , -0.4725148 ,\n",
              "         0.5152667 ,  0.51290333, -0.49239585, -0.5168247 , -0.45757425,\n",
              "        -0.48992297,  0.48444864,  0.48398882,  0.433632  , -0.56093323,\n",
              "        -0.5079657 ,  0.49013028,  0.43632698, -0.5073368 , -0.49958512,\n",
              "        -0.48130462,  0.49884427,  0.51289403,  0.42870206, -0.47028843,\n",
              "        -0.5228874 ,  0.46346256,  0.54788125,  0.46561202,  0.46609873,\n",
              "        -0.51599514,  0.49773228, -0.48154864, -0.47905192, -0.4814394 ,\n",
              "        -0.51453066,  0.5228781 , -0.46325868,  0.48616832, -0.47633213,\n",
              "         0.4429137 , -0.5312441 , -0.49605307, -0.47728574,  0.4702902 ,\n",
              "         0.5237103 ,  0.47991648,  0.51332426, -0.5141093 ,  0.44072253],\n",
              "       dtype=float32),\n",
              " 'applicable': array([ 0.465891  ,  0.483201  , -0.46169356, -0.5063215 , -0.4493353 ,\n",
              "         0.5329669 ,  0.45538703, -0.50757945, -0.4477293 , -0.48206496,\n",
              "        -0.5023119 ,  0.45014632,  0.49852607,  0.46545792, -0.4788046 ,\n",
              "        -0.47135994,  0.46358287,  0.53753537, -0.47427648, -0.4693141 ,\n",
              "        -0.5104203 ,  0.44670865,  0.48214197,  0.47858444, -0.46145135,\n",
              "        -0.43872595,  0.49276125,  0.5046457 ,  0.52458096,  0.52993965,\n",
              "        -0.5359831 ,  0.50967926, -0.5100687 , -0.49652952, -0.5071676 ,\n",
              "        -0.43997142,  0.53074914, -0.51716083,  0.4931001 , -0.43997374,\n",
              "         0.50740355, -0.5059004 , -0.5184148 , -0.43638787,  0.51861495,\n",
              "         0.50018007,  0.4599478 ,  0.50209177, -0.45163697,  0.5121272 ],\n",
              "       dtype=float32),\n",
              " 'trading': array([ 0.48416018,  0.43862325, -0.5001773 , -0.49445176, -0.44649258,\n",
              "         0.50488585,  0.4582216 , -0.4438674 , -0.5065922 , -0.45765346,\n",
              "        -0.47209707,  0.5179948 ,  0.5127455 ,  0.48343405, -0.5102115 ,\n",
              "        -0.5027673 ,  0.49875256,  0.4936321 , -0.4909351 , -0.5014775 ,\n",
              "        -0.48826748,  0.4942624 ,  0.4874649 ,  0.50580305, -0.46706983,\n",
              "        -0.42744634,  0.4621352 ,  0.47505847,  0.47766846,  0.45236698,\n",
              "        -0.49426708,  0.44461992, -0.47896546, -0.52094835, -0.51363593,\n",
              "        -0.4500633 ,  0.524807  , -0.523812  ,  0.5091809 , -0.51030755,\n",
              "         0.44629657, -0.48666996, -0.49715364, -0.47726396,  0.49577054,\n",
              "         0.4531823 ,  0.50248736,  0.48347926, -0.44509625,  0.46635222],\n",
              "       dtype=float32),\n",
              " 'reflected': array([ 0.5177152 ,  0.45541957, -0.5232804 , -0.49225965, -0.5241872 ,\n",
              "         0.49570912,  0.48779637, -0.5101142 , -0.46290883, -0.5064369 ,\n",
              "        -0.5211957 ,  0.46770063,  0.49741244,  0.46699703, -0.5697069 ,\n",
              "        -0.4573232 ,  0.47458026,  0.53007466, -0.48148036, -0.5212728 ,\n",
              "        -0.50019455,  0.45512208,  0.5379133 ,  0.47128236, -0.53877914,\n",
              "        -0.4520516 ,  0.52549183,  0.5437532 ,  0.50373834,  0.4981944 ,\n",
              "        -0.52360475,  0.46330822, -0.5223831 , -0.4824883 , -0.44619843,\n",
              "        -0.4956675 ,  0.55121773, -0.4766385 ,  0.48008367, -0.48483956,\n",
              "         0.49075812, -0.47575304, -0.5243868 , -0.52523166,  0.5148672 ,\n",
              "         0.45965457,  0.5158277 ,  0.53617203, -0.5242224 ,  0.4721276 ],\n",
              "       dtype=float32),\n",
              " 'quality': array([ 0.50352776,  0.4277327 , -0.50426894, -0.48742574, -0.50060225,\n",
              "         0.5205244 ,  0.5070801 , -0.52032304, -0.44856974, -0.43363926,\n",
              "        -0.51612884,  0.45311734,  0.47202554,  0.4465061 , -0.53095007,\n",
              "        -0.46595275,  0.4795812 ,  0.4442322 , -0.5062517 , -0.50174385,\n",
              "        -0.4890673 ,  0.4682517 ,  0.52339584,  0.42857283, -0.4932202 ,\n",
              "        -0.49836013,  0.47493282,  0.4568637 ,  0.5000381 ,  0.5083089 ,\n",
              "        -0.48842943,  0.48437196, -0.45797506, -0.45760164, -0.48081973,\n",
              "        -0.5238886 ,  0.5271162 , -0.5065442 ,  0.46516514, -0.5163191 ,\n",
              "         0.49202704, -0.44543576, -0.4351002 , -0.5153677 ,  0.5050011 ,\n",
              "         0.45751074,  0.48092863,  0.4462398 , -0.51778036,  0.48612276],\n",
              "       dtype=float32),\n",
              " 'hold': array([ 0.5331684 ,  0.49046484, -0.5347543 , -0.49692982, -0.49901754,\n",
              "         0.51527584,  0.48088422, -0.44735625, -0.489982  , -0.46121702,\n",
              "        -0.53803307,  0.46892565,  0.49097753,  0.5132475 , -0.56554914,\n",
              "        -0.5325133 ,  0.48166472,  0.50940305, -0.46624035, -0.4686314 ,\n",
              "        -0.5083645 ,  0.47230718,  0.52729785,  0.45508528, -0.45089182,\n",
              "        -0.4531289 ,  0.51307476,  0.51932067,  0.4694584 ,  0.5326841 ,\n",
              "        -0.5182754 ,  0.48640525, -0.48193538, -0.46821952, -0.48541284,\n",
              "        -0.49628723,  0.5452225 , -0.52542496,  0.52303886, -0.5289148 ,\n",
              "         0.4565923 , -0.54049295, -0.49645537, -0.51828146,  0.46662515,\n",
              "         0.497253  ,  0.5357688 ,  0.52459824, -0.4971883 ,  0.4638242 ],\n",
              "       dtype=float32),\n",
              " 'following': array([ 0.49692228,  0.43297282, -0.47545883, -0.5168126 , -0.46897155,\n",
              "         0.46493378,  0.46471447, -0.4478153 , -0.48351738, -0.4666373 ,\n",
              "        -0.5391867 ,  0.4743323 ,  0.5292834 ,  0.44988558, -0.577307  ,\n",
              "        -0.46228752,  0.47053906,  0.5241648 , -0.4661696 , -0.5176789 ,\n",
              "        -0.49211907,  0.4762023 ,  0.45531788,  0.50924605, -0.4849469 ,\n",
              "        -0.4522382 ,  0.5250406 ,  0.47151726,  0.47042662,  0.4459771 ,\n",
              "        -0.47450525,  0.47175315, -0.54297024, -0.46767086, -0.5108958 ,\n",
              "        -0.49788964,  0.53094083, -0.45286852,  0.5142231 , -0.5342212 ,\n",
              "         0.49340367, -0.54020864, -0.5173825 , -0.5113846 ,  0.5117729 ,\n",
              "         0.5272249 ,  0.5113442 ,  0.52397335, -0.47504032,  0.5297991 ],\n",
              "       dtype=float32),\n",
              " 'intermediaries': array([ 0.46456644,  0.47520632, -0.47185773, -0.46241024, -0.5178952 ,\n",
              "         0.4947095 ,  0.529322  , -0.44964907, -0.449432  , -0.46352205,\n",
              "        -0.51755023,  0.4992797 ,  0.52610207,  0.48572218, -0.51346236,\n",
              "        -0.4541268 ,  0.5076039 ,  0.46881336, -0.49037126, -0.45232722,\n",
              "        -0.5061332 ,  0.5155398 ,  0.53805166,  0.45968592, -0.5281092 ,\n",
              "        -0.44881818,  0.53013015,  0.51869416,  0.510385  ,  0.48682255,\n",
              "        -0.5262035 ,  0.4722414 , -0.51639324, -0.52971506, -0.52952665,\n",
              "        -0.5301009 ,  0.51093686, -0.50247693,  0.5300047 , -0.5045381 ,\n",
              "         0.47975183, -0.51194024, -0.47944745, -0.4787268 ,  0.48117298,\n",
              "         0.50111264,  0.48778912,  0.4427436 , -0.4743214 ,  0.4961604 ],\n",
              "       dtype=float32),\n",
              " 'buy': array([ 0.53847003,  0.51497823, -0.49150562, -0.46038812, -0.4641813 ,\n",
              "         0.53003746,  0.49363053, -0.45731512, -0.4643565 , -0.49659908,\n",
              "        -0.4682968 ,  0.47218776,  0.4517706 ,  0.5454823 , -0.5614296 ,\n",
              "        -0.5102254 ,  0.53087795,  0.4849558 , -0.47909418, -0.5166574 ,\n",
              "        -0.49526873,  0.5033501 ,  0.52819926,  0.4959237 , -0.4881703 ,\n",
              "        -0.50826776,  0.5164418 ,  0.55537355,  0.518769  ,  0.5058551 ,\n",
              "        -0.4789911 ,  0.5278185 , -0.5370427 , -0.51722836, -0.46590763,\n",
              "        -0.488205  ,  0.45994574, -0.4611739 ,  0.5327237 , -0.48633394,\n",
              "         0.46626192, -0.5588788 , -0.46783033, -0.5114217 ,  0.5202254 ,\n",
              "         0.5363707 ,  0.5213218 ,  0.45475912, -0.53249556,  0.50254285],\n",
              "       dtype=float32),\n",
              " 'similar': array([ 0.49036407,  0.4878521 , -0.50870156, -0.46681306, -0.4989912 ,\n",
              "         0.49071556,  0.4638228 , -0.45339814, -0.4663597 , -0.5239956 ,\n",
              "        -0.45207477,  0.5277628 ,  0.4757404 ,  0.5133093 , -0.505172  ,\n",
              "        -0.47569004,  0.45662227,  0.4913731 , -0.5030366 , -0.45835453,\n",
              "        -0.53177464,  0.5044583 ,  0.53495586,  0.4411743 , -0.5257973 ,\n",
              "        -0.4818747 ,  0.4656109 ,  0.52186394,  0.4930907 ,  0.4590092 ,\n",
              "        -0.49946243,  0.46201947, -0.51263344, -0.47975054, -0.46562505,\n",
              "        -0.54098755,  0.54610085, -0.48074567,  0.46734905, -0.46889886,\n",
              "         0.51170695, -0.49065816, -0.44080216, -0.48461944,  0.52430797,\n",
              "         0.48218334,  0.48940033,  0.47896343, -0.53648996,  0.44678354],\n",
              "       dtype=float32),\n",
              " 'losses': array([ 0.5048976 ,  0.43929422, -0.4981342 , -0.5064514 , -0.5035932 ,\n",
              "         0.48537263,  0.4603675 , -0.52837795, -0.4984287 , -0.43391925,\n",
              "        -0.45072728,  0.47481668,  0.4832001 ,  0.49987206, -0.5620321 ,\n",
              "        -0.455363  ,  0.473951  ,  0.46830708, -0.47255468, -0.4818244 ,\n",
              "        -0.46747106,  0.5189227 ,  0.52407074,  0.44794977, -0.47523814,\n",
              "        -0.49185085,  0.49863246,  0.46976   ,  0.48426664,  0.46492893,\n",
              "        -0.490667  ,  0.48668993, -0.4702562 , -0.50791204, -0.4828965 ,\n",
              "        -0.47939512,  0.4655578 , -0.447349  ,  0.5054428 , -0.5209105 ,\n",
              "         0.46055046, -0.53320473, -0.4932948 , -0.5106217 ,  0.52896607,\n",
              "         0.48413852,  0.45108014,  0.5053988 , -0.4505169 ,  0.47790304],\n",
              "       dtype=float32),\n",
              " 'quarter': array([ 0.4906701 ,  0.42198122, -0.5020946 , -0.52194226, -0.50819784,\n",
              "         0.4650267 ,  0.43410236, -0.4541035 , -0.43898675, -0.44500244,\n",
              "        -0.49871814,  0.4671159 ,  0.46317646,  0.4702472 , -0.51559204,\n",
              "        -0.4718012 ,  0.5301656 ,  0.44306153, -0.49972093, -0.49850643,\n",
              "        -0.5163324 ,  0.49631965,  0.4979276 ,  0.48160887, -0.47296938,\n",
              "        -0.5224125 ,  0.4300005 ,  0.48152497,  0.4689453 ,  0.5188168 ,\n",
              "        -0.49512166,  0.44229323, -0.48559585, -0.5123424 , -0.48581713,\n",
              "        -0.44972575,  0.49029547, -0.4815615 ,  0.4533109 , -0.48104775,\n",
              "         0.47644517, -0.5350089 , -0.49504334, -0.51414865,  0.47347584,\n",
              "         0.45191938,  0.43431512,  0.48407984, -0.46058863,  0.49533755],\n",
              "       dtype=float32),\n",
              " 'due': array([ 0.4820484 ,  0.4572728 , -0.5094549 , -0.5065752 , -0.5061186 ,\n",
              "         0.5318257 ,  0.48887122, -0.45759192, -0.50073826, -0.5027954 ,\n",
              "        -0.4953702 ,  0.5267013 ,  0.4554859 ,  0.50418216, -0.5184054 ,\n",
              "        -0.4414297 ,  0.5259623 ,  0.48986322, -0.4861916 , -0.5026412 ,\n",
              "        -0.5045836 ,  0.48171246,  0.44227996,  0.475668  , -0.47135782,\n",
              "        -0.5108091 ,  0.44837612,  0.5205263 ,  0.47602558,  0.4769803 ,\n",
              "        -0.5217168 ,  0.43876004, -0.50914264, -0.5071216 , -0.4541291 ,\n",
              "        -0.5189081 ,  0.4458116 , -0.5042926 ,  0.43923944, -0.4565892 ,\n",
              "         0.502993  , -0.48970428, -0.5159013 , -0.44815612,  0.49273112,\n",
              "         0.45312503,  0.5015747 ,  0.4462183 , -0.46814507,  0.48013833],\n",
              "       dtype=float32),\n",
              " 'current': array([ 0.5162583 ,  0.47598174, -0.4885545 , -0.4592777 , -0.47426113,\n",
              "         0.5208376 ,  0.48901552, -0.5304247 , -0.52553225, -0.4559077 ,\n",
              "        -0.4960969 ,  0.49098772,  0.49422428,  0.45017636, -0.56166047,\n",
              "        -0.48409685,  0.45089936,  0.5319654 , -0.46352813, -0.4789212 ,\n",
              "        -0.48557922,  0.47046423,  0.5029944 ,  0.45409077, -0.48540208,\n",
              "        -0.48170337,  0.47945452,  0.4851922 ,  0.49764806,  0.47100902,\n",
              "        -0.5362632 ,  0.50861335, -0.46665537, -0.45416504, -0.4861733 ,\n",
              "        -0.45034003,  0.4538427 , -0.52984583,  0.44483888, -0.48820478,\n",
              "         0.5157927 , -0.45721415, -0.46257663, -0.48814976,  0.4893837 ,\n",
              "         0.47831488,  0.46019068,  0.46951646, -0.52837735,  0.4626512 ],\n",
              "       dtype=float32),\n",
              " 'future': array([ 0.5452674 ,  0.431538  , -0.47927225, -0.5228784 , -0.46824965,\n",
              "         0.52705574,  0.440909  , -0.45909038, -0.5048084 , -0.47738516,\n",
              "        -0.49238378,  0.47671983,  0.46719652,  0.5184985 , -0.4908021 ,\n",
              "        -0.48503777,  0.4786667 ,  0.52094984, -0.44612917, -0.47388405,\n",
              "        -0.5057363 ,  0.4450241 ,  0.5082945 ,  0.46217045, -0.50154656,\n",
              "        -0.5210172 ,  0.5171966 ,  0.54387486,  0.5173626 ,  0.5026452 ,\n",
              "        -0.49759552,  0.4771164 , -0.4963795 , -0.52017933, -0.5119141 ,\n",
              "        -0.46065673,  0.4443395 , -0.49518996,  0.5242402 , -0.45805085,\n",
              "         0.5174971 , -0.4680524 , -0.4965867 , -0.4978199 ,  0.43618563,\n",
              "         0.53082097,  0.44399732,  0.5137667 , -0.43656656,  0.49996   ],\n",
              "       dtype=float32),\n",
              " 'bar': array([ 0.5416574 ,  0.4844467 , -0.46075127, -0.5216968 , -0.45566386,\n",
              "         0.5314139 ,  0.48979262, -0.5344065 , -0.45159948, -0.5000388 ,\n",
              "        -0.52739394,  0.5031473 ,  0.4910856 ,  0.483055  , -0.5019853 ,\n",
              "        -0.4564588 ,  0.47847557,  0.44661802, -0.5200443 , -0.5029911 ,\n",
              "        -0.48972413,  0.5216295 ,  0.51527864,  0.45376533, -0.5200746 ,\n",
              "        -0.51526487,  0.48594522,  0.47304475,  0.43806446,  0.45220003,\n",
              "        -0.47686765,  0.4849857 , -0.47773138, -0.4644295 , -0.5051412 ,\n",
              "        -0.51861364,  0.46041107, -0.533169  ,  0.4596762 , -0.47401795,\n",
              "         0.51137394, -0.5367094 , -0.49898183, -0.49483427,  0.46891966,\n",
              "         0.46163285,  0.4665144 ,  0.48942822, -0.46746743,  0.5074761 ],\n",
              "       dtype=float32),\n",
              " 'amount': array([ 0.5074749 ,  0.43216538, -0.5239076 , -0.49766153, -0.46001866,\n",
              "         0.53049594,  0.4672579 , -0.47256058, -0.4984696 , -0.48098877,\n",
              "        -0.51137954,  0.48461178,  0.48648763,  0.4573921 , -0.49331692,\n",
              "        -0.47194475,  0.48064354,  0.46042195, -0.4724161 , -0.5154939 ,\n",
              "        -0.4477118 ,  0.4903943 ,  0.47686   ,  0.43337584, -0.5119449 ,\n",
              "        -0.45129624,  0.52718437,  0.46719772,  0.465213  ,  0.47227022,\n",
              "        -0.51303035,  0.5000503 , -0.54491746, -0.52427   , -0.51622653,\n",
              "        -0.45207775,  0.45653403, -0.5018596 ,  0.4398996 , -0.51634115,\n",
              "         0.47508258, -0.46543133, -0.44066536, -0.5177947 ,  0.53361595,\n",
              "         0.45897207,  0.45733237,  0.5072472 , -0.51302224,  0.50555307],\n",
              "       dtype=float32),\n",
              " 'perform': array([ 0.5167426 ,  0.46863902, -0.46065947, -0.5304481 , -0.4729916 ,\n",
              "         0.4913604 ,  0.4711274 , -0.5306274 , -0.5064375 , -0.4582219 ,\n",
              "        -0.47310007,  0.5381634 ,  0.4984448 ,  0.5204585 , -0.55523276,\n",
              "        -0.45828286,  0.47473174,  0.5211727 , -0.46856987, -0.5261381 ,\n",
              "        -0.5135987 ,  0.46060437,  0.52027637,  0.51818496, -0.47634423,\n",
              "        -0.46408343,  0.5053386 ,  0.48188785,  0.46586734,  0.44576338,\n",
              "        -0.45630372,  0.50818974, -0.4703917 , -0.49564624, -0.46307874,\n",
              "        -0.48913717,  0.4651829 , -0.4807321 ,  0.52447265, -0.53151387,\n",
              "         0.4956249 , -0.52776873, -0.50121677, -0.48342553,  0.4636896 ,\n",
              "         0.5260476 ,  0.4490254 ,  0.5396322 , -0.45487103,  0.50215024],\n",
              "       dtype=float32),\n",
              " 'lose': array([ 0.48676535,  0.4719171 , -0.51984394, -0.45914605, -0.45209664,\n",
              "         0.48277774,  0.5197842 , -0.49735457, -0.5009338 , -0.50505334,\n",
              "        -0.51324165,  0.46794218,  0.47650453,  0.52219975, -0.55645066,\n",
              "        -0.51137805,  0.49075472,  0.46703854, -0.483765  , -0.5205133 ,\n",
              "        -0.5203387 ,  0.5200689 ,  0.47202092,  0.48580182, -0.5300374 ,\n",
              "        -0.5266435 ,  0.51577026,  0.5356757 ,  0.45464694,  0.45438874,\n",
              "        -0.48477724,  0.4718453 , -0.48542207, -0.5178987 , -0.4877039 ,\n",
              "        -0.5211543 ,  0.50412375, -0.4683706 ,  0.52654403, -0.49704817,\n",
              "         0.48294288, -0.5017153 , -0.44106913, -0.5135047 ,  0.47366711,\n",
              "         0.52714324,  0.53359497,  0.52452564, -0.50415885,  0.4855976 ],\n",
              "       dtype=float32),\n",
              " 'sector': array([ 0.47520316,  0.49398917, -0.5331272 , -0.4437357 , -0.531868  ,\n",
              "         0.4722864 ,  0.517136  , -0.45722407, -0.51859725, -0.5183422 ,\n",
              "        -0.5261152 ,  0.5255252 ,  0.5102157 ,  0.45064113, -0.5112415 ,\n",
              "        -0.46452338,  0.5115955 ,  0.51497716, -0.45767713, -0.45826352,\n",
              "        -0.4528629 ,  0.5015065 ,  0.45039493,  0.4719736 , -0.500564  ,\n",
              "        -0.46663913,  0.47201395,  0.513821  ,  0.45045418,  0.50471437,\n",
              "        -0.49168438,  0.4379582 , -0.4803842 , -0.5249628 , -0.48294407,\n",
              "        -0.48601097,  0.52623963, -0.4502295 ,  0.48434824, -0.5082149 ,\n",
              "         0.47796512, -0.4623872 , -0.44397944, -0.50625783,  0.4889137 ,\n",
              "         0.4428825 ,  0.53150016,  0.50059843, -0.45472598,  0.5009535 ],\n",
              "       dtype=float32),\n",
              " 'service': array([ 0.5037589 ,  0.43875116, -0.53010696, -0.4436973 , -0.5185694 ,\n",
              "         0.5122748 ,  0.46838403, -0.50262356, -0.46832883, -0.48389733,\n",
              "        -0.4555381 ,  0.46069455,  0.47824204,  0.49338531, -0.508599  ,\n",
              "        -0.5351448 ,  0.4487246 ,  0.46307665, -0.4458184 , -0.48889196,\n",
              "        -0.53436947,  0.51287943,  0.4908657 ,  0.4914894 , -0.4626745 ,\n",
              "        -0.48065385,  0.47361445,  0.46449357,  0.49834135,  0.5016399 ,\n",
              "        -0.46414366,  0.4647989 , -0.49755794, -0.49782592, -0.4899249 ,\n",
              "        -0.5174429 ,  0.51425755, -0.51551294,  0.50584394, -0.47683433,\n",
              "         0.5098956 , -0.5203347 , -0.47062048, -0.44918272,  0.5226791 ,\n",
              "         0.47314942,  0.49484745,  0.48937702, -0.50121874,  0.510113  ],\n",
              "       dtype=float32),\n",
              " 'percentage': array([ 0.54941684,  0.50615084, -0.51458466, -0.507694  , -0.52078134,\n",
              "         0.47310603,  0.47579864, -0.4431895 , -0.5079261 , -0.5167745 ,\n",
              "        -0.4696428 ,  0.4841632 ,  0.51134765,  0.5340561 , -0.55100214,\n",
              "        -0.4796625 ,  0.5176303 ,  0.51910573, -0.5217432 , -0.4577961 ,\n",
              "        -0.51958233,  0.4377039 ,  0.53541964,  0.5216435 , -0.45941144,\n",
              "        -0.4546095 ,  0.4422265 ,  0.4712817 ,  0.50477207,  0.49771982,\n",
              "        -0.45115295,  0.5048446 , -0.5185237 , -0.50407827, -0.51779234,\n",
              "        -0.477966  ,  0.46906215, -0.4489255 ,  0.5178219 , -0.47423458,\n",
              "         0.5358635 , -0.49140877, -0.4831366 , -0.49983078,  0.47076258,\n",
              "         0.5041251 ,  0.44690755,  0.45036447, -0.50223225,  0.45842335],\n",
              "       dtype=float32),\n",
              " 'business': array([ 0.5081217 ,  0.4394685 , -0.4780738 , -0.44953257, -0.5052141 ,\n",
              "         0.48872054,  0.521969  , -0.45602405, -0.4598819 , -0.5113637 ,\n",
              "        -0.46004504,  0.4460052 ,  0.50840616,  0.5036006 , -0.5042167 ,\n",
              "        -0.50314784,  0.52223873,  0.51753485, -0.45190233, -0.51242805,\n",
              "        -0.49268135,  0.5192899 ,  0.4992845 ,  0.5194789 , -0.47407097,\n",
              "        -0.46712837,  0.5191636 ,  0.5213132 ,  0.44157124,  0.4690978 ,\n",
              "        -0.4527598 ,  0.458551  , -0.4797497 , -0.52130467, -0.44503528,\n",
              "        -0.4880634 ,  0.5226177 , -0.4703103 ,  0.44382814, -0.51785344,\n",
              "         0.5118058 , -0.45068315, -0.4854306 , -0.5027301 ,  0.44760138,\n",
              "         0.44400588,  0.48046452,  0.45158374, -0.5101124 ,  0.46643662],\n",
              "       dtype=float32),\n",
              " 'bank': array([ 0.47634512,  0.45355636, -0.5244551 , -0.44840005, -0.48237765,\n",
              "         0.54286265,  0.49718732, -0.4609296 , -0.507231  , -0.5210212 ,\n",
              "        -0.5321506 ,  0.48147252,  0.533     ,  0.48379323, -0.4966031 ,\n",
              "        -0.45226187,  0.49963552,  0.53356206, -0.4599967 , -0.44530624,\n",
              "        -0.5371718 ,  0.5059232 ,  0.4653661 ,  0.51971596, -0.44879326,\n",
              "        -0.47915524,  0.4446618 ,  0.5579214 ,  0.5051158 ,  0.45193544,\n",
              "        -0.5476819 ,  0.44317076, -0.4765142 , -0.46513674, -0.44464198,\n",
              "        -0.47145388,  0.53208363, -0.4783017 ,  0.5359551 , -0.4776989 ,\n",
              "         0.51179403, -0.48802745, -0.49423245, -0.44673893,  0.4897283 ,\n",
              "         0.4805968 ,  0.50314504,  0.44994578, -0.46663016,  0.5265219 ],\n",
              "       dtype=float32),\n",
              " 'investor': array([ 0.5525288 ,  0.5083405 , -0.5170489 , -0.44517007, -0.49380064,\n",
              "         0.49552056,  0.47144887, -0.4451983 , -0.5166429 , -0.49977732,\n",
              "        -0.50228584,  0.49833897,  0.45806643,  0.4766301 , -0.5156982 ,\n",
              "        -0.48421162,  0.46888083,  0.45112267, -0.47253382, -0.43652928,\n",
              "        -0.45520836,  0.47604057,  0.5260521 ,  0.45037484, -0.50512856,\n",
              "        -0.4870046 ,  0.49066162,  0.52718145,  0.46167257,  0.5214088 ,\n",
              "        -0.52814406,  0.47118884, -0.5183625 , -0.49900895, -0.49098957,\n",
              "        -0.51263744,  0.5029107 , -0.45392427,  0.51002663, -0.4615121 ,\n",
              "         0.4935948 , -0.5373703 , -0.51802236, -0.49299237,  0.4851576 ,\n",
              "         0.48553786,  0.5153811 ,  0.4647962 , -0.47243667,  0.5263673 ],\n",
              "       dtype=float32),\n",
              " 'increase': array([ 0.54907167,  0.48063993, -0.48352298, -0.50456905, -0.545005  ,\n",
              "         0.5071404 ,  0.49255872, -0.51959705, -0.52636576, -0.43560728,\n",
              "        -0.5023317 ,  0.4562078 ,  0.4473611 ,  0.45722094, -0.49620622,\n",
              "        -0.4764852 ,  0.45690307,  0.53012466, -0.5201103 , -0.48658034,\n",
              "        -0.4543343 ,  0.45797503,  0.517902  ,  0.4713227 , -0.45860282,\n",
              "        -0.4417166 ,  0.5172829 ,  0.55652267,  0.44391212,  0.46576858,\n",
              "        -0.47507828,  0.48356512, -0.55263436, -0.4548049 , -0.43866453,\n",
              "        -0.519557  ,  0.45545432, -0.48229614,  0.533091  , -0.4506038 ,\n",
              "         0.48403046, -0.4703919 , -0.45263028, -0.48886308,  0.5044967 ,\n",
              "         0.5180368 ,  0.48437846,  0.50175595, -0.5055622 ,  0.5254723 ],\n",
              "       dtype=float32),\n",
              " 'particular': array([ 0.5005985 ,  0.4664589 , -0.4455575 , -0.4719909 , -0.47662184,\n",
              "         0.47413456,  0.5246966 , -0.44901118, -0.5235448 , -0.45068014,\n",
              "        -0.46308112,  0.519919  ,  0.50051963,  0.46255934, -0.48384455,\n",
              "        -0.50567263,  0.47420084,  0.5280025 , -0.49297693, -0.45560756,\n",
              "        -0.5078702 ,  0.5143049 ,  0.4979513 ,  0.514213  , -0.44438133,\n",
              "        -0.5293411 ,  0.49995866,  0.53687763,  0.4932717 ,  0.51934755,\n",
              "        -0.5229251 ,  0.4523405 , -0.515703  , -0.4790712 , -0.4861673 ,\n",
              "        -0.5010022 ,  0.5090157 , -0.4436285 ,  0.46623477, -0.5035399 ,\n",
              "         0.4765216 , -0.48449355, -0.50852144, -0.46882874,  0.4750494 ,\n",
              "         0.5300387 ,  0.4441874 ,  0.46375504, -0.5039259 ,  0.47583377],\n",
              "       dtype=float32),\n",
              " 'calculated': array([ 0.5007718 ,  0.45340812, -0.49206397, -0.5085952 , -0.53998566,\n",
              "         0.54094946,  0.51555717, -0.4702277 , -0.46632436, -0.51382136,\n",
              "        -0.50861853,  0.4639859 ,  0.5130023 ,  0.44732383, -0.48260236,\n",
              "        -0.52263665,  0.4549534 ,  0.47855762, -0.503525  , -0.48196065,\n",
              "        -0.5241166 ,  0.45383084,  0.4490454 ,  0.4365062 , -0.5245943 ,\n",
              "        -0.49940172,  0.50572366,  0.47608393,  0.52086395,  0.46756953,\n",
              "        -0.482083  ,  0.5108162 , -0.5406028 , -0.5292531 , -0.48295107,\n",
              "        -0.5331569 ,  0.5387218 , -0.5250848 ,  0.44361115, -0.46824723,\n",
              "         0.5219546 , -0.4881757 , -0.4515996 , -0.45838848,  0.47119504,\n",
              "         0.5161455 ,  0.45441374,  0.45788524, -0.4865855 ,  0.4756082 ],\n",
              "       dtype=float32),\n",
              " 'cause': array([ 0.5274404 ,  0.4758907 , -0.49383992, -0.47359344, -0.49403548,\n",
              "         0.4982114 ,  0.5148338 , -0.49991536, -0.5181693 , -0.5178279 ,\n",
              "        -0.49963167,  0.4706741 ,  0.47172832,  0.49274504, -0.49824345,\n",
              "        -0.5266007 ,  0.48495415,  0.5227992 , -0.47680363, -0.48943773,\n",
              "        -0.50865704,  0.50078034,  0.5059166 ,  0.5121648 , -0.46818167,\n",
              "        -0.46635163,  0.50748813,  0.5336113 ,  0.47977132,  0.47225034,\n",
              "        -0.47238696,  0.46083885, -0.45852745, -0.4685735 , -0.43968043,\n",
              "        -0.5294062 ,  0.4853273 , -0.49220777,  0.48542762, -0.5100557 ,\n",
              "         0.4571069 , -0.45691156, -0.45720577, -0.5048086 ,  0.5005795 ,\n",
              "         0.4662693 ,  0.48474982,  0.52724415, -0.50392866,  0.47174203],\n",
              "       dtype=float32),\n",
              " 'one': array([ 0.51559764,  0.5048674 , -0.52030927, -0.48598695, -0.46280068,\n",
              "         0.5275352 ,  0.45385537, -0.46263915, -0.5331611 , -0.5068352 ,\n",
              "        -0.50230163,  0.47260675,  0.50232697,  0.45994934, -0.5107204 ,\n",
              "        -0.5187155 ,  0.46878383,  0.45288762, -0.45589215, -0.45775604,\n",
              "        -0.46516585,  0.5164585 ,  0.5248269 ,  0.47500005, -0.45590794,\n",
              "        -0.51714265,  0.4767439 ,  0.53860354,  0.48418993,  0.528064  ,\n",
              "        -0.45919   ,  0.47851127, -0.5380706 , -0.4571862 , -0.49280888,\n",
              "        -0.49622652,  0.51576316, -0.48175144,  0.504646  , -0.47555566,\n",
              "         0.5206609 , -0.4669131 , -0.4540945 , -0.4462575 ,  0.498755  ,\n",
              "         0.45968306,  0.5362646 ,  0.4709913 , -0.45063797,  0.47880763],\n",
              "       dtype=float32),\n",
              " 'high': array([ 0.51002705,  0.49781924, -0.45463994, -0.49116015, -0.526912  ,\n",
              "         0.53778726,  0.4821961 , -0.5179656 , -0.4725236 , -0.4808676 ,\n",
              "        -0.4371889 ,  0.45480892,  0.48940945,  0.49169737, -0.49107662,\n",
              "        -0.4745609 ,  0.49407154,  0.45245117, -0.525811  , -0.43796587,\n",
              "        -0.50298107,  0.46905038,  0.4582559 ,  0.45943242, -0.5018364 ,\n",
              "        -0.47755796,  0.51849955,  0.49556354,  0.5178181 ,  0.52015084,\n",
              "        -0.50074697,  0.44250512, -0.5129561 , -0.49999467, -0.4371709 ,\n",
              "        -0.5178653 ,  0.47602957, -0.50078976,  0.47621456, -0.5013807 ,\n",
              "         0.5124075 , -0.5231386 , -0.4534378 , -0.4517718 ,  0.5082691 ,\n",
              "         0.4727329 ,  0.501194  ,  0.5039796 , -0.51832616,  0.46824807],\n",
              "       dtype=float32),\n",
              " 'charges': array([ 0.5557193 ,  0.45432046, -0.45651767, -0.5029094 , -0.49618253,\n",
              "         0.486547  ,  0.43287623, -0.55881584, -0.43442553, -0.41984883,\n",
              "        -0.46424577,  0.5175772 ,  0.45396885,  0.5191636 , -0.49698377,\n",
              "        -0.48707402,  0.5332509 ,  0.47614375, -0.512874  , -0.4700552 ,\n",
              "        -0.46827245,  0.4706625 ,  0.52168393,  0.5169643 , -0.492856  ,\n",
              "        -0.4529767 ,  0.5112806 ,  0.5202076 ,  0.47274336,  0.5218653 ,\n",
              "        -0.549506  ,  0.44387636, -0.5352464 , -0.4332174 , -0.4938521 ,\n",
              "        -0.51455   ,  0.51525384, -0.515986  ,  0.4682609 , -0.48767036,\n",
              "         0.5076151 , -0.5327791 , -0.43073037, -0.4382769 ,  0.5064814 ,\n",
              "         0.5451232 ,  0.43948668,  0.4843301 , -0.45664367,  0.5116693 ],\n",
              "       dtype=float32),\n",
              " 'managed': array([ 0.48572454,  0.48539808, -0.5260617 , -0.50017256, -0.4501341 ,\n",
              "         0.48760644,  0.44353998, -0.49241018, -0.47209516, -0.49016386,\n",
              "        -0.4687789 ,  0.5195664 ,  0.47716784,  0.5280318 , -0.5250032 ,\n",
              "        -0.5287206 ,  0.48311034,  0.48481435, -0.5155139 , -0.5094328 ,\n",
              "        -0.47923625,  0.45414746,  0.50767434,  0.49906218, -0.5048689 ,\n",
              "        -0.52056384,  0.49654436,  0.5302143 ,  0.45176247,  0.467986  ,\n",
              "        -0.51074314,  0.47851142, -0.52506655, -0.50619125, -0.49137112,\n",
              "        -0.51369333,  0.5217863 , -0.50589085,  0.4821156 , -0.49428064,\n",
              "         0.51432323, -0.5027718 , -0.4645306 , -0.5057399 ,  0.485113  ,\n",
              "         0.4494607 ,  0.47645923,  0.43950018, -0.46127284,  0.4766116 ],\n",
              "       dtype=float32),\n",
              " 'taxable': array([ 0.55775   ,  0.46075282, -0.48364848, -0.44055367, -0.5218376 ,\n",
              "         0.51332635,  0.5175064 , -0.52628607, -0.4812277 , -0.5195579 ,\n",
              "        -0.53923756,  0.46824175,  0.4679648 ,  0.45419085, -0.53785497,\n",
              "        -0.500792  ,  0.52891076,  0.53824604, -0.5183709 , -0.43776947,\n",
              "        -0.5205675 ,  0.44565696,  0.54418975,  0.49334806, -0.45687518,\n",
              "        -0.51301944,  0.5276953 ,  0.5439381 ,  0.4652598 ,  0.45151818,\n",
              "        -0.48828757,  0.51481557, -0.48317325, -0.52240515, -0.48464257,\n",
              "        -0.4800464 ,  0.48361903, -0.5116563 ,  0.5387255 , -0.44944513,\n",
              "         0.5132674 , -0.52112395, -0.49799246, -0.46365175,  0.5379627 ,\n",
              "         0.5190166 ,  0.46288726,  0.4554975 , -0.46916914,  0.4632155 ],\n",
              "       dtype=float32),\n",
              " 'least': array([ 0.5332879 ,  0.45640397, -0.4440935 , -0.5104104 , -0.45294943,\n",
              "         0.5377066 ,  0.4483967 , -0.44074976, -0.5190919 , -0.46797732,\n",
              "        -0.5075425 ,  0.51091605,  0.5107584 ,  0.5144821 , -0.49521744,\n",
              "        -0.46296063,  0.5024079 ,  0.50690204, -0.4637552 , -0.46645445,\n",
              "        -0.4902833 ,  0.4588222 ,  0.49748063,  0.46067458, -0.4640582 ,\n",
              "        -0.5233352 ,  0.44659904,  0.5083717 ,  0.5078429 ,  0.5191056 ,\n",
              "        -0.5053719 ,  0.5046888 , -0.4749558 , -0.4955405 , -0.4431588 ,\n",
              "        -0.4764468 ,  0.46557102, -0.423116  ,  0.47182587, -0.5293624 ,\n",
              "         0.45214933, -0.48077586, -0.47280747, -0.5003384 ,  0.51158834,\n",
              "         0.49658203,  0.50577235,  0.5187959 , -0.47237   ,  0.47457492],\n",
              "       dtype=float32),\n",
              " 'impact': array([ 0.5009935 ,  0.44096583, -0.45139828, -0.49954018, -0.5113089 ,\n",
              "         0.51226246,  0.5050813 , -0.45747554, -0.4749862 , -0.50953513,\n",
              "        -0.49461436,  0.45500913,  0.52547234,  0.46788114, -0.54812765,\n",
              "        -0.52351886,  0.5106955 ,  0.4674467 , -0.5178753 , -0.47654733,\n",
              "        -0.46301985,  0.5206368 ,  0.4510238 ,  0.4493671 , -0.49479413,\n",
              "        -0.45471692,  0.48952377,  0.5039676 ,  0.46166655,  0.43844247,\n",
              "        -0.5408581 ,  0.50194347, -0.5063073 , -0.52605104, -0.505468  ,\n",
              "        -0.5252931 ,  0.5293487 , -0.50252444,  0.50371444, -0.5239628 ,\n",
              "         0.45969936, -0.4705623 , -0.51716214, -0.52213824,  0.5046339 ,\n",
              "         0.48243222,  0.50374174,  0.47740537, -0.5101829 ,  0.4943767 ],\n",
              "       dtype=float32),\n",
              " 'developments': array([ 0.48485085,  0.518641  , -0.46777502, -0.45389658, -0.47296867,\n",
              "         0.5002176 ,  0.5185106 , -0.46249622, -0.43942904, -0.48117962,\n",
              "        -0.45665345,  0.5195635 ,  0.48542637,  0.524338  , -0.51231503,\n",
              "        -0.5281722 ,  0.49291897,  0.51662457, -0.5174954 , -0.523029  ,\n",
              "        -0.44484705,  0.50801855,  0.5096418 ,  0.47682732, -0.4975405 ,\n",
              "        -0.46954057,  0.44959396,  0.479103  ,  0.5055579 ,  0.47604322,\n",
              "        -0.45164117,  0.5161615 , -0.46330684, -0.5088209 , -0.46554473,\n",
              "        -0.49132067,  0.49532744, -0.5075727 ,  0.4610479 , -0.51835275,\n",
              "         0.48033002, -0.53549117, -0.48776513, -0.46373764,  0.47406137,\n",
              "         0.48439145,  0.5215277 ,  0.46543705, -0.4967304 ,  0.5248656 ],\n",
              "       dtype=float32),\n",
              " 'industry': array([ 0.5160496 ,  0.46873546, -0.4876747 , -0.51058614, -0.49428672,\n",
              "         0.46725053,  0.50666356, -0.45311797, -0.48646978, -0.49847862,\n",
              "        -0.4563773 ,  0.49680457,  0.4596539 ,  0.5112617 , -0.5018387 ,\n",
              "        -0.5233049 ,  0.45026362,  0.46532342, -0.5195751 , -0.48624608,\n",
              "        -0.4948235 ,  0.4942947 ,  0.48792782,  0.49593365, -0.44759598,\n",
              "        -0.52930176,  0.43928066,  0.5057429 ,  0.44457394,  0.44131076,\n",
              "        -0.5461416 ,  0.50946736, -0.47504306, -0.48012993, -0.45595565,\n",
              "        -0.525034  ,  0.5144939 , -0.4534985 ,  0.4746335 , -0.4605993 ,\n",
              "         0.46235812, -0.48654333, -0.48611748, -0.45014164,  0.45925874,\n",
              "         0.5224895 ,  0.50201124,  0.50679153, -0.49584353,  0.45167118],\n",
              "       dtype=float32),\n",
              " 'target': array([ 0.47480538,  0.49958384, -0.4625734 , -0.48768976, -0.48630023,\n",
              "         0.47813338,  0.44995704, -0.4473948 , -0.50692993, -0.45443213,\n",
              "        -0.5098659 ,  0.45351148,  0.45213675,  0.51813895, -0.52213216,\n",
              "        -0.508855  ,  0.4480555 ,  0.44904056, -0.51143986, -0.45196968,\n",
              "        -0.44883883,  0.5040009 ,  0.46421266,  0.4953056 , -0.4560351 ,\n",
              "        -0.48435444,  0.45462245,  0.46496746,  0.5154697 ,  0.45830965,\n",
              "        -0.47487062,  0.43468875, -0.5185689 , -0.5214132 , -0.4605425 ,\n",
              "        -0.5181662 ,  0.51762843, -0.49789444,  0.4952616 , -0.5145165 ,\n",
              "         0.5102983 , -0.48163664, -0.46648762, -0.46745518,  0.45572546,\n",
              "         0.4626063 ,  0.46561906,  0.4378657 , -0.47073492,  0.5080952 ],\n",
              "       dtype=float32),\n",
              " 'within': array([ 0.5073378 ,  0.45093435, -0.49969468, -0.47381535, -0.44560528,\n",
              "         0.5024199 ,  0.4976408 , -0.5172994 , -0.51900154, -0.5067064 ,\n",
              "        -0.456392  ,  0.5284653 ,  0.47451782,  0.46457952, -0.55159986,\n",
              "        -0.5095714 ,  0.4899525 ,  0.45156047, -0.47484344, -0.45007166,\n",
              "        -0.48774517,  0.44025928,  0.4682062 ,  0.4386959 , -0.45652068,\n",
              "        -0.43891507,  0.5124452 ,  0.52906346,  0.48071408,  0.50852346,\n",
              "        -0.508295  ,  0.43828252, -0.45993352, -0.47421744, -0.46533537,\n",
              "        -0.5263098 ,  0.49358833, -0.45083368,  0.48856974, -0.50145453,\n",
              "         0.4477862 , -0.4666598 , -0.51508397, -0.452367  ,  0.5078541 ,\n",
              "         0.51168203,  0.51173   ,  0.5102553 , -0.49884796,  0.5110847 ],\n",
              "       dtype=float32),\n",
              " 'issued': array([ 0.5235535 ,  0.4573285 , -0.5069459 , -0.50452787, -0.5127896 ,\n",
              "         0.48174232,  0.4698325 , -0.4719749 , -0.4648654 , -0.52028227,\n",
              "        -0.43455315,  0.5107951 ,  0.44818693,  0.4986809 , -0.5309689 ,\n",
              "        -0.4913833 ,  0.4788493 ,  0.4606678 , -0.4989992 , -0.49552506,\n",
              "        -0.46323702,  0.47607356,  0.46333614,  0.47970253, -0.4585982 ,\n",
              "        -0.524294  ,  0.52275497,  0.52553976,  0.49118543,  0.4929245 ,\n",
              "        -0.49677745,  0.5203564 , -0.55060863, -0.46754804, -0.4717099 ,\n",
              "        -0.4535852 ,  0.45542747, -0.4541914 ,  0.44771814, -0.5135321 ,\n",
              "         0.5077454 , -0.46052074, -0.4893782 , -0.50535494,  0.48719555,\n",
              "         0.50062335,  0.45658606,  0.453438  , -0.45809698,  0.52747977],\n",
              "       dtype=float32),\n",
              " 'ftse': array([ 0.47587177,  0.48994687, -0.48592627, -0.45456535, -0.46490556,\n",
              "         0.48530534,  0.47520363, -0.4494193 , -0.4418719 , -0.5094828 ,\n",
              "        -0.49941579,  0.4926414 ,  0.4456116 ,  0.43429723, -0.5389636 ,\n",
              "        -0.4962334 ,  0.51495266,  0.4772697 , -0.46951997, -0.4976319 ,\n",
              "        -0.47101054,  0.50422454,  0.52050674,  0.4940927 , -0.47101527,\n",
              "        -0.47917804,  0.5193093 ,  0.5273509 ,  0.45612597,  0.43638605,\n",
              "        -0.48389682,  0.45194346, -0.528561  , -0.46044   , -0.45872533,\n",
              "        -0.46671167,  0.48441237, -0.5124861 ,  0.50273746, -0.46996802,\n",
              "         0.49625763, -0.52765465, -0.44214022, -0.4580652 ,  0.514467  ,\n",
              "         0.4519294 ,  0.48946106,  0.48099747, -0.5112229 ,  0.49460644],\n",
              "       dtype=float32),\n",
              " 'deposit': array([ 0.506924  ,  0.511106  , -0.464187  , -0.46154094, -0.48471126,\n",
              "         0.5168071 ,  0.52123356, -0.4600812 , -0.43583897, -0.45367184,\n",
              "        -0.4563752 ,  0.5349904 ,  0.52089924,  0.4666326 , -0.5194618 ,\n",
              "        -0.5019702 ,  0.53284705,  0.4543928 , -0.474762  , -0.46058607,\n",
              "        -0.43939465,  0.52898765,  0.51949143,  0.48009035, -0.5088694 ,\n",
              "        -0.5200192 ,  0.46274614,  0.5259535 ,  0.49584067,  0.50642246,\n",
              "        -0.49182287,  0.5240038 , -0.48262128, -0.50530106, -0.4803206 ,\n",
              "        -0.52669567,  0.5322058 , -0.44712117,  0.5234021 , -0.46119535,\n",
              "         0.53487563, -0.44348678, -0.47781038, -0.5168855 ,  0.462866  ,\n",
              "         0.45048898,  0.4833678 ,  0.49237022, -0.46213335,  0.4465264 ],\n",
              "       dtype=float32),\n",
              " 'regulatory': array([ 0.48590922,  0.5117301 , -0.47981474, -0.46656772, -0.45967853,\n",
              "         0.4691691 ,  0.45718524, -0.49891183, -0.47176427, -0.44169027,\n",
              "        -0.52170074,  0.472139  ,  0.50528145,  0.45431674, -0.53097266,\n",
              "        -0.45000607,  0.53767735,  0.5252062 , -0.5193386 , -0.4626651 ,\n",
              "        -0.44584367,  0.44544578,  0.47329262,  0.50776243, -0.44876134,\n",
              "        -0.4525903 ,  0.5285229 ,  0.5178617 ,  0.49918032,  0.44716653,\n",
              "        -0.4799924 ,  0.5085605 , -0.51894104, -0.5203559 , -0.504083  ,\n",
              "        -0.5155155 ,  0.4522227 , -0.47078308,  0.5169966 , -0.4832691 ,\n",
              "         0.49358904, -0.520572  , -0.4901562 , -0.45784935,  0.45429578,\n",
              "         0.4802285 ,  0.454687  ,  0.51088554, -0.5116943 ,  0.4605371 ],\n",
              "       dtype=float32),\n",
              " 'contents': array([ 0.482127  ,  0.48999056, -0.52079505, -0.49264675, -0.55129313,\n",
              "         0.4634041 ,  0.5047773 , -0.46365085, -0.48662782, -0.45952192,\n",
              "        -0.5200985 ,  0.5154209 ,  0.45217627,  0.5066913 , -0.5276511 ,\n",
              "        -0.45123374,  0.46975484,  0.48465505, -0.51788455, -0.52181256,\n",
              "        -0.47063848,  0.44691697,  0.47808954,  0.44718778, -0.48313594,\n",
              "        -0.48301697,  0.5198575 ,  0.4767848 ,  0.5342272 ,  0.49570236,\n",
              "        -0.49952888,  0.49753624, -0.46962705, -0.4616904 , -0.5279654 ,\n",
              "        -0.472486  ,  0.5067041 , -0.51507384,  0.4652722 , -0.51956856,\n",
              "         0.52416545, -0.50812346, -0.4416245 , -0.47342777,  0.45639807,\n",
              "         0.49283093,  0.5006616 ,  0.52431077, -0.5306419 ,  0.50339216],\n",
              "       dtype=float32),\n",
              " 'initial': array([ 0.51912   ,  0.4760127 , -0.46240422, -0.46271822, -0.4590705 ,\n",
              "         0.5185156 ,  0.5161253 , -0.5322728 , -0.5003093 , -0.5039137 ,\n",
              "        -0.51147157,  0.50832   ,  0.45481247,  0.53592736, -0.4865944 ,\n",
              "        -0.5261463 ,  0.53002   ,  0.4734412 , -0.47493368, -0.47422737,\n",
              "        -0.48941952,  0.478849  ,  0.4588138 ,  0.4887484 , -0.48917964,\n",
              "        -0.46729574,  0.46191368,  0.46186137,  0.4684193 ,  0.4522672 ,\n",
              "        -0.52914023,  0.474071  , -0.48494473, -0.47974113, -0.47083735,\n",
              "        -0.49744892,  0.5337913 , -0.49733695,  0.49785185, -0.53400815,\n",
              "         0.50093126, -0.46005836, -0.48398548, -0.4705724 ,  0.5244448 ,\n",
              "         0.49701852,  0.4647192 ,  0.45245108, -0.53251624,  0.5279695 ],\n",
              "       dtype=float32),\n",
              " 'deduction': array([ 0.5162042 ,  0.4656656 , -0.52481663, -0.46486405, -0.5150615 ,\n",
              "         0.4917826 ,  0.5136848 , -0.4768495 , -0.44873413, -0.4817274 ,\n",
              "        -0.4393426 ,  0.5151284 ,  0.45724526,  0.5142543 , -0.5170106 ,\n",
              "        -0.4964525 ,  0.52576435,  0.44606483, -0.50684303, -0.51522505,\n",
              "        -0.44711223,  0.45976895,  0.5066837 ,  0.44301295, -0.4989279 ,\n",
              "        -0.48302954,  0.5196987 ,  0.5398271 ,  0.49497333,  0.5162191 ,\n",
              "        -0.51583064,  0.44898582, -0.500548  , -0.49584475, -0.48440322,\n",
              "        -0.50070256,  0.5169396 , -0.51489884,  0.4871211 , -0.49745232,\n",
              "         0.5205357 , -0.48771396, -0.49219063, -0.50216645,  0.531373  ,\n",
              "         0.53550196,  0.49770534,  0.46586078, -0.48381492,  0.5162842 ],\n",
              "       dtype=float32),\n",
              " 'december': array([ 0.5521646 ,  0.4326318 , -0.4773398 , -0.4653389 , -0.51405996,\n",
              "         0.49845377,  0.47894123, -0.48031345, -0.5268047 , -0.4824368 ,\n",
              "        -0.5041107 ,  0.46182865,  0.48528606,  0.4523762 , -0.49761572,\n",
              "        -0.53118557,  0.4569683 ,  0.49128258, -0.49451295, -0.48564363,\n",
              "        -0.4460831 ,  0.46554688,  0.49876222,  0.48853773, -0.5137154 ,\n",
              "        -0.47170952,  0.50292265,  0.4629769 ,  0.4861888 ,  0.45311117,\n",
              "        -0.49873784,  0.50861675, -0.51924735, -0.51499367, -0.4734714 ,\n",
              "        -0.5027094 ,  0.45979527, -0.5354037 ,  0.5199266 , -0.48201802,\n",
              "         0.4689018 , -0.54029644, -0.49274915, -0.51137674,  0.45727542,\n",
              "         0.5176071 ,  0.4491607 ,  0.49189216, -0.46831757,  0.46300402],\n",
              "       dtype=float32),\n",
              " 'make': array([ 0.53455544,  0.5013571 , -0.4657308 , -0.46892953, -0.45860654,\n",
              "         0.53094333,  0.4520441 , -0.4893076 , -0.51037014, -0.5173547 ,\n",
              "        -0.48451212,  0.44335544,  0.45710525,  0.52484286, -0.5438125 ,\n",
              "        -0.4588796 ,  0.5015527 ,  0.48980314, -0.49146098, -0.4826336 ,\n",
              "        -0.45951337,  0.45836404,  0.46761915,  0.4726673 , -0.45273378,\n",
              "        -0.52736336,  0.525531  ,  0.53716105,  0.5183098 ,  0.49503732,\n",
              "        -0.5308427 ,  0.4412704 , -0.4704208 , -0.5237617 , -0.50804615,\n",
              "        -0.51526064,  0.49263328, -0.5035717 ,  0.5285449 , -0.5099533 ,\n",
              "         0.48267084, -0.4552519 , -0.51206625, -0.46007323,  0.4741087 ,\n",
              "         0.46795717,  0.49410045,  0.4527871 , -0.44293445,  0.51081735],\n",
              "       dtype=float32),\n",
              " 'managers': array([ 0.4760154 ,  0.50183266, -0.49813315, -0.5203831 , -0.51630884,\n",
              "         0.5140164 ,  0.46200144, -0.48327476, -0.4537659 , -0.47631937,\n",
              "        -0.5045765 ,  0.4962859 ,  0.44323438,  0.50036097, -0.47277856,\n",
              "        -0.47607717,  0.5050919 ,  0.5203108 , -0.4783556 , -0.50151193,\n",
              "        -0.44391033,  0.5200388 ,  0.44473723,  0.47784984, -0.45703334,\n",
              "        -0.5080151 ,  0.51544666,  0.48686936,  0.48540977,  0.42192227,\n",
              "        -0.5380355 ,  0.52246267, -0.53270906, -0.4956914 , -0.42853075,\n",
              "        -0.46147504,  0.5364151 , -0.505695  ,  0.49853167, -0.520069  ,\n",
              "         0.5250805 , -0.5266775 , -0.48958713, -0.46986538,  0.5071436 ,\n",
              "         0.49245146,  0.44906983,  0.4636763 , -0.46116662,  0.46455708],\n",
              "       dtype=float32),\n",
              " 'additional': array([ 0.5325292 ,  0.50360984, -0.508312  , -0.4557246 , -0.5135918 ,\n",
              "         0.51206386,  0.4843108 , -0.48187575, -0.47338384, -0.5146261 ,\n",
              "        -0.46054944,  0.5229118 ,  0.5035124 ,  0.5132169 , -0.557421  ,\n",
              "        -0.51704025,  0.5232768 ,  0.45756564, -0.48068795, -0.50935566,\n",
              "        -0.47915086,  0.4989698 ,  0.5073612 ,  0.50782907, -0.48158905,\n",
              "        -0.48079684,  0.44446525,  0.53270936,  0.49535164,  0.49827588,\n",
              "        -0.4478761 ,  0.44070837, -0.513928  , -0.4655392 , -0.48345053,\n",
              "        -0.51689285,  0.4467047 , -0.47968912,  0.50026315, -0.4733321 ,\n",
              "         0.5270769 , -0.45507053, -0.47978398, -0.48628807,  0.47776183,\n",
              "         0.47846887,  0.44023794,  0.4782732 , -0.4648414 ,  0.45002595],\n",
              "       dtype=float32),\n",
              " 'limited': array([ 0.480354  ,  0.43727735, -0.5017857 , -0.4411035 , -0.45735562,\n",
              "         0.4538969 ,  0.52249974, -0.48334476, -0.48571336, -0.49396306,\n",
              "        -0.45947343,  0.47018784,  0.48141328,  0.5240308 , -0.5056405 ,\n",
              "        -0.506933  ,  0.46684188,  0.450259  , -0.5270389 , -0.4980979 ,\n",
              "        -0.47688714,  0.51271003,  0.5253891 ,  0.48886707, -0.51469725,\n",
              "        -0.5157872 ,  0.48910418,  0.52878916,  0.490662  ,  0.4840997 ,\n",
              "        -0.5104433 ,  0.45178372, -0.51525235, -0.4635044 , -0.44084692,\n",
              "        -0.44773456,  0.45892313, -0.4889651 ,  0.49189562, -0.49803966,\n",
              "         0.5327289 , -0.50009406, -0.4967044 , -0.46030077,  0.5149338 ,\n",
              "         0.4856729 ,  0.46415523,  0.4691208 , -0.4693635 ,  0.47552198],\n",
              "       dtype=float32),\n",
              " 'maximum': array([ 0.48099628,  0.48582232, -0.4974086 , -0.46485233, -0.50110203,\n",
              "         0.53831905,  0.49499285, -0.50038433, -0.45976788, -0.4675672 ,\n",
              "        -0.5047197 ,  0.4649503 ,  0.45905036,  0.5120092 , -0.5160115 ,\n",
              "        -0.52660453,  0.4729811 ,  0.49009332, -0.46243685, -0.45809606,\n",
              "        -0.5122773 ,  0.48713315,  0.495268  ,  0.51715887, -0.4866966 ,\n",
              "        -0.4913833 ,  0.4934572 ,  0.464288  ,  0.4990077 ,  0.49862728,\n",
              "        -0.46229258,  0.51803195, -0.47709957, -0.4581162 , -0.5024286 ,\n",
              "        -0.46566194,  0.4912178 , -0.4866702 ,  0.52544916, -0.48703152,\n",
              "         0.47054064, -0.49543762, -0.46543536, -0.46217552,  0.48316118,\n",
              "         0.4624452 ,  0.45842114,  0.52120227, -0.45702457,  0.5331221 ],\n",
              "       dtype=float32),\n",
              " 'types': array([ 0.5357014 ,  0.49094245, -0.4523226 , -0.48992637, -0.53103757,\n",
              "         0.46477133,  0.47459868, -0.5372009 , -0.50321764, -0.52499616,\n",
              "        -0.5150095 ,  0.49130645,  0.45091823,  0.5211245 , -0.5200155 ,\n",
              "        -0.4563736 ,  0.4896301 ,  0.45279297, -0.51187193, -0.5149117 ,\n",
              "        -0.5112257 ,  0.46145558,  0.49438706,  0.46238813, -0.47432482,\n",
              "        -0.46167123,  0.45700875,  0.5383682 ,  0.49420118,  0.44553143,\n",
              "        -0.5058464 ,  0.46092692, -0.5075583 , -0.5272193 , -0.5185964 ,\n",
              "        -0.534445  ,  0.4715047 , -0.5307222 ,  0.4786868 , -0.46432874,\n",
              "         0.4670055 , -0.47825393, -0.48433602, -0.4602653 ,  0.4991456 ,\n",
              "         0.49120483,  0.47526836,  0.5246245 , -0.5164842 ,  0.4647818 ],\n",
              "       dtype=float32),\n",
              " 'futures': array([ 0.4764679 ,  0.4869365 , -0.48113877, -0.51317656, -0.5280431 ,\n",
              "         0.51437384,  0.49782717, -0.49027458, -0.4980666 , -0.43464723,\n",
              "        -0.51899743,  0.5091059 ,  0.45402673,  0.48880008, -0.52094084,\n",
              "        -0.5089984 ,  0.5135149 ,  0.48620558, -0.44096863, -0.5081414 ,\n",
              "        -0.5234599 ,  0.44634768,  0.48785314,  0.47561553, -0.4923659 ,\n",
              "        -0.5159064 ,  0.51711005,  0.4676984 ,  0.48568636,  0.5067835 ,\n",
              "        -0.46916172,  0.45760822, -0.47328103, -0.47156942, -0.44472206,\n",
              "        -0.48499048,  0.49814326, -0.4853254 ,  0.4896138 , -0.52195615,\n",
              "         0.5227713 , -0.46200335, -0.4769459 , -0.49198318,  0.51339906,\n",
              "         0.45812413,  0.48921502,  0.46215776, -0.5229117 ,  0.47089496],\n",
              "       dtype=float32),\n",
              " 'country': array([ 0.52515894,  0.4409513 , -0.48441494, -0.5052287 , -0.46090314,\n",
              "         0.4559848 ,  0.48953703, -0.5164967 , -0.45275596, -0.47577894,\n",
              "        -0.47029004,  0.5177053 ,  0.43605146,  0.4751904 , -0.5440077 ,\n",
              "        -0.47896454,  0.515262  ,  0.46100345, -0.5026116 , -0.42758286,\n",
              "        -0.48697734,  0.47972748,  0.48284805,  0.46711498, -0.428982  ,\n",
              "        -0.44948325,  0.47167403,  0.5093445 ,  0.46202105,  0.4905489 ,\n",
              "        -0.47240466,  0.491763  , -0.47656515, -0.4442582 , -0.4956704 ,\n",
              "        -0.49639833,  0.46093452, -0.51119906,  0.4591016 , -0.49300912,\n",
              "         0.5088022 , -0.51017493, -0.47441727, -0.46801066,  0.5226315 ,\n",
              "         0.48000908,  0.4678519 ,  0.46905038, -0.47937638,  0.48674354],\n",
              "       dtype=float32),\n",
              " 'past': array([ 0.47573897,  0.43854663, -0.46529236, -0.45577973, -0.5141326 ,\n",
              "         0.5307626 ,  0.44614053, -0.47108144, -0.46000892, -0.5234463 ,\n",
              "        -0.50511044,  0.52360344,  0.44652   ,  0.5254905 , -0.57019055,\n",
              "        -0.50894725,  0.50842446,  0.5300037 , -0.46252874, -0.44174403,\n",
              "        -0.4613308 ,  0.5020489 ,  0.46096963,  0.5178437 , -0.48270577,\n",
              "        -0.44898164,  0.47526667,  0.52393794,  0.45779666,  0.50990665,\n",
              "        -0.5277065 ,  0.48091665, -0.5144733 , -0.46079376, -0.45745772,\n",
              "        -0.48314515,  0.5074612 , -0.50738764,  0.5048634 , -0.5045872 ,\n",
              "         0.48338404, -0.48834485, -0.49188635, -0.49078232,  0.48801944,\n",
              "         0.44824383,  0.47935158,  0.4939494 , -0.48493585,  0.49600452],\n",
              "       dtype=float32),\n",
              " 'liquid': array([ 0.50674367,  0.44974446, -0.47946113, -0.46855223, -0.5433883 ,\n",
              "         0.5217943 ,  0.48208496, -0.5430242 , -0.5205196 , -0.50368553,\n",
              "        -0.52618283,  0.5171268 ,  0.48319367,  0.50284463, -0.52137196,\n",
              "        -0.46932727,  0.45862037,  0.5340589 , -0.47509816, -0.4626795 ,\n",
              "        -0.4672115 ,  0.45036745,  0.5079496 ,  0.44282973, -0.43912128,\n",
              "        -0.4535941 ,  0.51879036,  0.4908042 ,  0.51310724,  0.4856407 ,\n",
              "        -0.4969497 ,  0.51163405, -0.52761436, -0.51044434, -0.46695822,\n",
              "        -0.46140614,  0.4724963 , -0.46163496,  0.4897485 , -0.4749201 ,\n",
              "         0.4546277 , -0.52229375, -0.51466316, -0.5054769 ,  0.5029581 ,\n",
              "         0.46007043,  0.51874006,  0.4976586 , -0.47603682,  0.48249343],\n",
              "       dtype=float32),\n",
              " 'various': array([ 0.47859293,  0.49018875, -0.48930424, -0.4745065 , -0.45898834,\n",
              "         0.49099177,  0.45026502, -0.5026161 , -0.45374838, -0.5080735 ,\n",
              "        -0.46689108,  0.44511792,  0.4627036 ,  0.49191332, -0.5670974 ,\n",
              "        -0.5176493 ,  0.51515067,  0.47197476, -0.4640582 , -0.5137547 ,\n",
              "        -0.4416578 ,  0.5203815 ,  0.47704488,  0.5049387 , -0.4875717 ,\n",
              "        -0.46758878,  0.45328677,  0.49999017,  0.4813359 ,  0.48464382,\n",
              "        -0.48582098,  0.48104766, -0.50253797, -0.5300132 , -0.44979432,\n",
              "        -0.5137278 ,  0.4501289 , -0.50853264,  0.4430249 , -0.5031692 ,\n",
              "         0.45979708, -0.5321446 , -0.46059346, -0.48830017,  0.50788873,\n",
              "         0.46042114,  0.5116261 ,  0.4888016 , -0.50263613,  0.5202602 ],\n",
              "       dtype=float32),\n",
              " 'intended': array([ 0.49800763,  0.47767645, -0.47899616, -0.5186324 , -0.5282042 ,\n",
              "         0.5197024 ,  0.46373856, -0.47961485, -0.4709597 , -0.52649325,\n",
              "        -0.44025618,  0.5084746 ,  0.47213778,  0.49159858, -0.49814248,\n",
              "        -0.47231874,  0.45875818,  0.47419462, -0.48201662, -0.46365595,\n",
              "        -0.44597167,  0.48920938,  0.49336064,  0.5248239 , -0.4512877 ,\n",
              "        -0.47057813,  0.46239042,  0.5082891 ,  0.49313733,  0.49864665,\n",
              "        -0.53227985,  0.4982797 , -0.46948645, -0.5277196 , -0.50723743,\n",
              "        -0.49738628,  0.50026137, -0.5311424 ,  0.4462271 , -0.4796408 ,\n",
              "         0.45100176, -0.5397029 , -0.5133124 , -0.4984443 ,  0.49169663,\n",
              "         0.46368462,  0.5236357 ,  0.48448387, -0.44178796,  0.4915181 ],\n",
              "       dtype=float32),\n",
              " 'gains': array([ 0.47704583,  0.52007765, -0.453559  , -0.474034  , -0.46095967,\n",
              "         0.46170408,  0.45551556, -0.45635164, -0.4604619 , -0.5182878 ,\n",
              "        -0.5134559 ,  0.49589372,  0.50313073,  0.48035893, -0.51555777,\n",
              "        -0.4876084 ,  0.481582  ,  0.4773747 , -0.47246933, -0.44343477,\n",
              "        -0.45553833,  0.44612738,  0.50355506,  0.5106568 , -0.4860116 ,\n",
              "        -0.4692201 ,  0.47128123,  0.5101738 ,  0.51056075,  0.50212896,\n",
              "        -0.5219049 ,  0.53147423, -0.54093957, -0.5377668 , -0.4836855 ,\n",
              "        -0.47194955,  0.47947192, -0.49376813,  0.4444372 , -0.455981  ,\n",
              "         0.45585018, -0.48334038, -0.49917734, -0.45385042,  0.49204153,\n",
              "         0.4910496 ,  0.5245918 ,  0.4551562 , -0.5005201 ,  0.5055313 ],\n",
              "       dtype=float32),\n",
              " 'mutual': array([ 0.54613256,  0.44696167, -0.46831965, -0.48024848, -0.5484395 ,\n",
              "         0.48499075,  0.49600762, -0.5034227 , -0.46122396, -0.46112227,\n",
              "        -0.4813828 ,  0.5233116 ,  0.50956094,  0.51505536, -0.5572924 ,\n",
              "        -0.527964  ,  0.5188457 ,  0.5265832 , -0.511913  , -0.45447305,\n",
              "        -0.46667632,  0.48294085,  0.5326222 ,  0.49231112, -0.46577284,\n",
              "        -0.4688638 ,  0.48173833,  0.5358824 ,  0.49044934,  0.45602039,\n",
              "        -0.49661934,  0.44585982, -0.48523602, -0.5087736 , -0.5315943 ,\n",
              "        -0.49613655,  0.4810999 , -0.48583913,  0.5167968 , -0.47450656,\n",
              "         0.53915524, -0.52856493, -0.46991533, -0.449395  ,  0.48748347,\n",
              "         0.51583976,  0.48524958,  0.4554857 , -0.45511356,  0.4438778 ],\n",
              "       dtype=float32),\n",
              " 'seeks': array([ 0.4849446 ,  0.5123358 , -0.5267851 , -0.47843733, -0.4667606 ,\n",
              "         0.50765526,  0.4858564 , -0.4980896 , -0.46105558, -0.49019173,\n",
              "        -0.527465  ,  0.5261508 ,  0.46499503,  0.48732927, -0.5380802 ,\n",
              "        -0.48888737,  0.52790797,  0.4430224 , -0.4407351 , -0.5152292 ,\n",
              "        -0.53741544,  0.46037006,  0.4929391 ,  0.45115402, -0.4724182 ,\n",
              "        -0.44103262,  0.51009876,  0.5194174 ,  0.4647141 ,  0.490987  ,\n",
              "        -0.4775816 ,  0.50895   , -0.49438998, -0.5098648 , -0.5244967 ,\n",
              "        -0.47312665,  0.53772247, -0.45195574,  0.47591975, -0.48537475,\n",
              "         0.5089432 , -0.46330795, -0.48196447, -0.5046053 ,  0.5344705 ,\n",
              "         0.5000745 ,  0.4734188 ,  0.45102596, -0.44839677,  0.46852797],\n",
              "       dtype=float32),\n",
              " 'variable': array([ 0.5468497 ,  0.4910658 , -0.45310238, -0.48679495, -0.52063507,\n",
              "         0.48171452,  0.48788014, -0.4678176 , -0.45545393, -0.45093334,\n",
              "        -0.49675214,  0.4394712 ,  0.46543598,  0.48640424, -0.51683414,\n",
              "        -0.49011198,  0.46064347,  0.43851316, -0.45732513, -0.5048653 ,\n",
              "        -0.4604867 ,  0.46791393,  0.5151196 ,  0.5123828 , -0.46914017,\n",
              "        -0.4566524 ,  0.45601305,  0.5315498 ,  0.51708496,  0.48784292,\n",
              "        -0.501375  ,  0.43905273, -0.5189108 , -0.50845623, -0.5083845 ,\n",
              "        -0.49394977,  0.49972013, -0.4422924 ,  0.50918305, -0.51676583,\n",
              "         0.46829835, -0.47887534, -0.46280453, -0.43664843,  0.44653183,\n",
              "         0.5136368 ,  0.44124037,  0.46832317, -0.449084  ,  0.4523495 ],\n",
              "       dtype=float32),\n",
              " 'purchases': array([ 0.4827732 ,  0.4999079 , -0.5028281 , -0.4764292 , -0.5325725 ,\n",
              "         0.49090254,  0.44944   , -0.4853035 , -0.49867946, -0.4914502 ,\n",
              "        -0.48734492,  0.52295643,  0.4408709 ,  0.45553124, -0.55937517,\n",
              "        -0.47890937,  0.4635885 ,  0.5330917 , -0.45483682, -0.46975797,\n",
              "        -0.47696412,  0.43823916,  0.49346036,  0.47277853, -0.4379329 ,\n",
              "        -0.4542719 ,  0.46036202,  0.47027075,  0.5222222 ,  0.47153303,\n",
              "        -0.5103098 ,  0.5009788 , -0.5555152 , -0.5053585 , -0.48987004,\n",
              "        -0.45636657,  0.48735482, -0.4940632 ,  0.5029116 , -0.5124638 ,\n",
              "         0.4978683 , -0.50747144, -0.5185739 , -0.4547181 ,  0.47911838,\n",
              "         0.46641618,  0.49093765,  0.51369613, -0.4614972 ,  0.46202224],\n",
              "       dtype=float32),\n",
              " 'global': array([ 0.4671747 ,  0.49959415, -0.50963473, -0.46267924, -0.46140906,\n",
              "         0.4758387 ,  0.48936218, -0.44233906, -0.49978402, -0.4864085 ,\n",
              "        -0.478331  ,  0.51983815,  0.46783212,  0.46797043, -0.50229716,\n",
              "        -0.47600225,  0.4859633 ,  0.4559688 , -0.4936836 , -0.4941424 ,\n",
              "        -0.52452105,  0.46825096,  0.5023204 ,  0.45949787, -0.4424195 ,\n",
              "        -0.5234744 ,  0.47536293,  0.49480522,  0.47849673,  0.4991899 ,\n",
              "        -0.49203533,  0.47698715, -0.490343  , -0.49378225, -0.48836863,\n",
              "        -0.48690695,  0.5128964 , -0.4535067 ,  0.4533554 , -0.5177694 ,\n",
              "         0.52466935, -0.5261942 , -0.4310519 , -0.4450112 ,  0.45104405,\n",
              "         0.49576718,  0.5011812 ,  0.45177844, -0.49362415,  0.50411403],\n",
              "       dtype=float32),\n",
              " 'commissions': array([ 0.53376037,  0.45602006, -0.5358104 , -0.526417  , -0.48074234,\n",
              "         0.53053284,  0.4499615 , -0.4840394 , -0.5179775 , -0.44224724,\n",
              "        -0.44373983,  0.47642872,  0.45072147,  0.47468442, -0.53803277,\n",
              "        -0.52356946,  0.49210188,  0.4520588 , -0.5197288 , -0.52816796,\n",
              "        -0.45325503,  0.45470393,  0.44642332,  0.45936596, -0.4523018 ,\n",
              "        -0.5133082 ,  0.46910378,  0.48121873,  0.52708757,  0.45153227,\n",
              "        -0.50605905,  0.50011766, -0.48074925, -0.4948622 , -0.45039475,\n",
              "        -0.5273907 ,  0.53219515, -0.46184677,  0.5179581 , -0.4814071 ,\n",
              "         0.4612385 , -0.5124995 , -0.49808347, -0.4991315 ,  0.52420676,\n",
              "         0.48651287,  0.46335182,  0.49328366, -0.47328824,  0.5002186 ],\n",
              "       dtype=float32),\n",
              " 'indicated': array([ 0.51232916,  0.45598814, -0.49504387, -0.47252616, -0.51455575,\n",
              "         0.4602275 ,  0.4939018 , -0.44687763, -0.47196364, -0.5047455 ,\n",
              "        -0.5069105 ,  0.492447  ,  0.5343898 ,  0.5023342 , -0.5099729 ,\n",
              "        -0.51777613,  0.51554835,  0.5029352 , -0.517149  , -0.45578954,\n",
              "        -0.51709104,  0.4911355 ,  0.49663052,  0.4701398 , -0.5143826 ,\n",
              "        -0.4792943 ,  0.5094415 ,  0.5472605 ,  0.46879652,  0.46378762,\n",
              "        -0.4895579 ,  0.48263043, -0.5061935 , -0.5152861 , -0.47334456,\n",
              "        -0.53817147,  0.46853176, -0.5196789 ,  0.47168082, -0.49834728,\n",
              "         0.46512336, -0.48931637, -0.4426411 , -0.53846157,  0.4532795 ,\n",
              "         0.52764344,  0.4901603 ,  0.46314695, -0.48542202,  0.4845161 ],\n",
              "       dtype=float32),\n",
              " 'b': array([ 0.43479392,  0.5181476 , -0.45641604, -0.48579335, -0.5055071 ,\n",
              "         0.5265338 ,  0.45676255, -0.5202764 , -0.44114372, -0.44798943,\n",
              "        -0.53376126,  0.46167544,  0.44463938,  0.49726105, -0.55882365,\n",
              "        -0.47587422,  0.47791183,  0.43598968, -0.45807475, -0.5012534 ,\n",
              "        -0.5549089 ,  0.5177731 ,  0.51480937,  0.47304446, -0.49717447,\n",
              "        -0.50462645,  0.4926003 ,  0.5321258 ,  0.4865548 ,  0.56541014,\n",
              "        -0.47626102,  0.48295152, -0.49925432, -0.5108185 , -0.5100297 ,\n",
              "        -0.5214918 ,  0.5314433 , -0.48681462,  0.5132411 , -0.52096134,\n",
              "         0.54567546, -0.5395553 , -0.471283  , -0.51667184,  0.54163104,\n",
              "         0.57144845,  0.5113233 ,  0.43640956, -0.54296684,  0.49837828],\n",
              "       dtype=float32),\n",
              " 'institutional': array([ 0.51905096,  0.48275572, -0.4592862 , -0.4498127 , -0.5164151 ,\n",
              "         0.51998913,  0.462036  , -0.45746028, -0.5078512 , -0.519375  ,\n",
              "        -0.5012633 ,  0.5369221 ,  0.48673016,  0.46575472, -0.49292645,\n",
              "        -0.4941196 ,  0.49129862,  0.5161186 , -0.45060474, -0.50024736,\n",
              "        -0.46852058,  0.52499896,  0.5388527 ,  0.448927  , -0.47757703,\n",
              "        -0.53233624,  0.44829366,  0.54922163,  0.48995233,  0.51474637,\n",
              "        -0.5230367 ,  0.46152705, -0.527957  , -0.52087957, -0.4381264 ,\n",
              "        -0.50954276,  0.502227  , -0.5311929 ,  0.47788155, -0.45118907,\n",
              "         0.54269946, -0.47340763, -0.44650733, -0.50730807,  0.46975207,\n",
              "         0.54186684,  0.46617237,  0.4620863 , -0.50754267,  0.49658072],\n",
              "       dtype=float32),\n",
              " 'cash': array([ 0.48653203,  0.49092075, -0.47507823, -0.4751671 , -0.5126722 ,\n",
              "         0.47378922,  0.51675004, -0.50215703, -0.45348907, -0.47647238,\n",
              "        -0.48233664,  0.47864082,  0.4867757 ,  0.43717426, -0.5499991 ,\n",
              "        -0.46714312,  0.5035485 ,  0.49687624, -0.51256657, -0.47774607,\n",
              "        -0.4996919 ,  0.515812  ,  0.47408265,  0.4418224 , -0.46875316,\n",
              "        -0.47464132,  0.49285918,  0.47677878,  0.5164119 ,  0.517736  ,\n",
              "        -0.5113781 ,  0.44570085, -0.4636886 , -0.48264563, -0.5008624 ,\n",
              "        -0.47125432,  0.4759525 , -0.44027779,  0.47763303, -0.49943572,\n",
              "         0.44134167, -0.45081693, -0.4400088 , -0.52144706,  0.48649827,\n",
              "         0.5153864 ,  0.4359689 ,  0.45045453, -0.4617483 ,  0.47252446],\n",
              "       dtype=float32),\n",
              " 'provide': array([ 0.5314542 ,  0.46944255, -0.46008518, -0.50153995, -0.4954098 ,\n",
              "         0.47227538,  0.51157725, -0.4544578 , -0.4598492 , -0.5038241 ,\n",
              "        -0.46694267,  0.49519032,  0.53103507,  0.53357846, -0.51220626,\n",
              "        -0.49620485,  0.53101397,  0.52993596, -0.45182228, -0.4365194 ,\n",
              "        -0.508545  ,  0.50488096,  0.47955558,  0.5178735 , -0.47169042,\n",
              "        -0.4571603 ,  0.4872627 ,  0.5296433 ,  0.43913326,  0.44114563,\n",
              "        -0.47607413,  0.49173594, -0.5503346 , -0.49743783, -0.43660146,\n",
              "        -0.49580362,  0.50755006, -0.51401484,  0.4958048 , -0.48827296,\n",
              "         0.5033652 , -0.4610047 , -0.44846267, -0.44585624,  0.46560094,\n",
              "         0.48078534,  0.51782805,  0.4555327 , -0.48950222,  0.4984229 ],\n",
              "       dtype=float32),\n",
              " 'ended': array([ 0.50107133,  0.46995336, -0.47205943, -0.47736746, -0.50685096,\n",
              "         0.4714322 ,  0.44160944, -0.4911464 , -0.5088456 , -0.49320322,\n",
              "        -0.48945114,  0.5165134 ,  0.49686322,  0.4646672 , -0.48994982,\n",
              "        -0.48439687,  0.5120553 ,  0.5030227 , -0.47980827, -0.5094886 ,\n",
              "        -0.5086224 ,  0.5120255 ,  0.5212584 ,  0.51904446, -0.4781815 ,\n",
              "        -0.47534594,  0.4379747 ,  0.47179276,  0.5048722 ,  0.52292347,\n",
              "        -0.50633615,  0.4395735 , -0.5468158 , -0.5145052 , -0.45138332,\n",
              "        -0.5115335 ,  0.45155498, -0.48335394,  0.49847543, -0.4603592 ,\n",
              "         0.47527403, -0.5172498 , -0.44948372, -0.50476915,  0.47714978,\n",
              "         0.48082748,  0.48508218,  0.4526021 , -0.49816602,  0.48649728],\n",
              "       dtype=float32),\n",
              " 'significant': array([ 0.5424396 ,  0.47961128, -0.47930348, -0.44088906, -0.4772933 ,\n",
              "         0.49445605,  0.4690923 , -0.46349597, -0.49901167, -0.5019569 ,\n",
              "        -0.5113821 ,  0.4595948 ,  0.4749156 ,  0.46025226, -0.5605021 ,\n",
              "        -0.4872352 ,  0.50270504,  0.50752395, -0.50604326, -0.51722866,\n",
              "        -0.49674934,  0.4900216 ,  0.51842445,  0.4736045 , -0.46307272,\n",
              "        -0.44611308,  0.4833793 ,  0.48328274,  0.46270195,  0.48258024,\n",
              "        -0.47476855,  0.5104385 , -0.49928018, -0.47641855, -0.43965608,\n",
              "        -0.45981896,  0.51192975, -0.47378555,  0.49210668, -0.507467  ,\n",
              "         0.53786737, -0.53821146, -0.49155623, -0.49757853,  0.5200698 ,\n",
              "         0.46107408,  0.53733027,  0.49892646, -0.4921562 ,  0.50764614],\n",
              "       dtype=float32),\n",
              " 'loss': array([ 0.49846676,  0.524539  , -0.4691939 , -0.46588475, -0.45432803,\n",
              "         0.50095063,  0.4802424 , -0.47538665, -0.459086  , -0.45122746,\n",
              "        -0.4755756 ,  0.50806797,  0.45824423,  0.45449057, -0.53329635,\n",
              "        -0.49555635,  0.5035191 ,  0.44875464, -0.44091344, -0.43790296,\n",
              "        -0.5093577 ,  0.4592315 ,  0.49619308,  0.5240477 , -0.5133162 ,\n",
              "        -0.5082849 ,  0.49167776,  0.55212164,  0.51428235,  0.5213814 ,\n",
              "        -0.53529334,  0.5132604 , -0.5487635 , -0.4671862 , -0.44116834,\n",
              "        -0.47290337,  0.48286885, -0.4530662 ,  0.49199396, -0.49794644,\n",
              "         0.50757766, -0.5137717 , -0.5220246 , -0.4476415 ,  0.539712  ,\n",
              "         0.47150427,  0.48385948,  0.45095965, -0.5192633 ,  0.4918691 ],\n",
              "       dtype=float32),\n",
              " 'using': array([ 0.5178145 ,  0.50701785, -0.47488374, -0.47050637, -0.49231258,\n",
              "         0.512599  ,  0.45058304, -0.51444954, -0.4973047 , -0.49036795,\n",
              "        -0.4634346 ,  0.47153413,  0.4647492 ,  0.4704134 , -0.5350641 ,\n",
              "        -0.4657977 ,  0.48305067,  0.51741517, -0.45044488, -0.4779961 ,\n",
              "        -0.53365064,  0.46811903,  0.50459945,  0.4833554 , -0.53638244,\n",
              "        -0.4494138 ,  0.48637515,  0.53535396,  0.53653586,  0.46278864,\n",
              "        -0.5318791 ,  0.49778783, -0.5076791 , -0.50237346, -0.437851  ,\n",
              "        -0.5271569 ,  0.5455904 , -0.49469042,  0.46270075, -0.5169263 ,\n",
              "         0.4828095 , -0.5211599 , -0.5087577 , -0.44783705,  0.47310627,\n",
              "         0.45845997,  0.49454862,  0.47363013, -0.5226296 ,  0.47760352],\n",
              "       dtype=float32),\n",
              " 'real': array([ 0.51819026,  0.44880134, -0.4847363 , -0.5017004 , -0.47331482,\n",
              "         0.50989383,  0.4464984 , -0.43936086, -0.48594654, -0.4280163 ,\n",
              "        -0.5065765 ,  0.4677652 ,  0.51195884,  0.44854593, -0.551935  ,\n",
              "        -0.48372602,  0.4468982 ,  0.43938968, -0.49037942, -0.49958727,\n",
              "        -0.5130548 ,  0.51918304,  0.5325443 ,  0.4826062 , -0.5069834 ,\n",
              "        -0.4516314 ,  0.4832515 ,  0.536779  ,  0.44563895,  0.43013114,\n",
              "        -0.48960918,  0.5029851 , -0.52638835, -0.5209513 , -0.51401   ,\n",
              "        -0.5057386 ,  0.4413028 , -0.47264975,  0.4470514 , -0.4517369 ,\n",
              "         0.50403655, -0.4852143 , -0.4945946 , -0.4868002 ,  0.5256225 ,\n",
              "         0.46917683,  0.5145676 ,  0.4942357 , -0.4574946 ,  0.5131273 ],\n",
              "       dtype=float32),\n",
              " 'involve': array([ 0.47879353,  0.50675744, -0.5106204 , -0.44367304, -0.46103033,\n",
              "         0.4998554 ,  0.5373684 , -0.51935655, -0.52059597, -0.50084585,\n",
              "        -0.47864637,  0.4591303 ,  0.48363248,  0.49268797, -0.52838385,\n",
              "        -0.46428394,  0.50516087,  0.45064586, -0.47489935, -0.46931618,\n",
              "        -0.5185615 ,  0.5127657 ,  0.50038904,  0.49478453, -0.4638558 ,\n",
              "        -0.517171  ,  0.5258916 ,  0.49579078,  0.5223381 ,  0.47970507,\n",
              "        -0.518505  ,  0.4934767 , -0.53181756, -0.49055973, -0.46349648,\n",
              "        -0.45519513,  0.5110301 , -0.5255413 ,  0.48364264, -0.47539005,\n",
              "         0.47100312, -0.4869703 , -0.4771158 , -0.44610873,  0.5109136 ,\n",
              "         0.49745524,  0.44730398,  0.5086464 , -0.44981375,  0.4699611 ],\n",
              "       dtype=float32),\n",
              " 'ability': array([ 0.4758163 ,  0.44970065, -0.47443238, -0.49256015, -0.5273059 ,\n",
              "         0.49637306,  0.5241856 , -0.44317764, -0.5108733 , -0.46339905,\n",
              "        -0.51852155,  0.51102567,  0.45470023,  0.5207381 , -0.5018101 ,\n",
              "        -0.5166905 ,  0.5304175 ,  0.5191722 , -0.45894063, -0.50426924,\n",
              "        -0.4447173 ,  0.46928668,  0.51878667,  0.4980831 , -0.45793533,\n",
              "        -0.44641533,  0.44981605,  0.53649503,  0.44330066,  0.4750517 ,\n",
              "        -0.4513737 ,  0.49056166, -0.46681845, -0.4468532 , -0.5208388 ,\n",
              "        -0.4471415 ,  0.44828415, -0.45084986,  0.5058776 , -0.49591228,\n",
              "         0.5166074 , -0.5080142 , -0.43255457, -0.5079503 ,  0.5024861 ,\n",
              "         0.47706464,  0.45167273,  0.49724016, -0.49257284,  0.49754158],\n",
              "       dtype=float32),\n",
              " 'prior': array([ 0.4883565 ,  0.4819915 , -0.48051804, -0.49136335, -0.47329274,\n",
              "         0.5170167 ,  0.4589143 , -0.5085786 , -0.49618033, -0.45836246,\n",
              "        -0.48037127,  0.48799178,  0.47542074,  0.50697273, -0.50722516,\n",
              "        -0.48466328,  0.51323247,  0.45414424, -0.46528816, -0.443056  ,\n",
              "        -0.47278646,  0.49462336,  0.47559762,  0.4479446 , -0.5202674 ,\n",
              "        -0.48720893,  0.45295373,  0.5352006 ,  0.51195097,  0.487054  ,\n",
              "        -0.52474827,  0.51826584, -0.5240545 , -0.47254795, -0.5031249 ,\n",
              "        -0.4611427 ,  0.52592975, -0.5306373 ,  0.5012903 , -0.5252291 ,\n",
              "         0.50003016, -0.48749894, -0.48237875, -0.44761282,  0.49283165,\n",
              "         0.5027017 ,  0.4987864 ,  0.45196748, -0.44976312,  0.4795391 ],\n",
              "       dtype=float32),\n",
              " 'inception': array([ 0.5259118 ,  0.44445956, -0.5286791 , -0.4925422 , -0.51702636,\n",
              "         0.53424144,  0.4849518 , -0.51306236, -0.45746803, -0.46524748,\n",
              "        -0.4976315 ,  0.5155479 ,  0.45100906,  0.53158784, -0.5561011 ,\n",
              "        -0.48033193,  0.4859173 ,  0.4906691 , -0.49332118, -0.44544184,\n",
              "        -0.48441702,  0.49824762,  0.5338735 ,  0.48364913, -0.4548511 ,\n",
              "        -0.48665926,  0.514011  ,  0.51366305,  0.45291534,  0.52198005,\n",
              "        -0.48089895,  0.49453747, -0.49706587, -0.48205397, -0.48523343,\n",
              "        -0.528692  ,  0.45891473, -0.4524056 ,  0.44992986, -0.5174204 ,\n",
              "         0.47778714, -0.5153245 , -0.52511525, -0.46107098,  0.45284647,\n",
              "         0.4688272 ,  0.52683836,  0.45730436, -0.5145488 ,  0.4799716 ],\n",
              "       dtype=float32),\n",
              " 'broker-dealer': array([ 0.4669741 ,  0.50752616, -0.47348157, -0.5282167 , -0.45654702,\n",
              "         0.4820462 ,  0.4821649 , -0.51420426, -0.4624915 , -0.5026961 ,\n",
              "        -0.5051575 ,  0.52846265,  0.50124013,  0.52188796, -0.5207267 ,\n",
              "        -0.45283633,  0.50695825,  0.503217  , -0.4608547 , -0.51940274,\n",
              "        -0.44673556,  0.5108196 ,  0.49846905,  0.45333824, -0.49673006,\n",
              "        -0.467362  ,  0.48162147,  0.50747085,  0.491741  ,  0.48382616,\n",
              "        -0.5185398 ,  0.47485125, -0.48332885, -0.48353946, -0.4966988 ,\n",
              "        -0.45058444,  0.534462  , -0.5130662 ,  0.50226206, -0.51278067,\n",
              "         0.486352  , -0.4466502 , -0.504003  , -0.5166244 ,  0.46019793,\n",
              "         0.49927998,  0.49012774,  0.47511238, -0.45302787,  0.48473066],\n",
              "       dtype=float32),\n",
              " 'pimco': array([ 0.4711908 ,  0.44350472, -0.4603648 , -0.5227884 , -0.5078664 ,\n",
              "         0.5258163 ,  0.4775962 , -0.44514287, -0.47396106, -0.4860813 ,\n",
              "        -0.45956364,  0.5117069 ,  0.48417622,  0.45087174, -0.5436711 ,\n",
              "        -0.5340616 ,  0.50948364,  0.511917  , -0.52148813, -0.48217922,\n",
              "        -0.4645741 ,  0.4398224 ,  0.4800798 ,  0.4869251 , -0.5153841 ,\n",
              "        -0.5166744 ,  0.45714813,  0.47683644,  0.47721925,  0.5032152 ,\n",
              "        -0.5065809 ,  0.44736907, -0.4572223 , -0.49661547, -0.50058013,\n",
              "        -0.44890672,  0.456438  , -0.44737402,  0.5010128 , -0.45429182,\n",
              "         0.4879978 , -0.46379563, -0.5135492 , -0.4487093 ,  0.49186072,\n",
              "         0.5222689 ,  0.45887747,  0.5170724 , -0.5168749 ,  0.5178599 ],\n",
              "       dtype=float32),\n",
              " 'increased': array([ 0.5026409 ,  0.46519879, -0.47129077, -0.52553666, -0.53230083,\n",
              "         0.49312457,  0.46167743, -0.4932904 , -0.4548168 , -0.51641536,\n",
              "        -0.44336   ,  0.4799146 ,  0.46662492,  0.514691  , -0.5143197 ,\n",
              "        -0.507413  ,  0.51508975,  0.5174124 , -0.48894036, -0.49679303,\n",
              "        -0.48501956,  0.5006385 ,  0.4701878 ,  0.47908586, -0.5041499 ,\n",
              "        -0.5052236 ,  0.44557342,  0.4826116 ,  0.4628507 ,  0.46448743,\n",
              "        -0.51874214,  0.46836963, -0.4896217 , -0.50905204, -0.493294  ,\n",
              "        -0.49454448,  0.48969543, -0.44615182,  0.47859377, -0.4912216 ,\n",
              "         0.5004203 , -0.50976026, -0.4841793 , -0.4521831 ,  0.52434194,\n",
              "         0.49941996,  0.48407453,  0.4929924 , -0.4785139 ,  0.44747975],\n",
              "       dtype=float32),\n",
              " 'new': array([ 0.53491414,  0.47067973, -0.5117581 , -0.45863113, -0.50577295,\n",
              "         0.515909  ,  0.5216708 , -0.48819333, -0.50830483, -0.47214556,\n",
              "        -0.45855975,  0.46183386,  0.45363626,  0.4669527 , -0.5281084 ,\n",
              "        -0.49504143,  0.5278059 ,  0.48022687, -0.43558353, -0.44899163,\n",
              "        -0.49999452,  0.4557143 ,  0.48850536,  0.4427353 , -0.5202378 ,\n",
              "        -0.5228088 ,  0.43864036,  0.54250073,  0.50501394,  0.44777566,\n",
              "        -0.45202646,  0.44784552, -0.5393759 , -0.529916  , -0.45483708,\n",
              "        -0.5069816 ,  0.52859193, -0.47329447,  0.45331925, -0.47168848,\n",
              "         0.45737684, -0.52075255, -0.44787624, -0.44471952,  0.4671509 ,\n",
              "         0.45437214,  0.44109172,  0.45083427, -0.508933  ,  0.5158224 ],\n",
              "       dtype=float32),\n",
              " 'general': array([ 0.50120676,  0.5149329 , -0.47559148, -0.4806301 , -0.46804065,\n",
              "         0.47770208,  0.49252725, -0.4891869 , -0.4429929 , -0.49989265,\n",
              "        -0.4348    ,  0.5038065 ,  0.5097928 ,  0.46329147, -0.5443865 ,\n",
              "        -0.4950547 ,  0.5305117 ,  0.4768922 , -0.43613565, -0.45316404,\n",
              "        -0.46041536,  0.49643612,  0.4799137 ,  0.46183538, -0.45636076,\n",
              "        -0.51877224,  0.5167798 ,  0.4790172 ,  0.50718546,  0.48838   ,\n",
              "        -0.48275325,  0.5250332 , -0.47212398, -0.48120207, -0.48616472,\n",
              "        -0.47682977,  0.45467883, -0.517932  ,  0.51130056, -0.49596405,\n",
              "         0.47799182, -0.492253  , -0.5159107 , -0.4644666 ,  0.50904   ,\n",
              "         0.49898094,  0.4831363 ,  0.4898916 , -0.46462414,  0.49150085],\n",
              "       dtype=float32),\n",
              " 'contract': array([ 0.5404155 ,  0.50638294, -0.46697935, -0.4881331 , -0.5167663 ,\n",
              "         0.45528606,  0.522723  , -0.51970804, -0.43988618, -0.47883037,\n",
              "        -0.48710078,  0.49565926,  0.4437689 ,  0.4970272 , -0.5668252 ,\n",
              "        -0.4473987 ,  0.4770337 ,  0.5324818 , -0.45083672, -0.44354486,\n",
              "        -0.46827716,  0.47617146,  0.51204026,  0.4636007 , -0.5080624 ,\n",
              "        -0.43934914,  0.5266145 ,  0.5088582 ,  0.5249786 ,  0.51294243,\n",
              "        -0.50441027,  0.52229923, -0.52516496, -0.4552264 , -0.47686028,\n",
              "        -0.52197456,  0.4535158 , -0.50045735,  0.4851104 , -0.51388687,\n",
              "         0.4993704 , -0.49338982, -0.45055428, -0.5082706 ,  0.46600458,\n",
              "         0.45447174,  0.52811766,  0.46810356, -0.46162763,  0.48417217],\n",
              "       dtype=float32),\n",
              " 'primarily': array([ 0.4684308 ,  0.5154629 , -0.49740446, -0.48126775, -0.49874336,\n",
              "         0.47663605,  0.44892055, -0.4487801 , -0.50239563, -0.5138495 ,\n",
              "        -0.45554978,  0.48962685,  0.4973502 ,  0.47480977, -0.49160916,\n",
              "        -0.49246955,  0.4715412 ,  0.48780316, -0.51001203, -0.44984955,\n",
              "        -0.5176922 ,  0.45338133,  0.48238283,  0.52451104, -0.4534658 ,\n",
              "        -0.5238425 ,  0.5124361 ,  0.53976595,  0.46865514,  0.50651073,\n",
              "        -0.48328656,  0.5076016 , -0.4597875 , -0.48989674, -0.465014  ,\n",
              "        -0.4586041 ,  0.4803036 , -0.5013688 ,  0.45084843, -0.5107608 ,\n",
              "         0.50551474, -0.51919645, -0.48795214, -0.49774855,  0.485131  ,\n",
              "         0.51158375,  0.47724962,  0.4503918 , -0.5254691 ,  0.44349998],\n",
              "       dtype=float32),\n",
              " 'redemptions': array([ 0.46692   ,  0.5112545 , -0.44651437, -0.4842965 , -0.48402888,\n",
              "         0.5283546 ,  0.4458963 , -0.49551338, -0.45027304, -0.4438962 ,\n",
              "        -0.44985467,  0.5342737 ,  0.4745013 ,  0.44020498, -0.50914806,\n",
              "        -0.51919436,  0.50511014,  0.52770424, -0.44626158, -0.44457114,\n",
              "        -0.4670774 ,  0.48377544,  0.46621174,  0.45858246, -0.44973484,\n",
              "        -0.46532905,  0.5100627 ,  0.5343967 ,  0.4851522 ,  0.4834445 ,\n",
              "        -0.5420033 ,  0.44462964, -0.529723  , -0.4685482 , -0.43640292,\n",
              "        -0.46482423,  0.5335675 , -0.4440736 ,  0.5199299 , -0.43897846,\n",
              "         0.5234827 , -0.5044365 , -0.42368516, -0.5144333 ,  0.531366  ,\n",
              "         0.5261132 ,  0.43686372,  0.46689907, -0.5012839 ,  0.47056708],\n",
              "       dtype=float32),\n",
              " 'capped': array([ 0.52728176,  0.51111203, -0.52351415, -0.43896663, -0.51271653,\n",
              "         0.5028455 ,  0.44406584, -0.48651642, -0.4259312 , -0.45203444,\n",
              "        -0.48624665,  0.5287694 ,  0.46885344,  0.45162767, -0.5465805 ,\n",
              "        -0.44846344,  0.44034356,  0.497827  , -0.43631107, -0.4592111 ,\n",
              "        -0.46892384,  0.49645278,  0.4590044 ,  0.47167647, -0.47580305,\n",
              "        -0.45860043,  0.47017208,  0.5210683 ,  0.50451577,  0.483726  ,\n",
              "        -0.5086212 ,  0.47176695, -0.45214137, -0.44438994, -0.46252576,\n",
              "        -0.46014422,  0.5131163 , -0.516172  ,  0.4320805 , -0.473904  ,\n",
              "         0.52135414, -0.5212636 , -0.50851494, -0.50949246,  0.49988395,\n",
              "         0.48842347,  0.48050758,  0.511554  , -0.51858765,  0.51331395],\n",
              "       dtype=float32),\n",
              " 'r': array([ 0.47314012,  0.54534197, -0.54001856, -0.5364742 , -0.5556571 ,\n",
              "         0.48856518,  0.49432924, -0.45257017, -0.5186757 , -0.45248562,\n",
              "        -0.5731685 ,  0.48283362,  0.5448321 ,  0.5216745 , -0.56257755,\n",
              "        -0.5671756 ,  0.5543755 ,  0.45406178, -0.46310318, -0.49021974,\n",
              "        -0.5693146 ,  0.51812315,  0.5166048 ,  0.5918928 , -0.5580887 ,\n",
              "        -0.44545105,  0.45910087,  0.46474698,  0.5095635 ,  0.5594356 ,\n",
              "        -0.45997536,  0.47655764, -0.502789  , -0.53564227, -0.4339396 ,\n",
              "        -0.5329428 ,  0.5000281 , -0.57183367,  0.5346306 , -0.53013265,\n",
              "         0.48598033, -0.6116908 , -0.4512541 , -0.53182226,  0.50642765,\n",
              "         0.55004174,  0.52892756,  0.511028  , -0.54964846,  0.53555614],\n",
              "       dtype=float32),\n",
              " 'state': array([ 0.5481908 ,  0.46444878, -0.45598024, -0.4904044 , -0.4510505 ,\n",
              "         0.5411655 ,  0.4663597 , -0.4900684 , -0.52563727, -0.4467426 ,\n",
              "        -0.46231487,  0.5212171 ,  0.4514693 ,  0.44870037, -0.5028867 ,\n",
              "        -0.53053063,  0.44930422,  0.48514628, -0.5048021 , -0.4985681 ,\n",
              "        -0.47614294,  0.5213455 ,  0.4708221 ,  0.4729179 , -0.520797  ,\n",
              "        -0.47071752,  0.45366693,  0.4743877 ,  0.45143983,  0.49655148,\n",
              "        -0.46186835,  0.5124093 , -0.54957086, -0.4611466 , -0.44585213,\n",
              "        -0.5209465 ,  0.4748683 , -0.4536557 ,  0.51126707, -0.53019273,\n",
              "         0.50071293, -0.47436875, -0.5138573 , -0.48092297,  0.4748754 ,\n",
              "         0.4869111 ,  0.5040129 ,  0.46412203, -0.50490016,  0.43977353],\n",
              "       dtype=float32),\n",
              " 'end': array([ 0.51234686,  0.4592605 , -0.46400052, -0.45477524, -0.47968805,\n",
              "         0.47562775,  0.47756258, -0.5211864 , -0.44996902, -0.48855886,\n",
              "        -0.5244375 ,  0.4488213 ,  0.50618696,  0.47040772, -0.56852126,\n",
              "        -0.46790922,  0.5317983 ,  0.50601244, -0.46478105, -0.44897848,\n",
              "        -0.50431937,  0.5097943 ,  0.4700862 ,  0.4723318 , -0.518672  ,\n",
              "        -0.47511712,  0.49479264,  0.5376331 ,  0.49231005,  0.4973597 ,\n",
              "        -0.5344901 ,  0.4807459 , -0.51125515, -0.48566082, -0.52331984,\n",
              "        -0.46841642,  0.45863613, -0.49645606,  0.5182713 , -0.4488963 ,\n",
              "         0.5226252 , -0.4651818 , -0.4982977 , -0.51196456,  0.47591466,\n",
              "         0.516235  ,  0.44410425,  0.51192504, -0.4720401 ,  0.5158406 ],\n",
              "       dtype=float32),\n",
              " 'extent': array([ 0.55502963,  0.49709165, -0.52853763, -0.474443  , -0.46271974,\n",
              "         0.49435836,  0.49218556, -0.4556113 , -0.48405725, -0.44395632,\n",
              "        -0.5292916 ,  0.47061813,  0.477078  ,  0.45559484, -0.5700973 ,\n",
              "        -0.51048285,  0.46078408,  0.4497094 , -0.46088374, -0.49087507,\n",
              "        -0.46757808,  0.47894198,  0.5333551 ,  0.46153697, -0.4499779 ,\n",
              "        -0.5273306 ,  0.5211959 ,  0.55188036,  0.4525624 ,  0.5218778 ,\n",
              "        -0.52651304,  0.45990184, -0.50906014, -0.47593844, -0.48855   ,\n",
              "        -0.5211578 ,  0.5312894 , -0.5137098 ,  0.49827713, -0.5213688 ,\n",
              "         0.4809514 , -0.5172395 , -0.4449951 , -0.4569088 ,  0.5269046 ,\n",
              "         0.45175   ,  0.47343752,  0.46413162, -0.51626414,  0.48626924],\n",
              "       dtype=float32),\n",
              " 'paid': array([ 0.5117942 ,  0.46525323, -0.52213556, -0.49114874, -0.47987235,\n",
              "         0.4876337 ,  0.5205038 , -0.48988375, -0.52279615, -0.5219191 ,\n",
              "        -0.47447383,  0.49008512,  0.5094109 ,  0.5299029 , -0.5336551 ,\n",
              "        -0.5222001 ,  0.50698787,  0.51796055, -0.4533987 , -0.44316357,\n",
              "        -0.4923018 ,  0.49843943,  0.47777155,  0.44899234, -0.50546634,\n",
              "        -0.45594487,  0.46507424,  0.48796356,  0.46601936,  0.48799416,\n",
              "        -0.50322986,  0.49172282, -0.5257299 , -0.53390795, -0.52278787,\n",
              "        -0.45451957,  0.5030358 , -0.5174469 ,  0.48466623, -0.4925886 ,\n",
              "         0.5434645 , -0.4633115 , -0.47807166, -0.46977526,  0.5223017 ,\n",
              "         0.50684506,  0.48488364,  0.44639444, -0.5073233 ,  0.5076591 ],\n",
              "       dtype=float32),\n",
              " 'potential': array([ 0.5361349 ,  0.4543964 , -0.4680631 , -0.44783854, -0.47345108,\n",
              "         0.5348596 ,  0.47835928, -0.5076374 , -0.52090096, -0.48127645,\n",
              "        -0.47112852,  0.45705098,  0.48746386,  0.44973743, -0.57246566,\n",
              "        -0.47481158,  0.5046302 ,  0.4563247 , -0.4964343 , -0.46478692,\n",
              "        -0.52834636,  0.45741963,  0.5131222 ,  0.43930948, -0.48851576,\n",
              "        -0.45242527,  0.5016575 ,  0.51965797,  0.4432899 ,  0.46562818,\n",
              "        -0.4787231 ,  0.45678705, -0.4759686 , -0.45756823, -0.47623378,\n",
              "        -0.47288865,  0.4724943 , -0.50657535,  0.45648214, -0.5192649 ,\n",
              "         0.49792808, -0.45224404, -0.50806063, -0.49895352,  0.5309696 ,\n",
              "         0.47275466,  0.4486401 ,  0.44703752, -0.51217496,  0.44970554],\n",
              "       dtype=float32),\n",
              " 'use': array([ 0.5366617 ,  0.43356675, -0.47671622, -0.47820258, -0.46616346,\n",
              "         0.520326  ,  0.48439145, -0.5164388 , -0.43161806, -0.50936705,\n",
              "        -0.5131138 ,  0.49500823,  0.4443639 ,  0.5024776 , -0.49301082,\n",
              "        -0.49463856,  0.51463646,  0.5086786 , -0.49547967, -0.47465494,\n",
              "        -0.4923753 ,  0.47623748,  0.45720896,  0.5089645 , -0.4711411 ,\n",
              "        -0.5094807 ,  0.4702415 ,  0.47496337,  0.4524799 ,  0.45447296,\n",
              "        -0.5041122 ,  0.48838   , -0.514126  , -0.517871  , -0.46897152,\n",
              "        -0.46775275,  0.44837674, -0.49618414,  0.4380756 , -0.49843645,\n",
              "         0.4834281 , -0.45259094, -0.51476437, -0.46904194,  0.46484256,\n",
              "         0.46017274,  0.5006402 ,  0.51727676, -0.5125049 ,  0.49343783],\n",
              "       dtype=float32),\n",
              " 'upon': array([ 0.55938   ,  0.45855644, -0.46171176, -0.44766387, -0.48756394,\n",
              "         0.4565524 ,  0.4691285 , -0.44858465, -0.45448613, -0.45821753,\n",
              "        -0.48210955,  0.49220818,  0.4692287 ,  0.45027435, -0.5185705 ,\n",
              "        -0.5235975 ,  0.48421168,  0.4816288 , -0.5286946 , -0.43906432,\n",
              "        -0.4566753 ,  0.4937111 ,  0.50496835,  0.45344755, -0.48315814,\n",
              "        -0.51836884,  0.52229357,  0.54375625,  0.47017404,  0.4848093 ,\n",
              "        -0.53112614,  0.44917178, -0.4929799 , -0.49769527, -0.5198009 ,\n",
              "        -0.45608968,  0.47244853, -0.4607576 ,  0.46465978, -0.52975005,\n",
              "         0.46273497, -0.47581074, -0.48572692, -0.507859  ,  0.46862096,\n",
              "         0.50813043,  0.46938482,  0.4936012 , -0.535815  ,  0.50807494],\n",
              "       dtype=float32),\n",
              " 'inc.': array([ 0.52215135,  0.48704937, -0.501954  , -0.47294268, -0.49663335,\n",
              "         0.52008206,  0.46944213, -0.48306984, -0.47543335, -0.47223657,\n",
              "        -0.4658126 ,  0.47048077,  0.5167158 ,  0.5147213 , -0.5258219 ,\n",
              "        -0.48967588,  0.48669565,  0.49159086, -0.49320012, -0.4547486 ,\n",
              "        -0.50369483,  0.51160157,  0.4662962 ,  0.5069353 , -0.4765165 ,\n",
              "        -0.4374474 ,  0.5097938 ,  0.46361277,  0.4671524 ,  0.48944414,\n",
              "        -0.4963767 ,  0.48614222, -0.45112315, -0.5033518 , -0.43144333,\n",
              "        -0.46617007,  0.5162664 , -0.44437546,  0.4748929 , -0.43513882,\n",
              "         0.48612761, -0.44742292, -0.49012473, -0.43115819,  0.47266525,\n",
              "         0.4425279 ,  0.48796076,  0.4844385 , -0.48935026,  0.46792358],\n",
              "       dtype=float32),\n",
              " 'invested': array([ 0.46153235,  0.50533587, -0.51468116, -0.5255412 , -0.50247437,\n",
              "         0.51954556,  0.52568394, -0.4875245 , -0.494281  , -0.46932146,\n",
              "        -0.48036072,  0.51734877,  0.50301695,  0.5298788 , -0.5308568 ,\n",
              "        -0.44590518,  0.47227797,  0.47672966, -0.447956  , -0.49531648,\n",
              "        -0.4788073 ,  0.46822348,  0.48003656,  0.5038041 , -0.45384514,\n",
              "        -0.5145906 ,  0.43812868,  0.5253605 ,  0.45587528,  0.46061277,\n",
              "        -0.503746  ,  0.47144014, -0.47736275, -0.52407956, -0.46061194,\n",
              "        -0.51220506,  0.5073413 , -0.5045694 ,  0.47002843, -0.5150944 ,\n",
              "         0.4648207 , -0.53481156, -0.43670657, -0.4705317 ,  0.47633824,\n",
              "         0.45345536,  0.49613363,  0.4983352 , -0.5123781 ,  0.4892816 ],\n",
              "       dtype=float32),\n",
              " 'although': array([ 0.49486113,  0.45596355, -0.4540786 , -0.4801386 , -0.5161721 ,\n",
              "         0.5212939 ,  0.44542807, -0.4922331 , -0.5184806 , -0.47287044,\n",
              "        -0.48719972,  0.5219998 ,  0.46700254,  0.46079895, -0.52535015,\n",
              "        -0.4611151 ,  0.4574447 ,  0.50091517, -0.5127456 , -0.46754146,\n",
              "        -0.5352062 ,  0.47971138,  0.46489707,  0.46490917, -0.5012902 ,\n",
              "        -0.5035661 ,  0.5325721 ,  0.5007156 ,  0.49668312,  0.47637093,\n",
              "        -0.52979237,  0.4761036 , -0.49157223, -0.5323829 , -0.5158693 ,\n",
              "        -0.45719332,  0.46938336, -0.45583752,  0.49651915, -0.498923  ,\n",
              "         0.472894  , -0.50384235, -0.5294219 , -0.47300342,  0.46608445,\n",
              "         0.5309096 ,  0.48008996,  0.5155846 , -0.4640349 ,  0.481906  ],\n",
              "       dtype=float32),\n",
              " 'adversely': array([ 0.5424645 ,  0.45058492, -0.47239214, -0.466415  , -0.49104673,\n",
              "         0.47761258,  0.46244797, -0.47783813, -0.5119605 , -0.4781335 ,\n",
              "        -0.5385867 ,  0.49984956,  0.48900014,  0.46115646, -0.5548709 ,\n",
              "        -0.52919894,  0.5026085 ,  0.50361824, -0.48493814, -0.4664539 ,\n",
              "        -0.47713754,  0.47897235,  0.5142201 ,  0.489524  , -0.48024556,\n",
              "        -0.5282861 ,  0.46132964,  0.52241683,  0.47804892,  0.4514258 ,\n",
              "        -0.5273658 ,  0.49340755, -0.5500308 , -0.50261766, -0.52545154,\n",
              "        -0.49154717,  0.46817842, -0.51735413,  0.51762426, -0.5038048 ,\n",
              "         0.4846598 , -0.502313  , -0.4481585 , -0.47273758,  0.49837503,\n",
              "         0.45484856,  0.50094664,  0.49272847, -0.45378205,  0.48005128],\n",
              "       dtype=float32),\n",
              " 'waivers': array([ 0.53518057,  0.4544567 , -0.5313385 , -0.49524927, -0.47379044,\n",
              "         0.47445378,  0.51652914, -0.4495549 , -0.46783233, -0.42780572,\n",
              "        -0.472753  ,  0.44091284,  0.50603276,  0.52140284, -0.50375545,\n",
              "        -0.5015558 ,  0.5038192 ,  0.4501014 , -0.50726324, -0.50311637,\n",
              "        -0.45691663,  0.43491215,  0.50176644,  0.48198578, -0.47883314,\n",
              "        -0.5014961 ,  0.47532284,  0.53421926,  0.52063465,  0.46002853,\n",
              "        -0.5359051 ,  0.50120085, -0.4705549 , -0.44141826, -0.5075082 ,\n",
              "        -0.5175872 ,  0.4561689 , -0.45145673,  0.487238  , -0.5061406 ,\n",
              "         0.4896525 , -0.5044204 , -0.46121025, -0.5015544 ,  0.46189702,\n",
              "         0.5045179 ,  0.45816633,  0.5098427 , -0.43911943,  0.5175705 ],\n",
              "       dtype=float32),\n",
              " 'states': array([ 0.49927133,  0.4935242 , -0.50496876, -0.46736652, -0.47263733,\n",
              "         0.47128922,  0.44568625, -0.51859194, -0.5020118 , -0.46481946,\n",
              "        -0.44014162,  0.4353028 ,  0.46318057,  0.47395507, -0.5473456 ,\n",
              "        -0.4639664 ,  0.4369697 ,  0.4527801 , -0.48971453, -0.4634001 ,\n",
              "        -0.52332425,  0.46369886,  0.5160011 ,  0.5071261 , -0.43440634,\n",
              "        -0.52728474,  0.53066677,  0.49909067,  0.52359927,  0.4201982 ,\n",
              "        -0.4796415 ,  0.51501083, -0.52507305, -0.4831186 , -0.44280297,\n",
              "        -0.45038497,  0.53038216, -0.45342046,  0.5198405 , -0.45782113,\n",
              "         0.51984096, -0.43403578, -0.46118304, -0.52497244,  0.48410764,\n",
              "         0.46861   ,  0.4837346 ,  0.54011846, -0.44812945,  0.46113163],\n",
              "       dtype=float32),\n",
              " 'indicate': array([ 0.51439774,  0.52232397, -0.4677546 , -0.500617  , -0.5454515 ,\n",
              "         0.520481  ,  0.45896706, -0.47219375, -0.46980897, -0.43853506,\n",
              "        -0.47237107,  0.49921328,  0.49774146,  0.49961707, -0.5600638 ,\n",
              "        -0.4566928 ,  0.49528444,  0.5337121 , -0.46506187, -0.5173341 ,\n",
              "        -0.5275489 ,  0.48148024,  0.49814457,  0.53563243, -0.52077055,\n",
              "        -0.5105673 ,  0.4996141 ,  0.46965685,  0.51784116,  0.48179823,\n",
              "        -0.49676165,  0.4806602 , -0.47655135, -0.53836316, -0.52497536,\n",
              "        -0.47486278,  0.46121114, -0.50099874,  0.49711466, -0.47394994,\n",
              "         0.487041  , -0.5123795 , -0.49815065, -0.5029991 ,  0.46565434,\n",
              "         0.531097  ,  0.48613352,  0.5027428 , -0.47020742,  0.50499344],\n",
              "       dtype=float32),\n",
              " 'larger': array([ 0.5020421 ,  0.4309976 , -0.50982726, -0.48105142, -0.47576866,\n",
              "         0.49574482,  0.49875665, -0.46517795, -0.45027336, -0.5113334 ,\n",
              "        -0.47819588,  0.452416  ,  0.5039567 ,  0.49168396, -0.5461752 ,\n",
              "        -0.47272435,  0.51547694,  0.5124707 , -0.5089804 , -0.5209962 ,\n",
              "        -0.5107759 ,  0.45359454,  0.50521594,  0.4743253 , -0.49457198,\n",
              "        -0.4584778 ,  0.46545443,  0.5083357 ,  0.5059908 ,  0.47053236,\n",
              "        -0.49332047,  0.51475877, -0.47485617, -0.46632677, -0.49893355,\n",
              "        -0.5391981 ,  0.4846917 , -0.5295028 ,  0.44790933, -0.49488854,\n",
              "         0.4663607 , -0.52953035, -0.49297762, -0.4728489 ,  0.51968956,\n",
              "         0.48217514,  0.4871117 ,  0.5380124 , -0.511027  ,  0.47741064],\n",
              "       dtype=float32),\n",
              " 'maturity': array([ 0.55025715,  0.4617237 , -0.4889992 , -0.45589718, -0.53760463,\n",
              "         0.5070044 ,  0.47805926, -0.44409218, -0.47420102, -0.46849975,\n",
              "        -0.43690452,  0.46508092,  0.49364248,  0.43807873, -0.50138175,\n",
              "        -0.5222862 ,  0.4946985 ,  0.44947296, -0.43879   , -0.50084835,\n",
              "        -0.49835205,  0.46972016,  0.48880833,  0.51004547, -0.5053958 ,\n",
              "        -0.47293815,  0.4766575 ,  0.48645446,  0.44327092,  0.4618114 ,\n",
              "        -0.4565607 ,  0.51133484, -0.5219525 , -0.51327056, -0.5054088 ,\n",
              "        -0.4488187 ,  0.47950685, -0.51047087,  0.4338316 , -0.5272926 ,\n",
              "         0.506897  , -0.46802908, -0.43031713, -0.46388578,  0.49997276,\n",
              "         0.50963145,  0.5222166 ,  0.45136362, -0.49979395,  0.48346043],\n",
              "       dtype=float32),\n",
              " 'effect': array([ 0.4942893 ,  0.47211364, -0.4787107 , -0.5031568 , -0.4499356 ,\n",
              "         0.52803034,  0.46296725, -0.4971353 , -0.475211  , -0.50303966,\n",
              "        -0.51456755,  0.4514747 ,  0.44105574,  0.48769602, -0.5417228 ,\n",
              "        -0.48055992,  0.5239946 ,  0.46477777, -0.5169308 , -0.47962964,\n",
              "        -0.49370724,  0.5157516 ,  0.48903129,  0.44691834, -0.44051522,\n",
              "        -0.47760087,  0.5123542 ,  0.47378597,  0.4905335 ,  0.49941078,\n",
              "        -0.48396286,  0.44264495, -0.5436193 , -0.5256295 , -0.48249155,\n",
              "        -0.46596056,  0.49436545, -0.5348499 ,  0.50842565, -0.4493923 ,\n",
              "         0.51519   , -0.5358839 , -0.49929672, -0.44317243,  0.49452555,\n",
              "         0.5147058 ,  0.50087667,  0.5049813 , -0.5075136 ,  0.50493777],\n",
              "       dtype=float32),\n",
              " 'achieve': array([ 0.5488409 ,  0.47409832, -0.48581922, -0.46051046, -0.4774089 ,\n",
              "         0.54267395,  0.44976503, -0.46595573, -0.48823473, -0.5269401 ,\n",
              "        -0.5110672 ,  0.5230745 ,  0.49906182,  0.538351  , -0.50395364,\n",
              "        -0.5304785 ,  0.51398426,  0.47281414, -0.454269  , -0.44809353,\n",
              "        -0.50785875,  0.5073926 ,  0.5313781 ,  0.47193143, -0.48490357,\n",
              "        -0.4824524 ,  0.47452945,  0.52470726,  0.47551435,  0.47208756,\n",
              "        -0.46119982,  0.5209508 , -0.4920481 , -0.49165776, -0.48629922,\n",
              "        -0.53989285,  0.5125794 , -0.48018768,  0.4748312 , -0.49609393,\n",
              "         0.5099792 , -0.53172123, -0.448911  , -0.4550945 ,  0.4972994 ,\n",
              "         0.53361917,  0.4700876 ,  0.50773335, -0.4516046 ,  0.49779877],\n",
              "       dtype=float32),\n",
              " 'local': array([ 0.5126024 ,  0.48050895, -0.44615397, -0.49260002, -0.48335543,\n",
              "         0.46630517,  0.50971   , -0.53576857, -0.49806195, -0.4629172 ,\n",
              "        -0.49898657,  0.45148572,  0.50225854,  0.4968547 , -0.56894624,\n",
              "        -0.45704097,  0.47759095,  0.47902912, -0.4494254 , -0.43245268,\n",
              "        -0.47390556,  0.44340715,  0.4885386 ,  0.48265696, -0.5271097 ,\n",
              "        -0.45768267,  0.5066933 ,  0.47717136,  0.5226766 ,  0.53893536,\n",
              "        -0.5038091 ,  0.46866682, -0.49172717, -0.46442258, -0.51072115,\n",
              "        -0.49226853,  0.49818173, -0.47230372,  0.51309144, -0.46619177,\n",
              "         0.47521812, -0.45974335, -0.5233705 , -0.44040608,  0.4945489 ,\n",
              "         0.5148775 ,  0.48599958,  0.47709057, -0.48739526,  0.4465704 ],\n",
              "       dtype=float32),\n",
              " 'assumes': array([ 0.5029577 ,  0.45952827, -0.5313489 , -0.4790082 , -0.5228655 ,\n",
              "         0.45851022,  0.480999  , -0.482503  , -0.47942677, -0.4940658 ,\n",
              "        -0.50001866,  0.5243085 ,  0.5166711 ,  0.47569728, -0.5266734 ,\n",
              "        -0.51653636,  0.53069985,  0.48909223, -0.44389206, -0.46404693,\n",
              "        -0.4476654 ,  0.45291   ,  0.475637  ,  0.5258902 , -0.50893384,\n",
              "        -0.51805973,  0.49238175,  0.5151121 ,  0.50493544,  0.50059223,\n",
              "        -0.46123663,  0.48115027, -0.5536525 , -0.5371277 , -0.5153991 ,\n",
              "        -0.4994948 ,  0.5299329 , -0.45343557,  0.52469194, -0.45953065,\n",
              "         0.48575497, -0.5026419 , -0.44820696, -0.505345  ,  0.5018047 ,\n",
              "         0.47834104,  0.45890453,  0.45648786, -0.52014554,  0.450974  ],\n",
              "       dtype=float32),\n",
              " 'strategy': array([ 0.4686264 ,  0.48622918, -0.52670515, -0.441956  , -0.53600794,\n",
              "         0.5005962 ,  0.48805866, -0.46892095, -0.48419487, -0.46323213,\n",
              "        -0.47046515,  0.52708197,  0.4544437 ,  0.47163   , -0.55945075,\n",
              "        -0.4500993 ,  0.5291952 ,  0.4433251 , -0.5200206 , -0.46458185,\n",
              "        -0.4813708 ,  0.47118545,  0.50389606,  0.48203272, -0.45958167,\n",
              "        -0.5220729 ,  0.43293115,  0.47230807,  0.51187086,  0.520067  ,\n",
              "        -0.4516051 ,  0.44040343, -0.52170944, -0.4624648 , -0.47653747,\n",
              "        -0.5057074 ,  0.48417377, -0.5256081 ,  0.49200314, -0.50225997,\n",
              "         0.5096473 , -0.49021685, -0.51073873, -0.44452643,  0.4751123 ,\n",
              "         0.5033672 ,  0.49570513,  0.49899262, -0.4716354 ,  0.49970704],\n",
              "       dtype=float32),\n",
              " 'another': array([ 0.53854954,  0.46844026, -0.46194574, -0.5199664 , -0.49341407,\n",
              "         0.5379339 ,  0.5268468 , -0.44535375, -0.5239841 , -0.46937993,\n",
              "        -0.5280343 ,  0.45405424,  0.49107417,  0.449791  , -0.4823638 ,\n",
              "        -0.4529372 ,  0.51703966,  0.5024576 , -0.50777537, -0.48765627,\n",
              "        -0.5056546 ,  0.4469702 ,  0.5265652 ,  0.5106963 , -0.4721822 ,\n",
              "        -0.47625875,  0.52074146,  0.54147387,  0.44324845,  0.5243533 ,\n",
              "        -0.5112829 ,  0.5023121 , -0.54639417, -0.4566072 , -0.47063857,\n",
              "        -0.45349365,  0.49249086, -0.49160105,  0.45800063, -0.5053849 ,\n",
              "         0.5307271 , -0.46746957, -0.5037551 , -0.45369494,  0.463253  ,\n",
              "         0.51317483,  0.43886858,  0.5005214 , -0.5083479 ,  0.48774442],\n",
              "       dtype=float32),\n",
              " 'shareholders': array([ 0.48956394,  0.45246822, -0.50254244, -0.50974417, -0.4613579 ,\n",
              "         0.4804362 ,  0.45866415, -0.49689484, -0.5251342 , -0.44712633,\n",
              "        -0.46426862,  0.45556444,  0.4586561 ,  0.4892707 , -0.53491086,\n",
              "        -0.44979587,  0.49608698,  0.45769155, -0.43195572, -0.42760688,\n",
              "        -0.45460033,  0.5147367 ,  0.48241127,  0.50905293, -0.5112644 ,\n",
              "        -0.461798  ,  0.4708769 ,  0.53199446,  0.46974024,  0.44893867,\n",
              "        -0.47549075,  0.43240145, -0.51755536, -0.4468463 , -0.5086106 ,\n",
              "        -0.47177756,  0.5294788 , -0.48252407,  0.47391897, -0.51545703,\n",
              "         0.530758  , -0.53435326, -0.4279007 , -0.5222417 ,  0.45588762,\n",
              "         0.4584144 ,  0.50582415,  0.50384945, -0.45295388,  0.46723002],\n",
              "       dtype=float32),\n",
              " 'help': array([ 0.5043409 ,  0.49774098, -0.45316407, -0.5030503 , -0.5183685 ,\n",
              "         0.5267603 ,  0.505945  , -0.5200343 , -0.45815945, -0.4608471 ,\n",
              "        -0.5287221 ,  0.527827  ,  0.4664838 ,  0.49203452, -0.49341637,\n",
              "        -0.50657684,  0.49102026,  0.53842264, -0.4711561 , -0.51501524,\n",
              "        -0.5163436 ,  0.4929474 ,  0.4614455 ,  0.4756332 , -0.49884543,\n",
              "        -0.45423016,  0.4929999 ,  0.52017266,  0.44482583,  0.46807304,\n",
              "        -0.46696883,  0.49470517, -0.54583967, -0.47620484, -0.49156117,\n",
              "        -0.44872722,  0.46868667, -0.44777113,  0.5196892 , -0.52337205,\n",
              "         0.5079126 , -0.48697504, -0.43810338, -0.5024643 ,  0.5367006 ,\n",
              "         0.45618683,  0.43882743,  0.4792861 , -0.47288078,  0.4996587 ],\n",
              "       dtype=float32),\n",
              " 'derivative': array([ 0.49453074,  0.48872095, -0.4507655 , -0.5100476 , -0.4731453 ,\n",
              "         0.46911982,  0.48082992, -0.47592175, -0.45514506, -0.49225742,\n",
              "        -0.44508058,  0.48436508,  0.4919688 ,  0.4620698 , -0.5535812 ,\n",
              "        -0.4634645 ,  0.45147875,  0.49774823, -0.52071506, -0.43078202,\n",
              "        -0.49234283,  0.47808018,  0.4935874 ,  0.4413758 , -0.46041894,\n",
              "        -0.5149399 ,  0.4472931 ,  0.53336185,  0.46592715,  0.5176091 ,\n",
              "        -0.4796972 ,  0.4296352 , -0.5192858 , -0.52322257, -0.44664857,\n",
              "        -0.4734624 ,  0.49001455, -0.51624477,  0.44708896, -0.46785408,\n",
              "         0.49559784, -0.48294818, -0.4378737 , -0.46503675,  0.4843457 ,\n",
              "         0.4819822 ,  0.5062726 ,  0.50196624, -0.47516647,  0.49735638],\n",
              "       dtype=float32),\n",
              " 'long-term': array([ 0.54155624,  0.4819222 , -0.49051112, -0.50943464, -0.54274046,\n",
              "         0.4482531 ,  0.48237118, -0.4880867 , -0.51223207, -0.5036564 ,\n",
              "        -0.50613815,  0.50246644,  0.50109535,  0.4849789 , -0.47966203,\n",
              "        -0.5133889 ,  0.51409614,  0.48727536, -0.4514518 , -0.46851957,\n",
              "        -0.46672207,  0.5108377 ,  0.45859987,  0.4412593 , -0.45646918,\n",
              "        -0.46270838,  0.4595401 ,  0.511773  ,  0.43922794,  0.4909896 ,\n",
              "        -0.51254255,  0.43739778, -0.48750123, -0.48959702, -0.5103975 ,\n",
              "        -0.5144657 ,  0.53653556, -0.44233468,  0.47683832, -0.48697156,\n",
              "         0.4544459 , -0.47978765, -0.45701414, -0.52511376,  0.48311555,\n",
              "         0.48952624,  0.4910949 ,  0.49613798, -0.513371  ,  0.5289307 ],\n",
              "       dtype=float32),\n",
              " 'developed': array([ 0.45993787,  0.4856245 , -0.48911566, -0.47706875, -0.48822334,\n",
              "         0.49206603,  0.4778831 , -0.4922126 , -0.48028398, -0.50995016,\n",
              "        -0.50917083,  0.5172709 ,  0.5202812 ,  0.47955495, -0.48580968,\n",
              "        -0.454843  ,  0.48511028,  0.49286485, -0.5006824 , -0.46498165,\n",
              "        -0.46604043,  0.44456238,  0.5145591 ,  0.43320635, -0.44227514,\n",
              "        -0.4767509 ,  0.5224951 ,  0.5273607 ,  0.47860157,  0.4635744 ,\n",
              "        -0.5126311 ,  0.47498116, -0.50180984, -0.526099  , -0.4666801 ,\n",
              "        -0.4713343 ,  0.49090803, -0.4423863 ,  0.46336755, -0.5006603 ,\n",
              "         0.52789795, -0.4441994 , -0.46569562, -0.48048458,  0.5299473 ,\n",
              "         0.457784  ,  0.4886217 ,  0.5063366 , -0.46120176,  0.4834824 ],\n",
              "       dtype=float32),\n",
              " 'guaranteed': array([ 0.51351655,  0.485076  , -0.4533072 , -0.490065  , -0.48513198,\n",
              "         0.52480286,  0.45873916, -0.49528897, -0.42683744, -0.44353426,\n",
              "        -0.47162336,  0.52650243,  0.4383689 ,  0.48094648, -0.5456601 ,\n",
              "        -0.5276578 ,  0.49970484,  0.49687597, -0.43574283, -0.47090128,\n",
              "        -0.50600576,  0.4705298 ,  0.4886602 ,  0.4555715 , -0.4761672 ,\n",
              "        -0.5040438 ,  0.5056395 ,  0.47286785,  0.50293195,  0.43826056,\n",
              "        -0.5307914 ,  0.49505135, -0.4534201 , -0.5180211 , -0.46149877,\n",
              "        -0.4812572 ,  0.4996335 , -0.48793906,  0.5096114 , -0.50620186,\n",
              "         0.5278843 , -0.5023132 , -0.42087838, -0.4802975 ,  0.45576838,\n",
              "         0.50376666,  0.45914966,  0.5283037 , -0.49372196,  0.48401874],\n",
              "       dtype=float32),\n",
              " 'sectors': array([ 0.49251854,  0.47790182, -0.5087419 , -0.52615315, -0.44761896,\n",
              "         0.51168746,  0.44089967, -0.51240367, -0.4968256 , -0.5009645 ,\n",
              "        -0.520096  ,  0.49818692,  0.45567685,  0.4495573 , -0.49917704,\n",
              "        -0.49345562,  0.52274024,  0.45758006, -0.5029105 , -0.46854925,\n",
              "        -0.513957  ,  0.45893735,  0.49503773,  0.47047174, -0.42979437,\n",
              "        -0.52212113,  0.4490572 ,  0.50924987,  0.52502203,  0.4437553 ,\n",
              "        -0.5088301 ,  0.51773787, -0.5322363 , -0.44653866, -0.5030401 ,\n",
              "        -0.45703766,  0.51644605, -0.45560378,  0.47965062, -0.4596305 ,\n",
              "         0.44511232, -0.44639048, -0.45500594, -0.4881594 ,  0.4806909 ,\n",
              "         0.45583808,  0.4933563 ,  0.47761077, -0.50619227,  0.43723696],\n",
              "       dtype=float32),\n",
              " 'affected': array([ 0.5192007 ,  0.44382212, -0.5135812 , -0.45495844, -0.5064019 ,\n",
              "         0.46511474,  0.4607745 , -0.5238355 , -0.5065446 , -0.44370365,\n",
              "        -0.4879926 ,  0.45498955,  0.47873333,  0.49041206, -0.54773283,\n",
              "        -0.51043713,  0.5355195 ,  0.52585167, -0.44463393, -0.4436872 ,\n",
              "        -0.52850395,  0.4797002 ,  0.5182467 ,  0.4773252 , -0.4614911 ,\n",
              "        -0.52899027,  0.44904944,  0.55142486,  0.43944722,  0.50898474,\n",
              "        -0.49626172,  0.51429003, -0.5196246 , -0.46272436, -0.5111685 ,\n",
              "        -0.49802244,  0.50872386, -0.47847307,  0.5113113 , -0.543428  ,\n",
              "         0.45715612, -0.5314297 , -0.51185274, -0.45047912,  0.4809169 ,\n",
              "         0.4913147 ,  0.47695506,  0.4798615 , -0.4448962 ,  0.526296  ],\n",
              "       dtype=float32),\n",
              " 'distribution': array([ 0.53296274,  0.4648577 , -0.48433775, -0.44108832, -0.5335245 ,\n",
              "         0.46097443,  0.47888014, -0.47281852, -0.47166732, -0.50143135,\n",
              "        -0.46637785,  0.50452113,  0.46539545,  0.5219757 , -0.5600476 ,\n",
              "        -0.5384339 ,  0.5124298 ,  0.46650496, -0.44439825, -0.47091082,\n",
              "        -0.47213238,  0.5001628 ,  0.5232644 ,  0.49915272, -0.46543682,\n",
              "        -0.47112194,  0.4610162 ,  0.51938796,  0.47849354,  0.5285591 ,\n",
              "        -0.50817937,  0.50004846, -0.49170363, -0.5094785 , -0.5230831 ,\n",
              "        -0.5233676 ,  0.5174681 , -0.47973382,  0.49716184, -0.49559808,\n",
              "         0.51781803, -0.45920184, -0.44149733, -0.48073018,  0.49002042,\n",
              "         0.53088546,  0.5157022 ,  0.511125  , -0.49239656,  0.4691006 ],\n",
              "       dtype=float32),\n",
              " 'yield': array([ 0.49393377,  0.44687375, -0.48232114, -0.43904242, -0.4721739 ,\n",
              "         0.46058807,  0.4428098 , -0.5269411 , -0.49347517, -0.5036461 ,\n",
              "        -0.4764959 ,  0.44551066,  0.4928961 ,  0.51013035, -0.54440683,\n",
              "        -0.4711579 ,  0.46065536,  0.48539424, -0.45429033, -0.5155507 ,\n",
              "        -0.49457958,  0.48376   ,  0.52671653,  0.43701506, -0.42843556,\n",
              "        -0.5083661 ,  0.4403162 ,  0.49527425,  0.46382588,  0.5073584 ,\n",
              "        -0.4473983 ,  0.51615477, -0.5155466 , -0.50716686, -0.44605044,\n",
              "        -0.50578034,  0.50019246, -0.49895033,  0.48306024, -0.48664832,\n",
              "         0.4761395 , -0.45268545, -0.46135214, -0.50565606,  0.5166343 ,\n",
              "         0.48547274,  0.48408914,  0.4372853 , -0.4737985 ,  0.49398148],\n",
              "       dtype=float32),\n",
              " 'highest': array([ 0.50812626,  0.44071755, -0.4819636 , -0.49014187, -0.5105067 ,\n",
              "         0.48685133,  0.45611686, -0.4842925 , -0.42606878, -0.48233843,\n",
              "        -0.43456256,  0.49825087,  0.51742876,  0.4778933 , -0.5111192 ,\n",
              "        -0.45308545,  0.51964676,  0.4657475 , -0.5156693 , -0.48263216,\n",
              "        -0.49714798,  0.46140665,  0.44291157,  0.4957198 , -0.5146352 ,\n",
              "        -0.44664443,  0.49876493,  0.5097317 ,  0.5225198 ,  0.5135485 ,\n",
              "        -0.51854223,  0.50296056, -0.48053324, -0.5054398 , -0.48083967,\n",
              "        -0.4565761 ,  0.5232674 , -0.45785803,  0.46279764, -0.50549847,\n",
              "         0.52553564, -0.44695273, -0.4574139 , -0.4729367 ,  0.53009605,\n",
              "         0.45156142,  0.45861343,  0.5053821 , -0.4667933 ,  0.49106148],\n",
              "       dtype=float32),\n",
              " 'united': array([ 0.5232758 ,  0.4704436 , -0.46970293, -0.4430863 , -0.48517683,\n",
              "         0.45887664,  0.45340624, -0.4855573 , -0.45124647, -0.469086  ,\n",
              "        -0.46739882,  0.49279442,  0.5004176 ,  0.48950112, -0.48894736,\n",
              "        -0.49162146,  0.4863829 ,  0.49966922, -0.46848288, -0.4680686 ,\n",
              "        -0.46104237,  0.45589903,  0.4650774 ,  0.5099069 , -0.43601122,\n",
              "        -0.46554574,  0.49227574,  0.5255248 ,  0.50614035,  0.44593623,\n",
              "        -0.51318556,  0.4807274 , -0.49288088, -0.5126771 , -0.52041477,\n",
              "        -0.48463792,  0.46541223, -0.49967107,  0.47179797, -0.45562735,\n",
              "         0.5117717 , -0.47374725, -0.44800037, -0.49740577,  0.5016543 ,\n",
              "         0.5228426 ,  0.44593453,  0.5432737 , -0.49648607,  0.4556825 ],\n",
              "       dtype=float32),\n",
              " 'compare': array([ 0.55913055,  0.438874  , -0.5042882 , -0.49763688, -0.48842633,\n",
              "         0.5072119 ,  0.487927  , -0.4924803 , -0.46833286, -0.53303564,\n",
              "        -0.50321805,  0.4703912 ,  0.5358264 ,  0.53549594, -0.50645983,\n",
              "        -0.49510255,  0.46553075,  0.46137765, -0.45063236, -0.46891072,\n",
              "        -0.5169706 ,  0.44412348,  0.5108464 ,  0.47870502, -0.4773899 ,\n",
              "        -0.4727577 ,  0.5045983 ,  0.5545239 ,  0.53416556,  0.4473915 ,\n",
              "        -0.47155127,  0.5139487 , -0.4742483 , -0.50600195, -0.4826687 ,\n",
              "        -0.4586101 ,  0.54105943, -0.46379185,  0.4554513 , -0.5012497 ,\n",
              "         0.47491762, -0.5521662 , -0.47002897, -0.51619935,  0.46315277,\n",
              "         0.47660512,  0.45608026,  0.53099173, -0.5166762 ,  0.47553486],\n",
              "       dtype=float32),\n",
              " 'agency': array([ 0.5381815 ,  0.49251357, -0.49535215, -0.5196766 , -0.51626414,\n",
              "         0.48759782,  0.50672454, -0.5102273 , -0.4962564 , -0.49627796,\n",
              "        -0.48393565,  0.5054186 ,  0.46627054,  0.52111274, -0.55141854,\n",
              "        -0.47227645,  0.54022545,  0.5212095 , -0.46391252, -0.47864017,\n",
              "        -0.48644796,  0.46899995,  0.44656485,  0.49180055, -0.47956324,\n",
              "        -0.4474657 ,  0.44931853,  0.48216805,  0.45910257,  0.4442245 ,\n",
              "        -0.45254156,  0.4732248 , -0.4718698 , -0.5207339 , -0.4421722 ,\n",
              "        -0.45454875,  0.47129935, -0.47508627,  0.474725  , -0.456214  ,\n",
              "         0.44316363, -0.46316203, -0.46951926, -0.49854758,  0.4957491 ,\n",
              "         0.51690716,  0.47946197,  0.5249318 , -0.46790472,  0.49849784],\n",
              "       dtype=float32),\n",
              " 'events': array([ 0.53401697,  0.45484465, -0.47846204, -0.46830052, -0.48251572,\n",
              "         0.5130857 ,  0.5266321 , -0.48164493, -0.4498504 , -0.5155114 ,\n",
              "        -0.46670184,  0.5225261 ,  0.52576   ,  0.5028495 , -0.56330013,\n",
              "        -0.47921136,  0.49665424,  0.5029094 , -0.5038183 , -0.44542986,\n",
              "        -0.4582051 ,  0.51545364,  0.4888488 ,  0.44683424, -0.51248753,\n",
              "        -0.5091077 ,  0.52115095,  0.5165875 ,  0.4871582 ,  0.44512755,\n",
              "        -0.45704713,  0.44763517, -0.4768043 , -0.5284897 , -0.4661502 ,\n",
              "        -0.49846038,  0.4507275 , -0.4798895 ,  0.47109464, -0.45579112,\n",
              "         0.46341574, -0.4869323 , -0.44539344, -0.52494663,  0.46836025,\n",
              "         0.44478926,  0.46377873,  0.47995186, -0.5122698 ,  0.50470436],\n",
              "       dtype=float32),\n",
              " 'recent': array([ 0.50854224,  0.47688794, -0.5343386 , -0.51019806, -0.5377567 ,\n",
              "         0.5343663 ,  0.52175   , -0.46088225, -0.5303269 , -0.49522656,\n",
              "        -0.47699434,  0.47676414,  0.5082851 ,  0.46932498, -0.54859716,\n",
              "        -0.522992  ,  0.5301616 ,  0.4815492 , -0.5237442 , -0.43198723,\n",
              "        -0.51686454,  0.48110574,  0.50800705,  0.49160323, -0.46123987,\n",
              "        -0.49617118,  0.44505608,  0.5281635 ,  0.5298054 ,  0.5219708 ,\n",
              "        -0.52947193,  0.46619952, -0.4653336 , -0.50610906, -0.52590835,\n",
              "        -0.5301555 ,  0.5213454 , -0.45445675,  0.45078465, -0.4585291 ,\n",
              "         0.4904576 , -0.46643478, -0.45800024, -0.46634978,  0.50981194,\n",
              "         0.4542758 ,  0.4819191 ,  0.5267131 , -0.48455805,  0.48967654],\n",
              "       dtype=float32),\n",
              " 'smaller': array([ 0.5588041 ,  0.43266234, -0.5206359 , -0.47809815, -0.475575  ,\n",
              "         0.54418695,  0.52344346, -0.49793994, -0.4904787 , -0.47151548,\n",
              "        -0.47453538,  0.48099518,  0.45083985,  0.47945693, -0.5582499 ,\n",
              "        -0.4439577 ,  0.47788808,  0.45016205, -0.45283327, -0.45265657,\n",
              "        -0.47877538,  0.49961013,  0.5221182 ,  0.44324052, -0.48529264,\n",
              "        -0.4503671 ,  0.49980962,  0.48401037,  0.5044346 ,  0.49627948,\n",
              "        -0.46707925,  0.44591942, -0.5295509 , -0.47176635, -0.5015097 ,\n",
              "        -0.5022505 ,  0.5359069 , -0.4472484 ,  0.5237466 , -0.5283173 ,\n",
              "         0.50359726, -0.5179657 , -0.49829805, -0.52264875,  0.47546422,\n",
              "         0.48760292,  0.46675223,  0.511914  , -0.48771137,  0.46235186],\n",
              "       dtype=float32),\n",
              " 'fiscal': array([ 0.4790458 ,  0.5058854 , -0.4822831 , -0.44222865, -0.520647  ,\n",
              "         0.52195066,  0.5139587 , -0.48936754, -0.49502358, -0.5160991 ,\n",
              "        -0.53037506,  0.4838301 ,  0.45984596,  0.5012526 , -0.5560494 ,\n",
              "        -0.5307172 ,  0.45897797,  0.48308253, -0.52181375, -0.47613472,\n",
              "        -0.5291984 ,  0.5274571 ,  0.50396454,  0.4853071 , -0.48505592,\n",
              "        -0.47613055,  0.5029169 ,  0.5024672 ,  0.5013546 ,  0.46972364,\n",
              "        -0.53230554,  0.50451654, -0.49717343, -0.48599088, -0.47202212,\n",
              "        -0.53284883,  0.4553308 , -0.46047512,  0.47940245, -0.49269095,\n",
              "         0.46606854, -0.49783   , -0.5250275 , -0.5235595 ,  0.5346471 ,\n",
              "         0.5273297 ,  0.5186802 ,  0.506762  , -0.48049098,  0.47374701],\n",
              "       dtype=float32),\n",
              " 'fidelity': array([ 0.5476793 ,  0.44236016, -0.50691307, -0.4948254 , -0.46594614,\n",
              "         0.5255921 ,  0.51145005, -0.4445481 , -0.4686046 , -0.49699906,\n",
              "        -0.44274098,  0.504077  ,  0.46797037,  0.4906867 , -0.50563353,\n",
              "        -0.5112344 ,  0.44367677,  0.47263247, -0.48262352, -0.4315618 ,\n",
              "        -0.46199292,  0.52157867,  0.4791624 ,  0.509043  , -0.43763173,\n",
              "        -0.45656446,  0.4628276 ,  0.5445376 ,  0.49980465,  0.525727  ,\n",
              "        -0.4968278 ,  0.45980492, -0.50579554, -0.5335705 , -0.43918037,\n",
              "        -0.47490054,  0.5249933 , -0.5013861 ,  0.52600884, -0.46774858,\n",
              "         0.4523944 , -0.50771827, -0.51360226, -0.5161488 ,  0.5012088 ,\n",
              "         0.46287245,  0.4591078 ,  0.4704324 , -0.5217377 ,  0.46508205],\n",
              "       dtype=float32),\n",
              " 'contracts': array([ 0.496589  ,  0.43484175, -0.52019495, -0.4591384 , -0.5117162 ,\n",
              "         0.4915076 ,  0.52608603, -0.4898075 , -0.49590456, -0.5012063 ,\n",
              "        -0.48321527,  0.50606376,  0.48072886,  0.47559923, -0.55560803,\n",
              "        -0.47817057,  0.45457253,  0.5141972 , -0.46173233, -0.50410724,\n",
              "        -0.454383  ,  0.4428841 ,  0.51445097,  0.44068196, -0.44046772,\n",
              "        -0.46868482,  0.45243728,  0.5072617 ,  0.48730022,  0.5172081 ,\n",
              "        -0.46319133,  0.44512075, -0.47126383, -0.48338675, -0.5199548 ,\n",
              "        -0.47697487,  0.45078322, -0.52263737,  0.4868415 , -0.48387957,\n",
              "         0.47423795, -0.51453936, -0.47649258, -0.4855328 ,  0.4660492 ,\n",
              "         0.5147138 ,  0.51389295,  0.45515624, -0.5006155 ,  0.47332197],\n",
              "       dtype=float32),\n",
              " 'corporation': array([ 0.5183618 ,  0.5141792 , -0.46042264, -0.4532251 , -0.45279747,\n",
              "         0.49333847,  0.51747656, -0.52994305, -0.44462803, -0.47171563,\n",
              "        -0.53137004,  0.4674173 ,  0.5077586 ,  0.47220558, -0.5533358 ,\n",
              "        -0.47355434,  0.448044  ,  0.5265187 , -0.4347115 , -0.46524343,\n",
              "        -0.44946834,  0.5209167 ,  0.4458577 ,  0.4825339 , -0.48617965,\n",
              "        -0.46960998,  0.49001318,  0.48695627,  0.45818248,  0.48902282,\n",
              "        -0.47024006,  0.4399391 , -0.45586717, -0.5276544 , -0.4509632 ,\n",
              "        -0.49311006,  0.45361567, -0.523337  ,  0.50205886, -0.46360883,\n",
              "         0.49818668, -0.4897431 , -0.44799668, -0.4609182 ,  0.50382406,\n",
              "         0.50470906,  0.4568429 ,  0.51771134, -0.5211599 ,  0.5092734 ],\n",
              "       dtype=float32),\n",
              " 'columbia': array([ 0.53537935,  0.44218737, -0.49174798, -0.47951552, -0.49960306,\n",
              "         0.4967515 ,  0.47849122, -0.51321936, -0.50838566, -0.4406724 ,\n",
              "        -0.47073483,  0.46944654,  0.5033438 ,  0.53631234, -0.5181196 ,\n",
              "        -0.49171412,  0.53953815,  0.50855166, -0.45580024, -0.50316995,\n",
              "        -0.44787338,  0.51218826,  0.49267673,  0.47493732, -0.5075988 ,\n",
              "        -0.5328185 ,  0.52573633,  0.4948634 ,  0.45494002,  0.51999295,\n",
              "        -0.5136986 ,  0.52894706, -0.5233238 , -0.5168112 , -0.46275926,\n",
              "        -0.53223014,  0.4573712 , -0.5051557 ,  0.5042096 , -0.5212378 ,\n",
              "         0.46446764, -0.45583636, -0.47080022, -0.5253895 ,  0.5385857 ,\n",
              "         0.5338652 ,  0.53223866,  0.44839776, -0.4950921 ,  0.52406126],\n",
              "       dtype=float32),\n",
              " 'r6': array([ 0.48500645,  0.5141913 , -0.52083087, -0.5103396 , -0.5052343 ,\n",
              "         0.4851848 ,  0.44489008, -0.5053593 , -0.45451534, -0.44088805,\n",
              "        -0.4464669 ,  0.48983186,  0.44616976,  0.5142266 , -0.51147836,\n",
              "        -0.5163848 ,  0.4607214 ,  0.4624965 , -0.48246962, -0.51800084,\n",
              "        -0.5026059 ,  0.45108625,  0.44314575,  0.49570692, -0.48434106,\n",
              "        -0.43417105,  0.51957875,  0.49529997,  0.5233674 ,  0.50484234,\n",
              "        -0.5120738 ,  0.5188914 , -0.4949938 , -0.47351173, -0.45847282,\n",
              "        -0.51661   ,  0.4654024 , -0.5137154 ,  0.44868553, -0.49009433,\n",
              "         0.5355663 , -0.5331889 , -0.43221128, -0.4534742 ,  0.530931  ,\n",
              "         0.50961757,  0.49988207,  0.4847111 , -0.45695585,  0.50518847],\n",
              "       dtype=float32),\n",
              " '12b-1': array([ 0.51946586,  0.5021359 , -0.45621136, -0.5096687 , -0.50792545,\n",
              "         0.53583604,  0.47508606, -0.5240497 , -0.51227874, -0.47450876,\n",
              "        -0.47037566,  0.46806592,  0.4989557 ,  0.50399554, -0.5383944 ,\n",
              "        -0.5098845 ,  0.51630324,  0.47909376, -0.468884  , -0.4977476 ,\n",
              "        -0.5004587 ,  0.50906223,  0.52152026,  0.5253233 , -0.5139149 ,\n",
              "        -0.4495371 ,  0.5224502 ,  0.51572275,  0.44776952,  0.50629675,\n",
              "        -0.5084336 ,  0.46524888, -0.5005603 , -0.50049615, -0.48800838,\n",
              "        -0.45816123,  0.48592716, -0.45443147,  0.5180229 , -0.47594258,\n",
              "         0.52387685, -0.4877052 , -0.4679387 , -0.48959237,  0.45308912,\n",
              "         0.4547983 ,  0.452299  ,  0.46980923, -0.47910243,  0.4722521 ],\n",
              "       dtype=float32),\n",
              " 'plans': array([ 0.5078032 ,  0.50352997, -0.4410384 , -0.51044065, -0.4859031 ,\n",
              "         0.47883528,  0.4373842 , -0.48702422, -0.489043  , -0.4942974 ,\n",
              "        -0.43586603,  0.5076909 ,  0.5110125 ,  0.51139486, -0.49239275,\n",
              "        -0.48167434,  0.47184178,  0.45936298, -0.5078268 , -0.48266584,\n",
              "        -0.4444404 ,  0.51341623,  0.44982055,  0.47626448, -0.4761917 ,\n",
              "        -0.46468762,  0.43636242,  0.45906785,  0.5085659 ,  0.47129595,\n",
              "        -0.5239945 ,  0.44407326, -0.50633943, -0.4751978 , -0.47526616,\n",
              "        -0.5052905 ,  0.5154953 , -0.48443443,  0.49264652, -0.44175988,\n",
              "         0.47220087, -0.48970294, -0.490376  , -0.500576  ,  0.45672348,\n",
              "         0.47283235,  0.45901424,  0.48848698, -0.4737773 ,  0.46624023],\n",
              "       dtype=float32),\n",
              " 'tax-deferred': array([ 0.46878046,  0.44195768, -0.47695878, -0.47464472, -0.48827195,\n",
              "         0.48402366,  0.46613154, -0.5242677 , -0.46344408, -0.42902732,\n",
              "        -0.5133087 ,  0.50628245,  0.45781878,  0.49524644, -0.55007404,\n",
              "        -0.50428003,  0.5224307 ,  0.53121656, -0.4529294 , -0.49126142,\n",
              "        -0.46252677,  0.4411698 ,  0.46484056,  0.51547915, -0.48195288,\n",
              "        -0.44318503,  0.44002274,  0.46763945,  0.46201518,  0.4749887 ,\n",
              "        -0.5131321 ,  0.4477443 , -0.50319993, -0.53505725, -0.4354337 ,\n",
              "        -0.51184154,  0.4660784 , -0.51027626,  0.49602222, -0.48310745,\n",
              "         0.5303341 , -0.47061124, -0.50237954, -0.5043762 ,  0.51746696,\n",
              "         0.44452485,  0.52166146,  0.44497594, -0.4419983 ,  0.47400576],\n",
              "       dtype=float32),\n",
              " 'expected': array([ 0.513106  ,  0.42955267, -0.49823195, -0.4587128 , -0.52572316,\n",
              "         0.52276516,  0.5081128 , -0.4516293 , -0.4868682 , -0.4969775 ,\n",
              "        -0.50233847,  0.5099615 ,  0.5071562 ,  0.5285755 , -0.52762485,\n",
              "        -0.4936651 ,  0.5094922 ,  0.4792532 , -0.455174  , -0.4571925 ,\n",
              "        -0.4971342 ,  0.4756866 ,  0.47036725,  0.4546513 , -0.44825634,\n",
              "        -0.47959977,  0.4720073 ,  0.49522984,  0.47204137,  0.45538637,\n",
              "        -0.46769238,  0.5237832 , -0.48290318, -0.4973896 , -0.4661601 ,\n",
              "        -0.45399517,  0.5018059 , -0.47693238,  0.44829872, -0.47791266,\n",
              "         0.5318918 , -0.53070575, -0.42917794, -0.5120266 ,  0.47473335,\n",
              "         0.48120767,  0.5138124 ,  0.45518333, -0.5071363 ,  0.4632771 ],\n",
              "       dtype=float32),\n",
              " 'industries': array([ 0.53315157,  0.45271212, -0.49101022, -0.49049166, -0.4923379 ,\n",
              "         0.46568227,  0.50651497, -0.4904808 , -0.47977203, -0.47977138,\n",
              "        -0.5133275 ,  0.47142982,  0.502251  ,  0.44268006, -0.55237776,\n",
              "        -0.44051358,  0.48579708,  0.4894049 , -0.51984084, -0.49925837,\n",
              "        -0.43687427,  0.50754815,  0.47705305,  0.4313103 , -0.44589794,\n",
              "        -0.43975577,  0.44774985,  0.46867052,  0.47047922,  0.43920094,\n",
              "        -0.5362193 ,  0.4705412 , -0.515862  , -0.53402245, -0.5118016 ,\n",
              "        -0.46403375,  0.4929548 , -0.46295333,  0.51602376, -0.4678772 ,\n",
              "         0.4782438 , -0.4500065 , -0.49078935, -0.50172603,  0.5198911 ,\n",
              "         0.46713975,  0.5092543 ,  0.4494092 , -0.5155298 ,  0.5098579 ],\n",
              "       dtype=float32),\n",
              " 'circumstances': array([ 0.51487875,  0.4336531 , -0.4618173 , -0.4934405 , -0.46106046,\n",
              "         0.48171508,  0.48636374, -0.52334774, -0.49303937, -0.4584612 ,\n",
              "        -0.44435585,  0.5360948 ,  0.46556184,  0.50583553, -0.51849157,\n",
              "        -0.5195458 ,  0.49828038,  0.45252156, -0.48860583, -0.5263662 ,\n",
              "        -0.48243588,  0.4993415 ,  0.5368881 ,  0.48795134, -0.507192  ,\n",
              "        -0.45809123,  0.48057073,  0.5158327 ,  0.450158  ,  0.43972063,\n",
              "        -0.48943505,  0.52792937, -0.5001088 , -0.5216228 , -0.4620718 ,\n",
              "        -0.45899865,  0.49456868, -0.51828617,  0.47316223, -0.5270922 ,\n",
              "         0.49298206, -0.5242558 , -0.45273104, -0.44844073,  0.46921456,\n",
              "         0.4801917 ,  0.51216966,  0.45895272, -0.5108477 ,  0.51220095],\n",
              "       dtype=float32),\n",
              " 'difficult': array([ 0.4852366 ,  0.50306284, -0.47664002, -0.4831926 , -0.54270595,\n",
              "         0.49549267,  0.462016  , -0.5247795 , -0.46951398, -0.49269915,\n",
              "        -0.44673002,  0.44652906,  0.4425123 ,  0.44625056, -0.5707645 ,\n",
              "        -0.46553725,  0.5100392 ,  0.458955  , -0.47475934, -0.49706358,\n",
              "        -0.47260138,  0.47724637,  0.51260644,  0.51495385, -0.49690688,\n",
              "        -0.47011197,  0.5159158 ,  0.47594395,  0.45545068,  0.49331892,\n",
              "        -0.4610906 ,  0.45294598, -0.5394351 , -0.4697346 , -0.4442117 ,\n",
              "        -0.46241805,  0.54951394, -0.54189926,  0.4913606 , -0.50207585,\n",
              "         0.5232946 , -0.5098412 , -0.53736377, -0.46486935,  0.5368873 ,\n",
              "         0.47828332,  0.4970199 ,  0.51508814, -0.48053876,  0.48711956],\n",
              "       dtype=float32),\n",
              " 'visit': array([ 0.5347816 ,  0.4645484 , -0.49705794, -0.45540556, -0.53352886,\n",
              "         0.4556086 ,  0.5266143 , -0.47942284, -0.46656576, -0.51199096,\n",
              "        -0.45633718,  0.47071958,  0.4790344 ,  0.4657343 , -0.5033532 ,\n",
              "        -0.45700568,  0.4585043 ,  0.44769168, -0.48231214, -0.43491897,\n",
              "        -0.46541744,  0.4511848 ,  0.53318226,  0.43374464, -0.52077574,\n",
              "        -0.48814422,  0.44975907,  0.49805105,  0.48827955,  0.48369014,\n",
              "        -0.5326505 ,  0.49132615, -0.5264507 , -0.5002643 , -0.46906042,\n",
              "        -0.5217875 ,  0.51376665, -0.54104745,  0.4624372 , -0.45491812,\n",
              "         0.49307883, -0.46177456, -0.46122745, -0.46762785,  0.5292312 ,\n",
              "         0.46881104,  0.43836597,  0.52140045, -0.44425294,  0.51936424],\n",
              "       dtype=float32),\n",
              " 'transactions': array([ 0.52522045,  0.45372856, -0.45558053, -0.49977368, -0.48332244,\n",
              "         0.45618263,  0.47414678, -0.44769156, -0.48199087, -0.45786124,\n",
              "        -0.46339   ,  0.5105872 ,  0.47560593,  0.47602844, -0.5076975 ,\n",
              "        -0.5033857 ,  0.4454159 ,  0.48019132, -0.5029581 , -0.48608297,\n",
              "        -0.47967082,  0.49832198,  0.4459316 ,  0.50769573, -0.5188847 ,\n",
              "        -0.4462764 ,  0.50012577,  0.47824204,  0.4532152 ,  0.45686647,\n",
              "        -0.45451266,  0.46493888, -0.4703158 , -0.5000597 , -0.4795074 ,\n",
              "        -0.44454166,  0.48062125, -0.5033413 ,  0.5275035 , -0.47949645,\n",
              "         0.46528977, -0.53494763, -0.44230893, -0.48069823,  0.4944387 ,\n",
              "         0.45806834,  0.51169443,  0.5097618 , -0.4394164 ,  0.44943163],\n",
              "       dtype=float32),\n",
              " 'p': array([ 0.5515021 ,  0.47775605, -0.44708735, -0.52612495, -0.49809113,\n",
              "         0.4695244 ,  0.49718615, -0.5125279 , -0.48561633, -0.47666287,\n",
              "        -0.46814334,  0.51939225,  0.5219461 ,  0.5181996 , -0.561247  ,\n",
              "        -0.476411  ,  0.5215917 ,  0.4483424 , -0.50521404, -0.5046712 ,\n",
              "        -0.52050793,  0.49511242,  0.51464474,  0.5171679 , -0.4946587 ,\n",
              "        -0.462268  ,  0.45306408,  0.5055176 ,  0.4549951 ,  0.44094157,\n",
              "        -0.51090676,  0.48766503, -0.49356464, -0.47761405, -0.47016862,\n",
              "        -0.5044357 ,  0.49324343, -0.48835552,  0.45013028, -0.5253701 ,\n",
              "         0.47972983, -0.45317844, -0.49246928, -0.4475958 ,  0.5033938 ,\n",
              "         0.49413297,  0.43548122,  0.45072943, -0.45240426,  0.51773113],\n",
              "       dtype=float32),\n",
              " 'reduce': array([ 0.5177247 ,  0.4728347 , -0.4453828 , -0.47473213, -0.479395  ,\n",
              "         0.5092693 ,  0.4948125 , -0.5058861 , -0.48043755, -0.43309525,\n",
              "        -0.53031486,  0.52606857,  0.46948618,  0.46115935, -0.49984148,\n",
              "        -0.50461775,  0.45428053,  0.5206304 , -0.44948587, -0.5104509 ,\n",
              "        -0.46142936,  0.47649476,  0.525465  ,  0.49479967, -0.4915827 ,\n",
              "        -0.45043576,  0.48730123,  0.508655  ,  0.44739816,  0.46754304,\n",
              "        -0.5261664 ,  0.5238128 , -0.46701425, -0.48289877, -0.5055223 ,\n",
              "        -0.4460939 ,  0.5281611 , -0.5110606 ,  0.4395922 , -0.52785873,\n",
              "         0.47097054, -0.47019625, -0.47868776, -0.5036859 ,  0.5166477 ,\n",
              "         0.45088962,  0.45886147,  0.47406995, -0.46330318,  0.46286714],\n",
              "       dtype=float32),\n",
              " 'american': array([ 0.53974986,  0.5151385 , -0.5175918 , -0.472643  , -0.49456745,\n",
              "         0.5117395 ,  0.4436329 , -0.5251501 , -0.52961534, -0.44591296,\n",
              "        -0.44362777,  0.45654327,  0.5383567 ,  0.47191525, -0.51268864,\n",
              "        -0.48491633,  0.48995507,  0.45144153, -0.4438648 , -0.5020355 ,\n",
              "        -0.5186578 ,  0.45688123,  0.53374547,  0.45738393, -0.50839424,\n",
              "        -0.5336076 ,  0.4430535 ,  0.54275906,  0.46996665,  0.52446955,\n",
              "        -0.52661645,  0.49753773, -0.45911467, -0.52712643, -0.4911754 ,\n",
              "        -0.48744145,  0.44966698, -0.46571958,  0.48200455, -0.5442304 ,\n",
              "         0.4925529 , -0.51520294, -0.44524118, -0.4715913 ,  0.46236336,\n",
              "         0.5059901 ,  0.4458002 ,  0.46662304, -0.43107593,  0.48853928],\n",
              "       dtype=float32),\n",
              " 'counterparty': array([ 0.52871454,  0.48733032, -0.4815341 , -0.4760221 , -0.52800053,\n",
              "         0.5094866 ,  0.4521605 , -0.44222108, -0.4738451 , -0.49285352,\n",
              "        -0.48847088,  0.45078406,  0.50883365,  0.44986224, -0.49717212,\n",
              "        -0.44247934,  0.5110838 ,  0.47393346, -0.4669356 , -0.47861037,\n",
              "        -0.44316003,  0.47839776,  0.52250075,  0.45028907, -0.4667968 ,\n",
              "        -0.4702565 ,  0.4704062 ,  0.515974  ,  0.45831436,  0.5073927 ,\n",
              "        -0.47420067,  0.49731535, -0.46224868, -0.5305277 , -0.5116642 ,\n",
              "        -0.46169627,  0.5136941 , -0.5274307 ,  0.4517811 , -0.44807953,\n",
              "         0.47053394, -0.4932971 , -0.5012306 , -0.45435557,  0.4963404 ,\n",
              "         0.45204163,  0.49515545,  0.44543242, -0.5150333 ,  0.5129722 ],\n",
              "       dtype=float32),\n",
              " 'portion': array([ 0.49202594,  0.47590625, -0.46663907, -0.4803236 , -0.4875378 ,\n",
              "         0.5141858 ,  0.45334005, -0.4620763 , -0.49220845, -0.49041992,\n",
              "        -0.47931498,  0.5139525 ,  0.5413228 ,  0.49805284, -0.53147936,\n",
              "        -0.52808243,  0.4966625 ,  0.45605803, -0.4831394 , -0.484804  ,\n",
              "        -0.52291656,  0.47521457,  0.48674908,  0.53232783, -0.4497379 ,\n",
              "        -0.5326961 ,  0.48248485,  0.5126213 ,  0.48697248,  0.48408145,\n",
              "        -0.46297747,  0.5313685 , -0.5072136 , -0.5171162 , -0.4433849 ,\n",
              "        -0.47602975,  0.4671788 , -0.47967422,  0.49853605, -0.48844737,\n",
              "         0.4535032 , -0.47855154, -0.46399173, -0.4649307 ,  0.45715448,\n",
              "         0.498446  ,  0.4729249 ,  0.486193  , -0.5055664 ,  0.51877373],\n",
              "       dtype=float32),\n",
              " 'number': array([ 0.45791626,  0.50728494, -0.44426206, -0.48425758, -0.47905222,\n",
              "         0.46708056,  0.44015315, -0.52184206, -0.45784983, -0.48716897,\n",
              "        -0.49034172,  0.45188397,  0.48370418,  0.46804193, -0.5607137 ,\n",
              "        -0.45166826,  0.47154772,  0.46451116, -0.4677503 , -0.47832566,\n",
              "        -0.5066416 ,  0.5147648 ,  0.48771745,  0.494626  , -0.43549225,\n",
              "        -0.481469  ,  0.45998475,  0.53454226,  0.50723433,  0.48649192,\n",
              "        -0.519664  ,  0.44262972, -0.4874433 , -0.500015  , -0.5109468 ,\n",
              "        -0.49599013,  0.49909595, -0.49564204,  0.5100695 , -0.5164192 ,\n",
              "         0.48668966, -0.5283278 , -0.46598625, -0.49313283,  0.46825737,\n",
              "         0.45199043,  0.5217405 ,  0.45138416, -0.5019531 ,  0.4778654 ],\n",
              "       dtype=float32),\n",
              " 'short': array([ 0.47050327,  0.45759124, -0.45319653, -0.48460543, -0.46499908,\n",
              "         0.48212355,  0.52523994, -0.46308526, -0.48855376, -0.44776604,\n",
              "        -0.46193126,  0.51121116,  0.49744493,  0.49735188, -0.5160971 ,\n",
              "        -0.44556794,  0.5326509 ,  0.46891314, -0.4670163 , -0.43768153,\n",
              "        -0.46770033,  0.47131303,  0.5341449 ,  0.518147  , -0.49243674,\n",
              "        -0.49663648,  0.46348408,  0.52663445,  0.52974534,  0.46603316,\n",
              "        -0.54251504,  0.5153706 , -0.4731627 , -0.47054067, -0.45421427,\n",
              "        -0.45378575,  0.48951873, -0.4794015 ,  0.47832298, -0.47051296,\n",
              "         0.53867424, -0.52551055, -0.5175985 , -0.5071725 ,  0.501564  ,\n",
              "         0.4788553 ,  0.44607803,  0.51353097, -0.46003005,  0.47948518],\n",
              "       dtype=float32),\n",
              " 'research': array([ 0.47782648,  0.49355915, -0.48589194, -0.47174224, -0.5345577 ,\n",
              "         0.53104395,  0.5043948 , -0.45574483, -0.4863468 , -0.48895642,\n",
              "        -0.4354394 ,  0.46212   ,  0.45941985,  0.48499176, -0.49595752,\n",
              "        -0.49130172,  0.458578  ,  0.4486927 , -0.49552628, -0.44235235,\n",
              "        -0.5077349 ,  0.48796517,  0.50453347,  0.4968534 , -0.47317386,\n",
              "        -0.5243169 ,  0.49966192,  0.4995553 ,  0.50847745,  0.46543205,\n",
              "        -0.510179  ,  0.5120499 , -0.46663043, -0.46441627, -0.5267348 ,\n",
              "        -0.46260345,  0.4542444 , -0.5119952 ,  0.4645951 , -0.5164498 ,\n",
              "         0.52757484, -0.4632719 , -0.43237042, -0.52684116,  0.5298078 ,\n",
              "         0.4591657 ,  0.50973785,  0.4469844 , -0.47601557,  0.5230025 ],\n",
              "       dtype=float32),\n",
              " 'load': array([ 0.53622586,  0.4803906 , -0.4604237 , -0.49793434, -0.53636426,\n",
              "         0.44887048,  0.49394497, -0.50267035, -0.4379796 , -0.45490453,\n",
              "        -0.5084385 ,  0.50251025,  0.43440792,  0.49343365, -0.56024826,\n",
              "        -0.46757162,  0.5231242 ,  0.4608278 , -0.5036208 , -0.46619502,\n",
              "        -0.44852075,  0.4740606 ,  0.44969442,  0.509771  , -0.4953229 ,\n",
              "        -0.45387757,  0.4751647 ,  0.51403004,  0.4608149 ,  0.46610898,\n",
              "        -0.46059054,  0.44823393, -0.5511373 , -0.44862005, -0.4432573 ,\n",
              "        -0.48742104,  0.4538307 , -0.52704185,  0.4849278 , -0.48392567,\n",
              "         0.49849015, -0.4544778 , -0.45013377, -0.46652392,  0.4444962 ,\n",
              "         0.5236977 ,  0.5125195 ,  0.49748263, -0.4497782 ,  0.52096546],\n",
              "       dtype=float32),\n",
              " 'fixed': array([ 0.4756425 ,  0.4920654 , -0.503561  , -0.4308717 , -0.4797204 ,\n",
              "         0.45436853,  0.50285596, -0.5182696 , -0.46606827, -0.4732571 ,\n",
              "        -0.47483572,  0.4757508 ,  0.45065656,  0.5051037 , -0.5617392 ,\n",
              "        -0.44408622,  0.4790632 ,  0.44280377, -0.43672323, -0.48504254,\n",
              "        -0.49382758,  0.44150114,  0.5077682 ,  0.4986854 , -0.4268725 ,\n",
              "        -0.50587094,  0.42779252,  0.51461214,  0.5004812 ,  0.5074816 ,\n",
              "        -0.49840635,  0.48324567, -0.5145431 , -0.45745113, -0.48113143,\n",
              "        -0.5032075 ,  0.5303112 , -0.43645504,  0.44182393, -0.4838672 ,\n",
              "         0.46162313, -0.5117472 , -0.49735615, -0.44680765,  0.49747685,\n",
              "         0.4429341 ,  0.50059676,  0.45412368, -0.48884463,  0.48126316],\n",
              "       dtype=float32),\n",
              " 'russell': array([ 0.49592957,  0.49175572, -0.5020775 , -0.46060452, -0.5244778 ,\n",
              "         0.48319474,  0.46196195, -0.50148445, -0.48693138, -0.48299745,\n",
              "        -0.44857034,  0.4405803 ,  0.5061072 ,  0.51087815, -0.53660536,\n",
              "        -0.519127  ,  0.4898596 ,  0.51590836, -0.48800075, -0.48951218,\n",
              "        -0.49445206,  0.5145346 ,  0.48536924,  0.4777794 , -0.4495437 ,\n",
              "        -0.47779766,  0.44231704,  0.5559147 ,  0.47165108,  0.4995255 ,\n",
              "        -0.53860074,  0.47704744, -0.46759486, -0.48888043, -0.44647688,\n",
              "        -0.47209257,  0.5113487 , -0.4721803 ,  0.5033155 , -0.46446782,\n",
              "         0.4642601 , -0.4893875 , -0.5049295 , -0.49099547,  0.4792561 ,\n",
              "         0.48954952,  0.501171  ,  0.5101379 , -0.48676857,  0.48715267],\n",
              "       dtype=float32),\n",
              " 'sells': array([ 0.4752401 ,  0.4958047 , -0.4988615 , -0.4750422 , -0.5037751 ,\n",
              "         0.5238267 ,  0.48986602, -0.46764734, -0.4700908 , -0.44541037,\n",
              "        -0.48131847,  0.4743788 ,  0.5044345 ,  0.5206705 , -0.51984155,\n",
              "        -0.45160344,  0.5252731 ,  0.52230024, -0.49364167, -0.5108603 ,\n",
              "        -0.50387865,  0.4398772 ,  0.5245161 ,  0.4863742 , -0.44137287,\n",
              "        -0.46093336,  0.4502625 ,  0.46624026,  0.4626506 ,  0.52254283,\n",
              "        -0.54539084,  0.51218194, -0.48335168, -0.44914207, -0.52261245,\n",
              "        -0.46138254,  0.48556566, -0.5229016 ,  0.46864885, -0.45375973,\n",
              "         0.50468653, -0.53505963, -0.45789227, -0.48528102,  0.4743117 ,\n",
              "         0.5047843 ,  0.53320163,  0.5244856 , -0.50007266,  0.47705472],\n",
              "       dtype=float32),\n",
              " 'leverage': array([ 0.49202782,  0.4811841 , -0.504856  , -0.4895495 , -0.48902348,\n",
              "         0.47643727,  0.4967609 , -0.508616  , -0.4512334 , -0.51289004,\n",
              "        -0.51337206,  0.445467  ,  0.47723883,  0.50550276, -0.51903594,\n",
              "        -0.51949185,  0.4509404 ,  0.450812  , -0.4377377 , -0.47402602,\n",
              "        -0.5127386 ,  0.510566  ,  0.46514577,  0.4529958 , -0.43652874,\n",
              "        -0.44126952,  0.52178776,  0.45879373,  0.44667712,  0.4996255 ,\n",
              "        -0.45038375,  0.4746278 , -0.52914137, -0.47804648, -0.478046  ,\n",
              "        -0.47925526,  0.51641315, -0.52199864,  0.5056418 , -0.5237448 ,\n",
              "         0.4418682 , -0.48089615, -0.4420122 , -0.5074378 ,  0.5282609 ,\n",
              "         0.4847183 ,  0.4972102 ,  0.5234673 , -0.48567975,  0.50762033],\n",
              "       dtype=float32),\n",
              " 'without': array([ 0.48371404,  0.5023759 , -0.44830602, -0.46727583, -0.50678325,\n",
              "         0.536189  ,  0.4688293 , -0.4735288 , -0.48603833, -0.4774942 ,\n",
              "        -0.48405612,  0.46755496,  0.5001475 ,  0.5266646 , -0.5420313 ,\n",
              "        -0.4936015 ,  0.48453963,  0.4784975 , -0.47239196, -0.43937957,\n",
              "        -0.49397862,  0.459     ,  0.4907601 ,  0.47005647, -0.45955682,\n",
              "        -0.5052104 ,  0.5265462 ,  0.5472661 ,  0.45335016,  0.4407948 ,\n",
              "        -0.49069875,  0.4388258 , -0.52078104, -0.47081342, -0.5039562 ,\n",
              "        -0.49844933,  0.5263866 , -0.4792611 ,  0.47713584, -0.4635765 ,\n",
              "         0.49646994, -0.47842726, -0.4628776 , -0.45826566,  0.47419512,\n",
              "         0.4477125 ,  0.4652702 ,  0.496795  , -0.45595428,  0.5207181 ],\n",
              "       dtype=float32),\n",
              " 'k': array([ 0.45841148,  0.5083044 , -0.5185077 , -0.4351828 , -0.50812924,\n",
              "         0.44568583,  0.5222211 , -0.46718147, -0.5057153 , -0.47112963,\n",
              "        -0.51471853,  0.45290694,  0.44939953,  0.44932595, -0.55233073,\n",
              "        -0.49519768,  0.52050143,  0.44739527, -0.45340547, -0.46558082,\n",
              "        -0.518413  ,  0.5212626 ,  0.45073515,  0.4933994 , -0.5200207 ,\n",
              "        -0.46464044,  0.50755656,  0.46256363,  0.51357603,  0.48226994,\n",
              "        -0.49004075,  0.44419116, -0.517864  , -0.45826295, -0.43947062,\n",
              "        -0.46463814,  0.46623316, -0.50825286,  0.46468472, -0.505065  ,\n",
              "         0.5113861 , -0.5207649 , -0.43369862, -0.5194477 ,  0.46425074,\n",
              "         0.48531353,  0.4995992 ,  0.4396184 , -0.45880467,  0.4348281 ],\n",
              "       dtype=float32),\n",
              " 'msci': array([ 0.47569048,  0.45470247, -0.4681191 , -0.43478376, -0.51930803,\n",
              "         0.44121373,  0.50133216, -0.48653093, -0.5074675 , -0.50934017,\n",
              "        -0.4574874 ,  0.48106796,  0.5046237 ,  0.46203598, -0.51148754,\n",
              "        -0.45691037,  0.51328593,  0.46429724, -0.4248801 , -0.48627013,\n",
              "        -0.51369   ,  0.47127914,  0.45086542,  0.45438817, -0.49319965,\n",
              "        -0.502095  ,  0.4250928 ,  0.5289822 ,  0.49699077,  0.4694983 ,\n",
              "        -0.52427113,  0.48467815, -0.53185534, -0.45206892, -0.4908839 ,\n",
              "        -0.4978137 ,  0.5218353 , -0.5050802 ,  0.4484163 , -0.43983358,\n",
              "         0.5158384 , -0.48693016, -0.43590757, -0.49123538,  0.5050289 ,\n",
              "         0.48335496,  0.44061023,  0.43223885, -0.42573032,  0.43616018],\n",
              "       dtype=float32),\n",
              " 'summary': array([ 0.4963039 ,  0.49059618, -0.46689484, -0.5092983 , -0.51058096,\n",
              "         0.49599537,  0.49190524, -0.5208921 , -0.5372658 , -0.48190796,\n",
              "        -0.45432627,  0.474288  ,  0.49186128,  0.53751487, -0.486279  ,\n",
              "        -0.51188207,  0.53019637,  0.52089566, -0.44498172, -0.4775509 ,\n",
              "        -0.47506672,  0.49984047,  0.49740022,  0.4756921 , -0.5177349 ,\n",
              "        -0.49099675,  0.478958  ,  0.5492018 ,  0.52619135,  0.45583478,\n",
              "        -0.5095864 ,  0.47966945, -0.46664667, -0.511215  , -0.5234753 ,\n",
              "        -0.45688814,  0.54206556, -0.49287435,  0.48082352, -0.51558155,\n",
              "         0.46299672, -0.527367  , -0.45170027, -0.5055001 ,  0.5301045 ,\n",
              "         0.47896585,  0.4619177 ,  0.53191936, -0.50385183,  0.4505656 ],\n",
              "       dtype=float32),\n",
              " 'fixed-income': array([ 0.49012306,  0.4467334 , -0.47769952, -0.5108701 , -0.48903397,\n",
              "         0.49760753,  0.4548813 , -0.47625864, -0.517112  , -0.43191695,\n",
              "        -0.46872663,  0.4840321 ,  0.49109757,  0.4562767 , -0.5108309 ,\n",
              "        -0.51775426,  0.4534899 ,  0.5009953 , -0.4710476 , -0.49909398,\n",
              "        -0.5213731 ,  0.4968335 ,  0.47671044,  0.43413273, -0.5086467 ,\n",
              "        -0.45227346,  0.47993258,  0.47410908,  0.49115893,  0.43818364,\n",
              "        -0.44709986,  0.46756357, -0.54201126, -0.5183824 , -0.43919078,\n",
              "        -0.5111115 ,  0.46525517, -0.49029323,  0.45001665, -0.49536937,\n",
              "         0.45707804, -0.49765623, -0.42782053, -0.44542736,  0.49382946,\n",
              "         0.45121315,  0.45418808,  0.5123852 , -0.4876156 ,  0.4515899 ],\n",
              "       dtype=float32),\n",
              " 'trade': array([ 0.54384434,  0.44626722, -0.5204929 , -0.4469052 , -0.52323496,\n",
              "         0.46887675,  0.5108175 , -0.51905185, -0.4697837 , -0.49568003,\n",
              "        -0.43836805,  0.5224821 ,  0.4396037 ,  0.4877547 , -0.5704614 ,\n",
              "        -0.50660294,  0.46660432,  0.53620464, -0.47961825, -0.47903427,\n",
              "        -0.44017282,  0.49807614,  0.45920703,  0.44475013, -0.4739977 ,\n",
              "        -0.47356027,  0.5249755 ,  0.5081551 ,  0.45800012,  0.5166029 ,\n",
              "        -0.4792431 ,  0.480111  , -0.47615057, -0.5190478 , -0.44147563,\n",
              "        -0.49965537,  0.45452648, -0.4973904 ,  0.46708274, -0.46781757,\n",
              "         0.52467495, -0.47172388, -0.46940443, -0.53003305,  0.5342939 ,\n",
              "         0.5279812 ,  0.46993217,  0.45518002, -0.48612982,  0.49154285],\n",
              "       dtype=float32),\n",
              " 'common': array([ 0.46254492,  0.49286908, -0.47096294, -0.49712127, -0.5174617 ,\n",
              "         0.54073346,  0.52810025, -0.5476409 , -0.5367909 , -0.44057167,\n",
              "        -0.4574833 ,  0.44574636,  0.49508414,  0.47709647, -0.55143964,\n",
              "        -0.44451255,  0.5033002 ,  0.46709985, -0.48443907, -0.5002318 ,\n",
              "        -0.47569427,  0.51435196,  0.5290712 ,  0.4393798 , -0.44355142,\n",
              "        -0.4496637 ,  0.48908463,  0.48601496,  0.46193358,  0.48360047,\n",
              "        -0.50890017,  0.44941065, -0.45927343, -0.48316178, -0.53172314,\n",
              "        -0.54566455,  0.5005566 , -0.46328247,  0.5147263 , -0.47183448,\n",
              "         0.49820474, -0.45945808, -0.48537058, -0.5152213 ,  0.5111638 ,\n",
              "         0.47182602,  0.4785237 ,  0.49023345, -0.46375555,  0.5115581 ],\n",
              "       dtype=float32),\n",
              " 'overall': array([ 0.5327031 ,  0.43230408, -0.5208734 , -0.45845112, -0.50073105,\n",
              "         0.45560446,  0.44439158, -0.4528948 , -0.48350924, -0.4610439 ,\n",
              "        -0.47342682,  0.5081962 ,  0.45831636,  0.4638446 , -0.55575186,\n",
              "        -0.5050922 ,  0.49618292,  0.47118896, -0.48332542, -0.45005164,\n",
              "        -0.4467152 ,  0.48476568,  0.4581045 ,  0.4608322 , -0.47351682,\n",
              "        -0.512877  ,  0.5059446 ,  0.5050192 ,  0.5075707 ,  0.5165424 ,\n",
              "        -0.46961412,  0.53046244, -0.51903194, -0.5049239 , -0.43346784,\n",
              "        -0.444904  ,  0.52774626, -0.49584198,  0.48327696, -0.49172944,\n",
              "         0.5095544 , -0.526415  , -0.50422645, -0.47732708,  0.51215893,\n",
              "         0.5122534 ,  0.503905  ,  0.50451183, -0.47760084,  0.51341176],\n",
              "       dtype=float32),\n",
              " 'relevant': array([ 0.46883518,  0.4932289 , -0.4570463 , -0.4504366 , -0.49481508,\n",
              "         0.47572538,  0.46482563, -0.4895213 , -0.50283587, -0.48587587,\n",
              "        -0.5184194 ,  0.5373224 ,  0.45683834,  0.48224115, -0.5511964 ,\n",
              "        -0.53091794,  0.4612416 ,  0.45381808, -0.48102635, -0.44493568,\n",
              "        -0.46740216,  0.47463533,  0.5317204 ,  0.45740062, -0.50407314,\n",
              "        -0.46762928,  0.4772191 ,  0.5101549 ,  0.52268076,  0.5136882 ,\n",
              "        -0.53284967,  0.44674808, -0.48902422, -0.4794014 , -0.44842663,\n",
              "        -0.4822068 ,  0.5216733 , -0.48048452,  0.46912447, -0.47768342,\n",
              "         0.49332488, -0.47594905, -0.51333123, -0.5108799 ,  0.4572584 ,\n",
              "         0.46020755,  0.49657995,  0.46687996, -0.49296755,  0.49847916],\n",
              "       dtype=float32),\n",
              " 'create': array([ 0.51468277,  0.42988294, -0.49173105, -0.47357032, -0.5148815 ,\n",
              "         0.5127673 ,  0.48650366, -0.5077154 , -0.46355665, -0.47111258,\n",
              "        -0.44377857,  0.49426618,  0.49658537,  0.51245815, -0.5312022 ,\n",
              "        -0.4759728 ,  0.52869344,  0.4970233 , -0.48808876, -0.51480186,\n",
              "        -0.5121662 ,  0.50646067,  0.46179417,  0.45505548, -0.46756008,\n",
              "        -0.5069728 ,  0.46288016,  0.4726605 ,  0.44153675,  0.4860838 ,\n",
              "        -0.5234385 ,  0.48700708, -0.46937212, -0.5255437 , -0.44734544,\n",
              "        -0.5144765 ,  0.5163075 , -0.45748365,  0.51216215, -0.48245668,\n",
              "         0.45436928, -0.48538774, -0.43547422, -0.44283125,  0.49561733,\n",
              "         0.5168549 ,  0.45174682,  0.47139698, -0.49202326,  0.50095886],\n",
              "       dtype=float32),\n",
              " 'estate': array([ 0.50935495,  0.44142404, -0.45762178, -0.48361126, -0.5163787 ,\n",
              "         0.46823657,  0.4695711 , -0.47998577, -0.50358045, -0.48467135,\n",
              "        -0.49411094,  0.51111937,  0.48425528,  0.4785879 , -0.5193434 ,\n",
              "        -0.49684882,  0.4344576 ,  0.43521726, -0.49103126, -0.50612783,\n",
              "        -0.4849672 ,  0.49280113,  0.47927096,  0.42311168, -0.4613695 ,\n",
              "        -0.42512265,  0.49456424,  0.4864293 ,  0.48639598,  0.46145672,\n",
              "        -0.51363385,  0.5100401 , -0.52355516, -0.49850172, -0.4990706 ,\n",
              "        -0.46505263,  0.48640183, -0.51559174,  0.5045841 , -0.44560438,\n",
              "         0.46049336, -0.51545495, -0.5049819 , -0.49766916,  0.50996643,\n",
              "         0.51496315,  0.44519383,  0.49404123, -0.49884242,  0.4700965 ],\n",
              "       dtype=float32),\n",
              " 'fluctuations': array([ 0.510059  ,  0.5048349 , -0.46803948, -0.5028473 , -0.49751797,\n",
              "         0.5082583 ,  0.4996582 , -0.51626563, -0.48648965, -0.51145124,\n",
              "        -0.4819602 ,  0.444451  ,  0.45374054,  0.47215915, -0.52877706,\n",
              "        -0.48357776,  0.46538106,  0.44203705, -0.49197793, -0.43638554,\n",
              "        -0.45653597,  0.4747693 ,  0.52245027,  0.49284798, -0.44775712,\n",
              "        -0.4475881 ,  0.4355391 ,  0.54307884,  0.49113703,  0.50125605,\n",
              "        -0.5166979 ,  0.47716454, -0.5173598 , -0.5321772 , -0.45324498,\n",
              "        -0.47251007,  0.4493438 , -0.5216101 ,  0.5185813 , -0.48433185,\n",
              "         0.47750837, -0.4451101 , -0.51135343, -0.434269  ,  0.46328986,\n",
              "         0.5155166 ,  0.51820254,  0.51819324, -0.46620172,  0.45603922],\n",
              "       dtype=float32),\n",
              " 'imposed': array([ 0.47479212,  0.45994928, -0.4517609 , -0.5240146 , -0.454746  ,\n",
              "         0.49154544,  0.5125035 , -0.51937866, -0.50093997, -0.4274151 ,\n",
              "        -0.4623428 ,  0.46666288,  0.51769465,  0.52965   , -0.47299328,\n",
              "        -0.48782986,  0.4942124 ,  0.51753145, -0.51587594, -0.4766373 ,\n",
              "        -0.4882866 ,  0.48722225,  0.48862505,  0.50828195, -0.44852182,\n",
              "        -0.48235255,  0.45419672,  0.4735548 ,  0.49242944,  0.4491675 ,\n",
              "        -0.5133658 ,  0.46902472, -0.49854374, -0.51975155, -0.4987987 ,\n",
              "        -0.49839294,  0.47787088, -0.46233955,  0.44890463, -0.48958895,\n",
              "         0.4717857 , -0.48131353, -0.4618694 , -0.45913056,  0.48298633,\n",
              "         0.52141255,  0.50728595,  0.49387196, -0.44075277,  0.5187649 ],\n",
              "       dtype=float32),\n",
              " 'experience': array([ 0.4933529 ,  0.43820998, -0.4907785 , -0.43395883, -0.52535045,\n",
              "         0.5097375 ,  0.44677886, -0.5203104 , -0.517713  , -0.45263398,\n",
              "        -0.5012792 ,  0.47225565,  0.4961908 ,  0.4995172 , -0.5446201 ,\n",
              "        -0.50311786,  0.51690185,  0.46398905, -0.50373375, -0.47938883,\n",
              "        -0.44831035,  0.43731186,  0.5028277 ,  0.48507515, -0.52895576,\n",
              "        -0.4759505 ,  0.45020148,  0.5071013 ,  0.4988352 ,  0.50428545,\n",
              "        -0.54512507,  0.49577266, -0.51190335, -0.50021595, -0.4724258 ,\n",
              "        -0.4466459 ,  0.52414924, -0.4990081 ,  0.49137   , -0.44550526,\n",
              "         0.53019965, -0.5020959 , -0.48233077, -0.49084902,  0.49299407,\n",
              "         0.48243785,  0.47320625,  0.5048638 , -0.47030687,  0.43505296],\n",
              "       dtype=float32),\n",
              " 'broker-dealers': array([ 0.5046142 ,  0.47885346, -0.45497093, -0.49244508, -0.53661615,\n",
              "         0.44736758,  0.52491754, -0.46745858, -0.45005286, -0.46421096,\n",
              "        -0.51092875,  0.48919117,  0.47254708,  0.4428659 , -0.55276614,\n",
              "        -0.45440194,  0.4444815 ,  0.51014596, -0.5034365 , -0.48364675,\n",
              "        -0.5102403 ,  0.5124093 ,  0.5172054 ,  0.5115179 , -0.5061161 ,\n",
              "        -0.49075443,  0.43470743,  0.51919794,  0.50121725,  0.5001936 ,\n",
              "        -0.4742236 ,  0.5162927 , -0.5127647 , -0.5078483 , -0.48561385,\n",
              "        -0.53342867,  0.52487093, -0.44473863,  0.52072424, -0.48408118,\n",
              "         0.5240262 , -0.46918967, -0.48762837, -0.4477334 ,  0.4775643 ,\n",
              "         0.46906957,  0.48163995,  0.43510956, -0.50249666,  0.5076901 ],\n",
              "       dtype=float32),\n",
              " 'iico': array([ 0.5285304 ,  0.46627745, -0.48455325, -0.50460786, -0.46393162,\n",
              "         0.47368708,  0.47962433, -0.47077265, -0.5279198 , -0.5002117 ,\n",
              "        -0.49800348,  0.46126598,  0.46379918,  0.45772448, -0.49112022,\n",
              "        -0.4363832 ,  0.47245815,  0.4957797 , -0.539394  , -0.52622956,\n",
              "        -0.41933513,  0.51768607,  0.50446963,  0.48575056, -0.5205138 ,\n",
              "        -0.5068661 ,  0.51331556,  0.50525516,  0.480812  ,  0.42512   ,\n",
              "        -0.50976104,  0.49645746, -0.5467572 , -0.5066501 , -0.4671478 ,\n",
              "        -0.4969887 ,  0.5094677 , -0.47319803,  0.5315339 , -0.5292523 ,\n",
              "         0.528131  , -0.44423342, -0.5081267 , -0.5031878 ,  0.42488652,\n",
              "         0.49368548,  0.49305952,  0.4865216 , -0.47417724,  0.52728903],\n",
              "       dtype=float32),\n",
              " 'significantly': array([ 0.47431573,  0.47514954, -0.477111  , -0.50560844, -0.4599937 ,\n",
              "         0.5282972 ,  0.51340836, -0.46251485, -0.52266693, -0.4966104 ,\n",
              "        -0.5186328 ,  0.51969856,  0.46374655,  0.46709195, -0.5144212 ,\n",
              "        -0.45429993,  0.50404555,  0.523428  , -0.48054796, -0.44621393,\n",
              "        -0.47172326,  0.48978475,  0.5161929 ,  0.47319424, -0.48033768,\n",
              "        -0.50521374,  0.45750928,  0.4797875 ,  0.43225518,  0.44986752,\n",
              "        -0.47853628,  0.5050879 , -0.51515585, -0.47066337, -0.51264954,\n",
              "        -0.52070004,  0.5107757 , -0.5146782 ,  0.44369665, -0.5257735 ,\n",
              "         0.49073243, -0.47653133, -0.4682246 , -0.47065067,  0.51809543,\n",
              "         0.4924997 ,  0.5132544 ,  0.4839408 , -0.4854102 ,  0.4797035 ],\n",
              "       dtype=float32),\n",
              " 'meet': array([ 0.5144973 ,  0.5105557 , -0.50029397, -0.49551967, -0.50318664,\n",
              "         0.49328986,  0.5046976 , -0.50531256, -0.51432276, -0.46314344,\n",
              "        -0.46900672,  0.47127086,  0.44245267,  0.5058051 , -0.49088305,\n",
              "        -0.5174026 ,  0.4708547 ,  0.4412193 , -0.4649028 , -0.47994283,\n",
              "        -0.4864634 ,  0.44037485,  0.47452724,  0.4827876 , -0.4952328 ,\n",
              "        -0.47180927,  0.50876045,  0.49790925,  0.4605215 ,  0.48577338,\n",
              "        -0.4841341 ,  0.4539096 , -0.46564534, -0.5245571 , -0.47025695,\n",
              "        -0.46287006,  0.48022908, -0.52582   ,  0.48975262, -0.44034868,\n",
              "         0.5250499 , -0.5157059 , -0.44695008, -0.43367863,  0.45704997,\n",
              "         0.46377066,  0.5084361 ,  0.514631  , -0.47028664,  0.45190233],\n",
              "       dtype=float32),\n",
              " 'instrument': array([ 0.50921106,  0.51445997, -0.4788053 , -0.515412  , -0.49192235,\n",
              "         0.5074149 ,  0.51298547, -0.45840973, -0.43386897, -0.4348306 ,\n",
              "        -0.49583656,  0.5139691 ,  0.42329615,  0.48747492, -0.5608298 ,\n",
              "        -0.4738406 ,  0.5008714 ,  0.5041569 , -0.5019912 , -0.4558862 ,\n",
              "        -0.515374  ,  0.45481366,  0.45258343,  0.42966115, -0.4424653 ,\n",
              "        -0.44415426,  0.482243  ,  0.47120965,  0.5123435 ,  0.460946  ,\n",
              "        -0.47172424,  0.5146621 , -0.4641379 , -0.45107278, -0.45348978,\n",
              "        -0.49772424,  0.5020028 , -0.45220938,  0.487633  , -0.49493277,\n",
              "         0.46587223, -0.45303726, -0.44303676, -0.50497925,  0.5282489 ,\n",
              "         0.468673  ,  0.4737021 ,  0.4350901 , -0.47086143,  0.52239287],\n",
              "       dtype=float32),\n",
              " 'whether': array([ 0.45185772,  0.46369594, -0.43946934, -0.4562548 , -0.5078953 ,\n",
              "         0.4913684 ,  0.48687243, -0.5225235 , -0.5219668 , -0.5063305 ,\n",
              "        -0.44399518,  0.44244787,  0.5140079 ,  0.5204945 , -0.5483375 ,\n",
              "        -0.48860633,  0.5277569 ,  0.44358683, -0.51449615, -0.4979977 ,\n",
              "        -0.5228196 ,  0.5063746 ,  0.4639213 ,  0.4928519 , -0.5195258 ,\n",
              "        -0.50187975,  0.5042936 ,  0.5050861 ,  0.4527081 ,  0.46247926,\n",
              "        -0.46192762,  0.47604543, -0.49917522, -0.49343076, -0.47293025,\n",
              "        -0.52171755,  0.50083506, -0.5247092 ,  0.4588403 , -0.4889259 ,\n",
              "         0.51716983, -0.5002603 , -0.5109643 , -0.4846114 ,  0.4610989 ,\n",
              "         0.47515035,  0.4805526 ,  0.45677134, -0.51207757,  0.44821543],\n",
              "       dtype=float32),\n",
              " 'buys': array([ 0.48224258,  0.47161585, -0.52573556, -0.4822104 , -0.48273507,\n",
              "         0.45947695,  0.46255502, -0.45701727, -0.5069331 , -0.47468504,\n",
              "        -0.51521707,  0.47676444,  0.48381543,  0.46388397, -0.56662977,\n",
              "        -0.43885735,  0.4874269 ,  0.52272546, -0.49167037, -0.5088104 ,\n",
              "        -0.4867974 ,  0.50537103,  0.462987  ,  0.46047962, -0.48141098,\n",
              "        -0.52801263,  0.4774987 ,  0.521594  ,  0.49156955,  0.47747904,\n",
              "        -0.46929714,  0.52169967, -0.5095101 , -0.49584615, -0.43700302,\n",
              "        -0.5051208 ,  0.49580258, -0.49023157,  0.44705787, -0.5100766 ,\n",
              "         0.48956302, -0.46174723, -0.4384977 , -0.45422727,  0.46037504,\n",
              "         0.49942505,  0.5286121 ,  0.528862  , -0.52758896,  0.51487416],\n",
              "       dtype=float32),\n",
              " 'section': array([ 0.5354609 ,  0.4511033 , -0.47175542, -0.47900352, -0.49042815,\n",
              "         0.49124792,  0.4372959 , -0.5036559 , -0.4870527 , -0.43359724,\n",
              "        -0.5230115 ,  0.50336397,  0.47325766,  0.48191836, -0.51207083,\n",
              "        -0.4694841 ,  0.4811821 ,  0.4850406 , -0.5013448 , -0.50224936,\n",
              "        -0.44748193,  0.47651726,  0.47194958,  0.43442085, -0.51428026,\n",
              "        -0.5007285 ,  0.5139858 ,  0.5497383 ,  0.49036443,  0.4640403 ,\n",
              "        -0.4805873 ,  0.44656357, -0.53771895, -0.53884184, -0.43840602,\n",
              "        -0.4477746 ,  0.524394  , -0.53256017,  0.4842857 , -0.51731473,\n",
              "         0.48103276, -0.44983384, -0.48560485, -0.5288212 ,  0.4623301 ,\n",
              "         0.53531915,  0.52455175,  0.48769122, -0.47429577,  0.51328903],\n",
              "       dtype=float32),\n",
              " 'world': array([ 0.5294653 ,  0.447373  , -0.46407682, -0.49999964, -0.47594145,\n",
              "         0.50687516,  0.49274015, -0.48989433, -0.42666948, -0.51105183,\n",
              "        -0.4765972 ,  0.5155511 ,  0.5008893 ,  0.47586083, -0.4686885 ,\n",
              "        -0.45201737,  0.4440914 ,  0.46800378, -0.4893495 , -0.4321162 ,\n",
              "        -0.44355172,  0.4623738 ,  0.51367724,  0.47458473, -0.46713677,\n",
              "        -0.51959574,  0.44554538,  0.4757431 ,  0.5150626 ,  0.4918889 ,\n",
              "        -0.4462101 ,  0.5012261 , -0.4580711 , -0.46988004, -0.48970583,\n",
              "        -0.4975914 ,  0.47544408, -0.4599953 ,  0.42854533, -0.5165853 ,\n",
              "         0.5196183 , -0.43961367, -0.44435912, -0.4437272 ,  0.471877  ,\n",
              "         0.48580366,  0.51081395,  0.470063  , -0.508812  ,  0.45856848],\n",
              "       dtype=float32),\n",
              " 'large': array([ 0.47923517,  0.49124995, -0.5114166 , -0.49185964, -0.44446838,\n",
              "         0.48699513,  0.44608957, -0.51827717, -0.5172258 , -0.50113535,\n",
              "        -0.46240535,  0.49886426,  0.47647655,  0.49614277, -0.56404495,\n",
              "        -0.46316585,  0.491539  ,  0.45207542, -0.4716052 , -0.47463042,\n",
              "        -0.51024306,  0.4807566 ,  0.46507448,  0.44734773, -0.44492698,\n",
              "        -0.4779112 ,  0.49335945,  0.52561474,  0.43646026,  0.48102653,\n",
              "        -0.5223821 ,  0.44650963, -0.46261132, -0.47754198, -0.4668965 ,\n",
              "        -0.4744919 ,  0.44716763, -0.5231841 ,  0.4814254 , -0.47519502,\n",
              "         0.47199878, -0.5066381 , -0.47148693, -0.46537864,  0.50352967,\n",
              "         0.45093858,  0.50557506,  0.44763315, -0.48962998,  0.49424973],\n",
              "       dtype=float32),\n",
              " 'indication': array([ 0.5276783 ,  0.5054373 , -0.4469537 , -0.5216626 , -0.5432155 ,\n",
              "         0.47432482,  0.48683363, -0.5175809 , -0.5224876 , -0.4781048 ,\n",
              "        -0.46073446,  0.46669197,  0.5245087 ,  0.47622493, -0.57370913,\n",
              "        -0.48948762,  0.4728559 ,  0.460553  , -0.52702945, -0.44148514,\n",
              "        -0.47079483,  0.5231011 ,  0.5243157 ,  0.44528404, -0.4947816 ,\n",
              "        -0.52701867,  0.4864906 ,  0.48776346,  0.5026774 ,  0.44957182,\n",
              "        -0.4884582 ,  0.45638776, -0.50102687, -0.53453636, -0.46191925,\n",
              "        -0.4567994 ,  0.48035428, -0.5348375 ,  0.51748776, -0.4658923 ,\n",
              "         0.51319873, -0.5105356 , -0.5098757 , -0.4668138 ,  0.4791544 ,\n",
              "         0.4720199 ,  0.46162042,  0.49514863, -0.43914545,  0.46755266],\n",
              "       dtype=float32),\n",
              " 'well': array([ 0.49726814,  0.51213706, -0.50062907, -0.47661996, -0.5053202 ,\n",
              "         0.45743456,  0.4537793 , -0.45226038, -0.4479885 , -0.4522562 ,\n",
              "        -0.5112775 ,  0.4816688 ,  0.4544628 ,  0.5278969 , -0.52037156,\n",
              "        -0.509755  ,  0.5266148 ,  0.47599977, -0.4744484 , -0.5147724 ,\n",
              "        -0.46042347,  0.45601323,  0.46166632,  0.47213876, -0.49407887,\n",
              "        -0.486659  ,  0.5225599 ,  0.48541737,  0.484128  ,  0.48847535,\n",
              "        -0.5443599 ,  0.4900767 , -0.5055777 , -0.50823206, -0.45535603,\n",
              "        -0.50101185,  0.46730453, -0.49732092,  0.49343744, -0.49038142,\n",
              "         0.47960857, -0.48659778, -0.52200216, -0.45047265,  0.52674335,\n",
              "         0.51670706,  0.4472989 ,  0.5039433 , -0.47455117,  0.46339524],\n",
              "       dtype=float32),\n",
              " 'discounts': array([ 0.46034747,  0.43565693, -0.475396  , -0.5121484 , -0.513182  ,\n",
              "         0.4936662 ,  0.46474928, -0.5268213 , -0.48774952, -0.4959749 ,\n",
              "        -0.514987  ,  0.50122523,  0.43169004,  0.48744258, -0.5611043 ,\n",
              "        -0.50814027,  0.47021297,  0.47684532, -0.48592097, -0.4383551 ,\n",
              "        -0.5040459 ,  0.4821089 ,  0.52333736,  0.43567702, -0.5020127 ,\n",
              "        -0.5245173 ,  0.49988988,  0.51640725,  0.4997847 ,  0.5183128 ,\n",
              "        -0.50565743,  0.4063104 , -0.5258992 , -0.44333157, -0.46675763,\n",
              "        -0.4454391 ,  0.44547457, -0.4682886 ,  0.46299943, -0.51169986,\n",
              "         0.53326654, -0.51071554, -0.5141046 , -0.46925747,  0.4492415 ,\n",
              "         0.4740229 ,  0.4415919 ,  0.5202373 , -0.5017581 ,  0.46892655],\n",
              "       dtype=float32),\n",
              " 'seek': array([ 0.5262533 ,  0.5067544 , -0.45892286, -0.4596674 , -0.47033545,\n",
              "         0.50971675,  0.485375  , -0.4413651 , -0.48101723, -0.43292874,\n",
              "        -0.47915307,  0.52954245,  0.49435693,  0.44745666, -0.47865126,\n",
              "        -0.48842052,  0.5077936 ,  0.5182377 , -0.4709142 , -0.44450048,\n",
              "        -0.5062197 ,  0.5169239 ,  0.47181988,  0.4817981 , -0.4949224 ,\n",
              "        -0.5006701 ,  0.5149913 ,  0.5287339 ,  0.5237792 ,  0.43345022,\n",
              "        -0.4918812 ,  0.518871  , -0.54742527, -0.4870897 , -0.51003605,\n",
              "        -0.5243444 ,  0.45056093, -0.4467334 ,  0.45516855, -0.44212198,\n",
              "         0.48765802, -0.5182953 , -0.46792105, -0.44208133,  0.5182603 ,\n",
              "         0.5253718 ,  0.4808976 ,  0.46166548, -0.5238912 ,  0.469523  ],\n",
              "       dtype=float32),\n",
              " 'salesperson': array([ 0.5353911 ,  0.48673365, -0.46028426, -0.49601406, -0.4532981 ,\n",
              "         0.5189523 ,  0.45322427, -0.46747643, -0.48964337, -0.5135629 ,\n",
              "        -0.47114122,  0.45462796,  0.49398774,  0.4513848 , -0.47165865,\n",
              "        -0.46855366,  0.47812557,  0.4473004 , -0.45319316, -0.44436166,\n",
              "        -0.4504773 ,  0.4591447 ,  0.5353001 ,  0.45417255, -0.50576913,\n",
              "        -0.50744   ,  0.5060757 ,  0.51841277,  0.48573765,  0.5164362 ,\n",
              "        -0.5328729 ,  0.50395536, -0.50540596, -0.52207756, -0.43653792,\n",
              "        -0.46395418,  0.4994849 , -0.4950028 ,  0.47127396, -0.47463056,\n",
              "         0.45296842, -0.4460435 , -0.4298305 , -0.44231373,  0.50604254,\n",
              "         0.43937373,  0.5180636 ,  0.48138767, -0.51870453,  0.43174672],\n",
              "       dtype=float32),\n",
              " 'included': array([ 0.49967775,  0.44694227, -0.48820877, -0.47417256, -0.48474413,\n",
              "         0.49742934,  0.51319194, -0.50158685, -0.44840544, -0.48785007,\n",
              "        -0.4424589 ,  0.5333114 ,  0.51228374,  0.53874093, -0.50662833,\n",
              "        -0.4684015 ,  0.48469082,  0.48482218, -0.46055079, -0.45670652,\n",
              "        -0.5230429 ,  0.4803973 ,  0.48731697,  0.5281718 , -0.522849  ,\n",
              "        -0.5038097 ,  0.5066868 ,  0.4830672 ,  0.5001961 ,  0.5293713 ,\n",
              "        -0.46719617,  0.44737074, -0.4849383 , -0.46605176, -0.46169305,\n",
              "        -0.48565465,  0.5133458 , -0.5129088 ,  0.47399533, -0.5117112 ,\n",
              "         0.45977616, -0.5192904 , -0.51723677, -0.44506097,  0.5400704 ,\n",
              "         0.52682596,  0.44635734,  0.5088852 , -0.46804813,  0.4977277 ],\n",
              "       dtype=float32),\n",
              " 'dividend': array([ 0.48267102,  0.45985663, -0.4860025 , -0.53366625, -0.505821  ,\n",
              "         0.52890795,  0.5310312 , -0.49617934, -0.5255197 , -0.45237687,\n",
              "        -0.48219255,  0.46336782,  0.4897192 ,  0.47819367, -0.54877937,\n",
              "        -0.4584086 ,  0.46208534,  0.51765215, -0.43676865, -0.47218093,\n",
              "        -0.45829734,  0.52204335,  0.47137788,  0.48479083, -0.4894987 ,\n",
              "        -0.4580808 ,  0.45631212,  0.54888594,  0.4879906 ,  0.46403247,\n",
              "        -0.46302423,  0.5021397 , -0.51904607, -0.5164309 , -0.5037261 ,\n",
              "        -0.46319306,  0.4733702 , -0.4486261 ,  0.5024958 , -0.529245  ,\n",
              "         0.48752773, -0.44865566, -0.42842335, -0.5088226 ,  0.51282907,\n",
              "         0.45603278,  0.4982199 ,  0.44604814, -0.5173641 ,  0.5203786 ],\n",
              "       dtype=float32),\n",
              " 'policies': array([ 0.5428063 ,  0.4926503 , -0.46622854, -0.43897733, -0.49484137,\n",
              "         0.49645975,  0.5038367 , -0.44151455, -0.43268803, -0.4697393 ,\n",
              "        -0.5098685 ,  0.46388158,  0.44536904,  0.50549334, -0.4899842 ,\n",
              "        -0.5221456 ,  0.44571722,  0.46866226, -0.43338066, -0.45804724,\n",
              "        -0.4674583 ,  0.42891657,  0.4418246 ,  0.51021445, -0.5150154 ,\n",
              "        -0.44478303,  0.4817045 ,  0.5099698 ,  0.47845447,  0.47144696,\n",
              "        -0.4938774 ,  0.51553994, -0.53230894, -0.4617269 , -0.4836943 ,\n",
              "        -0.4946333 ,  0.46475667, -0.5279579 ,  0.5213761 , -0.44235015,\n",
              "         0.4747366 , -0.49732256, -0.4572469 , -0.47837648,  0.47865528,\n",
              "         0.4666605 ,  0.46778926,  0.51365674, -0.49151117,  0.50874   ],\n",
              "       dtype=float32),\n",
              " 'advisor': array([ 0.5443062 ,  0.48635846, -0.4969892 , -0.45942745, -0.4957422 ,\n",
              "         0.50849235,  0.45633668, -0.47604218, -0.45826232, -0.45788366,\n",
              "        -0.44809988,  0.46497005,  0.46975952,  0.4591692 , -0.50770134,\n",
              "        -0.48493275,  0.49722847,  0.5169075 , -0.4445608 , -0.5017883 ,\n",
              "        -0.5143344 ,  0.52163017,  0.5005722 ,  0.44304866, -0.436618  ,\n",
              "        -0.50030327,  0.48757958,  0.50191426,  0.46911263,  0.44159967,\n",
              "        -0.49758223,  0.49886063, -0.47682446, -0.51914793, -0.45859027,\n",
              "        -0.5217881 ,  0.47374764, -0.44305426,  0.44966045, -0.49006975,\n",
              "         0.46971604, -0.46958882, -0.42922264, -0.45897695,  0.44841385,\n",
              "         0.4937033 ,  0.49182755,  0.43881086, -0.46407238,  0.48922268],\n",
              "       dtype=float32),\n",
              " 'turns': array([ 0.5221717 ,  0.52800846, -0.51469845, -0.44789216, -0.50519115,\n",
              "         0.52164215,  0.46404475, -0.4987436 , -0.51885116, -0.46264127,\n",
              "        -0.47617543,  0.47562966,  0.54199845,  0.53281003, -0.5679036 ,\n",
              "        -0.5297793 ,  0.46132234,  0.5018093 , -0.5331334 , -0.48738652,\n",
              "        -0.46701884,  0.5149532 ,  0.4488696 ,  0.48657516, -0.4784088 ,\n",
              "        -0.48570925,  0.5283397 ,  0.5264335 ,  0.47248435,  0.47589707,\n",
              "        -0.49978876,  0.46222478, -0.48820737, -0.50812465, -0.47567454,\n",
              "        -0.4854231 ,  0.53234065, -0.52202237,  0.47160795, -0.49466312,\n",
              "         0.4686132 , -0.47105357, -0.43831104, -0.4738781 ,  0.46345833,\n",
              "         0.45152336,  0.47765717,  0.5182642 , -0.51455724,  0.5215131 ],\n",
              "       dtype=float32),\n",
              " 'international': array([ 0.4681738 ,  0.45019674, -0.510781  , -0.51823425, -0.5104012 ,\n",
              "         0.5252024 ,  0.49180576, -0.46582723, -0.441285  , -0.49265492,\n",
              "        -0.5039779 ,  0.4693935 ,  0.4672284 ,  0.53330123, -0.56877553,\n",
              "        -0.47471103,  0.5334658 ,  0.47313687, -0.4953223 , -0.44816953,\n",
              "        -0.49910596,  0.4484307 ,  0.45421445,  0.45097783, -0.4522966 ,\n",
              "        -0.4502635 ,  0.4908179 ,  0.48972544,  0.47744718,  0.50958616,\n",
              "        -0.52237093,  0.4800076 , -0.469825  , -0.48020342, -0.4747066 ,\n",
              "        -0.5109907 ,  0.5119193 , -0.5086502 ,  0.48457122, -0.46368134,\n",
              "         0.48681402, -0.4729769 , -0.4775021 , -0.45156577,  0.47497544,\n",
              "         0.5260769 ,  0.44586617,  0.47410992, -0.47651145,  0.5084407 ],\n",
              "       dtype=float32),\n",
              " 'dividends': array([ 0.5383315 ,  0.43930453, -0.48485467, -0.51856095, -0.49018404,\n",
              "         0.48495898,  0.47018752, -0.45159364, -0.5238793 , -0.47027373,\n",
              "        -0.48640424,  0.5164724 ,  0.5141185 ,  0.5139768 , -0.4939443 ,\n",
              "        -0.5077452 ,  0.5093871 ,  0.4787773 , -0.48080957, -0.45576447,\n",
              "        -0.52264047,  0.45272967,  0.50835526,  0.50132877, -0.50393325,\n",
              "        -0.50777286,  0.46564725,  0.51002413,  0.43797693,  0.47478324,\n",
              "        -0.5155925 ,  0.45318303, -0.4739283 , -0.53090423, -0.52114385,\n",
              "        -0.49053177,  0.48024204, -0.5094082 ,  0.4600966 , -0.44916236,\n",
              "         0.48300767, -0.5014751 , -0.4663068 , -0.4974693 ,  0.4398385 ,\n",
              "         0.44573817,  0.46320546,  0.51368684, -0.50639355,  0.4388145 ],\n",
              "       dtype=float32),\n",
              " 'june': array([ 0.5042361 ,  0.47541776, -0.5099937 , -0.43611783, -0.5144735 ,\n",
              "         0.5128355 ,  0.5170053 , -0.48853484, -0.50558805, -0.5092607 ,\n",
              "        -0.45801046,  0.5144639 ,  0.4604511 ,  0.46274942, -0.52181494,\n",
              "        -0.48598546,  0.5014256 ,  0.4576063 , -0.46463534, -0.47367057,\n",
              "        -0.44628692,  0.51142645,  0.5254549 ,  0.4660407 , -0.51593196,\n",
              "        -0.45628154,  0.4453948 ,  0.51435995,  0.45152193,  0.5148665 ,\n",
              "        -0.49424654,  0.49080092, -0.47694582, -0.53189445, -0.44487873,\n",
              "        -0.46049443,  0.4765903 , -0.52483004,  0.5123186 , -0.50473315,\n",
              "         0.49683   , -0.45150468, -0.47835076, -0.4741495 ,  0.4831954 ,\n",
              "         0.5159827 ,  0.50874007,  0.4866618 , -0.4757406 ,  0.49568623],\n",
              "       dtype=float32),\n",
              " 'cap': array([ 0.56258553,  0.48767462, -0.5323592 , -0.4402512 , -0.4783161 ,\n",
              "         0.5396911 ,  0.45534322, -0.4674388 , -0.51062834, -0.46362874,\n",
              "        -0.5271268 ,  0.4823977 ,  0.44315165,  0.51421666, -0.5469736 ,\n",
              "        -0.472999  ,  0.5216577 ,  0.5200173 , -0.4389464 , -0.51720583,\n",
              "        -0.50215125,  0.47180578,  0.5247305 ,  0.51992273, -0.50313175,\n",
              "        -0.516531  ,  0.462394  ,  0.5404499 ,  0.47035202,  0.50314003,\n",
              "        -0.52498364,  0.5185035 , -0.5119739 , -0.47706878, -0.5067379 ,\n",
              "        -0.46289125,  0.5120944 , -0.48132706,  0.45731157, -0.47869715,\n",
              "         0.50988275, -0.4592616 , -0.4294356 , -0.51854557,  0.5277393 ,\n",
              "         0.4954316 ,  0.4334746 ,  0.43917918, -0.4999277 ,  0.503016  ],\n",
              "       dtype=float32),\n",
              " 'relative': array([ 0.47876158,  0.42949468, -0.49122274, -0.48549753, -0.48627517,\n",
              "         0.45696703,  0.48825344, -0.5045222 , -0.4483088 , -0.46845758,\n",
              "        -0.43695682,  0.5195647 ,  0.4987625 ,  0.46418062, -0.54050857,\n",
              "        -0.4796782 ,  0.52947134,  0.5005575 , -0.4503338 , -0.45036417,\n",
              "        -0.4887267 ,  0.4639129 ,  0.53594446,  0.44715914, -0.43636826,\n",
              "        -0.48321483,  0.48735088,  0.54452837,  0.47512513,  0.4622385 ,\n",
              "        -0.4554543 ,  0.5011593 , -0.5183212 , -0.50128657, -0.4391076 ,\n",
              "        -0.50318485,  0.49698505, -0.47936645,  0.5005808 , -0.47565535,\n",
              "         0.48489627, -0.49408096, -0.4802854 , -0.50179565,  0.47993305,\n",
              "         0.5186597 ,  0.5235009 ,  0.505396  , -0.46439147,  0.5253076 ],\n",
              "       dtype=float32),\n",
              " 'n': array([ 0.41245902,  0.54389757, -0.4946261 , -0.44687897, -0.4737457 ,\n",
              "         0.52678984,  0.44077528, -0.45344138, -0.47052407, -0.47653854,\n",
              "        -0.52422446,  0.5244434 ,  0.49936798,  0.47248167, -0.49220738,\n",
              "        -0.5193882 ,  0.5256312 ,  0.45585924, -0.471005  , -0.48777756,\n",
              "        -0.58972925,  0.52976775,  0.5221238 ,  0.58397484, -0.5606047 ,\n",
              "        -0.43907592,  0.5120924 ,  0.52076256,  0.47236842,  0.5697567 ,\n",
              "        -0.4560206 ,  0.5131836 , -0.54577005, -0.5513476 , -0.4512372 ,\n",
              "        -0.47835314,  0.484225  , -0.6019627 ,  0.47070846, -0.5259661 ,\n",
              "         0.51183945, -0.6142075 , -0.4861504 , -0.51099664,  0.5839465 ,\n",
              "         0.531372  ,  0.53328854,  0.49125695, -0.5563185 ,  0.48215535],\n",
              "       dtype=float32),\n",
              " 'mfs': array([ 0.511797  ,  0.504794  , -0.4556702 , -0.43183228, -0.44920605,\n",
              "         0.5136762 ,  0.4951735 , -0.51447666, -0.46895143, -0.4772668 ,\n",
              "        -0.4951884 ,  0.48496887,  0.47508538,  0.529212  , -0.5265999 ,\n",
              "        -0.49834025,  0.46268386,  0.44819584, -0.49710238, -0.49799204,\n",
              "        -0.45060763,  0.5200041 ,  0.5191593 ,  0.45799065, -0.47663763,\n",
              "        -0.51956666,  0.47106802,  0.5076296 ,  0.44575188,  0.533126  ,\n",
              "        -0.4779107 ,  0.47368807, -0.55115986, -0.46339247, -0.45478266,\n",
              "        -0.5194277 ,  0.4591829 , -0.49528173,  0.4906108 , -0.5387944 ,\n",
              "         0.5592701 , -0.5180954 , -0.52698714, -0.49452502,  0.5149731 ,\n",
              "         0.50770444,  0.5330827 ,  0.4504776 , -0.5265666 ,  0.46836835],\n",
              "       dtype=float32),\n",
              " 'tend': array([ 0.5237427 ,  0.4985224 , -0.45256013, -0.49812472, -0.48329115,\n",
              "         0.5208923 ,  0.49023277, -0.4387036 , -0.4860127 , -0.47813517,\n",
              "        -0.45407173,  0.52081513,  0.43416417,  0.52006316, -0.5116513 ,\n",
              "        -0.51798105,  0.48623365,  0.43589568, -0.50909203, -0.49640435,\n",
              "        -0.50442785,  0.4722809 ,  0.5265063 ,  0.5028357 , -0.5100767 ,\n",
              "        -0.4925514 ,  0.42697555,  0.49196213,  0.46013817,  0.45748812,\n",
              "        -0.5227023 ,  0.43812582, -0.45149526, -0.49855244, -0.49012744,\n",
              "        -0.47655702,  0.44027096, -0.44262806,  0.51518863, -0.43775916,\n",
              "         0.5074708 , -0.43640867, -0.48582178, -0.43385777,  0.4681519 ,\n",
              "         0.49862966,  0.44357803,  0.46792763, -0.4692264 ,  0.42898917],\n",
              "       dtype=float32),\n",
              " 'vary': array([ 0.5020106 ,  0.44607598, -0.4939592 , -0.46514645, -0.5016849 ,\n",
              "         0.4608716 ,  0.5100431 , -0.49974167, -0.46569192, -0.47928664,\n",
              "        -0.4679602 ,  0.46230972,  0.49502823,  0.5341729 , -0.52113616,\n",
              "        -0.501925  ,  0.46178818,  0.52723604, -0.46725905, -0.5176703 ,\n",
              "        -0.46278363,  0.44386122,  0.50642836,  0.50214463, -0.53376573,\n",
              "        -0.49407595,  0.45045   ,  0.52200603,  0.4607586 ,  0.4605863 ,\n",
              "        -0.46297878,  0.44546947, -0.5108704 , -0.4919551 , -0.47665846,\n",
              "        -0.5141461 ,  0.5343117 , -0.4839346 ,  0.5227198 , -0.5076902 ,\n",
              "         0.51094985, -0.5139735 , -0.5168643 , -0.48826757,  0.4876494 ,\n",
              "         0.45623153,  0.46543074,  0.51489663, -0.4518195 ,  0.44869846],\n",
              "       dtype=float32),\n",
              " 'deferred': array([ 0.5441226 ,  0.46184063, -0.44334158, -0.45156386, -0.48706004,\n",
              "         0.509546  ,  0.5030276 , -0.52551657, -0.47534478, -0.44700548,\n",
              "        -0.5219617 ,  0.5085149 ,  0.4324725 ,  0.5129345 , -0.57276314,\n",
              "        -0.47817612,  0.53626955,  0.49717253, -0.45045555, -0.43705255,\n",
              "        -0.49165058,  0.47150162,  0.4878892 ,  0.46486673, -0.47038493,\n",
              "        -0.4309361 ,  0.4479075 ,  0.49102008,  0.49323237,  0.50274575,\n",
              "        -0.53173846,  0.4997008 , -0.51941466, -0.489464  , -0.44911107,\n",
              "        -0.5253832 ,  0.48192415, -0.48956504,  0.49892652, -0.4717476 ,\n",
              "         0.47758085, -0.50244826, -0.45824894, -0.46565208,  0.4757132 ,\n",
              "         0.45530117,  0.5145549 ,  0.5101222 , -0.4454925 ,  0.5301796 ],\n",
              "       dtype=float32),\n",
              " 'describes': array([ 0.54575807,  0.48241568, -0.48152003, -0.49758542, -0.5162211 ,\n",
              "         0.53640926,  0.4894235 , -0.47499833, -0.4765089 , -0.50481313,\n",
              "        -0.52910876,  0.46702445,  0.5431224 ,  0.46264145, -0.5123254 ,\n",
              "        -0.51130205,  0.4569603 ,  0.50975543, -0.48754138, -0.522478  ,\n",
              "        -0.459443  ,  0.49638906,  0.49180326,  0.47130957, -0.4642553 ,\n",
              "        -0.51807445,  0.55665237,  0.50073844,  0.48858333,  0.53879154,\n",
              "        -0.5377198 ,  0.48224795, -0.5216698 , -0.49859717, -0.47438854,\n",
              "        -0.51359546,  0.46897438, -0.5354356 ,  0.49231565, -0.5248357 ,\n",
              "         0.5096198 , -0.5620001 , -0.524196  , -0.49226516,  0.5040582 ,\n",
              "         0.51632583,  0.5124789 ,  0.53062385, -0.5473805 ,  0.4880404 ],\n",
              "       dtype=float32),\n",
              " 'insured': array([ 0.47568938,  0.44550842, -0.4650852 , -0.44406858, -0.5292063 ,\n",
              "         0.46637586,  0.45844337, -0.52463907, -0.48704398, -0.45377713,\n",
              "        -0.4993585 ,  0.46251556,  0.44812912,  0.53982365, -0.4973603 ,\n",
              "        -0.5301946 ,  0.45325258,  0.513593  , -0.4807244 , -0.44811934,\n",
              "        -0.4633562 ,  0.5253149 ,  0.532192  ,  0.47215676, -0.52147233,\n",
              "        -0.47850513,  0.50644505,  0.5666446 ,  0.46265665,  0.46970087,\n",
              "        -0.4788693 ,  0.44941178, -0.54153734, -0.49185693, -0.4456311 ,\n",
              "        -0.46088958,  0.48334193, -0.53945756,  0.47629452, -0.5228183 ,\n",
              "         0.49959952, -0.51675534, -0.443637  , -0.45034674,  0.49564946,\n",
              "         0.45009136,  0.4660003 ,  0.495838  , -0.48972178,  0.44681692],\n",
              "       dtype=float32),\n",
              " 'described': array([ 0.5010941 ,  0.4621074 , -0.5255835 , -0.51059514, -0.53942484,\n",
              "         0.47295034,  0.4551987 , -0.4975116 , -0.4485422 , -0.44785097,\n",
              "        -0.49059093,  0.52196366,  0.44659075,  0.54051983, -0.56934917,\n",
              "        -0.4754345 ,  0.48549855,  0.4846668 , -0.48554373, -0.46536016,\n",
              "        -0.44790933,  0.4734045 ,  0.52144885,  0.5111486 , -0.4653503 ,\n",
              "        -0.46242014,  0.47754064,  0.4744414 ,  0.49728662,  0.47158298,\n",
              "        -0.51142347,  0.51236796, -0.48705518, -0.53882384, -0.47369325,\n",
              "        -0.50172466,  0.47830153, -0.45054063,  0.49289787, -0.4588605 ,\n",
              "         0.52931166, -0.48922205, -0.4477244 , -0.5038849 ,  0.48906705,\n",
              "         0.5125176 ,  0.48337334,  0.4986424 , -0.49956587,  0.48873264],\n",
              "       dtype=float32),\n",
              " 'default': array([ 0.5459492 ,  0.44876888, -0.47258812, -0.5052692 , -0.5300504 ,\n",
              "         0.5406433 ,  0.43999803, -0.48671818, -0.4396917 , -0.46957827,\n",
              "        -0.45469376,  0.4676753 ,  0.4808483 ,  0.45559645, -0.52828366,\n",
              "        -0.490863  ,  0.46418846,  0.5288439 , -0.522293  , -0.44493395,\n",
              "        -0.43538627,  0.45144227,  0.53391516,  0.52519816, -0.47517335,\n",
              "        -0.5234192 ,  0.50762904,  0.52447057,  0.52672416,  0.49322987,\n",
              "        -0.4837725 ,  0.46026936, -0.51051307, -0.49944523, -0.45794973,\n",
              "        -0.49513367,  0.45166615, -0.46991846,  0.515064  , -0.51895696,\n",
              "         0.49103245, -0.52156335, -0.45990062, -0.4513507 ,  0.4628965 ,\n",
              "         0.44250986,  0.44125077,  0.44187963, -0.44840088,  0.44598743],\n",
              "       dtype=float32),\n",
              " 'redeemed': array([ 0.51428694,  0.4988594 , -0.49639133, -0.457824  , -0.51740885,\n",
              "         0.530671  ,  0.49306446, -0.49273655, -0.45807126, -0.51354825,\n",
              "        -0.47084567,  0.49494177,  0.48008814,  0.5293577 , -0.48441124,\n",
              "        -0.4608598 ,  0.4669815 ,  0.49719048, -0.46416542, -0.45603698,\n",
              "        -0.51679355,  0.52927995,  0.5113858 ,  0.44657874, -0.49465644,\n",
              "        -0.49189442,  0.49372557,  0.4906315 ,  0.5290085 ,  0.46597084,\n",
              "        -0.45820493,  0.5171778 , -0.4592826 , -0.45596775, -0.46647367,\n",
              "        -0.44899917,  0.5122882 , -0.46900135,  0.5036289 , -0.5327523 ,\n",
              "         0.45993397, -0.48863384, -0.47393692, -0.5213684 ,  0.4843283 ,\n",
              "         0.5254881 ,  0.50996673,  0.44263473, -0.524152  ,  0.46216148],\n",
              "       dtype=float32),\n",
              " 'president': array([ 0.4724712 ,  0.48711818, -0.43688878, -0.4357754 , -0.4923342 ,\n",
              "         0.4817527 ,  0.45028788, -0.5022114 , -0.4941607 , -0.4462701 ,\n",
              "        -0.47283465,  0.44201753,  0.4518901 ,  0.47705695, -0.546402  ,\n",
              "        -0.47370347,  0.51042837,  0.4705558 , -0.46308112, -0.4885777 ,\n",
              "        -0.43833593,  0.44390216,  0.49463457,  0.49370512, -0.44854856,\n",
              "        -0.4262173 ,  0.4713339 ,  0.51290315,  0.46354437,  0.44465768,\n",
              "        -0.45640782,  0.4724412 , -0.52233064, -0.45801505, -0.46358225,\n",
              "        -0.5172249 ,  0.43973166, -0.48600802,  0.42732656, -0.5188897 ,\n",
              "         0.50124353, -0.44218376, -0.4261597 , -0.50851923,  0.47437772,\n",
              "         0.50155675,  0.49583048,  0.515685  , -0.49623877,  0.5037407 ],\n",
              "       dtype=float32),\n",
              " 'remain': array([ 0.53443605,  0.46585038, -0.4814641 , -0.46746057, -0.5322228 ,\n",
              "         0.4758917 ,  0.471598  , -0.4720584 , -0.45714596, -0.48603263,\n",
              "        -0.47254163,  0.44782493,  0.46588594,  0.5154722 , -0.4951797 ,\n",
              "        -0.47883102,  0.49226967,  0.4832217 , -0.5119947 , -0.5234956 ,\n",
              "        -0.5270036 ,  0.4683537 ,  0.4697364 ,  0.44812903, -0.4886399 ,\n",
              "        -0.4946948 ,  0.4554229 ,  0.53648627,  0.5341406 ,  0.48170394,\n",
              "        -0.5064243 ,  0.49028587, -0.5073131 , -0.45484814, -0.4545805 ,\n",
              "        -0.51077   ,  0.5141808 , -0.52105224,  0.52540445, -0.5167095 ,\n",
              "         0.521798  , -0.46868208, -0.47090417, -0.5255014 ,  0.52055484,\n",
              "         0.5265335 ,  0.46336713,  0.46017778, -0.48945236,  0.46527928],\n",
              "       dtype=float32),\n",
              " 'typically': array([ 0.5433453 ,  0.46363723, -0.49268743, -0.50720453, -0.4708745 ,\n",
              "         0.4994611 ,  0.45455045, -0.46135822, -0.48812404, -0.49523434,\n",
              "        -0.4863361 ,  0.4884578 ,  0.46801117,  0.52322555, -0.499308  ,\n",
              "        -0.46868038,  0.52902734,  0.5013326 , -0.4570015 , -0.5206457 ,\n",
              "        -0.51225877,  0.4706807 ,  0.5026941 ,  0.46649224, -0.43946275,\n",
              "        -0.46490213,  0.4478719 ,  0.5144744 ,  0.47282514,  0.44228134,\n",
              "        -0.519156  ,  0.47538275, -0.49928975, -0.527327  , -0.43822485,\n",
              "        -0.5015855 ,  0.47107714, -0.4910693 ,  0.4823269 , -0.4557674 ,\n",
              "         0.48729804, -0.49656805, -0.47942865, -0.4895702 ,  0.53410745,\n",
              "         0.47064245,  0.4970414 ,  0.44484332, -0.49895644,  0.49410856],\n",
              "       dtype=float32),\n",
              " 'order': array([ 0.5051323 ,  0.44080192, -0.48373547, -0.46152538, -0.45476735,\n",
              "         0.4951559 ,  0.43673846, -0.50786847, -0.49792063, -0.47046414,\n",
              "        -0.43777743,  0.48479286,  0.47043777,  0.48200956, -0.5386849 ,\n",
              "        -0.4908383 ,  0.52668434,  0.43696237, -0.44979933, -0.42640156,\n",
              "        -0.51089084,  0.50969434,  0.44160232,  0.45240185, -0.4636239 ,\n",
              "        -0.46342373,  0.44724357,  0.52985317,  0.45234004,  0.44359788,\n",
              "        -0.49858445,  0.46130967, -0.49297637, -0.4723795 , -0.5055919 ,\n",
              "        -0.4488001 ,  0.5109384 , -0.4654954 ,  0.4723911 , -0.47918692,\n",
              "         0.5197815 , -0.4908473 , -0.45956627, -0.49339443,  0.49622783,\n",
              "         0.49548376,  0.44607508,  0.45279965, -0.5101752 ,  0.48961228],\n",
              "       dtype=float32),\n",
              " 'differ': array([ 0.546521  ,  0.44571102, -0.44336906, -0.47577465, -0.4555415 ,\n",
              "         0.51639843,  0.5196102 , -0.45251277, -0.43836668, -0.46685743,\n",
              "        -0.51008856,  0.50519824,  0.5111714 ,  0.51447105, -0.49878043,\n",
              "        -0.4902628 ,  0.4948398 ,  0.46148163, -0.46902075, -0.45876417,\n",
              "        -0.4683764 ,  0.51659924,  0.45757297,  0.45811352, -0.49788716,\n",
              "        -0.45362222,  0.48818418,  0.47039804,  0.49921513,  0.50263083,\n",
              "        -0.52650666,  0.5097014 , -0.48136097, -0.5272297 , -0.51869094,\n",
              "        -0.49840233,  0.47644812, -0.50403523,  0.45597696, -0.5261181 ,\n",
              "         0.49667263, -0.5181972 , -0.51358616, -0.46878004,  0.48915446,\n",
              "         0.48337206,  0.5035611 ,  0.48656377, -0.5087161 ,  0.49419236],\n",
              "       dtype=float32),\n",
              " 'ordinary': array([ 0.53308684,  0.48328152, -0.49958664, -0.4972941 , -0.5314496 ,\n",
              "         0.47189808,  0.4914959 , -0.5256053 , -0.4547891 , -0.4731664 ,\n",
              "        -0.44865337,  0.49180496,  0.48219898,  0.45424616, -0.4989639 ,\n",
              "        -0.4697332 ,  0.5025249 ,  0.47733125, -0.51753783, -0.4327441 ,\n",
              "        -0.49515194,  0.4780808 ,  0.51600575,  0.47993824, -0.459872  ,\n",
              "        -0.45474952,  0.5041197 ,  0.47412843,  0.5028859 ,  0.46526635,\n",
              "        -0.524073  ,  0.46953493, -0.5121781 , -0.48140556, -0.5142406 ,\n",
              "        -0.5075736 ,  0.49848047, -0.46576425,  0.4873084 , -0.4657583 ,\n",
              "         0.47694224, -0.47591192, -0.5264667 , -0.48743182,  0.5147898 ,\n",
              "         0.51932055,  0.49192637,  0.46551666, -0.49591434,  0.501865  ],\n",
              "       dtype=float32),\n",
              " 'life': array([ 0.5656133 ,  0.49613327, -0.4605658 , -0.5055611 , -0.5095144 ,\n",
              "         0.46449748,  0.47798932, -0.49878195, -0.5025419 , -0.4502719 ,\n",
              "        -0.48717627,  0.43604416,  0.49862224,  0.48845342, -0.5747194 ,\n",
              "        -0.46134466,  0.5211658 ,  0.4972659 , -0.44515833, -0.45920414,\n",
              "        -0.43207172,  0.44559935,  0.5035243 ,  0.46042517, -0.49096033,\n",
              "        -0.43926483,  0.5095054 ,  0.50667024,  0.4612583 ,  0.45152485,\n",
              "        -0.5278329 ,  0.49087653, -0.4740445 , -0.5146504 , -0.44839513,\n",
              "        -0.45971453,  0.5222002 , -0.5022127 ,  0.45989263, -0.4738615 ,\n",
              "         0.45250657, -0.47663882, -0.47465867, -0.5155146 ,  0.44365132,\n",
              "         0.48516747,  0.48225248,  0.45304692, -0.44499794,  0.5173308 ],\n",
              "       dtype=float32),\n",
              " 'times': array([ 0.48656386,  0.49859253, -0.49929243, -0.4989669 , -0.50438654,\n",
              "         0.46734455,  0.52106327, -0.5179512 , -0.43367183, -0.5186355 ,\n",
              "        -0.48760706,  0.4712021 ,  0.5103834 ,  0.46436778, -0.5350591 ,\n",
              "        -0.48628044,  0.47545803,  0.49257588, -0.492651  , -0.44131842,\n",
              "        -0.4860328 ,  0.5139059 ,  0.52940893,  0.47763893, -0.49565828,\n",
              "        -0.4479708 ,  0.45079878,  0.5062999 ,  0.4404406 ,  0.51484036,\n",
              "        -0.48306385,  0.43286708, -0.4715235 , -0.5441773 , -0.4289631 ,\n",
              "        -0.5204745 ,  0.49041596, -0.4770338 ,  0.5057001 , -0.5335062 ,\n",
              "         0.49238503, -0.5068908 , -0.48443553, -0.5104333 ,  0.44299448,\n",
              "         0.49229085,  0.50309956,  0.50856835, -0.51107115,  0.4533155 ],\n",
              "       dtype=float32),\n",
              " 'calendar': array([ 0.5042448 ,  0.4890753 , -0.48726243, -0.48861626, -0.5261436 ,\n",
              "         0.50513786,  0.49961776, -0.52343976, -0.5124228 , -0.45423004,\n",
              "        -0.47912967,  0.47244748,  0.50283176,  0.44929814, -0.50432134,\n",
              "        -0.4616042 ,  0.5099518 ,  0.51246756, -0.4385737 , -0.4466957 ,\n",
              "        -0.49131924,  0.502939  ,  0.51680887,  0.5259924 , -0.4621718 ,\n",
              "        -0.53110135,  0.45106608,  0.49297723,  0.48244488,  0.46051958,\n",
              "        -0.46835604,  0.4576301 , -0.475047  , -0.46657225, -0.46104598,\n",
              "        -0.49222124,  0.53360105, -0.5339749 ,  0.51097924, -0.45058858,\n",
              "         0.4521425 , -0.49750957, -0.45828032, -0.5142746 ,  0.4952479 ,\n",
              "         0.47586367,  0.51453716,  0.48373705, -0.4798583 ,  0.45366472],\n",
              "       dtype=float32),\n",
              " 'rated': array([ 0.48322955,  0.49789155, -0.50391847, -0.50652003, -0.5292903 ,\n",
              "         0.46710563,  0.47572815, -0.46364367, -0.46571648, -0.44913942,\n",
              "        -0.4640406 ,  0.50582635,  0.48655242,  0.45974636, -0.5813054 ,\n",
              "        -0.49463716,  0.45293579,  0.46499586, -0.49912494, -0.4666832 ,\n",
              "        -0.4838686 ,  0.44773525,  0.5265461 ,  0.45040983, -0.4810807 ,\n",
              "        -0.44677696,  0.49917734,  0.54806954,  0.44403327,  0.44647634,\n",
              "        -0.48054463,  0.45712876, -0.50589156, -0.508574  , -0.52456707,\n",
              "        -0.5277089 ,  0.50108653, -0.52182835,  0.4511891 , -0.46017584,\n",
              "         0.52412283, -0.47813472, -0.43307894, -0.47783184,  0.49910527,\n",
              "         0.4620229 ,  0.49254015,  0.511767  , -0.5164211 ,  0.51008356],\n",
              "       dtype=float32),\n",
              " 'purchased': array([ 0.55520463,  0.51168346, -0.47491834, -0.45619184, -0.53777695,\n",
              "         0.47912416,  0.48130396, -0.5161253 , -0.49097946, -0.46187744,\n",
              "        -0.51897013,  0.52710843,  0.47727743,  0.5246574 , -0.54931563,\n",
              "        -0.4519664 ,  0.4803899 ,  0.46981618, -0.4645491 , -0.43754235,\n",
              "        -0.46193966,  0.4535106 ,  0.48489395,  0.5351491 , -0.53133404,\n",
              "        -0.48934603,  0.4810857 ,  0.48494622,  0.4800252 ,  0.48577583,\n",
              "        -0.5253974 ,  0.4661023 , -0.5413467 , -0.5400699 , -0.5099596 ,\n",
              "        -0.49487287,  0.50381595, -0.49441403,  0.5183637 , -0.5377804 ,\n",
              "         0.4901682 , -0.5130151 , -0.5143657 , -0.49732506,  0.49947304,\n",
              "         0.4632158 ,  0.5036801 ,  0.452729  , -0.45710072,  0.447535  ],\n",
              "       dtype=float32),\n",
              " 'shows': array([ 0.4989541 ,  0.47111842, -0.47648332, -0.4754318 , -0.5383664 ,\n",
              "         0.4545787 ,  0.50007695, -0.4814192 , -0.46954465, -0.48267958,\n",
              "        -0.4975611 ,  0.52833724,  0.4497644 ,  0.4877886 , -0.5469103 ,\n",
              "        -0.45586315,  0.4488769 ,  0.5283445 , -0.5038151 , -0.5134028 ,\n",
              "        -0.5245197 ,  0.50675166,  0.4833841 ,  0.5007827 , -0.485769  ,\n",
              "        -0.442348  ,  0.51045454,  0.5064921 ,  0.4347621 ,  0.51918244,\n",
              "        -0.51609474,  0.43934596, -0.50951105, -0.5126892 , -0.5148067 ,\n",
              "        -0.52440953,  0.45154542, -0.46433952,  0.4778703 , -0.46447307,\n",
              "         0.46002072, -0.51282996, -0.4647427 , -0.5136775 ,  0.47047898,\n",
              "         0.45495954,  0.49072468,  0.47092935, -0.47901255,  0.5263885 ],\n",
              "       dtype=float32),\n",
              " 'please': array([ 0.47833207,  0.459009  , -0.46337345, -0.45146063, -0.45790175,\n",
              "         0.4663896 ,  0.5212996 , -0.4639714 , -0.43382782, -0.4608389 ,\n",
              "        -0.49129826,  0.48764557,  0.48842323,  0.45650056, -0.53976154,\n",
              "        -0.50549775,  0.4966201 ,  0.46472177, -0.49609697, -0.49129462,\n",
              "        -0.4618249 ,  0.51463133,  0.47228628,  0.4798702 , -0.49109924,\n",
              "        -0.46969014,  0.43293107,  0.50890607,  0.51274765,  0.49320143,\n",
              "        -0.5297694 ,  0.49639863, -0.47032994, -0.47547647, -0.4370871 ,\n",
              "        -0.4622466 ,  0.53323495, -0.45300332,  0.45789537, -0.5146125 ,\n",
              "         0.44722584, -0.50193727, -0.47341853, -0.51790315,  0.5072144 ,\n",
              "         0.49918795,  0.46633682,  0.504416  , -0.47775576,  0.47091767],\n",
              "       dtype=float32),\n",
              " 'rise': array([ 0.46712008,  0.4342115 , -0.4798637 , -0.43831006, -0.50198495,\n",
              "         0.49043036,  0.4635084 , -0.44663528, -0.5096697 , -0.50014055,\n",
              "        -0.5185454 ,  0.5159083 ,  0.48518872,  0.43745354, -0.52285314,\n",
              "        -0.467295  ,  0.49260306,  0.51705146, -0.4557377 , -0.4350705 ,\n",
              "        -0.48401228,  0.43732944,  0.52350575,  0.46224353, -0.43952838,\n",
              "        -0.5022721 ,  0.44412538,  0.4577266 ,  0.47770235,  0.45854396,\n",
              "        -0.46063158,  0.50924575, -0.5025643 , -0.47455308, -0.45777422,\n",
              "        -0.4823229 ,  0.47635326, -0.51721376,  0.5020983 , -0.45453125,\n",
              "         0.48295885, -0.5098452 , -0.42690578, -0.50929165,  0.45346367,\n",
              "         0.459817  ,  0.49295434,  0.48482984, -0.44971806,  0.4785941 ],\n",
              "       dtype=float32),\n",
              " 'holdings': array([ 0.49090335,  0.50579274, -0.45670682, -0.48402017, -0.4587147 ,\n",
              "         0.48316044,  0.44270396, -0.45990896, -0.4692411 , -0.4912003 ,\n",
              "        -0.51013213,  0.5245282 ,  0.46204054,  0.52844167, -0.5630825 ,\n",
              "        -0.50086725,  0.45877793,  0.48027137, -0.49476406, -0.49507675,\n",
              "        -0.46020567,  0.5174892 ,  0.48464334,  0.43864882, -0.51989275,\n",
              "        -0.43992412,  0.46757144,  0.51310116,  0.4428191 ,  0.5065223 ,\n",
              "        -0.46847388,  0.47889367, -0.47769248, -0.52603227, -0.46639052,\n",
              "        -0.545067  ,  0.48743758, -0.4517192 ,  0.46530798, -0.48689964,\n",
              "         0.47783634, -0.49720585, -0.48014015, -0.5217415 ,  0.45117897,\n",
              "         0.4881779 ,  0.5083134 ,  0.5183621 , -0.46477228,  0.49681145],\n",
              "       dtype=float32),\n",
              " 'vice': array([ 0.48356172,  0.45284498, -0.5029453 , -0.4357783 , -0.5124484 ,\n",
              "         0.5234011 ,  0.4342278 , -0.48136503, -0.42486778, -0.48282948,\n",
              "        -0.49324137,  0.47844473,  0.4628392 ,  0.48624766, -0.5424293 ,\n",
              "        -0.49701488,  0.49143544,  0.4593632 , -0.47494018, -0.44942552,\n",
              "        -0.47475576,  0.4605851 ,  0.4819753 ,  0.44971707, -0.48353648,\n",
              "        -0.4461764 ,  0.47189364,  0.48272517,  0.43984264,  0.49604028,\n",
              "        -0.5201053 ,  0.5040307 , -0.4964997 , -0.4461633 , -0.44709438,\n",
              "        -0.50858295,  0.49692473, -0.5012483 ,  0.49629313, -0.4719572 ,\n",
              "         0.44201174, -0.495251  , -0.4294814 , -0.46592414,  0.4709921 ,\n",
              "         0.44195905,  0.43775463,  0.47210413, -0.47064194,  0.45302492],\n",
              "       dtype=float32),\n",
              " 'purposes': array([ 0.47296995,  0.45917064, -0.4846857 , -0.45331624, -0.52186286,\n",
              "         0.49736974,  0.48279023, -0.51081824, -0.52709955, -0.45946   ,\n",
              "        -0.45302507,  0.505273  ,  0.44822463,  0.48619938, -0.50188124,\n",
              "        -0.44787714,  0.49540856,  0.45754698, -0.434464  , -0.44986126,\n",
              "        -0.5024277 ,  0.45387056,  0.45068395,  0.5202092 , -0.5259674 ,\n",
              "        -0.48891687,  0.522165  ,  0.5083044 ,  0.4816026 ,  0.5071751 ,\n",
              "        -0.5155911 ,  0.5271981 , -0.47899392, -0.48856902, -0.45750046,\n",
              "        -0.47440222,  0.5097634 , -0.48412654,  0.5280582 , -0.52072483,\n",
              "         0.46309704, -0.45066714, -0.5056798 , -0.45309728,  0.52682734,\n",
              "         0.48239404,  0.47673345,  0.52790457, -0.4617262 ,  0.53321546],\n",
              "       dtype=float32),\n",
              " 'objectives': array([ 0.49591044,  0.4483948 , -0.5207076 , -0.4641402 , -0.48482773,\n",
              "         0.45936984,  0.5307205 , -0.47883004, -0.5153896 , -0.5056286 ,\n",
              "        -0.52557194,  0.4530613 ,  0.5054087 ,  0.5225326 , -0.47409448,\n",
              "        -0.4853726 ,  0.5272405 ,  0.47811723, -0.4635628 , -0.4954328 ,\n",
              "        -0.50738645,  0.49089894,  0.48693627,  0.44384164, -0.5263585 ,\n",
              "        -0.507518  ,  0.4791283 ,  0.46829286,  0.47580373,  0.49351928,\n",
              "        -0.47589654,  0.5220817 , -0.5493156 , -0.4622499 , -0.45543236,\n",
              "        -0.47672704,  0.47231576, -0.48932093,  0.46932232, -0.52489334,\n",
              "         0.4642131 , -0.508797  , -0.44730926, -0.5255571 ,  0.53935975,\n",
              "         0.47423518,  0.51823944,  0.5259339 , -0.51403433,  0.5305085 ],\n",
              "       dtype=float32),\n",
              " 'ask': array([ 0.46360308,  0.4363998 , -0.4879961 , -0.46211776, -0.5161877 ,\n",
              "         0.49421003,  0.52801335, -0.49471772, -0.4793387 , -0.4596041 ,\n",
              "        -0.5205068 ,  0.49088064,  0.52664286,  0.47053775, -0.48756823,\n",
              "        -0.47840863,  0.47709644,  0.47599792, -0.5199498 , -0.44441068,\n",
              "        -0.5271688 ,  0.4865577 ,  0.54382277,  0.48194224, -0.46278605,\n",
              "        -0.48304135,  0.5245059 ,  0.5267992 ,  0.492168  ,  0.48068485,\n",
              "        -0.48573658,  0.46138993, -0.49232763, -0.5073666 , -0.44427264,\n",
              "        -0.53103435,  0.5428395 , -0.52966595,  0.49985906, -0.5191551 ,\n",
              "         0.4701933 , -0.46532807, -0.4782431 , -0.45801976,  0.534136  ,\n",
              "         0.52592313,  0.4511277 ,  0.5020351 , -0.43856314,  0.49226466],\n",
              "       dtype=float32),\n",
              " 'acquired': array([ 0.52999055,  0.46647203, -0.47767785, -0.43466842, -0.4767856 ,\n",
              "         0.4622106 ,  0.46352884, -0.4480699 , -0.43808946, -0.4776358 ,\n",
              "        -0.45657703,  0.4488506 ,  0.5295831 ,  0.52049947, -0.5494376 ,\n",
              "        -0.48738623,  0.50460964,  0.4873699 , -0.45715162, -0.4942056 ,\n",
              "        -0.5245983 ,  0.50327975,  0.4892645 ,  0.50100434, -0.49127758,\n",
              "        -0.5278178 ,  0.5215582 ,  0.53840727,  0.51888007,  0.4431554 ,\n",
              "        -0.45291537,  0.4875486 , -0.4931957 , -0.49181485, -0.51774776,\n",
              "        -0.450601  ,  0.49354824, -0.48617288,  0.49499562, -0.51196337,\n",
              "         0.45717582, -0.4864483 , -0.4440247 , -0.5080885 ,  0.45305464,\n",
              "         0.500926  ,  0.4890101 ,  0.47461718, -0.4666789 ,  0.5132557 ],\n",
              "       dtype=float32),\n",
              " 'depositary': array([ 0.50310373,  0.4850747 , -0.49961048, -0.48788556, -0.5263023 ,\n",
              "         0.4621987 ,  0.5105209 , -0.46803525, -0.51763475, -0.4316506 ,\n",
              "        -0.46155787,  0.4522605 ,  0.5129811 ,  0.5202086 , -0.5187619 ,\n",
              "        -0.46336076,  0.44230437,  0.4786765 , -0.46840575, -0.49046108,\n",
              "        -0.45102715,  0.48104852,  0.4918633 ,  0.47913206, -0.49991393,\n",
              "        -0.44304633,  0.5045089 ,  0.52411497,  0.4872343 ,  0.44079247,\n",
              "        -0.46675843,  0.5072954 , -0.5197679 , -0.4590516 , -0.48244765,\n",
              "        -0.52249384,  0.44678718, -0.50000554,  0.4844362 , -0.5122907 ,\n",
              "         0.47428954, -0.49924392, -0.4522617 , -0.51845515,  0.49821895,\n",
              "         0.45618868,  0.48120737,  0.5142061 , -0.52251387,  0.48239136],\n",
              "       dtype=float32),\n",
              " 'change': array([ 0.47189355,  0.5071165 , -0.52255964, -0.4983695 , -0.48041615,\n",
              "         0.50819653,  0.45667008, -0.4524641 , -0.4604097 , -0.45862386,\n",
              "        -0.48541018,  0.4367415 ,  0.45573786,  0.46182334, -0.493922  ,\n",
              "        -0.50006765,  0.48191985,  0.4392569 , -0.461032  , -0.43861324,\n",
              "        -0.5122536 ,  0.44291312,  0.4935012 ,  0.5040613 , -0.49634272,\n",
              "        -0.4757677 ,  0.5062117 ,  0.5091042 ,  0.4931952 ,  0.510329  ,\n",
              "        -0.50978637,  0.5046866 , -0.5021815 , -0.5212766 , -0.45829788,\n",
              "        -0.46238476,  0.5052684 , -0.5042013 ,  0.46950984, -0.5076678 ,\n",
              "         0.51770926, -0.4941486 , -0.4292815 , -0.43058366,  0.44511238,\n",
              "         0.4619107 ,  0.44788602,  0.48631078, -0.4593813 ,  0.46628082],\n",
              "       dtype=float32),\n",
              " 'conflict': array([ 0.5342323 ,  0.48110977, -0.5030538 , -0.43530262, -0.54308414,\n",
              "         0.4740994 ,  0.43810096, -0.49962908, -0.5116199 , -0.44442415,\n",
              "        -0.5080472 ,  0.51263297,  0.49494892,  0.47903132, -0.5119297 ,\n",
              "        -0.48252386,  0.45652685,  0.48842835, -0.45801264, -0.51875687,\n",
              "        -0.4990132 ,  0.43675542,  0.46686944,  0.48246393, -0.43407837,\n",
              "        -0.46876815,  0.4885212 ,  0.5179121 ,  0.4916963 ,  0.5172026 ,\n",
              "        -0.47079226,  0.4406847 , -0.5123531 , -0.51376367, -0.5013435 ,\n",
              "        -0.5261992 ,  0.4672437 , -0.53342324,  0.52433026, -0.49777785,\n",
              "         0.5161766 , -0.45254558, -0.47111115, -0.45533082,  0.5109979 ,\n",
              "         0.5046644 ,  0.47241354,  0.49021667, -0.4554472 ,  0.48071173],\n",
              "       dtype=float32),\n",
              " 'recommend': array([ 0.5091704 ,  0.44758272, -0.45057628, -0.5309945 , -0.5374826 ,\n",
              "         0.45834893,  0.53501433, -0.5159487 , -0.5289497 , -0.47394264,\n",
              "        -0.45424914,  0.45850152,  0.530745  ,  0.49918246, -0.5463501 ,\n",
              "        -0.5155537 ,  0.4811147 ,  0.53464246, -0.4674893 , -0.45045456,\n",
              "        -0.48473832,  0.50389373,  0.4896247 ,  0.43712062, -0.46743947,\n",
              "        -0.49044803,  0.51793987,  0.5084877 ,  0.48512423,  0.51137406,\n",
              "        -0.4740847 ,  0.52152485, -0.51718986, -0.4618529 , -0.46938494,\n",
              "        -0.45907456,  0.48035064, -0.48315036,  0.47598135, -0.4894864 ,\n",
              "         0.5138481 , -0.53755105, -0.43594682, -0.5320441 ,  0.4570161 ,\n",
              "         0.47162285,  0.5158261 ,  0.5320729 , -0.44378778,  0.4481292 ],\n",
              "       dtype=float32),\n",
              " 'website': array([ 0.53220016,  0.43536887, -0.4672329 , -0.45689344, -0.47707057,\n",
              "         0.48286146,  0.47992727, -0.5226807 , -0.46585393, -0.4374575 ,\n",
              "        -0.5205529 ,  0.44312516,  0.49937734,  0.4961338 , -0.56558573,\n",
              "        -0.46196795,  0.48186874,  0.4554095 , -0.46191815, -0.46103036,\n",
              "        -0.51334924,  0.46061644,  0.51743406,  0.4452619 , -0.51531005,\n",
              "        -0.4518611 ,  0.45825455,  0.5139569 ,  0.5167872 ,  0.51658505,\n",
              "        -0.5343799 ,  0.49333185, -0.4948156 , -0.48278454, -0.5131003 ,\n",
              "        -0.4869356 ,  0.47544652, -0.5227825 ,  0.45178738, -0.5030935 ,\n",
              "         0.52468   , -0.4466953 , -0.4661983 , -0.44062728,  0.48533976,\n",
              "         0.45774433,  0.5011339 ,  0.5137764 , -0.44590256,  0.46071365],\n",
              "       dtype=float32),\n",
              " 'believes': array([ 0.49434686,  0.4748666 , -0.5192424 , -0.4911958 , -0.46564776,\n",
              "         0.45481452,  0.5256622 , -0.4897532 , -0.50924313, -0.51600623,\n",
              "        -0.45756543,  0.47845048,  0.48829716,  0.46384346, -0.54302776,\n",
              "        -0.5052196 ,  0.45412695,  0.48856995, -0.47666776, -0.5026382 ,\n",
              "        -0.50288075,  0.4461523 ,  0.46485183,  0.5198287 , -0.4450925 ,\n",
              "        -0.44471923,  0.4570965 ,  0.47265965,  0.43951893,  0.4459352 ,\n",
              "        -0.47612187,  0.5110942 , -0.50324273, -0.4840391 , -0.51008224,\n",
              "        -0.45396876,  0.4663026 , -0.4929909 ,  0.4827037 , -0.4585357 ,\n",
              "         0.46047857, -0.5314786 , -0.48173907, -0.49713945,  0.4668237 ,\n",
              "         0.4639815 ,  0.50786   ,  0.53197837, -0.46001765,  0.51359534],\n",
              "       dtype=float32),\n",
              " 'loans': array([ 0.5118078 ,  0.45305   , -0.51398236, -0.5118258 , -0.53605056,\n",
              "         0.47945952,  0.4755397 , -0.467112  , -0.47231668, -0.44607532,\n",
              "        -0.44750872,  0.484441  ,  0.45885813,  0.5312207 , -0.5348135 ,\n",
              "        -0.5147111 ,  0.44907975,  0.49920106, -0.44532666, -0.48535725,\n",
              "        -0.46097323,  0.46427163,  0.5283848 ,  0.45096374, -0.44566298,\n",
              "        -0.488647  ,  0.44304785,  0.5213912 ,  0.52448857,  0.5007061 ,\n",
              "        -0.48868778,  0.4626934 , -0.46003145, -0.48253682, -0.48252276,\n",
              "        -0.5130533 ,  0.45813474, -0.45055392,  0.46841276, -0.4775727 ,\n",
              "         0.49885625, -0.5123888 , -0.4580282 , -0.45693803,  0.523987  ,\n",
              "         0.48877856,  0.46463794,  0.5014323 , -0.5280449 ,  0.44223204],\n",
              "       dtype=float32),\n",
              " 'outside': array([ 0.49449283,  0.4842029 , -0.49885178, -0.442434  , -0.49872094,\n",
              "         0.5075156 ,  0.52231175, -0.5198632 , -0.49559498, -0.47436723,\n",
              "        -0.41193053,  0.5142044 ,  0.5116384 ,  0.47108862, -0.5387163 ,\n",
              "        -0.4874229 ,  0.47689554,  0.48246586, -0.4876903 , -0.43775946,\n",
              "        -0.43098468,  0.44341078,  0.5007215 ,  0.5294836 , -0.50236267,\n",
              "        -0.46486   ,  0.4734674 ,  0.49085483,  0.43655753,  0.48681563,\n",
              "        -0.46497193,  0.4624935 , -0.49074978, -0.5111364 , -0.45101976,\n",
              "        -0.49243653,  0.44972977, -0.5024693 ,  0.4828025 , -0.48524222,\n",
              "         0.4634887 , -0.48219153, -0.46633112, -0.4549433 ,  0.5195815 ,\n",
              "         0.52339745,  0.45168725,  0.51130825, -0.4859529 ,  0.5100542 ],\n",
              "       dtype=float32),\n",
              " 'policy': array([ 0.49936056,  0.5035276 , -0.4554344 , -0.49051017, -0.46287072,\n",
              "         0.5157951 ,  0.43014938, -0.49949408, -0.45654145, -0.497119  ,\n",
              "        -0.4476577 ,  0.50732625,  0.519266  ,  0.47060713, -0.54178125,\n",
              "        -0.46385112,  0.49300635,  0.47655177, -0.49947435, -0.46003056,\n",
              "        -0.46729946,  0.49199226,  0.47101465,  0.44306886, -0.43173477,\n",
              "        -0.44852668,  0.495237  ,  0.46846652,  0.5005075 ,  0.5018193 ,\n",
              "        -0.49068493,  0.46017745, -0.5149423 , -0.4805344 , -0.4535439 ,\n",
              "        -0.45146045,  0.4730309 , -0.47286588,  0.4458044 , -0.43414894,\n",
              "         0.45871267, -0.46159366, -0.47286746, -0.4645478 ,  0.46675482,\n",
              "         0.51565653,  0.44724005,  0.5246072 , -0.47731555,  0.44215596],\n",
              "       dtype=float32),\n",
              " 'reduced': array([ 0.47463655,  0.47068205, -0.47071752, -0.50095284, -0.5200682 ,\n",
              "         0.51461047,  0.46786663, -0.48844206, -0.5148272 , -0.44770318,\n",
              "        -0.46441185,  0.4916587 ,  0.51431996,  0.5179137 , -0.47651377,\n",
              "        -0.47077584,  0.52674574,  0.44467667, -0.4465285 , -0.4958619 ,\n",
              "        -0.45875728,  0.45583725,  0.5306195 ,  0.51438856, -0.4309333 ,\n",
              "        -0.48539895,  0.50083953,  0.53541124,  0.51881534,  0.44399843,\n",
              "        -0.45397115,  0.45254225, -0.4571243 , -0.48406994, -0.49682963,\n",
              "        -0.50169486,  0.4549087 , -0.5151373 ,  0.50197023, -0.48893154,\n",
              "         0.4521682 , -0.45238972, -0.47906852, -0.5102423 ,  0.4522671 ,\n",
              "         0.44702134,  0.5147817 ,  0.4704918 , -0.4928282 ,  0.46219078],\n",
              "       dtype=float32),\n",
              " 'influencing': array([ 0.49587917,  0.468818  , -0.45158014, -0.45736322, -0.5006693 ,\n",
              "         0.53307575,  0.47429347, -0.45726994, -0.4561965 , -0.45593825,\n",
              "        -0.4890839 ,  0.5189065 ,  0.43461132,  0.4701927 , -0.51924866,\n",
              "        -0.45038956,  0.48357746,  0.4616948 , -0.48079318, -0.43424734,\n",
              "        -0.50479746,  0.4559257 ,  0.5294262 ,  0.44504318, -0.46898714,\n",
              "        -0.47805008,  0.4698805 ,  0.50160843,  0.48774475,  0.5182506 ,\n",
              "        -0.5033916 ,  0.4680072 , -0.5247725 , -0.4861861 , -0.43832913,\n",
              "        -0.47807306,  0.4906971 , -0.453791  ,  0.46555448, -0.49429286,\n",
              "         0.5171542 , -0.48532605, -0.52678245, -0.46871296,  0.4692165 ,\n",
              "         0.4527468 ,  0.48077148,  0.50947523, -0.45936564,  0.52214855],\n",
              "       dtype=float32),\n",
              " 'page': array([ 0.5516282 ,  0.49262097, -0.5353362 , -0.5177697 , -0.48285875,\n",
              "         0.5049452 ,  0.44527113, -0.44935778, -0.4522966 , -0.49003726,\n",
              "        -0.50547093,  0.5166288 ,  0.48232812,  0.5280116 , -0.51301116,\n",
              "        -0.5179649 ,  0.49247068,  0.4431884 , -0.47986692, -0.45361224,\n",
              "        -0.49234158,  0.5013247 ,  0.52143383,  0.5151473 , -0.49920708,\n",
              "        -0.50565124,  0.46458516,  0.5591662 ,  0.4779435 ,  0.49269468,\n",
              "        -0.48077908,  0.45570084, -0.5155631 , -0.47292987, -0.45736438,\n",
              "        -0.4648944 ,  0.47040313, -0.45542586,  0.47533357, -0.45731682,\n",
              "         0.4611591 , -0.5037514 , -0.4799755 , -0.47771698,  0.51421964,\n",
              "         0.5172888 ,  0.504431  ,  0.4640785 , -0.45292762,  0.44665045],\n",
              "       dtype=float32),\n",
              " 'required': array([ 0.521186  ,  0.5001751 , -0.5250414 , -0.50443166, -0.45025057,\n",
              "         0.46092188,  0.52578336, -0.52558887, -0.50438493, -0.487248  ,\n",
              "        -0.49442837,  0.5007689 ,  0.45937607,  0.49560192, -0.49518642,\n",
              "        -0.46488586,  0.4499036 ,  0.47754505, -0.5122657 , -0.4607867 ,\n",
              "        -0.45561397,  0.51922035,  0.4953153 ,  0.4394418 , -0.5112405 ,\n",
              "        -0.4827915 ,  0.5206273 ,  0.5006716 ,  0.5255517 ,  0.5033399 ,\n",
              "        -0.45540297,  0.48733118, -0.5073605 , -0.4783877 , -0.4409282 ,\n",
              "        -0.49163854,  0.53146696, -0.49894238,  0.4526256 , -0.5196929 ,\n",
              "         0.46011287, -0.45880786, -0.49257204, -0.45974815,  0.4643033 ,\n",
              "         0.47530597,  0.45033383,  0.4407205 , -0.5232299 ,  0.46337706],\n",
              "       dtype=float32),\n",
              " 'day': array([ 0.4860794 ,  0.4889289 , -0.5171701 , -0.432113  , -0.45478374,\n",
              "         0.478676  ,  0.4535272 , -0.47069198, -0.5092352 , -0.4578768 ,\n",
              "        -0.48409402,  0.4631989 ,  0.4953174 ,  0.5152663 , -0.50449055,\n",
              "        -0.51268   ,  0.47407314,  0.51950306, -0.43821105, -0.44468585,\n",
              "        -0.46942374,  0.5125156 ,  0.51967305,  0.47626242, -0.50850767,\n",
              "        -0.47974026,  0.46241802,  0.5159379 ,  0.45955545,  0.47766778,\n",
              "        -0.5139725 ,  0.4476971 , -0.4736263 , -0.4459966 , -0.5093    ,\n",
              "        -0.52319324,  0.46358392, -0.47952026,  0.4787785 , -0.5102083 ,\n",
              "         0.448474  , -0.5200893 , -0.4302776 , -0.51274645,  0.49077457,\n",
              "         0.43925396,  0.4277685 ,  0.45867115, -0.46741694,  0.4311166 ],\n",
              "       dtype=float32),\n",
              " 'assumptions': array([ 0.496268  ,  0.4283494 , -0.5067529 , -0.46054816, -0.51383114,\n",
              "         0.48663852,  0.4738665 , -0.4595738 , -0.45641983, -0.5264526 ,\n",
              "        -0.44171545,  0.47943354,  0.44243297,  0.52057695, -0.53283846,\n",
              "        -0.4832893 ,  0.5425005 ,  0.5228638 , -0.523173  , -0.42852026,\n",
              "        -0.4695172 ,  0.43933812,  0.45616224,  0.46972305, -0.49051818,\n",
              "        -0.45987418,  0.48324892,  0.51262903,  0.51688296,  0.46392953,\n",
              "        -0.46160817,  0.44961745, -0.5415609 , -0.46975455, -0.46217585,\n",
              "        -0.5127479 ,  0.5353039 , -0.5317382 ,  0.48108596, -0.44297183,\n",
              "         0.5257713 , -0.54009247, -0.5116205 , -0.51656973,  0.53355193,\n",
              "         0.4808379 ,  0.50898474,  0.53400934, -0.4686582 ,  0.5066591 ],\n",
              "       dtype=float32),\n",
              " 'therefore': array([ 0.50553966,  0.45838237, -0.48379472, -0.456732  , -0.5224853 ,\n",
              "         0.5105983 ,  0.52419436, -0.48897442, -0.46456242, -0.4736908 ,\n",
              "        -0.51092196,  0.5041417 ,  0.47731662,  0.5048466 , -0.5776646 ,\n",
              "        -0.4556188 ,  0.4930076 ,  0.47863185, -0.5075939 , -0.48719293,\n",
              "        -0.52581507,  0.4470893 ,  0.50328207,  0.4931025 , -0.47097158,\n",
              "        -0.46362805,  0.47549742,  0.50422055,  0.51039684,  0.48339105,\n",
              "        -0.5393222 ,  0.5370687 , -0.49935186, -0.47376534, -0.44504204,\n",
              "        -0.5252763 ,  0.5220234 , -0.51834947,  0.45426744, -0.515313  ,\n",
              "         0.50340384, -0.508724  , -0.49321336, -0.45773673,  0.49272093,\n",
              "         0.5092105 ,  0.4575246 ,  0.51055497, -0.5108296 ,  0.52997714],\n",
              "       dtype=float32),\n",
              " 'sold': array([ 0.5088536 ,  0.52169156, -0.5413692 , -0.51410556, -0.45853367,\n",
              "         0.47147912,  0.478071  , -0.48270738, -0.54213583, -0.48916793,\n",
              "        -0.46031496,  0.45019338,  0.4501101 ,  0.5050483 , -0.567948  ,\n",
              "        -0.46833068,  0.5109798 ,  0.47060597, -0.4574716 , -0.5013757 ,\n",
              "        -0.47285515,  0.51360554,  0.4570673 ,  0.4782439 , -0.53804815,\n",
              "        -0.46766093,  0.46558812,  0.5274724 ,  0.44622126,  0.5107885 ,\n",
              "        -0.49503908,  0.47043142, -0.55853873, -0.52983236, -0.461681  ,\n",
              "        -0.4747905 ,  0.46992755, -0.51205   ,  0.4626117 , -0.48558256,\n",
              "         0.51577395, -0.50609744, -0.48434836, -0.50080913,  0.4919261 ,\n",
              "         0.46785912,  0.49928552,  0.50789917, -0.5382509 ,  0.48750556],\n",
              "       dtype=float32),\n",
              " 'among': array([ 0.544881  ,  0.47726998, -0.4472367 , -0.5090186 , -0.52189285,\n",
              "         0.49501368,  0.46168047, -0.52711904, -0.46489766, -0.4375371 ,\n",
              "        -0.441141  ,  0.5139003 ,  0.4739607 ,  0.47161376, -0.5430709 ,\n",
              "        -0.44152772,  0.5172759 ,  0.49394205, -0.48796737, -0.43853906,\n",
              "        -0.50690573,  0.49101445,  0.4667639 ,  0.5198469 , -0.43382433,\n",
              "        -0.5199573 ,  0.44092956,  0.5338386 ,  0.4653985 ,  0.50312835,\n",
              "        -0.48774365,  0.46417803, -0.46848068, -0.46694058, -0.4618134 ,\n",
              "        -0.47330856,  0.4882445 , -0.49568596,  0.5009674 , -0.46157888,\n",
              "         0.49552414, -0.5212801 , -0.457707  , -0.52217704,  0.47095257,\n",
              "         0.49631226,  0.4575398 ,  0.4706802 , -0.45444733,  0.4961022 ],\n",
              "       dtype=float32),\n",
              " 'short-term': array([ 0.47852743,  0.42649367, -0.50655127, -0.43990067, -0.46214455,\n",
              "         0.52584285,  0.48392117, -0.49167544, -0.49547702, -0.5010238 ,\n",
              "        -0.50062734,  0.43368912,  0.4487025 ,  0.51317424, -0.5093507 ,\n",
              "        -0.51830447,  0.44374645,  0.51939815, -0.45082745, -0.50727075,\n",
              "        -0.5098941 ,  0.4517153 ,  0.50013983,  0.50055665, -0.4674802 ,\n",
              "        -0.465678  ,  0.5091139 ,  0.4560882 ,  0.5085952 ,  0.48864603,\n",
              "        -0.4840198 ,  0.4604942 , -0.5090011 , -0.51602334, -0.43781555,\n",
              "        -0.52107656,  0.4634956 , -0.49598283,  0.50928426, -0.49266064,\n",
              "         0.51851994, -0.50772303, -0.5112771 , -0.5022655 ,  0.46661848,\n",
              "         0.45925298,  0.43065405,  0.4859577 , -0.44548216,  0.4448583 ],\n",
              "       dtype=float32),\n",
              " 'unless': array([ 0.4869908 ,  0.4862099 , -0.4991656 , -0.43609688, -0.49370593,\n",
              "         0.47720802,  0.48972592, -0.48724896, -0.48348606, -0.45025182,\n",
              "        -0.4706782 ,  0.4745441 ,  0.4648446 ,  0.4769525 , -0.49401334,\n",
              "        -0.46753287,  0.49256557,  0.51996213, -0.48565143, -0.47335863,\n",
              "        -0.51380587,  0.4775753 ,  0.51842725,  0.50133985, -0.47909707,\n",
              "        -0.4562698 ,  0.5142518 ,  0.45972708,  0.47717708,  0.5131767 ,\n",
              "        -0.5205591 ,  0.49864978, -0.5176369 , -0.47789016, -0.4274743 ,\n",
              "        -0.51216847,  0.48233423, -0.49251008,  0.5123011 , -0.50157934,\n",
              "         0.47543392, -0.46587765, -0.46327883, -0.47882116,  0.47515163,\n",
              "         0.453399  ,  0.50613564,  0.45930153, -0.453747  ,  0.43656012],\n",
              "       dtype=float32),\n",
              " 'active': array([ 0.4799588 ,  0.5150241 , -0.48397553, -0.43734568, -0.5114689 ,\n",
              "         0.5013223 ,  0.49514577, -0.46186924, -0.50731885, -0.5014137 ,\n",
              "        -0.4666176 ,  0.45870674,  0.50608647,  0.5004064 , -0.5615236 ,\n",
              "        -0.5108554 ,  0.48732698,  0.45587122, -0.4928013 , -0.47618723,\n",
              "        -0.44947767,  0.44276446,  0.4885204 ,  0.4815204 , -0.493932  ,\n",
              "        -0.44430882,  0.47530288,  0.51483285,  0.50681615,  0.47369242,\n",
              "        -0.500289  ,  0.49852175, -0.45876306, -0.50280833, -0.45539722,\n",
              "        -0.49919507,  0.49183944, -0.44295   ,  0.4630566 , -0.47494057,\n",
              "         0.48910198, -0.53263086, -0.48681456, -0.4813913 ,  0.5063845 ,\n",
              "         0.45143166,  0.49817485,  0.5102665 , -0.491805  ,  0.4651395 ],\n",
              "       dtype=float32),\n",
              " 'fall': array([ 0.5334592 ,  0.501528  , -0.4597417 , -0.48311964, -0.5289702 ,\n",
              "         0.47707134,  0.49669954, -0.506101  , -0.47042075, -0.4562719 ,\n",
              "        -0.48423997,  0.48941615,  0.43352926,  0.496813  , -0.5315244 ,\n",
              "        -0.4284006 ,  0.49390388,  0.4378509 , -0.42610773, -0.48037142,\n",
              "        -0.4739591 ,  0.42967397,  0.47368902,  0.4961094 , -0.5110936 ,\n",
              "        -0.49196282,  0.4428852 ,  0.49615836,  0.46842438,  0.4532056 ,\n",
              "        -0.49128598,  0.4884634 , -0.4726671 , -0.45618364, -0.5061104 ,\n",
              "        -0.5251671 ,  0.48153573, -0.46147949,  0.44162747, -0.44994202,\n",
              "         0.502167  , -0.4363452 , -0.42847168, -0.51019406,  0.4639934 ,\n",
              "         0.50006783,  0.48039204,  0.48327643, -0.50750536,  0.4901021 ],\n",
              "       dtype=float32),\n",
              " 'decrease': array([ 0.5513068 ,  0.48843533, -0.45298523, -0.4354324 , -0.5272796 ,\n",
              "         0.47241393,  0.50722486, -0.46065587, -0.49675775, -0.5182619 ,\n",
              "        -0.45241883,  0.5246085 ,  0.4885531 ,  0.47779453, -0.52959794,\n",
              "        -0.46334997,  0.49182174,  0.47816283, -0.48210523, -0.5166861 ,\n",
              "        -0.4530059 ,  0.4815936 ,  0.5155145 ,  0.45119116, -0.51551604,\n",
              "        -0.44564515,  0.46103996,  0.51149106,  0.51563907,  0.47042495,\n",
              "        -0.52310836,  0.44963557, -0.53157616, -0.45974082, -0.450734  ,\n",
              "        -0.49605066,  0.49380773, -0.4701718 ,  0.49449113, -0.46766147,\n",
              "         0.51757634, -0.52074456, -0.4789456 , -0.5144505 ,  0.48246878,\n",
              "         0.5176877 ,  0.4760869 ,  0.44125292, -0.45708552,  0.49124792],\n",
              "       dtype=float32),\n",
              " 'pays': array([ 0.47163448,  0.5222693 , -0.509223  , -0.5129914 , -0.5059303 ,\n",
              "         0.5321464 ,  0.4716513 , -0.45629078, -0.47706443, -0.4450988 ,\n",
              "        -0.48248348,  0.52431023,  0.45032227,  0.44889465, -0.5714656 ,\n",
              "        -0.53868204,  0.4590749 ,  0.46749508, -0.48646632, -0.4616102 ,\n",
              "        -0.5091023 ,  0.5218165 ,  0.48269528,  0.5182663 , -0.48561862,\n",
              "        -0.47898528,  0.48516122,  0.5481837 ,  0.44647682,  0.46953532,\n",
              "        -0.47735754,  0.50784844, -0.4790422 , -0.48625702, -0.45043725,\n",
              "        -0.45609367,  0.51228666, -0.47660902,  0.54088104, -0.46695125,\n",
              "         0.49997532, -0.5146093 , -0.46914417, -0.4840088 ,  0.5221712 ,\n",
              "         0.49959278,  0.48338318,  0.46692723, -0.52948195,  0.4845357 ],\n",
              "       dtype=float32),\n",
              " 'offering': array([ 0.5438142 ,  0.5163143 , -0.5029655 , -0.49446318, -0.5178995 ,\n",
              "         0.47982228,  0.5128042 , -0.53386855, -0.43092537, -0.46037117,\n",
              "        -0.52576727,  0.48388368,  0.4677087 ,  0.4601971 , -0.51536584,\n",
              "        -0.5239347 ,  0.44451022,  0.45152646, -0.5016508 , -0.45290118,\n",
              "        -0.4544187 ,  0.4811898 ,  0.5055518 ,  0.48612872, -0.4979849 ,\n",
              "        -0.47318804,  0.48131484,  0.47431567,  0.46799743,  0.48398516,\n",
              "        -0.4437627 ,  0.4731387 , -0.48921   , -0.5296014 , -0.5133889 ,\n",
              "        -0.51568085,  0.4724276 , -0.50477266,  0.4765429 , -0.47380853,\n",
              "         0.45675656, -0.5053804 , -0.48348612, -0.47737935,  0.48448777,\n",
              "         0.4864066 ,  0.47827286,  0.5048067 , -0.48929992,  0.4706421 ],\n",
              "       dtype=float32),\n",
              " 'small': array([ 0.5067384 ,  0.50694066, -0.4623533 , -0.4436344 , -0.4635242 ,\n",
              "         0.4856196 ,  0.4530729 , -0.5164242 , -0.44957796, -0.5021114 ,\n",
              "        -0.50222737,  0.51984966,  0.5227988 ,  0.49961   , -0.55804753,\n",
              "        -0.45375177,  0.52706474,  0.46373367, -0.45881763, -0.46392316,\n",
              "        -0.44917962,  0.45386878,  0.53558487,  0.4497606 , -0.4922391 ,\n",
              "        -0.4444588 ,  0.440746  ,  0.5365507 ,  0.49187246,  0.46819195,\n",
              "        -0.5231807 ,  0.50312746, -0.49607223, -0.52860254, -0.46363634,\n",
              "        -0.47443503,  0.51985943, -0.5182559 ,  0.4837417 , -0.49508643,\n",
              "         0.52280897, -0.49419504, -0.47692835, -0.5198849 ,  0.52336645,\n",
              "         0.49466556,  0.46255094,  0.529922  , -0.44905347,  0.5175849 ],\n",
              "       dtype=float32),\n",
              " 'waiver': array([ 0.5162848 ,  0.46156725, -0.4624504 , -0.44976124, -0.455963  ,\n",
              "         0.45619026,  0.47154462, -0.51332766, -0.4914197 , -0.46796876,\n",
              "        -0.43232155,  0.44698718,  0.50655186,  0.4700034 , -0.5492208 ,\n",
              "        -0.49040186,  0.5190645 ,  0.47018892, -0.4953955 , -0.4329454 ,\n",
              "        -0.50693107,  0.49377012,  0.47240245,  0.46085787, -0.44518986,\n",
              "        -0.43347868,  0.46035224,  0.49426186,  0.47904688,  0.4415496 ,\n",
              "        -0.45970884,  0.45990038, -0.48131815, -0.5074985 , -0.49491918,\n",
              "        -0.47783592,  0.48885363, -0.45532727,  0.4714238 , -0.50226533,\n",
              "         0.4635074 , -0.47672877, -0.5036464 , -0.4317687 ,  0.5007838 ,\n",
              "         0.4692722 ,  0.49346215,  0.43870124, -0.45005327,  0.48764515],\n",
              "       dtype=float32),\n",
              " 'designed': array([ 0.5181044 ,  0.5145883 , -0.4985988 , -0.4881981 , -0.49227422,\n",
              "         0.45579305,  0.52075505, -0.43939131, -0.45496717, -0.4302649 ,\n",
              "        -0.47979718,  0.5040628 ,  0.4610814 ,  0.46035188, -0.5421674 ,\n",
              "        -0.5149365 ,  0.5032012 ,  0.4426367 , -0.44657204, -0.4348814 ,\n",
              "        -0.52576387,  0.5200819 ,  0.45218384,  0.47154227, -0.455584  ,\n",
              "        -0.5158936 ,  0.51873326,  0.51614773,  0.5111534 ,  0.51474035,\n",
              "        -0.47828773,  0.5018826 , -0.48336262, -0.50817037, -0.49456733,\n",
              "        -0.49934915,  0.46551716, -0.45116723,  0.48489702, -0.47096667,\n",
              "         0.49854434, -0.50460875, -0.5030415 , -0.5169715 ,  0.45846188,\n",
              "         0.4539157 ,  0.48074466,  0.4401873 , -0.4383302 ,  0.448287  ],\n",
              "       dtype=float32),\n",
              " 'historical': array([ 0.5236068 ,  0.4311514 , -0.4678278 , -0.47911015, -0.49873435,\n",
              "         0.49795592,  0.4774801 , -0.5296777 , -0.44305915, -0.46766427,\n",
              "        -0.52246344,  0.46543422,  0.4898831 ,  0.44703135, -0.4952171 ,\n",
              "        -0.52238315,  0.4469775 ,  0.5007799 , -0.49084222, -0.5022554 ,\n",
              "        -0.49673265,  0.5153102 ,  0.52661645,  0.4846688 , -0.48067024,\n",
              "        -0.51292807,  0.43632725,  0.540943  ,  0.4692545 ,  0.5000204 ,\n",
              "        -0.53654546,  0.4552381 , -0.46559462, -0.46423098, -0.51617867,\n",
              "        -0.51354647,  0.506967  , -0.51135373,  0.4418619 , -0.49967235,\n",
              "         0.46643788, -0.47023922, -0.45068717, -0.47244075,  0.48807752,\n",
              "         0.45837057,  0.5168731 ,  0.45541131, -0.5193469 ,  0.44017875],\n",
              "       dtype=float32),\n",
              " 'reimbursement': array([ 0.44652697,  0.49670714, -0.44125018, -0.48987937, -0.46363294,\n",
              "         0.43831128,  0.50339615, -0.48531204, -0.477557  , -0.5074269 ,\n",
              "        -0.45523462,  0.50074375,  0.4968909 ,  0.4886922 , -0.52275074,\n",
              "        -0.43617684,  0.50348014,  0.48725432, -0.50695014, -0.43928427,\n",
              "        -0.47928458,  0.49526083,  0.49271196,  0.49402505, -0.4476703 ,\n",
              "        -0.46577096,  0.50988144,  0.48079357,  0.5010506 ,  0.49292356,\n",
              "        -0.49466914,  0.46211532, -0.4926726 , -0.46448582, -0.42550784,\n",
              "        -0.487299  ,  0.50394905, -0.46741655,  0.45886147, -0.45213613,\n",
              "         0.4773404 , -0.50419384, -0.45119298, -0.49105155,  0.44587183,\n",
              "         0.43007284,  0.45318735,  0.5065246 , -0.5008352 ,  0.48912287],\n",
              "       dtype=float32),\n",
              " 'proceeds': array([ 0.51611453,  0.4501265 , -0.4481727 , -0.4631948 , -0.5317387 ,\n",
              "         0.5185765 ,  0.48924565, -0.46454763, -0.50807077, -0.4361599 ,\n",
              "        -0.47811964,  0.49702597,  0.44852963,  0.43949714, -0.5040358 ,\n",
              "        -0.46303907,  0.5142013 ,  0.4996321 , -0.49580598, -0.47410882,\n",
              "        -0.431264  ,  0.45075917,  0.51736104,  0.4697172 , -0.4479255 ,\n",
              "        -0.4761759 ,  0.50583184,  0.48713347,  0.49045792,  0.47582716,\n",
              "        -0.46894333,  0.47263145, -0.51940066, -0.4955532 , -0.4631753 ,\n",
              "        -0.5140145 ,  0.5280871 , -0.47738102,  0.47659507, -0.49400154,\n",
              "         0.52592826, -0.5103461 , -0.46955624, -0.5111479 ,  0.51672965,\n",
              "         0.4798577 ,  0.46408293,  0.50344133, -0.51010996,  0.4866159 ],\n",
              "       dtype=float32),\n",
              " 'longer': array([ 0.5315639 ,  0.4633494 , -0.45614564, -0.49149233, -0.46335962,\n",
              "         0.50969297,  0.46724227, -0.43758503, -0.52183497, -0.5033537 ,\n",
              "        -0.4841218 ,  0.5251815 ,  0.47214705,  0.43364292, -0.54709285,\n",
              "        -0.49085638,  0.48564366,  0.4357726 , -0.4703048 , -0.45994407,\n",
              "        -0.4767325 ,  0.4585567 ,  0.45172876,  0.5075778 , -0.48176146,\n",
              "        -0.45024377,  0.50705904,  0.5428488 ,  0.49874204,  0.4284223 ,\n",
              "        -0.5231545 ,  0.46460873, -0.47571313, -0.4522668 , -0.4924643 ,\n",
              "        -0.44846585,  0.505636  , -0.5069734 ,  0.51698864, -0.48215103,\n",
              "         0.5114256 , -0.5149937 , -0.48156276, -0.5125546 ,  0.5179398 ,\n",
              "         0.49893576,  0.470691  ,  0.43520364, -0.46514213,  0.4716148 ],\n",
              "       dtype=float32),\n",
              " 'able': array([ 0.5269564 ,  0.45753324, -0.49635047, -0.4797717 , -0.45737782,\n",
              "         0.48412877,  0.4591761 , -0.5227819 , -0.49403596, -0.50843567,\n",
              "        -0.5126613 ,  0.5095048 ,  0.48441163,  0.5101635 , -0.55920196,\n",
              "        -0.4558757 ,  0.44231802,  0.5046693 , -0.4402119 , -0.50806993,\n",
              "        -0.52593136,  0.45638895,  0.5155388 ,  0.47279984, -0.48125258,\n",
              "        -0.47400326,  0.4656448 ,  0.48413908,  0.47244263,  0.48987222,\n",
              "        -0.5292213 ,  0.5168167 , -0.5155426 , -0.4974417 , -0.47627893,\n",
              "        -0.5121582 ,  0.47877592, -0.46203342,  0.47391307, -0.52610636,\n",
              "         0.5326138 , -0.4828065 , -0.46051255, -0.43862027,  0.45016274,\n",
              "         0.47172126,  0.47462338,  0.4456477 , -0.4701679 ,  0.5157069 ],\n",
              "       dtype=float32),\n",
              " 'taxed': array([ 0.51425755,  0.45851424, -0.48767525, -0.48565573, -0.5175722 ,\n",
              "         0.4812047 ,  0.49578914, -0.4671804 , -0.4766778 , -0.464723  ,\n",
              "        -0.44890827,  0.47730967,  0.5069587 ,  0.47423872, -0.50067663,\n",
              "        -0.49918956,  0.45598605,  0.45703948, -0.48612434, -0.44168508,\n",
              "        -0.4608196 ,  0.45350194,  0.4620524 ,  0.49742717, -0.49548098,\n",
              "        -0.45357472,  0.46210653,  0.52576995,  0.450032  ,  0.50331175,\n",
              "        -0.48721424,  0.5035113 , -0.5138982 , -0.44278032, -0.45843655,\n",
              "        -0.52246207,  0.5311629 , -0.4868047 ,  0.4385148 , -0.49768576,\n",
              "         0.4420006 , -0.5248135 , -0.46495453, -0.46156722,  0.45649073,\n",
              "         0.48616213,  0.47454247,  0.445595  , -0.44502157,  0.4901124 ],\n",
              "       dtype=float32),\n",
              " 'differently': array([ 0.48008922,  0.4917686 , -0.4496156 , -0.47821847, -0.4714367 ,\n",
              "         0.48519018,  0.46019104, -0.49573064, -0.47910082, -0.5177587 ,\n",
              "        -0.51643556,  0.50667006,  0.50263464,  0.44540337, -0.5145834 ,\n",
              "        -0.46301666,  0.52534   ,  0.5091869 , -0.43830898, -0.43478745,\n",
              "        -0.46244308,  0.4556775 ,  0.51275456,  0.45033124, -0.5056843 ,\n",
              "        -0.50883275,  0.49344897,  0.4767934 ,  0.45751047,  0.502972  ,\n",
              "        -0.49483114,  0.45660108, -0.47640035, -0.4708782 , -0.4698022 ,\n",
              "        -0.46509907,  0.49343807, -0.49072614,  0.47580945, -0.49330232,\n",
              "         0.45871672, -0.5115374 , -0.44938022, -0.50534797,  0.45553368,\n",
              "         0.49213448,  0.4978297 ,  0.51921666, -0.52420974,  0.4558432 ],\n",
              "       dtype=float32),\n",
              " 'lead': array([ 0.54271173,  0.48405647, -0.50818694, -0.5283981 , -0.47468254,\n",
              "         0.5012823 ,  0.46402732, -0.4585407 , -0.4430312 , -0.48945394,\n",
              "        -0.4514723 ,  0.44854286,  0.5027964 ,  0.4443965 , -0.50029033,\n",
              "        -0.44686165,  0.52935076,  0.448806  , -0.45598435, -0.52500516,\n",
              "        -0.46713287,  0.51050794,  0.53566885,  0.46295372, -0.50453055,\n",
              "        -0.5175403 ,  0.4764497 ,  0.5018043 ,  0.49044293,  0.48801517,\n",
              "        -0.49797094,  0.51761734, -0.47028732, -0.49194005, -0.47000787,\n",
              "        -0.48486048,  0.5053727 , -0.4829477 ,  0.5155813 , -0.48717725,\n",
              "         0.49983603, -0.5328217 , -0.5186688 , -0.4813032 ,  0.4815444 ,\n",
              "         0.43875262,  0.4823587 ,  0.4518994 , -0.52431107,  0.44861594],\n",
              "       dtype=float32),\n",
              " 'open': array([ 0.48195115,  0.49687597, -0.4935861 , -0.4767182 , -0.53600717,\n",
              "         0.4591497 ,  0.520821  , -0.516322  , -0.5281493 , -0.44482905,\n",
              "        -0.4899084 ,  0.46635848,  0.46828967,  0.5216592 , -0.5111022 ,\n",
              "        -0.5159486 ,  0.4809277 ,  0.46085283, -0.4603191 , -0.50167173,\n",
              "        -0.5183967 ,  0.5305058 ,  0.5248746 ,  0.5220127 , -0.44853425,\n",
              "        -0.4475332 ,  0.5040538 ,  0.4884581 ,  0.5130666 ,  0.47102615,\n",
              "        -0.4607693 ,  0.52439517, -0.470784  , -0.4863173 , -0.44612947,\n",
              "        -0.449114  ,  0.5422369 , -0.46324462,  0.471565  , -0.48323327,\n",
              "         0.53112274, -0.4674008 , -0.45536596, -0.45044366,  0.48125827,\n",
              "         0.4628083 ,  0.4564471 ,  0.48369014, -0.5234259 ,  0.5327007 ],\n",
              "       dtype=float32),\n",
              " 'eligible': array([ 0.49652013,  0.47807986, -0.45507807, -0.4678371 , -0.48124355,\n",
              "         0.5210075 ,  0.50395083, -0.47960365, -0.4606176 , -0.49542618,\n",
              "        -0.4777427 ,  0.45161363,  0.46837184,  0.449151  , -0.51414704,\n",
              "        -0.45385602,  0.52935153,  0.4817652 , -0.5196135 , -0.51222473,\n",
              "        -0.50836277,  0.44535112,  0.45648727,  0.47331503, -0.45733142,\n",
              "        -0.5115538 ,  0.49106094,  0.48443708,  0.47471836,  0.51670134,\n",
              "        -0.5227795 ,  0.45294327, -0.50867766, -0.52892137, -0.44517314,\n",
              "        -0.5383415 ,  0.5043431 , -0.49378484,  0.52086985, -0.5366922 ,\n",
              "         0.5242402 , -0.4511668 , -0.43848652, -0.44815192,  0.47223806,\n",
              "         0.51287603,  0.5178955 ,  0.47936144, -0.51788837,  0.48015988],\n",
              "       dtype=float32),\n",
              " 'senior': array([ 0.5222284 ,  0.48047534, -0.4881568 , -0.5342595 , -0.4509403 ,\n",
              "         0.5057591 ,  0.48655093, -0.44124746, -0.51947457, -0.46861786,\n",
              "        -0.48656657,  0.47174057,  0.50034773,  0.49840468, -0.48058712,\n",
              "        -0.43451777,  0.460086  ,  0.49942404, -0.44582152, -0.4382705 ,\n",
              "        -0.4465658 ,  0.4788987 ,  0.44767678,  0.4930071 , -0.47110245,\n",
              "        -0.44335568,  0.49587688,  0.49475533,  0.4561867 ,  0.5130069 ,\n",
              "        -0.469676  ,  0.44587833, -0.46250448, -0.5139441 , -0.44736657,\n",
              "        -0.51531917,  0.51637614, -0.44519714,  0.45188588, -0.5137528 ,\n",
              "         0.5073371 , -0.48818842, -0.5080698 , -0.4756804 ,  0.50743884,\n",
              "         0.52205014,  0.48265845,  0.45926708, -0.45381296,  0.50875336],\n",
              "       dtype=float32),\n",
              " 'professional': array([ 0.49879208,  0.50310767, -0.52808654, -0.48533532, -0.49115133,\n",
              "         0.4975917 ,  0.49134606, -0.44882277, -0.49888325, -0.51507574,\n",
              "        -0.4952708 ,  0.5223639 ,  0.48203266,  0.5225753 , -0.508101  ,\n",
              "        -0.46507308,  0.47378182,  0.46361393, -0.46305823, -0.5241648 ,\n",
              "        -0.46086925,  0.51096785,  0.5364441 ,  0.52220505, -0.48057052,\n",
              "        -0.47448334,  0.4499325 ,  0.45568025,  0.448078  ,  0.4828671 ,\n",
              "        -0.49236643,  0.44706753, -0.52089024, -0.5349642 , -0.4848771 ,\n",
              "        -0.44563892,  0.4717627 , -0.53215736,  0.49975508, -0.5219611 ,\n",
              "         0.4576202 , -0.53458595, -0.5139337 , -0.46627218,  0.4550688 ,\n",
              "         0.4909802 ,  0.48993745,  0.51326984, -0.48404935,  0.45944905],\n",
              "       dtype=float32),\n",
              " 'full': array([ 0.5482518 ,  0.47043633, -0.47113717, -0.46679264, -0.5229599 ,\n",
              "         0.44991294,  0.4654721 , -0.44620648, -0.47262335, -0.51679456,\n",
              "        -0.49217308,  0.5115202 ,  0.5163617 ,  0.50493073, -0.5124795 ,\n",
              "        -0.4442758 ,  0.4743495 ,  0.47365108, -0.43675467, -0.4485869 ,\n",
              "        -0.44354576,  0.4948867 ,  0.52778494,  0.43996143, -0.51148856,\n",
              "        -0.43940526,  0.47730643,  0.5114868 ,  0.44220772,  0.48167223,\n",
              "        -0.49483868,  0.48342663, -0.4559361 , -0.47176316, -0.51869625,\n",
              "        -0.5187939 ,  0.5331641 , -0.49028623,  0.45551777, -0.48976567,\n",
              "         0.50035095, -0.45163077, -0.46971425, -0.5229452 ,  0.52478695,\n",
              "         0.46179375,  0.44430968,  0.45870766, -0.4871062 ,  0.51751256],\n",
              "       dtype=float32),\n",
              " 'values': array([ 0.49513397,  0.47376847, -0.47913697, -0.4561866 , -0.45400423,\n",
              "         0.4775303 ,  0.4831531 , -0.48552316, -0.5210255 , -0.45712018,\n",
              "        -0.4994477 ,  0.52736884,  0.45135015,  0.51578104, -0.5436517 ,\n",
              "        -0.47838017,  0.5125748 ,  0.4453079 , -0.45952943, -0.44632822,\n",
              "        -0.44201005,  0.46475062,  0.48799527,  0.45542327, -0.50488937,\n",
              "        -0.49328956,  0.46064082,  0.5527984 ,  0.49408475,  0.4707737 ,\n",
              "        -0.4754648 ,  0.47004268, -0.5024987 , -0.5023447 , -0.4365531 ,\n",
              "        -0.4729269 ,  0.5241216 , -0.46886683,  0.52563465, -0.5234337 ,\n",
              "         0.47406676, -0.45016563, -0.49162698, -0.49533743,  0.4887102 ,\n",
              "         0.46394283,  0.45584142,  0.4842866 , -0.44845718,  0.4907279 ],\n",
              "       dtype=float32),\n",
              " 'potentially': array([ 0.5427685 ,  0.4371728 , -0.51587   , -0.45806473, -0.48408893,\n",
              "         0.49497774,  0.4761493 , -0.5143379 , -0.49733716, -0.4619667 ,\n",
              "        -0.47868034,  0.4973277 ,  0.4600783 ,  0.48431975, -0.4837805 ,\n",
              "        -0.4496044 ,  0.46482152,  0.474852  , -0.48218653, -0.5012377 ,\n",
              "        -0.45846725,  0.48835012,  0.4581929 ,  0.44302836, -0.5115671 ,\n",
              "        -0.46228188,  0.52155757,  0.49811718,  0.45505267,  0.4988545 ,\n",
              "        -0.54020524,  0.47428632, -0.5411957 , -0.48054677, -0.5086371 ,\n",
              "        -0.46716702,  0.48150328, -0.5337898 ,  0.4399489 , -0.4787461 ,\n",
              "         0.494073  , -0.48366535, -0.47152042, -0.46475717,  0.48155344,\n",
              "         0.45615712,  0.52317464,  0.4670554 , -0.47487384,  0.46447015],\n",
              "       dtype=float32),\n",
              " 'receive': array([ 0.47430915,  0.4610579 , -0.44726408, -0.45876586, -0.46676114,\n",
              "         0.46854907,  0.4596659 , -0.508077  , -0.45963138, -0.43038234,\n",
              "        -0.521388  ,  0.4670485 ,  0.45493847,  0.49266672, -0.505919  ,\n",
              "        -0.5114376 ,  0.4590917 ,  0.51841044, -0.51968837, -0.46564603,\n",
              "        -0.51587176,  0.5167929 ,  0.50167406,  0.48601413, -0.48214123,\n",
              "        -0.4630594 ,  0.4855189 ,  0.5301537 ,  0.51853275,  0.50053567,\n",
              "        -0.5218669 ,  0.51880044, -0.540701  , -0.49512467, -0.43056595,\n",
              "        -0.46302536,  0.50653607, -0.48806667,  0.49670124, -0.46289164,\n",
              "         0.527344  , -0.5012911 , -0.51291513, -0.508934  ,  0.5015743 ,\n",
              "         0.49482426,  0.44519943,  0.4719307 , -0.49324113,  0.51302373],\n",
              "       dtype=float32),\n",
              " 'receipts': array([ 0.50796366,  0.48735982, -0.4990657 , -0.4434873 , -0.46413526,\n",
              "         0.4844309 ,  0.4441317 , -0.45720586, -0.48326963, -0.46785864,\n",
              "        -0.4586541 ,  0.5304838 ,  0.47330686,  0.47114742, -0.56340224,\n",
              "        -0.49443197,  0.47553402,  0.49934053, -0.51531076, -0.48383185,\n",
              "        -0.465213  ,  0.449814  ,  0.50472224,  0.48246622, -0.45574045,\n",
              "        -0.46125075,  0.49318686,  0.5419115 ,  0.5261785 ,  0.43288952,\n",
              "        -0.46346033,  0.48379168, -0.50360847, -0.46832567, -0.50534725,\n",
              "        -0.44944537,  0.50167596, -0.4825252 ,  0.44224012, -0.52525395,\n",
              "         0.46105275, -0.51209176, -0.46952212, -0.5151374 ,  0.48623592,\n",
              "         0.5186298 ,  0.51828974,  0.51735914, -0.5088123 ,  0.49248326],\n",
              "       dtype=float32),\n",
              " 'normally': array([ 0.463591  ,  0.48370317, -0.52331614, -0.4660177 , -0.49411726,\n",
              "         0.4699422 ,  0.51881856, -0.46117562, -0.48875928, -0.4928024 ,\n",
              "        -0.52565193,  0.4733826 ,  0.49370146,  0.5300136 , -0.50273246,\n",
              "        -0.44355977,  0.5231687 ,  0.5028062 , -0.46162045, -0.4872741 ,\n",
              "        -0.506195  ,  0.514182  ,  0.51000094,  0.52537835, -0.4508495 ,\n",
              "        -0.45944348,  0.47481528,  0.550554  ,  0.47715345,  0.53098154,\n",
              "        -0.49309224,  0.4708157 , -0.49736902, -0.48650765, -0.51149476,\n",
              "        -0.4877097 ,  0.45032418, -0.46021995,  0.44530177, -0.49075916,\n",
              "         0.44787148, -0.53795654, -0.47751296, -0.48032546,  0.4759888 ,\n",
              "         0.48189706,  0.48844966,  0.53880227, -0.47592437,  0.5053107 ],\n",
              "       dtype=float32),\n",
              " 'social': array([ 0.502226  ,  0.44380566, -0.4484263 , -0.46078068, -0.48887002,\n",
              "         0.44458786,  0.49575058, -0.46685675, -0.49932542, -0.5020598 ,\n",
              "        -0.4853717 ,  0.5230264 ,  0.5077077 ,  0.49368656, -0.5103737 ,\n",
              "        -0.44015735,  0.47036666,  0.5066878 , -0.5052232 , -0.5146468 ,\n",
              "        -0.4388462 ,  0.45657468,  0.49028307,  0.5044636 , -0.48106885,\n",
              "        -0.49018675,  0.5083866 ,  0.5029223 ,  0.46787488,  0.43849364,\n",
              "        -0.48301703,  0.5086551 , -0.4899777 , -0.5296848 , -0.50273484,\n",
              "        -0.45916802,  0.45791113, -0.51052177,  0.5108307 , -0.48166025,\n",
              "         0.50021344, -0.5232318 , -0.50080854, -0.46438852,  0.476743  ,\n",
              "         0.49485832,  0.44316992,  0.46648055, -0.5146605 ,  0.5187117 ],\n",
              "       dtype=float32),\n",
              " 'currencies': array([ 0.5578191 ,  0.507087  , -0.45761877, -0.4687147 , -0.5042236 ,\n",
              "         0.47263303,  0.51344556, -0.47784168, -0.46075624, -0.51337206,\n",
              "        -0.44379902,  0.5350115 ,  0.4583941 ,  0.4858735 , -0.57046777,\n",
              "        -0.45307404,  0.5200517 ,  0.4888687 , -0.44748574, -0.4872098 ,\n",
              "        -0.46901834,  0.47565633,  0.54823714,  0.4623325 , -0.49729002,\n",
              "        -0.5319679 ,  0.47798026,  0.5667854 ,  0.5085241 ,  0.47931483,\n",
              "        -0.4801542 ,  0.46486545, -0.50263065, -0.49063823, -0.46346653,\n",
              "        -0.53004956,  0.5277331 , -0.51531917,  0.45927155, -0.5481096 ,\n",
              "         0.5286892 , -0.52979904, -0.50341505, -0.46852493,  0.48138022,\n",
              "         0.46818435,  0.47714183,  0.46240643, -0.44985256,  0.4631401 ],\n",
              "       dtype=float32),\n",
              " 'capitalization': array([ 0.5337206 ,  0.50451505, -0.50124556, -0.49958837, -0.4578072 ,\n",
              "         0.5432164 ,  0.46460846, -0.5209945 , -0.4810345 , -0.49799484,\n",
              "        -0.48998308,  0.43725184,  0.47753504,  0.48968205, -0.5253611 ,\n",
              "        -0.44228294,  0.5039036 ,  0.4697737 , -0.44444317, -0.47131628,\n",
              "        -0.45036426,  0.50458646,  0.5093516 ,  0.43285614, -0.4397517 ,\n",
              "        -0.5135658 ,  0.4898226 ,  0.482727  ,  0.4573695 ,  0.45893666,\n",
              "        -0.49828097,  0.5135639 , -0.4856723 , -0.52738696, -0.49134943,\n",
              "        -0.46036878,  0.47416857, -0.5245938 ,  0.46709138, -0.5336363 ,\n",
              "         0.49664992, -0.49010473, -0.44179952, -0.43978274,  0.4831804 ,\n",
              "         0.4814564 ,  0.45401546,  0.5027382 , -0.49022388,  0.4909172 ],\n",
              "       dtype=float32),\n",
              " 'guarantee': array([ 0.5403739 ,  0.419577  , -0.4425627 , -0.50107324, -0.45244482,\n",
              "         0.45144972,  0.5202196 , -0.43607658, -0.46024206, -0.5071578 ,\n",
              "        -0.43772143,  0.52183783,  0.5055846 ,  0.5009788 , -0.5331137 ,\n",
              "        -0.46429175,  0.4685864 ,  0.4648167 , -0.46536535, -0.5031949 ,\n",
              "        -0.50112337,  0.49485284,  0.52357733,  0.47918832, -0.4757888 ,\n",
              "        -0.43780175,  0.49120396,  0.4945509 ,  0.43741572,  0.43872207,\n",
              "        -0.5062224 ,  0.49013525, -0.5192052 , -0.4835576 , -0.4912119 ,\n",
              "        -0.44873106,  0.5287772 , -0.4986592 ,  0.50057226, -0.4627023 ,\n",
              "         0.48688   , -0.45938405, -0.4724904 , -0.45311967,  0.5149432 ,\n",
              "         0.46097523,  0.4437455 ,  0.50985324, -0.4698634 ,  0.50256205],\n",
              "       dtype=float32),\n",
              " 'p.o': array([ 0.52285904,  0.4416585 , -0.5042232 , -0.4543868 , -0.49153924,\n",
              "         0.50649965,  0.4568975 , -0.49271902, -0.50469416, -0.469477  ,\n",
              "        -0.4573318 ,  0.45367068,  0.44917527,  0.51385367, -0.46931103,\n",
              "        -0.4317666 ,  0.4835894 ,  0.4996188 , -0.48125085, -0.4413524 ,\n",
              "        -0.48237514,  0.47269988,  0.45944002,  0.49720356, -0.42765132,\n",
              "        -0.49060857,  0.42384747,  0.49018398,  0.46044046,  0.505502  ,\n",
              "        -0.49859142,  0.4416855 , -0.5148817 , -0.5104545 , -0.45243025,\n",
              "        -0.49082476,  0.4831785 , -0.47354755,  0.43798944, -0.49750406,\n",
              "         0.5025267 , -0.43342528, -0.50515276, -0.49532646,  0.44746074,\n",
              "         0.44208893,  0.4597073 ,  0.4624806 , -0.4442262 ,  0.47999197],\n",
              "       dtype=float32),\n",
              " 'box': array([ 0.45131242,  0.49004608, -0.46182132, -0.4947033 , -0.5148441 ,\n",
              "         0.46003246,  0.43147835, -0.43694738, -0.4478074 , -0.42768306,\n",
              "        -0.4955694 ,  0.49319428,  0.42425153,  0.5021181 , -0.4623798 ,\n",
              "        -0.4380222 ,  0.49670166,  0.45865557, -0.46321574, -0.420343  ,\n",
              "        -0.44639474,  0.46332476,  0.5009328 ,  0.44314465, -0.46346968,\n",
              "        -0.47339696,  0.4250019 ,  0.479892  ,  0.49203843,  0.41929996,\n",
              "        -0.50423473,  0.5078596 , -0.5105954 , -0.5061035 , -0.4909505 ,\n",
              "        -0.49494818,  0.4966584 , -0.43594754,  0.4505483 , -0.42888314,\n",
              "         0.45767283, -0.5089405 , -0.4995517 , -0.46699387,  0.45477784,\n",
              "         0.47757515,  0.49864218,  0.47391033, -0.50026363,  0.49969864],\n",
              "       dtype=float32),\n",
              " 'incur': array([ 0.54251957,  0.473712  , -0.49287528, -0.48341823, -0.52550364,\n",
              "         0.4847889 ,  0.43041408, -0.4789768 , -0.43510863, -0.43118826,\n",
              "        -0.51609623,  0.49013957,  0.503147  ,  0.4477257 , -0.5665826 ,\n",
              "        -0.4791266 ,  0.51263523,  0.459855  , -0.44809505, -0.46067652,\n",
              "        -0.5191649 ,  0.51426107,  0.5238476 ,  0.4847862 , -0.5066527 ,\n",
              "        -0.49502334,  0.51494116,  0.45746967,  0.44964784,  0.49609178,\n",
              "        -0.48677087,  0.4315484 , -0.5205257 , -0.49027118, -0.4806195 ,\n",
              "        -0.46621975,  0.45231554, -0.47398728,  0.48445755, -0.45530677,\n",
              "         0.506079  , -0.47594306, -0.49440616, -0.45282736,  0.5010089 ,\n",
              "         0.48230422,  0.5174567 ,  0.46582472, -0.4593985 ,  0.4886277 ],\n",
              "       dtype=float32),\n",
              " 'received': array([ 0.47706485,  0.502957  , -0.51286346, -0.451797  , -0.5155692 ,\n",
              "         0.507276  ,  0.5213711 , -0.45172685, -0.5227683 , -0.4995788 ,\n",
              "        -0.48808986,  0.44550124,  0.4367771 ,  0.44199157, -0.54594016,\n",
              "        -0.4405328 ,  0.45334426,  0.4851345 , -0.44322014, -0.50539076,\n",
              "        -0.5025402 ,  0.4367199 ,  0.452319  ,  0.42906895, -0.5029956 ,\n",
              "        -0.50261414,  0.4314404 ,  0.48367336,  0.4566231 ,  0.49097306,\n",
              "        -0.490547  ,  0.50684065, -0.52757406, -0.49463803, -0.46746525,\n",
              "        -0.44266868,  0.49071646, -0.47110635,  0.49740487, -0.46161115,\n",
              "         0.46556914, -0.4618779 , -0.4416161 , -0.46274266,  0.46491492,\n",
              "         0.46343696,  0.46551415,  0.49874693, -0.5085584 ,  0.47352153],\n",
              "       dtype=float32),\n",
              " 'response': array([ 0.47300956,  0.43610764, -0.48477632, -0.44530654, -0.49856037,\n",
              "         0.46403491,  0.47047776, -0.46881977, -0.51320237, -0.50204504,\n",
              "        -0.45661902,  0.47857925,  0.4522577 ,  0.49185807, -0.5197757 ,\n",
              "        -0.46382785,  0.4452204 ,  0.5277625 , -0.5160152 , -0.4533617 ,\n",
              "        -0.49609482,  0.44905862,  0.5209555 ,  0.44014472, -0.51403445,\n",
              "        -0.51271224,  0.4992297 ,  0.51416236,  0.50311327,  0.45258212,\n",
              "        -0.5009928 ,  0.49626946, -0.4680215 , -0.48664415, -0.50044173,\n",
              "        -0.4617122 ,  0.4677623 , -0.4545543 ,  0.5094156 , -0.44187382,\n",
              "         0.47133827, -0.5236015 , -0.46804938, -0.47112378,  0.5121282 ,\n",
              "         0.50065076,  0.46313605,  0.5105231 , -0.47192517,  0.43691656],\n",
              "       dtype=float32),\n",
              " 'board': array([ 0.45087868,  0.49127668, -0.42196232, -0.49817207, -0.4898029 ,\n",
              "         0.4403801 ,  0.4262648 , -0.47304606, -0.4827918 , -0.483994  ,\n",
              "        -0.42259783,  0.5106286 ,  0.44660366,  0.509636  , -0.5251093 ,\n",
              "        -0.43655953,  0.50028235,  0.48894528, -0.47721207, -0.44707495,\n",
              "        -0.4207028 ,  0.46487516,  0.44255438,  0.5014378 , -0.4434194 ,\n",
              "        -0.48602736,  0.42076677,  0.5273028 ,  0.42345315,  0.48155576,\n",
              "        -0.48256138,  0.4977185 , -0.4537534 , -0.4783873 , -0.47884542,\n",
              "        -0.45446375,  0.4668293 , -0.4451088 ,  0.4655827 , -0.5152523 ,\n",
              "         0.48283482, -0.51620007, -0.4454641 , -0.49769303,  0.43026596,\n",
              "         0.4810423 ,  0.5021922 ,  0.5025167 , -0.48449948,  0.5050401 ],\n",
              "       dtype=float32),\n",
              " 'arrangement': array([ 0.5061498 ,  0.51041806, -0.49188027, -0.46827284, -0.50564414,\n",
              "         0.4606737 ,  0.5156228 , -0.4561649 , -0.4936119 , -0.4841064 ,\n",
              "        -0.48553368,  0.44472894,  0.43915197,  0.5029274 , -0.5305497 ,\n",
              "        -0.50908417,  0.495435  ,  0.4894537 , -0.5112709 , -0.4581937 ,\n",
              "        -0.45535758,  0.44505095,  0.50488526,  0.4836494 , -0.45219862,\n",
              "        -0.4535575 ,  0.48511714,  0.45517457,  0.44705147,  0.48784488,\n",
              "        -0.5301953 ,  0.43464744, -0.44575736, -0.45709106, -0.4805841 ,\n",
              "        -0.52327657,  0.5293767 , -0.52046925,  0.44640306, -0.48569906,\n",
              "         0.48695314, -0.52691627, -0.4775387 , -0.46715093,  0.46708494,\n",
              "         0.49491954,  0.44329786,  0.476452  , -0.47958314,  0.45777464],\n",
              "       dtype=float32),\n",
              " 'allocations': array([ 0.48369947,  0.4197956 , -0.46483195, -0.42846176, -0.4657487 ,\n",
              "         0.5186933 ,  0.5170836 , -0.4614844 , -0.4712406 , -0.43367848,\n",
              "        -0.48327836,  0.44076407,  0.45753592,  0.4813751 , -0.47238436,\n",
              "        -0.49309537,  0.48732546,  0.50477797, -0.48033825, -0.4513163 ,\n",
              "        -0.49250627,  0.44567803,  0.46333042,  0.4831661 , -0.48629314,\n",
              "        -0.45841166,  0.4688964 ,  0.501102  ,  0.4762961 ,  0.4640193 ,\n",
              "        -0.45479345,  0.435581  , -0.52115685, -0.47075355, -0.48299298,\n",
              "        -0.44596115,  0.4545449 , -0.48325104,  0.4900145 , -0.461039  ,\n",
              "         0.5004399 , -0.5082282 , -0.5126395 , -0.4692202 ,  0.43254647,\n",
              "         0.5032357 ,  0.49436903,  0.4391681 , -0.51543874,  0.50157577],\n",
              "       dtype=float32),\n",
              " 'operations': array([ 0.48408127,  0.45985854, -0.46725327, -0.5005045 , -0.5042549 ,\n",
              "         0.5098526 ,  0.51860803, -0.51394504, -0.5114012 , -0.49010193,\n",
              "        -0.5155473 ,  0.46519083,  0.47297037,  0.44514525, -0.49937114,\n",
              "        -0.52663994,  0.44706473,  0.4914906 , -0.43498948, -0.4739612 ,\n",
              "        -0.4600877 ,  0.4501928 ,  0.4823128 ,  0.505204  , -0.5028057 ,\n",
              "        -0.44441625,  0.44016844,  0.50782305,  0.4671849 ,  0.45502856,\n",
              "        -0.4529909 ,  0.4659828 , -0.53827214, -0.53037   , -0.4897227 ,\n",
              "        -0.47425514,  0.49134967, -0.46536297,  0.45336682, -0.45783618,\n",
              "         0.4777647 , -0.5350408 , -0.46756396, -0.4815492 ,  0.47790304,\n",
              "         0.45205802,  0.50786245,  0.48710513, -0.46841514,  0.4987417 ],\n",
              "       dtype=float32),\n",
              " 'affecting': array([ 0.48414144,  0.50890744, -0.48852926, -0.49436414, -0.5354685 ,\n",
              "         0.5279255 ,  0.46816796, -0.5095533 , -0.47933856, -0.49932778,\n",
              "        -0.47378933,  0.5076106 ,  0.50372607,  0.43902677, -0.5130269 ,\n",
              "        -0.51621807,  0.5266057 ,  0.4554887 , -0.43762428, -0.43352592,\n",
              "        -0.44222623,  0.48750237,  0.46920106,  0.4521315 , -0.4287333 ,\n",
              "        -0.4374558 ,  0.43784305,  0.49888474,  0.5116355 ,  0.4775947 ,\n",
              "        -0.46733418,  0.47176448, -0.53266835, -0.48936185, -0.4687944 ,\n",
              "        -0.5302738 ,  0.5055041 , -0.45058563,  0.47857076, -0.47883755,\n",
              "         0.48592502, -0.44560763, -0.45506582, -0.49109936,  0.47930896,\n",
              "         0.52339566,  0.5243659 ,  0.5114301 , -0.47873586,  0.4520644 ],\n",
              "       dtype=float32),\n",
              " 'unable': array([ 0.5204755 ,  0.441242  , -0.4501013 , -0.44164136, -0.456059  ,\n",
              "         0.46090424,  0.4912618 , -0.49841323, -0.4412113 , -0.4581748 ,\n",
              "        -0.50202245,  0.47695693,  0.44172305,  0.49004215, -0.48582342,\n",
              "        -0.45590413,  0.48962972,  0.5095497 , -0.45446742, -0.4713464 ,\n",
              "        -0.5059279 ,  0.47623634,  0.4466731 ,  0.4511946 , -0.5049007 ,\n",
              "        -0.5005408 ,  0.46819475,  0.46085092,  0.5066048 ,  0.44202754,\n",
              "        -0.4979778 ,  0.43630043, -0.47322753, -0.438692  , -0.46213543,\n",
              "        -0.5001501 ,  0.5177077 , -0.43315965,  0.47984168, -0.5165608 ,\n",
              "         0.5238994 , -0.436694  , -0.46410602, -0.4407714 ,  0.47464722,\n",
              "         0.45332825,  0.43265328,  0.47433645, -0.461491  ,  0.43414342],\n",
              "       dtype=float32),\n",
              " 'go': array([ 0.47966245,  0.45553473, -0.45683858, -0.50094175, -0.46507424,\n",
              "         0.47645226,  0.5282048 , -0.5186667 , -0.46379203, -0.4545374 ,\n",
              "        -0.4549665 ,  0.4459813 ,  0.47542387,  0.47470134, -0.5379118 ,\n",
              "        -0.43861154,  0.47630534,  0.5258156 , -0.4754782 , -0.44206727,\n",
              "        -0.5194907 ,  0.49959168,  0.49700356,  0.51773196, -0.47936326,\n",
              "        -0.48044458,  0.4499362 ,  0.45675832,  0.4371683 ,  0.4360535 ,\n",
              "        -0.4876156 ,  0.48212346, -0.4654688 , -0.5068041 , -0.484607  ,\n",
              "        -0.49760264,  0.45720252, -0.51183945,  0.46451083, -0.4925084 ,\n",
              "         0.485353  , -0.45068076, -0.4715358 , -0.5253671 ,  0.4575606 ,\n",
              "         0.44335854,  0.527393  ,  0.4843381 , -0.46276096,  0.44552174],\n",
              "       dtype=float32),\n",
              " 'technology': array([ 0.51753   ,  0.4271409 , -0.5231696 , -0.5088845 , -0.47957635,\n",
              "         0.50127006,  0.43210056, -0.46451953, -0.47341797, -0.48427755,\n",
              "        -0.47993296,  0.5045282 ,  0.48602667,  0.514363  , -0.4898978 ,\n",
              "        -0.43166044,  0.47867018,  0.5169292 , -0.5029677 , -0.47487128,\n",
              "        -0.45109445,  0.50104785,  0.49996102,  0.4383897 , -0.4295073 ,\n",
              "        -0.44339854,  0.51276875,  0.50804985,  0.43780887,  0.47653985,\n",
              "        -0.46676683,  0.45445785, -0.4833864 , -0.4373299 , -0.5044742 ,\n",
              "        -0.45035854,  0.51912266, -0.4962144 ,  0.50222605, -0.44976327,\n",
              "         0.48102552, -0.44465518, -0.44752708, -0.4824319 ,  0.43463236,\n",
              "         0.4304333 ,  0.44626445,  0.5078071 , -0.47299093,  0.47111756],\n",
              "       dtype=float32),\n",
              " 'region': array([ 0.53779554,  0.47165865, -0.4904514 , -0.49734756, -0.47209075,\n",
              "         0.4519207 ,  0.46127242, -0.5065059 , -0.43863577, -0.50699615,\n",
              "        -0.5204627 ,  0.47449386,  0.53232235,  0.48833856, -0.5396383 ,\n",
              "        -0.46660796,  0.5163846 ,  0.4546953 , -0.5233927 , -0.44301072,\n",
              "        -0.4669293 ,  0.47065333,  0.47788045,  0.45847562, -0.51268387,\n",
              "        -0.50930154,  0.4540765 ,  0.47734994,  0.44584033,  0.4628946 ,\n",
              "        -0.49175262,  0.4900344 , -0.4799528 , -0.49749708, -0.4516075 ,\n",
              "        -0.53292733,  0.5030797 , -0.4457049 ,  0.5017472 , -0.46997142,\n",
              "         0.44974378, -0.5090984 , -0.44556883, -0.47331673,  0.44792932,\n",
              "         0.5145863 ,  0.4488588 ,  0.46510363, -0.46323147,  0.50076365],\n",
              "       dtype=float32),\n",
              " 'llc': array([ 0.4818654 ,  0.42614043, -0.48650265, -0.52780676, -0.47486192,\n",
              "         0.46681103,  0.52518886, -0.4947496 , -0.47990528, -0.50521386,\n",
              "        -0.43463498,  0.48801255,  0.45126784,  0.53578025, -0.5192847 ,\n",
              "        -0.44474265,  0.47519347,  0.5283604 , -0.50598943, -0.4293922 ,\n",
              "        -0.51943487,  0.461338  ,  0.44945532,  0.4939009 , -0.45737964,\n",
              "        -0.5225467 ,  0.45688114,  0.5428439 ,  0.47625238,  0.50875646,\n",
              "        -0.49789804,  0.45419022, -0.5237033 , -0.4709467 , -0.45963675,\n",
              "        -0.49353552,  0.5394669 , -0.46248296,  0.45569646, -0.5274014 ,\n",
              "         0.5000359 , -0.4652595 , -0.49254555, -0.5140056 ,  0.4927944 ,\n",
              "         0.45764908,  0.524656  ,  0.4897024 , -0.52449477,  0.47838563],\n",
              "       dtype=float32),\n",
              " 'normal': array([ 0.5343344 ,  0.49351686, -0.51120955, -0.4961882 , -0.48162997,\n",
              "         0.48062453,  0.4447127 , -0.45563865, -0.4688363 , -0.49998093,\n",
              "        -0.5105493 ,  0.45297107,  0.44372416,  0.48176467, -0.48812696,\n",
              "        -0.52228063,  0.50670767,  0.459592  , -0.46789908, -0.45930165,\n",
              "        -0.45343927,  0.4858207 ,  0.48835713,  0.5216656 , -0.52592796,\n",
              "        -0.5290319 ,  0.47081244,  0.48217645,  0.519981  ,  0.5164427 ,\n",
              "        -0.48279107,  0.46664503, -0.5283108 , -0.4771946 , -0.44928384,\n",
              "        -0.45197538,  0.45417416, -0.5044227 ,  0.523883  , -0.53491914,\n",
              "         0.50617087, -0.5283083 , -0.45076808, -0.45266253,  0.52245677,\n",
              "         0.52126366,  0.4836203 ,  0.50015604, -0.45075563,  0.43782797],\n",
              "       dtype=float32),\n",
              " 'rating': array([ 0.49798974,  0.490218  , -0.51501346, -0.4422817 , -0.46865565,\n",
              "         0.49023184,  0.4503654 , -0.44208285, -0.48894447, -0.4511777 ,\n",
              "        -0.49310887,  0.4623984 ,  0.43758565,  0.4780332 , -0.49735758,\n",
              "        -0.44851553,  0.51998806,  0.48997208, -0.49974456, -0.44079772,\n",
              "        -0.4455316 ,  0.4440421 ,  0.5083005 ,  0.47721884, -0.4832957 ,\n",
              "        -0.43237153,  0.43699145,  0.49787152,  0.48236272,  0.46677858,\n",
              "        -0.4941934 ,  0.5182519 , -0.44912606, -0.509875  , -0.4717482 ,\n",
              "        -0.47357836,  0.5106375 , -0.50887954,  0.5109846 , -0.46678764,\n",
              "         0.47331604, -0.45522475, -0.43087578, -0.5031963 ,  0.4529639 ,\n",
              "         0.468914  ,  0.43024164,  0.45231378, -0.5001818 ,  0.45544812],\n",
              "       dtype=float32),\n",
              " 'mail': array([ 0.49068213,  0.49037802, -0.48377192, -0.4889269 , -0.48879996,\n",
              "         0.4567325 ,  0.42292443, -0.456039  , -0.47840393, -0.48464707,\n",
              "        -0.44682196,  0.5087423 ,  0.5020104 ,  0.5146511 , -0.5477292 ,\n",
              "        -0.4688045 ,  0.46660632,  0.49709612, -0.48387393, -0.4678878 ,\n",
              "        -0.49488363,  0.48184317,  0.48441282,  0.46414152, -0.50187504,\n",
              "        -0.4570392 ,  0.45815343,  0.44238436,  0.45643032,  0.49500653,\n",
              "        -0.45754936,  0.46832526, -0.50530547, -0.43118277, -0.41797763,\n",
              "        -0.45080882,  0.50196934, -0.48881793,  0.49769872, -0.42908567,\n",
              "         0.4663827 , -0.5076091 , -0.4152503 , -0.41918376,  0.4607705 ,\n",
              "         0.5139793 ,  0.43602648,  0.42493746, -0.4932362 ,  0.4834336 ],\n",
              "       dtype=float32),\n",
              " 'n/a': array([ 0.5039728 ,  0.46345142, -0.5128079 , -0.48378968, -0.45405447,\n",
              "         0.4978604 ,  0.5194609 , -0.4943775 , -0.43879583, -0.45738456,\n",
              "        -0.47624007,  0.491098  ,  0.50005454,  0.50351256, -0.5134204 ,\n",
              "        -0.4507868 ,  0.5256248 ,  0.4761659 , -0.47483906, -0.49549633,\n",
              "        -0.4622179 ,  0.4708492 ,  0.5049443 ,  0.43293014, -0.4350816 ,\n",
              "        -0.45361036,  0.4619351 ,  0.4956833 ,  0.51405525,  0.51920915,\n",
              "        -0.48376992,  0.5249743 , -0.54086727, -0.4657328 , -0.43660647,\n",
              "        -0.45275888,  0.48208973, -0.5162076 ,  0.48128575, -0.48631635,\n",
              "         0.45064697, -0.46578303, -0.45194674, -0.44644713,  0.44088936,\n",
              "         0.5005894 ,  0.4366431 ,  0.48771986, -0.4674055 ,  0.48662573],\n",
              "       dtype=float32),\n",
              " 'series': array([ 0.48910597,  0.43389255, -0.5428362 , -0.4996229 , -0.4529297 ,\n",
              "         0.5041281 ,  0.48807245, -0.49111068, -0.5128635 , -0.48516035,\n",
              "        -0.48962837,  0.52984357,  0.5150265 ,  0.5057086 , -0.55914617,\n",
              "        -0.46817005,  0.46329212,  0.53153825, -0.4982315 , -0.5044569 ,\n",
              "        -0.49928814,  0.51174104,  0.5135836 ,  0.4858286 , -0.49267977,\n",
              "        -0.52999526,  0.5199496 ,  0.49715045,  0.5118104 ,  0.45623493,\n",
              "        -0.5217306 ,  0.50375485, -0.51515055, -0.4958371 , -0.48096845,\n",
              "        -0.45238805,  0.4848512 , -0.4579259 ,  0.43655816, -0.47354195,\n",
              "         0.45631304, -0.44442874, -0.47688603, -0.4587769 ,  0.5115765 ,\n",
              "         0.49859384,  0.48750114,  0.45661023, -0.4390371 ,  0.48770353],\n",
              "       dtype=float32),\n",
              " 'brokerage': array([ 0.5058072 ,  0.4669044 , -0.4917153 , -0.47089833, -0.48042762,\n",
              "         0.46709478,  0.43651316, -0.47200343, -0.46498936, -0.44351682,\n",
              "        -0.49664682,  0.5238557 ,  0.4837555 ,  0.48341268, -0.55865633,\n",
              "        -0.4492132 ,  0.47969985,  0.47253215, -0.48829144, -0.4526158 ,\n",
              "        -0.4395193 ,  0.5128804 ,  0.52487785,  0.48387736, -0.4901498 ,\n",
              "        -0.47032273,  0.4540286 ,  0.53572464,  0.51084447,  0.50749904,\n",
              "        -0.49859196,  0.51722395, -0.49606806, -0.5002873 , -0.45941284,\n",
              "        -0.45924094,  0.52833456, -0.51615804,  0.51292413, -0.43783873,\n",
              "         0.50656027, -0.44857636, -0.43911827, -0.5070106 ,  0.52261305,\n",
              "         0.46541393,  0.44750643,  0.52554476, -0.44916987,  0.4983308 ],\n",
              "       dtype=float32),\n",
              " 'analysis': array([ 0.50392   ,  0.5037988 , -0.44261733, -0.47855425, -0.4774474 ,\n",
              "         0.45048466,  0.45661378, -0.5071359 , -0.4935839 , -0.42945462,\n",
              "        -0.43829688,  0.43127102,  0.43954355,  0.43721974, -0.48311198,\n",
              "        -0.43811008,  0.5134802 ,  0.44746116, -0.50508577, -0.43550023,\n",
              "        -0.43163922,  0.45993894,  0.45524874,  0.51027775, -0.4610946 ,\n",
              "        -0.48269197,  0.4373965 ,  0.51063806,  0.439256  ,  0.48611608,\n",
              "        -0.45362723,  0.5084976 , -0.5161421 , -0.49156338, -0.47139174,\n",
              "        -0.4913456 ,  0.5159059 , -0.44820964,  0.44818696, -0.5063491 ,\n",
              "         0.46705046, -0.4499199 , -0.4263719 , -0.4917818 ,  0.4938854 ,\n",
              "         0.44576415,  0.48388487,  0.43914458, -0.50391424,  0.49935395],\n",
              "       dtype=float32),\n",
              " 'million': array([ 0.5249498 ,  0.50634843, -0.49711257, -0.45417893, -0.5165788 ,\n",
              "         0.4546767 ,  0.4944573 , -0.4404269 , -0.517008  , -0.48168412,\n",
              "        -0.48395026,  0.5227532 ,  0.4899953 ,  0.5145519 , -0.51414275,\n",
              "        -0.50279593,  0.47114545,  0.46507934, -0.43520632, -0.51484823,\n",
              "        -0.4418381 ,  0.52126926,  0.44332933,  0.4466075 , -0.5147247 ,\n",
              "        -0.47653785,  0.4987234 ,  0.512987  ,  0.52442175,  0.4770946 ,\n",
              "        -0.51820743,  0.44659042, -0.45656312, -0.4515148 , -0.51962125,\n",
              "        -0.50690097,  0.4580925 , -0.4440126 ,  0.5107392 , -0.44909438,\n",
              "         0.49675995, -0.49824265, -0.4533401 , -0.49106288,  0.45695904,\n",
              "         0.5040262 ,  0.4821003 ,  0.4440705 , -0.51901424,  0.47823733],\n",
              "       dtype=float32),\n",
              " 'depend': array([ 0.52598363,  0.43200687, -0.47548148, -0.52574533, -0.4742148 ,\n",
              "         0.5050185 ,  0.44165665, -0.51588416, -0.4348002 , -0.48801652,\n",
              "        -0.47922838,  0.48969242,  0.44756618,  0.51233983, -0.5529156 ,\n",
              "        -0.46544132,  0.49790126,  0.50441295, -0.48068303, -0.441829  ,\n",
              "        -0.45262036,  0.46434915,  0.52242815,  0.44913554, -0.4584197 ,\n",
              "        -0.4577062 ,  0.4333097 ,  0.52992773,  0.49350256,  0.50636196,\n",
              "        -0.4707847 ,  0.49583098, -0.4580865 , -0.45984316, -0.50912094,\n",
              "        -0.46221977,  0.53420895, -0.5264468 ,  0.43723488, -0.47154725,\n",
              "         0.4966038 , -0.4503674 , -0.45488238, -0.4741556 ,  0.5073971 ,\n",
              "         0.47320098,  0.50809854,  0.51296264, -0.44933093,  0.49511376],\n",
              "       dtype=float32),\n",
              " 'cfa': array([ 0.49790892,  0.46829587, -0.44552106, -0.49028364, -0.49236587,\n",
              "         0.5028161 ,  0.5121634 , -0.4722239 , -0.46846804, -0.48930085,\n",
              "        -0.50486916,  0.51264083,  0.44020763,  0.4370076 , -0.50991935,\n",
              "        -0.49918604,  0.4421479 ,  0.51964563, -0.4735565 , -0.48512688,\n",
              "        -0.5002952 ,  0.47328505,  0.49882457,  0.48409528, -0.49651277,\n",
              "        -0.45945534,  0.43889478,  0.48495966,  0.4575646 ,  0.45532024,\n",
              "        -0.47846386,  0.46273783, -0.46974376, -0.45805886, -0.43988594,\n",
              "        -0.5129441 ,  0.48030353, -0.48363602,  0.4554495 , -0.50720567,\n",
              "         0.52311563, -0.5027069 , -0.44301614, -0.44912156,  0.4942608 ,\n",
              "         0.5146996 ,  0.47643423,  0.48313498, -0.45327604,  0.43283576],\n",
              "       dtype=float32),\n",
              " 'janus': array([ 0.5201189 ,  0.44109517, -0.51232505, -0.49689206, -0.45372877,\n",
              "         0.5305692 ,  0.42832887, -0.5008258 , -0.5387596 , -0.52125555,\n",
              "        -0.53288025,  0.4971341 ,  0.5214273 ,  0.5405131 , -0.51115847,\n",
              "        -0.48917696,  0.49307352,  0.49836963, -0.52470195, -0.4421772 ,\n",
              "        -0.5190154 ,  0.47460198,  0.4997502 ,  0.45327568, -0.5266494 ,\n",
              "        -0.44603705,  0.47036135,  0.47008452,  0.4694232 ,  0.4841128 ,\n",
              "        -0.4911829 ,  0.43713686, -0.4980877 , -0.503058  , -0.45371664,\n",
              "        -0.44972384,  0.53321373, -0.4708883 ,  0.53933096, -0.5243476 ,\n",
              "         0.51127607, -0.5274421 , -0.48448855, -0.4633377 ,  0.4557205 ,\n",
              "         0.47971952,  0.4555031 ,  0.48990873, -0.5170869 ,  0.4927879 ],\n",
              "       dtype=float32),\n",
              " 'continued': array([ 0.49610883,  0.48683584, -0.5022739 , -0.5273791 , -0.48232427,\n",
              "         0.4740964 ,  0.4425139 , -0.5318295 , -0.46694466, -0.4481749 ,\n",
              "        -0.505029  ,  0.5023991 ,  0.515868  ,  0.5341971 , -0.48373398,\n",
              "        -0.51476   ,  0.48229784,  0.5138835 , -0.5274904 , -0.48804432,\n",
              "        -0.50304246,  0.49391246,  0.46290222,  0.5278509 , -0.5196876 ,\n",
              "        -0.47945932,  0.47362706,  0.5109836 ,  0.4691268 ,  0.5220743 ,\n",
              "        -0.49155343,  0.47777957, -0.51614803, -0.50284797, -0.49199933,\n",
              "        -0.5184281 ,  0.5158711 , -0.4695086 ,  0.48593134, -0.52435946,\n",
              "         0.50622517, -0.47009137, -0.4937097 , -0.47914752,  0.44926584,\n",
              "         0.45440948,  0.44391844,  0.52567565, -0.44397995,  0.4411027 ],\n",
              "       dtype=float32),\n",
              " 'earnings': array([ 0.5454194 ,  0.45027986, -0.5091518 , -0.4999288 , -0.5411296 ,\n",
              "         0.5029527 ,  0.5006968 , -0.4761247 , -0.506039  , -0.49584562,\n",
              "        -0.45929193,  0.52572864,  0.45895085,  0.483218  , -0.5694741 ,\n",
              "        -0.52608943,  0.4897675 ,  0.49615458, -0.42931414, -0.46792355,\n",
              "        -0.45478302,  0.4698891 ,  0.4494103 ,  0.51248366, -0.47307912,\n",
              "        -0.48224753,  0.5095767 ,  0.5586886 ,  0.44403818,  0.43667337,\n",
              "        -0.4601311 ,  0.4930232 , -0.49425328, -0.4612269 , -0.4440261 ,\n",
              "        -0.48020774,  0.45307538, -0.5218039 ,  0.44159317, -0.5103458 ,\n",
              "         0.46156734, -0.52579224, -0.45694503, -0.460058  ,  0.45908496,\n",
              "         0.47089732,  0.44969633,  0.4698936 , -0.46898684,  0.4632919 ],\n",
              "       dtype=float32),\n",
              " 'etf': array([ 0.51772946,  0.4859397 , -0.43708092, -0.46519646, -0.51887923,\n",
              "         0.47719324,  0.49564868, -0.4944931 , -0.47630125, -0.45155647,\n",
              "        -0.51456064,  0.49351767,  0.4325264 ,  0.4883895 , -0.528019  ,\n",
              "        -0.4744417 ,  0.5107497 ,  0.5109931 , -0.47236747, -0.4522784 ,\n",
              "        -0.47444272,  0.42162412,  0.5129724 ,  0.46869165, -0.42490593,\n",
              "        -0.45768103,  0.45938718,  0.45761725,  0.44807923,  0.42321056,\n",
              "        -0.50757265,  0.45036566, -0.4687392 , -0.43299094, -0.42309833,\n",
              "        -0.51222503,  0.5063786 , -0.4439918 ,  0.47566143, -0.44634855,\n",
              "         0.44956163, -0.4874127 , -0.44816718, -0.49927104,  0.500658  ,\n",
              "         0.51613307,  0.4592855 ,  0.47786316, -0.4746277 ,  0.5110953 ],\n",
              "       dtype=float32),\n",
              " 'respect': array([ 0.49888915,  0.49669626, -0.43700626, -0.49391747, -0.49406353,\n",
              "         0.4503714 ,  0.4745087 , -0.4717169 , -0.5192333 , -0.47002804,\n",
              "        -0.5211188 ,  0.46933216,  0.4994831 ,  0.47196862, -0.5063541 ,\n",
              "        -0.44585505,  0.48966038,  0.5019871 , -0.45463064, -0.47162756,\n",
              "        -0.5022156 ,  0.49184075,  0.5085364 ,  0.47770578, -0.44737425,\n",
              "        -0.4444482 ,  0.43587524,  0.4787234 ,  0.51444566,  0.46156132,\n",
              "        -0.5310144 ,  0.50030863, -0.45454073, -0.4913628 , -0.48677886,\n",
              "        -0.5119528 ,  0.48747116, -0.4672122 ,  0.4800672 , -0.44004896,\n",
              "         0.46049523, -0.4415569 , -0.5070136 , -0.4991958 ,  0.47165358,\n",
              "         0.50153637,  0.5122624 ,  0.44227168, -0.46897218,  0.47826147],\n",
              "       dtype=float32),\n",
              " 'qualified': array([ 0.5076297 ,  0.45950508, -0.4435387 , -0.48865852, -0.5076664 ,\n",
              "         0.48600024,  0.4577951 , -0.49682277, -0.50768346, -0.47433028,\n",
              "        -0.49097094,  0.48796692,  0.471954  ,  0.49114817, -0.494387  ,\n",
              "        -0.4422043 ,  0.5168792 ,  0.50222063, -0.4334961 , -0.48122135,\n",
              "        -0.46719116,  0.5009325 ,  0.47042754,  0.50003976, -0.51246583,\n",
              "        -0.45606652,  0.5228961 ,  0.49701348,  0.48868236,  0.50251967,\n",
              "        -0.45188716,  0.48992983, -0.54203546, -0.4478417 , -0.46145684,\n",
              "        -0.50221914,  0.5062574 , -0.51652914,  0.47041565, -0.5062617 ,\n",
              "         0.510555  , -0.48833188, -0.4541446 , -0.44497973,  0.4485293 ,\n",
              "         0.52316624,  0.47747934,  0.4806292 , -0.44067368,  0.503798  ],\n",
              "       dtype=float32),\n",
              " 'opportunities': array([ 0.47763777,  0.49192816, -0.5014245 , -0.48967004, -0.52365696,\n",
              "         0.4915442 ,  0.513692  , -0.496032  , -0.4795347 , -0.5073424 ,\n",
              "        -0.50506115,  0.5121318 ,  0.4701132 ,  0.4644692 , -0.5441607 ,\n",
              "        -0.482805  ,  0.45894954,  0.46282902, -0.43725538, -0.44174096,\n",
              "        -0.47102642,  0.45017973,  0.51039064,  0.49080062, -0.4967004 ,\n",
              "        -0.46637428,  0.46340102,  0.49311972,  0.4826711 ,  0.49754688,\n",
              "        -0.4524947 ,  0.4492052 , -0.51611584, -0.5076782 , -0.519009  ,\n",
              "        -0.52586067,  0.49282682, -0.49516457,  0.51984316, -0.5197946 ,\n",
              "         0.5157965 , -0.45730203, -0.42379975, -0.5239365 ,  0.45391056,\n",
              "         0.4437322 ,  0.46970546,  0.45937413, -0.44484442,  0.46230137],\n",
              "       dtype=float32),\n",
              " 'continue': array([ 0.5341952 ,  0.43085238, -0.48590082, -0.5018333 , -0.44707352,\n",
              "         0.48563656,  0.46684098, -0.47159865, -0.43268913, -0.44817197,\n",
              "        -0.5065408 ,  0.45020497,  0.44883794,  0.5138785 , -0.50513875,\n",
              "        -0.49363574,  0.44786763,  0.51521325, -0.51667416, -0.42726505,\n",
              "        -0.4330914 ,  0.4586948 ,  0.4805027 ,  0.43871298, -0.44108915,\n",
              "        -0.52242976,  0.49654758,  0.5157717 ,  0.45778626,  0.5164279 ,\n",
              "        -0.53543293,  0.48245767, -0.48629165, -0.5059656 , -0.4860977 ,\n",
              "        -0.5039263 ,  0.4646034 , -0.48701254,  0.4597454 , -0.5286917 ,\n",
              "         0.46576905, -0.4455386 , -0.43389347, -0.47386846,  0.5179312 ,\n",
              "         0.52459353,  0.4587131 ,  0.44264257, -0.5222519 ,  0.4988927 ],\n",
              "       dtype=float32),\n",
              " 'single': array([ 0.51077074,  0.47302523, -0.44874066, -0.44343838, -0.50789094,\n",
              "         0.4710395 ,  0.46026504, -0.4850266 , -0.459808  , -0.5281031 ,\n",
              "        -0.48832095,  0.47798014,  0.473282  ,  0.51079184, -0.52336   ,\n",
              "        -0.46545184,  0.49710935,  0.52332866, -0.49983937, -0.44163314,\n",
              "        -0.5288051 ,  0.45938516,  0.5119688 ,  0.45672384, -0.46368557,\n",
              "        -0.47698653,  0.45930615,  0.51569647,  0.5223556 ,  0.51295584,\n",
              "        -0.50890684,  0.4468218 , -0.48276854, -0.5162883 , -0.45310685,\n",
              "        -0.50779605,  0.47681564, -0.50822276,  0.45492616, -0.48480338,\n",
              "         0.5309313 , -0.45962906, -0.46781474, -0.4833073 ,  0.5289739 ,\n",
              "         0.50605303,  0.4636181 ,  0.4595477 , -0.51588726,  0.48574927],\n",
              "       dtype=float32),\n",
              " 'lack': array([ 0.5443662 ,  0.43211213, -0.49388227, -0.4830331 , -0.4921671 ,\n",
              "         0.48112777,  0.5153347 , -0.52221704, -0.48240188, -0.49449688,\n",
              "        -0.47311208,  0.5216639 ,  0.48811406,  0.47427854, -0.52953225,\n",
              "        -0.4718824 ,  0.4842939 ,  0.5119667 , -0.4929814 , -0.5163393 ,\n",
              "        -0.49568155,  0.4518994 ,  0.50735027,  0.484783  , -0.49729723,\n",
              "        -0.4664338 ,  0.45002374,  0.48271233,  0.43826658,  0.45437485,\n",
              "        -0.4524493 ,  0.46597132, -0.5224295 , -0.4943196 , -0.42634293,\n",
              "        -0.5337667 ,  0.5293349 , -0.4625129 ,  0.44209248, -0.522587  ,\n",
              "         0.4765077 , -0.5001975 , -0.44252005, -0.47430164,  0.49461633,\n",
              "         0.4524124 ,  0.4547387 ,  0.45173118, -0.4755953 ,  0.48951244],\n",
              "       dtype=float32),\n",
              " 'illiquid': array([ 0.52868277,  0.46403012, -0.4980478 , -0.4517759 , -0.48055643,\n",
              "         0.47571677,  0.47751686, -0.4980816 , -0.48287442, -0.42918265,\n",
              "        -0.51283556,  0.4403444 ,  0.4476514 ,  0.46836329, -0.5146256 ,\n",
              "        -0.51152384,  0.45903948,  0.44297406, -0.5048866 , -0.45334733,\n",
              "        -0.44865617,  0.44239673,  0.50965875,  0.47035208, -0.49623117,\n",
              "        -0.5029838 ,  0.46546742,  0.5290453 ,  0.46491933,  0.50034416,\n",
              "        -0.52716714,  0.5009392 , -0.4833166 , -0.47475538, -0.44659024,\n",
              "        -0.52241665,  0.50870204, -0.5072968 ,  0.4726536 , -0.52474177,\n",
              "         0.44808912, -0.5229969 , -0.4381011 , -0.43603787,  0.456856  ,\n",
              "         0.46704656,  0.47557113,  0.44378698, -0.49498835,  0.4297716 ],\n",
              "       dtype=float32),\n",
              " 'specific': array([ 0.4799547 ,  0.46394032, -0.4977777 , -0.47747982, -0.50258386,\n",
              "         0.5231495 ,  0.51557636, -0.44680712, -0.46239892, -0.49932554,\n",
              "        -0.5162793 ,  0.48718897,  0.43546152,  0.48341522, -0.4841663 ,\n",
              "        -0.43581262,  0.45692632,  0.45409396, -0.50506866, -0.48794505,\n",
              "        -0.49685183,  0.45878187,  0.4802215 ,  0.44829488, -0.4439849 ,\n",
              "        -0.45206046,  0.5109533 ,  0.515995  ,  0.43426913,  0.4411685 ,\n",
              "        -0.46295092,  0.5071412 , -0.50502133, -0.51886785, -0.49320954,\n",
              "        -0.46228716,  0.523446  , -0.50466406,  0.44711962, -0.48541632,\n",
              "         0.47063416, -0.52681303, -0.5124879 , -0.44727704,  0.51730883,\n",
              "         0.44514567,  0.49803603,  0.46090505, -0.50247484,  0.47879633],\n",
              "       dtype=float32),\n",
              " 'sensitive': array([ 0.5025642 ,  0.50957835, -0.52620816, -0.5096513 , -0.45387518,\n",
              "         0.45616075,  0.45870486, -0.5169401 , -0.43765467, -0.5038228 ,\n",
              "        -0.51107746,  0.5115992 ,  0.49441698,  0.47753322, -0.5406559 ,\n",
              "        -0.4606909 ,  0.46630085,  0.5139841 , -0.46410054, -0.49561173,\n",
              "        -0.4404507 ,  0.46738383,  0.51536405,  0.4698755 , -0.47827512,\n",
              "        -0.4373797 ,  0.45602566,  0.46228668,  0.5185898 ,  0.44756457,\n",
              "        -0.45924923,  0.458439  , -0.5364691 , -0.46658507, -0.45213473,\n",
              "        -0.50385815,  0.5244699 , -0.5323552 ,  0.4371612 , -0.5044842 ,\n",
              "         0.45190406, -0.5038277 , -0.45361134, -0.49368778,  0.466724  ,\n",
              "         0.5298151 ,  0.4738722 ,  0.49771366, -0.51504767,  0.46199977],\n",
              "       dtype=float32),\n",
              " 'case': array([ 0.54020596,  0.47134405, -0.5186733 , -0.50415874, -0.49258077,\n",
              "         0.4723938 ,  0.481985  , -0.4471712 , -0.4944054 , -0.47447073,\n",
              "        -0.4886976 ,  0.43769673,  0.49774638,  0.43123472, -0.4946838 ,\n",
              "        -0.49123034,  0.45106816,  0.5169352 , -0.50941616, -0.43328607,\n",
              "        -0.4750683 ,  0.45973974,  0.5203333 ,  0.4468252 , -0.49967927,\n",
              "        -0.4276278 ,  0.47973046,  0.504754  ,  0.46610388,  0.5064709 ,\n",
              "        -0.45941627,  0.43709782, -0.49912956, -0.51092774, -0.43506873,\n",
              "        -0.46837667,  0.51413125, -0.5242979 ,  0.47179112, -0.4656996 ,\n",
              "         0.52452254, -0.47359282, -0.50497025, -0.4569046 ,  0.50041735,\n",
              "         0.51865166,  0.43774986,  0.48587233, -0.46561474,  0.50468564],\n",
              "       dtype=float32),\n",
              " 'particularly': array([ 0.48304525,  0.51136327, -0.5059884 , -0.48447537, -0.5310548 ,\n",
              "         0.44957653,  0.47766262, -0.48348102, -0.48975185, -0.49927032,\n",
              "        -0.4917731 ,  0.49440676,  0.4490794 ,  0.51242936, -0.5676544 ,\n",
              "        -0.48442686,  0.44633842,  0.48149878, -0.4801029 , -0.4921329 ,\n",
              "        -0.48218793,  0.5242896 ,  0.5341086 ,  0.45423335, -0.47580072,\n",
              "        -0.452333  ,  0.44230035,  0.46274155,  0.49143603,  0.44023228,\n",
              "        -0.48178598,  0.44830388, -0.50449413, -0.5221004 , -0.5194732 ,\n",
              "        -0.44618693,  0.524875  , -0.5067976 ,  0.49082187, -0.47031888,\n",
              "         0.49770352, -0.5338932 , -0.48605493, -0.48634717,  0.45578653,\n",
              "         0.5088668 ,  0.51785696,  0.45497835, -0.4698874 ,  0.48382747],\n",
              "       dtype=float32),\n",
              " 'process': array([ 0.48108688,  0.49377865, -0.45272458, -0.4543321 , -0.48816487,\n",
              "         0.48292845,  0.5134299 , -0.5022452 , -0.51435715, -0.48741555,\n",
              "        -0.48048294,  0.46032488,  0.5178261 ,  0.5273417 , -0.5166061 ,\n",
              "        -0.45583498,  0.5192407 ,  0.51424295, -0.4926443 , -0.43041703,\n",
              "        -0.4563789 ,  0.5129047 ,  0.48620212,  0.4778461 , -0.45212403,\n",
              "        -0.492832  ,  0.5177096 ,  0.4867358 ,  0.44208965,  0.44913277,\n",
              "        -0.5063219 ,  0.5010694 , -0.46829012, -0.51079   , -0.471575  ,\n",
              "        -0.45279422,  0.48100367, -0.4740664 ,  0.4885369 , -0.49338853,\n",
              "         0.51394975, -0.4663267 , -0.47022888, -0.49696183,  0.46576783,\n",
              "         0.47574794,  0.5137085 ,  0.50987554, -0.47942492,  0.48156986],\n",
              "       dtype=float32),\n",
              " 'selling': array([ 0.54040337,  0.5071649 , -0.48135132, -0.4335848 , -0.51617765,\n",
              "         0.45430863,  0.43138582, -0.49634427, -0.48551017, -0.49120358,\n",
              "        -0.484265  ,  0.5041463 ,  0.49058878,  0.4770251 , -0.4962859 ,\n",
              "        -0.50335264,  0.5197824 ,  0.5031852 , -0.48150924, -0.42800173,\n",
              "        -0.45883933,  0.5008715 ,  0.44553068,  0.51492375, -0.44388402,\n",
              "        -0.4554621 ,  0.5125383 ,  0.5372719 ,  0.45709324,  0.49259964,\n",
              "        -0.4544126 ,  0.4618108 , -0.450335  , -0.5072314 , -0.47331583,\n",
              "        -0.46958998,  0.4535709 , -0.50844187,  0.4411533 , -0.44459194,\n",
              "         0.44382736, -0.530902  , -0.5008392 , -0.49517274,  0.47643343,\n",
              "         0.5099946 ,  0.5097198 ,  0.46292225, -0.485207  ,  0.46840355],\n",
              "       dtype=float32),\n",
              " 'call': array([ 0.46707192,  0.47440284, -0.49003217, -0.4721632 , -0.44502166,\n",
              "         0.45817548,  0.480613  , -0.48901814, -0.43984726, -0.45193148,\n",
              "        -0.44845363,  0.5275038 ,  0.4327179 ,  0.47025487, -0.5434006 ,\n",
              "        -0.5150555 ,  0.45985624,  0.51489824, -0.48239738, -0.4284128 ,\n",
              "        -0.46369815,  0.5171135 ,  0.5145163 ,  0.4810091 , -0.4911318 ,\n",
              "        -0.45200434,  0.45534393,  0.5201883 ,  0.45093268,  0.43166953,\n",
              "        -0.48017544,  0.48192036, -0.48781323, -0.507453  , -0.5121409 ,\n",
              "        -0.51276976,  0.47753507, -0.4726575 ,  0.45634168, -0.508431  ,\n",
              "         0.4473965 , -0.5237372 , -0.46311545, -0.45064113,  0.50193137,\n",
              "         0.50275743,  0.44691733,  0.52027386, -0.50723606,  0.4525101 ],\n",
              "       dtype=float32),\n",
              " 'authorized': array([ 0.4733174 ,  0.49406564, -0.51605797, -0.4343218 , -0.51928914,\n",
              "         0.48726627,  0.4986582 , -0.4948349 , -0.49230662, -0.4750228 ,\n",
              "        -0.4310797 ,  0.4295702 ,  0.47661754,  0.5085067 , -0.49689707,\n",
              "        -0.4598011 ,  0.50098395,  0.4899078 , -0.49296343, -0.46577448,\n",
              "        -0.44882187,  0.49084318,  0.46359935,  0.49957645, -0.44197655,\n",
              "        -0.47276217,  0.43865314,  0.4906267 ,  0.42632934,  0.48981968,\n",
              "        -0.47132546,  0.47651133, -0.47475508, -0.49572092, -0.4797654 ,\n",
              "        -0.47957152,  0.47112286, -0.45477238,  0.49823296, -0.4720637 ,\n",
              "         0.4943043 , -0.49375784, -0.46762517, -0.45075297,  0.49473822,\n",
              "         0.49162441,  0.41908714,  0.44238776, -0.48998043,  0.47952697],\n",
              "       dtype=float32),\n",
              " 'york': array([ 0.54878086,  0.45151725, -0.52085507, -0.49370232, -0.46534312,\n",
              "         0.47199205,  0.5018934 , -0.4996798 , -0.4735765 , -0.47463012,\n",
              "        -0.4837856 ,  0.4413157 ,  0.4622325 ,  0.4424533 , -0.48785144,\n",
              "        -0.50045127,  0.4614126 ,  0.51414436, -0.45474058, -0.46902558,\n",
              "        -0.44492   ,  0.5011057 ,  0.50461566,  0.46382785, -0.42843708,\n",
              "        -0.4548678 ,  0.4634199 ,  0.54441005,  0.51617914,  0.47532144,\n",
              "        -0.44693556,  0.46386862, -0.54548603, -0.5366752 , -0.48869002,\n",
              "        -0.51382947,  0.5302123 , -0.45626882,  0.48220468, -0.5367172 ,\n",
              "         0.4952973 , -0.5160883 , -0.4700718 , -0.51512736,  0.4507504 ,\n",
              "         0.4568787 ,  0.49456656,  0.5112424 , -0.46259624,  0.5043173 ],\n",
              "       dtype=float32),\n",
              " 'subsequent': array([ 0.509747  ,  0.44903466, -0.45938793, -0.4956947 , -0.44783914,\n",
              "         0.47590995,  0.50410026, -0.49369416, -0.5218537 , -0.4511055 ,\n",
              "        -0.5133299 ,  0.44342706,  0.4837647 ,  0.4793107 , -0.51741076,\n",
              "        -0.45188048,  0.52887154,  0.4530077 , -0.46275318, -0.46985722,\n",
              "        -0.50332326,  0.4499362 ,  0.46099815,  0.48464483, -0.5001229 ,\n",
              "        -0.517762  ,  0.48539203,  0.4714932 ,  0.44175208,  0.49661043,\n",
              "        -0.46405134,  0.49874777, -0.5302633 , -0.50021595, -0.47342288,\n",
              "        -0.47343323,  0.49849153, -0.45387658,  0.5143141 , -0.5073133 ,\n",
              "         0.5162957 , -0.44712558, -0.4712405 , -0.47993073,  0.5064349 ,\n",
              "         0.44210145,  0.5182849 ,  0.5063617 , -0.45225993,  0.501982  ],\n",
              "       dtype=float32),\n",
              " 'asset-backed': array([ 0.5106613 ,  0.4772526 , -0.5116074 , -0.5083106 , -0.48924077,\n",
              "         0.44840372,  0.46750274, -0.4935392 , -0.46220985, -0.4527095 ,\n",
              "        -0.4531757 ,  0.45073688,  0.48474166,  0.4479034 , -0.50551546,\n",
              "        -0.440392  ,  0.49658203,  0.45396438, -0.5020212 , -0.443745  ,\n",
              "        -0.44262585,  0.43813896,  0.50500757,  0.49662554, -0.49590033,\n",
              "        -0.45275655,  0.42391622,  0.5329667 ,  0.47174898,  0.44323415,\n",
              "        -0.5176955 ,  0.45037863, -0.4689261 , -0.46761668, -0.4370817 ,\n",
              "        -0.4417059 ,  0.4572351 , -0.43633935,  0.5097674 , -0.48549196,\n",
              "         0.43063337, -0.47793266, -0.43700638, -0.47315776,  0.45502794,\n",
              "         0.42581257,  0.509331  ,  0.44795212, -0.5040754 ,  0.45648918],\n",
              "       dtype=float32),\n",
              " 'non-u.s.': array([ 0.5209222 ,  0.49813476, -0.4671391 , -0.4771194 , -0.5306222 ,\n",
              "         0.47713643,  0.4580154 , -0.5262483 , -0.4347962 , -0.50507677,\n",
              "        -0.45873478,  0.5147897 ,  0.46697447,  0.5147214 , -0.5391659 ,\n",
              "        -0.49922794,  0.50075674,  0.44995052, -0.47124293, -0.49439457,\n",
              "        -0.5049391 ,  0.5094819 ,  0.46384475,  0.51024663, -0.48435184,\n",
              "        -0.44981834,  0.46604264,  0.52922034,  0.4610333 ,  0.5064516 ,\n",
              "        -0.44955072,  0.44170362, -0.49197885, -0.5203364 , -0.48100677,\n",
              "        -0.47228104,  0.4810551 , -0.45714593,  0.43556085, -0.45727238,\n",
              "         0.46768045, -0.51053905, -0.45065373, -0.47848925,  0.44679576,\n",
              "         0.47137877,  0.44194773,  0.52699673, -0.4419986 ,  0.48184308],\n",
              "       dtype=float32),\n",
              " 'benchmark': array([ 0.479924  ,  0.48297438, -0.47649208, -0.4938162 , -0.48844546,\n",
              "         0.4575003 ,  0.5236228 , -0.47837606, -0.4702473 , -0.43693852,\n",
              "        -0.5179624 ,  0.52947676,  0.50785166,  0.47698978, -0.57614005,\n",
              "        -0.5239685 ,  0.46773955,  0.5082546 , -0.4497387 , -0.43658486,\n",
              "        -0.47280717,  0.511358  ,  0.45378807,  0.4880561 , -0.44626683,\n",
              "        -0.4449079 ,  0.44086424,  0.5575662 ,  0.45379263,  0.51968   ,\n",
              "        -0.49986252,  0.48213258, -0.48760036, -0.5279286 , -0.43334103,\n",
              "        -0.4728727 ,  0.5390384 , -0.5050069 ,  0.48352888, -0.48399827,\n",
              "         0.47662982, -0.5315928 , -0.51937354, -0.45924813,  0.46647477,\n",
              "         0.47572494,  0.49464414,  0.47646585, -0.4895537 ,  0.5025866 ],\n",
              "       dtype=float32),\n",
              " 'many': array([ 0.53247654,  0.4782405 , -0.5194514 , -0.49408388, -0.5236222 ,\n",
              "         0.508958  ,  0.49564597, -0.47843754, -0.51994425, -0.4892683 ,\n",
              "        -0.51937264,  0.46171567,  0.4500489 ,  0.49184483, -0.51402724,\n",
              "        -0.46186373,  0.50014055,  0.45448694, -0.4573798 , -0.44971398,\n",
              "        -0.45300353,  0.44742453,  0.4638071 ,  0.46374708, -0.4633155 ,\n",
              "        -0.4580112 ,  0.5151595 ,  0.5187559 ,  0.5196084 ,  0.48257685,\n",
              "        -0.46231252,  0.45897543, -0.5398485 , -0.4889773 , -0.43988347,\n",
              "        -0.4602329 ,  0.4790253 , -0.4708864 ,  0.5205407 , -0.49979818,\n",
              "         0.5250728 , -0.4767004 , -0.43164676, -0.47155344,  0.5137439 ,\n",
              "         0.5144955 ,  0.50271726,  0.5150054 , -0.5238623 ,  0.47431126],\n",
              "       dtype=float32),\n",
              " 'quarterly': array([ 0.4948911 ,  0.4684438 , -0.44663396, -0.5133867 , -0.4562134 ,\n",
              "         0.5366404 ,  0.46143553, -0.44532   , -0.4622096 , -0.4443908 ,\n",
              "        -0.447651  ,  0.52238   ,  0.45850065,  0.49090657, -0.5543185 ,\n",
              "        -0.48245233,  0.47913247,  0.4950573 , -0.47228682, -0.42713875,\n",
              "        -0.5153585 ,  0.44302863,  0.51828665,  0.49043596, -0.46183944,\n",
              "        -0.52266014,  0.519347  ,  0.45390493,  0.4407103 ,  0.48642367,\n",
              "        -0.48730543,  0.49208415, -0.5216712 , -0.4951498 , -0.46964014,\n",
              "        -0.5052528 ,  0.49366054, -0.4615606 ,  0.41898236, -0.5066882 ,\n",
              "         0.45507208, -0.4648025 , -0.49349236, -0.51451457,  0.5064913 ,\n",
              "         0.44603497,  0.5024323 ,  0.49645886, -0.47622716,  0.49466974],\n",
              "       dtype=float32),\n",
              " 'fluctuate': array([ 0.48510692,  0.44251886, -0.46030253, -0.5094857 , -0.5002029 ,\n",
              "         0.47739527,  0.498979  , -0.45538503, -0.5159811 , -0.47887635,\n",
              "        -0.5133845 ,  0.44818658,  0.5230863 ,  0.47681502, -0.58334076,\n",
              "        -0.4550936 ,  0.5046752 ,  0.47099692, -0.45401686, -0.4871776 ,\n",
              "        -0.5262347 ,  0.51838636,  0.54080623,  0.5213375 , -0.46154296,\n",
              "        -0.46218917,  0.51254404,  0.4956169 ,  0.5015711 ,  0.51832324,\n",
              "        -0.46921465,  0.5100243 , -0.50530833, -0.5056025 , -0.47858799,\n",
              "        -0.4692324 ,  0.5193609 , -0.5355407 ,  0.46552813, -0.51353896,\n",
              "         0.4614751 , -0.4919662 , -0.44197318, -0.49628243,  0.517721  ,\n",
              "         0.49780118,  0.48045078,  0.44495896, -0.46324927,  0.49197698],\n",
              "       dtype=float32),\n",
              " 'used': array([ 0.48493192,  0.4419997 , -0.49887908, -0.47281376, -0.5342437 ,\n",
              "         0.50276077,  0.48673132, -0.4637218 , -0.4631093 , -0.45671782,\n",
              "        -0.5205386 ,  0.4812741 ,  0.4686048 ,  0.4531379 , -0.5326336 ,\n",
              "        -0.52170175,  0.4815186 ,  0.45150155, -0.45614862, -0.47580093,\n",
              "        -0.49799496,  0.43631402,  0.47673473,  0.49512663, -0.4627346 ,\n",
              "        -0.5141753 ,  0.47305238,  0.4799704 ,  0.43716916,  0.43570656,\n",
              "        -0.4983769 ,  0.4728386 , -0.49618113, -0.4983309 , -0.50243247,\n",
              "        -0.51467997,  0.4693992 , -0.5189335 ,  0.5097289 , -0.47972453,\n",
              "         0.48534948, -0.47686523, -0.5139512 , -0.4680091 ,  0.4610118 ,\n",
              "         0.48574835,  0.50869584,  0.46046844, -0.50561416,  0.4730619 ],\n",
              "       dtype=float32),\n",
              " 'differences': array([ 0.4732709 ,  0.5043479 , -0.4455637 , -0.47066498, -0.46160612,\n",
              "         0.49945015,  0.49245554, -0.5330187 , -0.5218801 , -0.46740955,\n",
              "        -0.4767874 ,  0.46992406,  0.46387827,  0.46196103, -0.50481975,\n",
              "        -0.4982415 ,  0.50907695,  0.5146915 , -0.5180114 , -0.51448035,\n",
              "        -0.48205042,  0.5127713 ,  0.4934201 ,  0.46345717, -0.5198622 ,\n",
              "        -0.518358  ,  0.48294377,  0.49513394,  0.47969377,  0.47670418,\n",
              "        -0.50202954,  0.4375358 , -0.48995906, -0.47156602, -0.49424487,\n",
              "        -0.4512718 ,  0.4896037 , -0.49506724,  0.4469985 , -0.48593718,\n",
              "         0.48346582, -0.49369326, -0.48551518, -0.49635482,  0.49114677,\n",
              "         0.47735146,  0.5226229 ,  0.5206889 , -0.48237544,  0.5122716 ],\n",
              "       dtype=float32),\n",
              " 'governmental': array([ 0.50130856,  0.4635991 , -0.45279044, -0.4735077 , -0.5056918 ,\n",
              "         0.448619  ,  0.50950396, -0.5208372 , -0.4431342 , -0.50906414,\n",
              "        -0.4650999 ,  0.4540261 ,  0.50475043,  0.48758325, -0.5532403 ,\n",
              "        -0.47952092,  0.49615657,  0.5336859 , -0.50606763, -0.4395274 ,\n",
              "        -0.4648634 ,  0.49390858,  0.5068084 ,  0.4823111 , -0.45950904,\n",
              "        -0.45819756,  0.51176715,  0.4723612 ,  0.42338678,  0.4547518 ,\n",
              "        -0.46562043,  0.52150434, -0.513408  , -0.500225  , -0.500302  ,\n",
              "        -0.46260616,  0.44155893, -0.43297565,  0.4495075 , -0.45551795,\n",
              "         0.5127812 , -0.498832  , -0.41730303, -0.47886094,  0.45564955,\n",
              "         0.45656958,  0.48235685,  0.499639  , -0.5253594 ,  0.43339974],\n",
              "       dtype=float32),\n",
              " 'otherwise': array([ 0.49978992,  0.47541574, -0.513158  , -0.51869607, -0.46285924,\n",
              "         0.4633516 ,  0.45406505, -0.5002917 , -0.50110054, -0.44496188,\n",
              "        -0.4647799 ,  0.527203  ,  0.47342268,  0.4903814 , -0.55597085,\n",
              "        -0.5326056 ,  0.4517929 ,  0.4636239 , -0.45962924, -0.44396627,\n",
              "        -0.50177085,  0.5278979 ,  0.45040438,  0.4730007 , -0.46994853,\n",
              "        -0.5078014 ,  0.52187485,  0.4861638 ,  0.4706604 ,  0.5159039 ,\n",
              "        -0.5074151 ,  0.46137333, -0.49738157, -0.51607114, -0.5133204 ,\n",
              "        -0.5054463 ,  0.49051815, -0.5026072 ,  0.47403136, -0.4509408 ,\n",
              "         0.5009818 , -0.45965266, -0.454629  , -0.52826697,  0.46027812,\n",
              "         0.5138345 ,  0.4746891 ,  0.48440444, -0.5245176 ,  0.46313584],\n",
              "       dtype=float32),\n",
              " 'valuation': array([ 0.49809414,  0.433124  , -0.4429334 , -0.4909411 , -0.4932854 ,\n",
              "         0.5220533 ,  0.5157583 , -0.4796398 , -0.45288056, -0.5051196 ,\n",
              "        -0.4594159 ,  0.45874634,  0.43584162,  0.4618947 , -0.5372294 ,\n",
              "        -0.49755892,  0.43641827,  0.47064552, -0.42468113, -0.50174   ,\n",
              "        -0.509733  ,  0.45543075,  0.4787652 ,  0.49492234, -0.5042908 ,\n",
              "        -0.4964103 ,  0.43108624,  0.49118277,  0.50737405,  0.45204324,\n",
              "        -0.4704072 ,  0.47595048, -0.456105  , -0.4541232 , -0.5123126 ,\n",
              "        -0.45782375,  0.5258713 , -0.5108763 ,  0.46828914, -0.49104545,\n",
              "         0.5203868 , -0.5177622 , -0.43493274, -0.47145733,  0.5144901 ,\n",
              "         0.50387   ,  0.50342286,  0.43653268, -0.4609619 ,  0.51127857],\n",
              "       dtype=float32),\n",
              " 'separate': array([ 0.48162556,  0.49785167, -0.48765948, -0.4428574 , -0.4890764 ,\n",
              "         0.525329  ,  0.45474392, -0.5204494 , -0.44703716, -0.46731195,\n",
              "        -0.5002126 ,  0.47544363,  0.5214621 ,  0.48665008, -0.5654597 ,\n",
              "        -0.4750065 ,  0.46958983,  0.49541318, -0.50684685, -0.5188578 ,\n",
              "        -0.51199454,  0.48853076,  0.5295123 ,  0.44364452, -0.49191064,\n",
              "        -0.51352847,  0.44440997,  0.46707925,  0.4574187 ,  0.4487187 ,\n",
              "        -0.5141387 ,  0.46595997, -0.47322965, -0.45756844, -0.47156692,\n",
              "        -0.5085342 ,  0.48832908, -0.45473832,  0.49914205, -0.46230677,\n",
              "         0.49818653, -0.47093752, -0.4296963 , -0.46760294,  0.46639675,\n",
              "         0.52839506,  0.46838096,  0.48915276, -0.49585533,  0.44478709],\n",
              "       dtype=float32),\n",
              " 'etfs': array([ 0.4573122 ,  0.4806955 , -0.51229084, -0.48065227, -0.46476695,\n",
              "         0.47463536,  0.43751335, -0.5019901 , -0.43394285, -0.43410933,\n",
              "        -0.4389663 ,  0.4660819 ,  0.42144492,  0.4484143 , -0.48343593,\n",
              "        -0.42491376,  0.43694034,  0.48240674, -0.46804905, -0.4250062 ,\n",
              "        -0.4466344 ,  0.43400872,  0.4445981 ,  0.4662165 , -0.4290027 ,\n",
              "        -0.47948536,  0.4291649 ,  0.5146545 ,  0.49251598,  0.43647948,\n",
              "        -0.49412456,  0.4268443 , -0.4898659 , -0.4730846 , -0.45082024,\n",
              "        -0.49576318,  0.4883126 , -0.45302618,  0.46190366, -0.51045597,\n",
              "         0.49925783, -0.46847957, -0.4874216 , -0.42342937,  0.4818166 ,\n",
              "         0.4713897 ,  0.49401876,  0.46107244, -0.48181745,  0.44709057],\n",
              "       dtype=float32),\n",
              " 'situation': array([ 0.5338484 ,  0.4704029 , -0.4431393 , -0.49722895, -0.4809684 ,\n",
              "         0.48019338,  0.51009405, -0.46070492, -0.43123654, -0.47516605,\n",
              "        -0.44242063,  0.4503731 ,  0.45227644,  0.47880825, -0.52733946,\n",
              "        -0.52516884,  0.46052745,  0.447085  , -0.46721312, -0.48025313,\n",
              "        -0.48749268,  0.50609547,  0.48281288,  0.4808181 , -0.4797881 ,\n",
              "        -0.5105318 ,  0.4568676 ,  0.5113902 ,  0.46194923,  0.5092797 ,\n",
              "        -0.48406908,  0.48878774, -0.46017915, -0.46872687, -0.49263638,\n",
              "        -0.50828147,  0.48273066, -0.45636636,  0.50991124, -0.51347774,\n",
              "         0.4608057 , -0.45747387, -0.48109508, -0.48352808,  0.5028276 ,\n",
              "         0.46674287,  0.5037935 ,  0.502958  , -0.49544385,  0.4431407 ],\n",
              "       dtype=float32),\n",
              " 'necessarily': array([ 0.47054464,  0.4244358 , -0.4892336 , -0.52503437, -0.52734166,\n",
              "         0.48339084,  0.4902848 , -0.44255245, -0.47206187, -0.5138618 ,\n",
              "        -0.4453453 ,  0.51919156,  0.49685368,  0.47392997, -0.47922716,\n",
              "        -0.44744527,  0.4692366 ,  0.46883345, -0.45716125, -0.4675018 ,\n",
              "        -0.4496299 ,  0.48078197,  0.5008544 ,  0.44854182, -0.5107303 ,\n",
              "        -0.4797845 ,  0.4730708 ,  0.47396773,  0.45441532,  0.4907752 ,\n",
              "        -0.49012524,  0.5196279 , -0.5396539 , -0.50692964, -0.45625135,\n",
              "        -0.45191056,  0.48025578, -0.50921434,  0.44983408, -0.5230129 ,\n",
              "         0.5020389 , -0.4946769 , -0.43265265, -0.4922161 ,  0.52575105,\n",
              "         0.5099187 ,  0.49583283,  0.4619789 , -0.47538298,  0.51965606],\n",
              "       dtype=float32),\n",
              " 'characteristics': array([ 0.5408343 ,  0.4964198 , -0.5053991 , -0.4776911 , -0.48410183,\n",
              "         0.495605  ,  0.50819546, -0.43451595, -0.5042112 , -0.46608973,\n",
              "        -0.5189011 ,  0.4533768 ,  0.48773518,  0.49770448, -0.5540524 ,\n",
              "        -0.50398344,  0.4661618 ,  0.46023396, -0.510821  , -0.4761973 ,\n",
              "        -0.4652866 ,  0.4420027 ,  0.45092714,  0.47659007, -0.46649936,\n",
              "        -0.4548452 ,  0.48890388,  0.5015909 ,  0.47906724,  0.48496363,\n",
              "        -0.47506985,  0.5012971 , -0.50972134, -0.49135548, -0.4310889 ,\n",
              "        -0.44584262,  0.47337264, -0.52252305,  0.4542044 , -0.4626016 ,\n",
              "         0.5188185 , -0.4684109 , -0.49542397, -0.5106189 ,  0.4602576 ,\n",
              "         0.47788045,  0.50991887,  0.48226842, -0.45423046,  0.5088758 ],\n",
              "       dtype=float32),\n",
              " 'barclays': array([ 0.5020848 ,  0.42659038, -0.5160605 , -0.49470013, -0.4474422 ,\n",
              "         0.48120892,  0.47197855, -0.44611606, -0.4533487 , -0.45172372,\n",
              "        -0.47454557,  0.5039579 ,  0.46917993,  0.47892293, -0.5230572 ,\n",
              "        -0.47481462,  0.52099407,  0.46857396, -0.47931373, -0.5101055 ,\n",
              "        -0.44365683,  0.507109  ,  0.52092975,  0.4749941 , -0.43157095,\n",
              "        -0.47565573,  0.5190463 ,  0.5475097 ,  0.43769786,  0.4415856 ,\n",
              "        -0.5071158 ,  0.4386559 , -0.530302  , -0.5287684 , -0.44145787,\n",
              "        -0.52926236,  0.5288887 , -0.48334962,  0.4419519 , -0.44529444,\n",
              "         0.52716213, -0.45978743, -0.43686172, -0.48290548,  0.4909461 ,\n",
              "         0.5173465 ,  0.51871336,  0.49470085, -0.4727389 ,  0.5209925 ],\n",
              "       dtype=float32),\n",
              " 'updated': array([ 0.47134674,  0.42555243, -0.4779083 , -0.4682175 , -0.48906082,\n",
              "         0.5402848 ,  0.4853101 , -0.5002977 , -0.52595013, -0.43969852,\n",
              "        -0.5258104 ,  0.44285765,  0.4969683 ,  0.51736325, -0.48138648,\n",
              "        -0.47587636,  0.50683546,  0.50483114, -0.5001541 , -0.43220267,\n",
              "        -0.520194  ,  0.5122531 ,  0.5078446 ,  0.460814  , -0.46932536,\n",
              "        -0.5269212 ,  0.44746846,  0.5026443 ,  0.4701785 ,  0.4536955 ,\n",
              "        -0.49651194,  0.43572497, -0.45458654, -0.46793687, -0.49128595,\n",
              "        -0.47212982,  0.5274064 , -0.44453955,  0.5027066 , -0.47540787,\n",
              "         0.5143903 , -0.53300714, -0.4582571 , -0.51247245,  0.4555743 ,\n",
              "         0.52253026,  0.51158154,  0.50782484, -0.46203148,  0.49393156],\n",
              "       dtype=float32),\n",
              " 'level': array([ 0.50324833,  0.49482775, -0.52209866, -0.47067976, -0.47369736,\n",
              "         0.44299778,  0.4375571 , -0.4775256 , -0.44013074, -0.50130475,\n",
              "        -0.49752086,  0.49232244,  0.453818  ,  0.5205105 , -0.5150926 ,\n",
              "        -0.48314735,  0.48389578,  0.45837164, -0.4442826 , -0.45444235,\n",
              "        -0.4842525 ,  0.42437404,  0.4502494 ,  0.46595493, -0.4688052 ,\n",
              "        -0.4841672 ,  0.50657594,  0.47347572,  0.48743075,  0.43160504,\n",
              "        -0.47022742,  0.49242246, -0.519573  , -0.4656563 , -0.48964456,\n",
              "        -0.5111642 ,  0.4769868 , -0.5032386 ,  0.5105628 , -0.48302656,\n",
              "         0.4428828 , -0.52837664, -0.45609984, -0.5076359 ,  0.4869979 ,\n",
              "         0.49278384,  0.44417134,  0.46956044, -0.49081957,  0.5101169 ],\n",
              "       dtype=float32),\n",
              " 'september': array([ 0.45349175,  0.4749284 , -0.5184016 , -0.46079093, -0.509731  ,\n",
              "         0.53034747,  0.45177627, -0.51413286, -0.44907224, -0.48346567,\n",
              "        -0.4544584 ,  0.51852435,  0.4490778 ,  0.4484991 , -0.5074302 ,\n",
              "        -0.48422343,  0.52390444,  0.47642297, -0.46090624, -0.5037198 ,\n",
              "        -0.4706739 ,  0.46399137,  0.50655895,  0.5103505 , -0.44727844,\n",
              "        -0.45998323,  0.48146206,  0.4583877 ,  0.5150745 ,  0.45575395,\n",
              "        -0.4884503 ,  0.5102091 , -0.52244186, -0.5206444 , -0.45641527,\n",
              "        -0.52102125,  0.46094567, -0.4708143 ,  0.5025302 , -0.49335453,\n",
              "         0.5170635 , -0.47306773, -0.50823396, -0.4864869 ,  0.46525732,\n",
              "         0.47261196,  0.5044025 ,  0.48471797, -0.45401666,  0.44941387],\n",
              "       dtype=float32),\n",
              " 'dollar': array([ 0.52030003,  0.46089923, -0.4939437 , -0.43109334, -0.51476943,\n",
              "         0.5295571 ,  0.4651425 , -0.47551677, -0.44701555, -0.47761956,\n",
              "        -0.48726764,  0.48942506,  0.49322435,  0.44780675, -0.51338416,\n",
              "        -0.51444405,  0.46993926,  0.5174817 , -0.5098014 , -0.45202172,\n",
              "        -0.46891493,  0.48370814,  0.45157486,  0.46009442, -0.50755835,\n",
              "        -0.50796485,  0.4335447 ,  0.509396  ,  0.4339987 ,  0.4910676 ,\n",
              "        -0.47848853,  0.4798181 , -0.46301118, -0.51465815, -0.48437116,\n",
              "        -0.4991904 ,  0.5232468 , -0.4334902 ,  0.44618753, -0.5168461 ,\n",
              "         0.48700157, -0.4729486 , -0.5102321 , -0.5165622 ,  0.5005125 ,\n",
              "         0.44038612,  0.4793328 ,  0.47107738, -0.51182675,  0.48507363],\n",
              "       dtype=float32),\n",
              " 'advisers': array([ 0.44090962,  0.46543384, -0.4601733 , -0.46370655, -0.44749743,\n",
              "         0.47715986,  0.48517773, -0.417694  , -0.50161517, -0.446833  ,\n",
              "        -0.49501982,  0.45058367,  0.4157717 ,  0.43026817, -0.49807322,\n",
              "        -0.5032165 ,  0.48752648,  0.48187518, -0.49092552, -0.4076761 ,\n",
              "        -0.43256044,  0.47049195,  0.42672014,  0.4395991 , -0.4829491 ,\n",
              "        -0.4267516 ,  0.42745173,  0.43841696,  0.45700812,  0.47985446,\n",
              "        -0.45453227,  0.47014678, -0.45940727, -0.4790165 , -0.47502762,\n",
              "        -0.4619036 ,  0.42337364, -0.45740256,  0.41844016, -0.4381893 ,\n",
              "         0.51033586, -0.47696063, -0.42234564, -0.4912973 ,  0.48106393,\n",
              "         0.49422517,  0.49434724,  0.47953653, -0.41684422,  0.4589985 ],\n",
              "       dtype=float32),\n",
              " 'whole': array([ 0.51602155,  0.4821455 , -0.47550467, -0.4584422 , -0.4873864 ,\n",
              "         0.5102798 ,  0.49342072, -0.5270496 , -0.48817092, -0.4772797 ,\n",
              "        -0.4937327 ,  0.49850774,  0.47868308,  0.45403206, -0.476557  ,\n",
              "        -0.49615994,  0.44358632,  0.50298005, -0.47097883, -0.50862443,\n",
              "        -0.5153778 ,  0.482706  ,  0.49733686,  0.49291012, -0.43995994,\n",
              "        -0.48987392,  0.4905189 ,  0.48591405,  0.46414953,  0.5191941 ,\n",
              "        -0.5259821 ,  0.4865746 , -0.4532087 , -0.45562953, -0.4871206 ,\n",
              "        -0.5237205 ,  0.4492237 , -0.4441006 ,  0.49474722, -0.49333268,\n",
              "         0.47600514, -0.4876504 , -0.4910056 , -0.4386466 ,  0.469342  ,\n",
              "         0.4506524 ,  0.48141503,  0.49921542, -0.5134214 ,  0.46470478],\n",
              "       dtype=float32),\n",
              " 'bloomberg': array([ 0.51961446,  0.50577956, -0.50363964, -0.50029516, -0.4909976 ,\n",
              "         0.5111952 ,  0.4820755 , -0.46656173, -0.5121614 , -0.43371183,\n",
              "        -0.46468407,  0.507301  ,  0.44246787,  0.518646  , -0.5188077 ,\n",
              "        -0.5297443 ,  0.46481735,  0.49767423, -0.48194325, -0.49152148,\n",
              "        -0.477805  ,  0.5066937 ,  0.5144129 ,  0.46250027, -0.47527182,\n",
              "        -0.47738683,  0.4648559 ,  0.5028395 ,  0.5044566 ,  0.46711317,\n",
              "        -0.4703193 ,  0.47929057, -0.52418447, -0.48082927, -0.4484274 ,\n",
              "        -0.47786885,  0.45749706, -0.5336665 ,  0.50495625, -0.5216627 ,\n",
              "         0.45076528, -0.4966417 , -0.50745785, -0.4679569 ,  0.43556464,\n",
              "         0.44725147,  0.46080926,  0.4364853 , -0.44091472,  0.4405927 ],\n",
              "       dtype=float32),\n",
              " 'requirements': array([ 0.4706505 ,  0.42641452, -0.49113327, -0.5077936 , -0.46997488,\n",
              "         0.45732805,  0.52329   , -0.49730873, -0.5124131 , -0.46696922,\n",
              "        -0.48478556,  0.5121726 ,  0.51378566,  0.48495367, -0.48326126,\n",
              "        -0.49528527,  0.47670245,  0.44153187, -0.45136142, -0.445903  ,\n",
              "        -0.5017149 ,  0.46281505,  0.47868568,  0.452622  , -0.4915353 ,\n",
              "        -0.47336733,  0.50082517,  0.4668025 ,  0.4686855 ,  0.5175616 ,\n",
              "        -0.46140212,  0.47382772, -0.53828967, -0.4708233 , -0.48073995,\n",
              "        -0.47516263,  0.48378986, -0.48125666,  0.44992512, -0.50538373,\n",
              "         0.4902545 , -0.48393247, -0.5178567 , -0.4385506 ,  0.5216371 ,\n",
              "         0.44657594,  0.4525608 ,  0.49333745, -0.5021586 ,  0.49466354],\n",
              "       dtype=float32),\n",
              " 'economies': array([ 0.505834  ,  0.5062954 , -0.5153499 , -0.50103706, -0.45240366,\n",
              "         0.4909442 ,  0.43191496, -0.5079078 , -0.49626598, -0.4659087 ,\n",
              "        -0.5085937 ,  0.47800636,  0.48690534,  0.44840878, -0.5193342 ,\n",
              "        -0.49861974,  0.46986216,  0.49173212, -0.51486623, -0.48948625,\n",
              "        -0.4857754 ,  0.4958627 ,  0.4892693 ,  0.46653724, -0.5002896 ,\n",
              "        -0.47545922,  0.51716095,  0.50275147,  0.47349232,  0.50019485,\n",
              "        -0.4521061 ,  0.5038216 , -0.5118288 , -0.50133336, -0.44804516,\n",
              "        -0.45229375,  0.4973871 , -0.4828314 ,  0.4995544 , -0.48455715,\n",
              "         0.4825598 , -0.50370586, -0.49457324, -0.4767118 ,  0.49930683,\n",
              "         0.46720943,  0.44547078,  0.52045417, -0.48606896,  0.4886123 ],\n",
              "       dtype=float32),\n",
              " 'diversified': array([ 0.54621637,  0.4887631 , -0.52296495, -0.47468725, -0.48897973,\n",
              "         0.52736485,  0.44488364, -0.50973904, -0.4573596 , -0.48376733,\n",
              "        -0.5239357 ,  0.5190512 ,  0.5142514 ,  0.47112873, -0.52640826,\n",
              "        -0.44274864,  0.44417638,  0.4391038 , -0.49210435, -0.48582906,\n",
              "        -0.51592535,  0.47105372,  0.5205359 ,  0.5079003 , -0.4423538 ,\n",
              "        -0.5076741 ,  0.5226917 ,  0.50969356,  0.4915734 ,  0.48230147,\n",
              "        -0.5399349 ,  0.4916533 , -0.5261759 , -0.5274954 , -0.52440137,\n",
              "        -0.44479683,  0.49455038, -0.5031869 ,  0.45877644, -0.52378166,\n",
              "         0.5320644 , -0.5037794 , -0.49875814, -0.4652582 ,  0.45110518,\n",
              "         0.50881237,  0.47540554,  0.5048996 , -0.45117742,  0.45845604],\n",
              "       dtype=float32),\n",
              " 'duration': array([ 0.46450785,  0.4422787 , -0.48005852, -0.45211625, -0.491171  ,\n",
              "         0.5009916 ,  0.5212703 , -0.44019216, -0.46068808, -0.47271594,\n",
              "        -0.5069932 ,  0.48399174,  0.45892832,  0.50008285, -0.5288596 ,\n",
              "        -0.4866588 ,  0.47599554,  0.5149631 , -0.43861264, -0.5120071 ,\n",
              "        -0.47301126,  0.50948995,  0.44581944,  0.45555797, -0.4409759 ,\n",
              "        -0.48598635,  0.49886873,  0.48459667,  0.4994029 ,  0.42959803,\n",
              "        -0.51249236,  0.49439207, -0.506138  , -0.46705955, -0.42403167,\n",
              "        -0.46459848,  0.5213545 , -0.5012237 ,  0.43467304, -0.44709638,\n",
              "         0.46896246, -0.48670447, -0.50044185, -0.51819956,  0.46299717,\n",
              "         0.4868109 ,  0.49189174,  0.5199661 , -0.43385336,  0.49299994],\n",
              "       dtype=float32),\n",
              " 'group': array([ 0.46098006,  0.4854194 , -0.47590038, -0.45015368, -0.454926  ,\n",
              "         0.5215189 ,  0.4973873 , -0.44544628, -0.48071924, -0.43016306,\n",
              "        -0.51065385,  0.46503955,  0.5004669 ,  0.46716246, -0.554176  ,\n",
              "        -0.47982946,  0.52127635,  0.4705447 , -0.45690528, -0.50402117,\n",
              "        -0.46186283,  0.50087863,  0.45565504,  0.46573782, -0.47241524,\n",
              "        -0.48713568,  0.42945343,  0.46100056,  0.518331  ,  0.51604605,\n",
              "        -0.5070867 ,  0.5111058 , -0.46528524, -0.5077066 , -0.439445  ,\n",
              "        -0.46602917,  0.46753407, -0.5033637 ,  0.47561228, -0.5110673 ,\n",
              "         0.5036118 , -0.49774885, -0.4784269 , -0.49853247,  0.4946589 ,\n",
              "         0.4675138 ,  0.46593925,  0.45999208, -0.4666553 ,  0.4754421 ],\n",
              "       dtype=float32),\n",
              " 'july': array([ 0.47162366,  0.42941427, -0.4610687 , -0.4960446 , -0.49014738,\n",
              "         0.51318425,  0.49717972, -0.5029824 , -0.45924   , -0.4403463 ,\n",
              "        -0.50423074,  0.49162266,  0.46761462,  0.48025113, -0.5155568 ,\n",
              "        -0.5168942 ,  0.49582136,  0.52730805, -0.43718123, -0.48735073,\n",
              "        -0.48341078,  0.4761213 ,  0.49842733,  0.4987456 , -0.49075675,\n",
              "        -0.4557925 ,  0.5132981 ,  0.5450675 ,  0.52428794,  0.4708203 ,\n",
              "        -0.52175057,  0.47648013, -0.53827995, -0.5180963 , -0.46958673,\n",
              "        -0.49634844,  0.53077507, -0.5117175 ,  0.52043056, -0.48067442,\n",
              "         0.5089652 , -0.48131853, -0.49672073, -0.51316893,  0.4590198 ,\n",
              "         0.49021643,  0.46102098,  0.46748194, -0.5357179 ,  0.53035593],\n",
              "       dtype=float32),\n",
              " 'grade': array([ 0.54950124,  0.487191  , -0.45641363, -0.44027537, -0.46847114,\n",
              "         0.49725527,  0.48951447, -0.46634465, -0.48168305, -0.47738725,\n",
              "        -0.46004337,  0.52544266,  0.49474633,  0.5185255 , -0.47489628,\n",
              "        -0.49769714,  0.49054995,  0.5208034 , -0.43718293, -0.50131845,\n",
              "        -0.44996798,  0.44426292,  0.5175458 ,  0.44563663, -0.4979316 ,\n",
              "        -0.512578  ,  0.43791464,  0.5031136 ,  0.49186283,  0.475704  ,\n",
              "        -0.46108213,  0.44933137, -0.50195086, -0.5224892 , -0.444444  ,\n",
              "        -0.5265802 ,  0.46019533, -0.47058144,  0.43691605, -0.50151515,\n",
              "         0.48138466, -0.5050826 , -0.4341977 , -0.46451896,  0.4737952 ,\n",
              "         0.4448568 ,  0.50656533,  0.4401321 , -0.49014157,  0.48964348],\n",
              "       dtype=float32),\n",
              " 'affiliates': array([ 0.53602713,  0.4774027 , -0.4722529 , -0.5286889 , -0.49109387,\n",
              "         0.4551534 ,  0.46168032, -0.49089435, -0.477908  , -0.48114827,\n",
              "        -0.45055127,  0.48256263,  0.5110253 ,  0.46089932, -0.4968858 ,\n",
              "        -0.4696015 ,  0.47346133,  0.49043715, -0.4798043 , -0.445738  ,\n",
              "        -0.4705831 ,  0.465037  ,  0.51077217,  0.5174849 , -0.5169363 ,\n",
              "        -0.49650764,  0.49048737,  0.5302417 ,  0.49867147,  0.4468281 ,\n",
              "        -0.4993112 ,  0.43956178, -0.5094949 , -0.4914376 , -0.5242555 ,\n",
              "        -0.5247394 ,  0.47513494, -0.5005541 ,  0.46491864, -0.49127728,\n",
              "         0.46615243, -0.46008688, -0.48108467, -0.47657493,  0.49226028,\n",
              "         0.5008134 ,  0.4905516 ,  0.45287094, -0.47586852,  0.47900674],\n",
              "       dtype=float32),\n",
              " 'exceed': array([ 0.4735511 ,  0.45086244, -0.5212541 , -0.45888817, -0.504959  ,\n",
              "         0.52649343,  0.48239708, -0.5228239 , -0.4802094 , -0.4354347 ,\n",
              "        -0.5100566 ,  0.501043  ,  0.5023072 ,  0.43973726, -0.56060314,\n",
              "        -0.5151772 ,  0.45146567,  0.45648187, -0.49061477, -0.44432002,\n",
              "        -0.51141477,  0.43210748,  0.5182807 ,  0.4566304 , -0.46715826,\n",
              "        -0.48867604,  0.50228447,  0.5403829 ,  0.4474273 ,  0.44252753,\n",
              "        -0.49679217,  0.43919367, -0.5365639 , -0.46068698, -0.4605874 ,\n",
              "        -0.5068461 ,  0.53382087, -0.4713734 ,  0.43280235, -0.48787925,\n",
              "         0.44912025, -0.4899884 , -0.47724298, -0.50195533,  0.4596866 ,\n",
              "         0.49154407,  0.46246204,  0.4894689 , -0.49910524,  0.47705126],\n",
              "       dtype=float32),\n",
              " 'approach': array([ 0.4663122 ,  0.4714104 , -0.5216842 , -0.45309913, -0.45400628,\n",
              "         0.4516876 ,  0.46259403, -0.48482087, -0.47331682, -0.46780214,\n",
              "        -0.43344805,  0.4746847 ,  0.5142639 ,  0.52602875, -0.47271776,\n",
              "        -0.5242679 ,  0.4509843 ,  0.4693457 , -0.50623757, -0.47219452,\n",
              "        -0.42827287,  0.44938922,  0.52546847,  0.43069345, -0.48698178,\n",
              "        -0.5169915 ,  0.5027579 ,  0.50018656,  0.48112804,  0.5129546 ,\n",
              "        -0.4646814 ,  0.5103501 , -0.4861325 , -0.5110711 , -0.45432672,\n",
              "        -0.52287656,  0.47791266, -0.4976531 ,  0.44434005, -0.450051  ,\n",
              "         0.47232336, -0.48695743, -0.46083617, -0.5106287 ,  0.48012578,\n",
              "         0.51389647,  0.4471597 ,  0.47477588, -0.4925514 ,  0.476823  ],\n",
              "       dtype=float32),\n",
              " 'decisions': array([ 0.4920672 ,  0.42484757, -0.49852717, -0.4670715 , -0.4899598 ,\n",
              "         0.44220367,  0.44799203, -0.51323056, -0.42173865, -0.49665323,\n",
              "        -0.43526232,  0.45695645,  0.4370597 ,  0.4712329 , -0.47939825,\n",
              "        -0.46927136,  0.43139872,  0.4288644 , -0.5075802 , -0.49230975,\n",
              "        -0.4956242 ,  0.45228064,  0.4307701 ,  0.5096267 , -0.4759861 ,\n",
              "        -0.4724169 ,  0.4449911 ,  0.44816267,  0.45232987,  0.47887817,\n",
              "        -0.45225206,  0.4288997 , -0.5134517 , -0.51397413, -0.44174054,\n",
              "        -0.48974136,  0.5018711 , -0.4922978 ,  0.4762397 , -0.4978825 ,\n",
              "         0.49702364, -0.4495031 , -0.47737846, -0.46573842,  0.4802049 ,\n",
              "         0.45304626,  0.49753356,  0.50502723, -0.4249064 ,  0.4872592 ],\n",
              "       dtype=float32),\n",
              " 'minimums': array([ 0.46545556,  0.4597953 , -0.47187507, -0.47738314, -0.5281193 ,\n",
              "         0.47552383,  0.51444006, -0.47412187, -0.47497874, -0.50046194,\n",
              "        -0.44377956,  0.4884968 ,  0.49525988,  0.46484256, -0.5332242 ,\n",
              "        -0.48397407,  0.4948901 ,  0.4918093 , -0.51808894, -0.4574814 ,\n",
              "        -0.49612865,  0.44416714,  0.46581987,  0.48719653, -0.45508373,\n",
              "        -0.5135572 ,  0.5088903 ,  0.501548  ,  0.444128  ,  0.49279308,\n",
              "        -0.46605745,  0.4993618 , -0.46762335, -0.4795299 , -0.46740273,\n",
              "        -0.46813443,  0.49478438, -0.48801112,  0.5287872 , -0.4958821 ,\n",
              "         0.463705  , -0.45759997, -0.43665823, -0.505593  ,  0.5168035 ,\n",
              "         0.49821794,  0.48357892,  0.50078404, -0.49814934,  0.48436263],\n",
              "       dtype=float32),\n",
              " 'form': array([ 0.5014045 ,  0.48198172, -0.5165026 , -0.44479963, -0.5323104 ,\n",
              "         0.51451534,  0.5000497 , -0.5175678 , -0.50822145, -0.4865898 ,\n",
              "        -0.45388374,  0.47823042,  0.4934044 ,  0.5004287 , -0.4762882 ,\n",
              "        -0.4831157 ,  0.4362602 ,  0.513636  , -0.47895068, -0.45530802,\n",
              "        -0.49140528,  0.49644125,  0.4612612 ,  0.485067  , -0.4600749 ,\n",
              "        -0.4660414 ,  0.50213796,  0.45666838,  0.4654317 ,  0.43565437,\n",
              "        -0.5131779 ,  0.44324464, -0.46933088, -0.49646372, -0.43829924,\n",
              "        -0.49103144,  0.46648118, -0.4979117 ,  0.49239093, -0.43885383,\n",
              "         0.5198509 , -0.50433326, -0.45054752, -0.4268765 ,  0.46681705,\n",
              "         0.4812869 ,  0.42792895,  0.43529168, -0.4546502 ,  0.43094364],\n",
              "       dtype=float32),\n",
              " 'established': array([ 0.503907  ,  0.47231576, -0.4963963 , -0.52394783, -0.5083364 ,\n",
              "         0.4833421 ,  0.45420125, -0.4711776 , -0.5087039 , -0.50077134,\n",
              "        -0.4369268 ,  0.53521305,  0.49060005,  0.4598061 , -0.5517091 ,\n",
              "        -0.49592984,  0.48002997,  0.46807763, -0.47970933, -0.47306132,\n",
              "        -0.5264575 ,  0.50092334,  0.5115029 ,  0.46905223, -0.49802524,\n",
              "        -0.45921195,  0.49014658,  0.46917686,  0.46027964,  0.50450087,\n",
              "        -0.47299585,  0.46276137, -0.47099122, -0.5167407 , -0.47471446,\n",
              "        -0.50191265,  0.5275128 , -0.4549452 ,  0.45696107, -0.5052394 ,\n",
              "         0.51827997, -0.4696116 , -0.50487924, -0.48017266,  0.49546248,\n",
              "         0.44596937,  0.4487222 ,  0.5137509 , -0.5121312 ,  0.4607696 ],\n",
              "       dtype=float32),\n",
              " 'withdrawal': array([ 0.51193404,  0.41692764, -0.51577544, -0.44786948, -0.4642116 ,\n",
              "         0.52829117,  0.43772212, -0.46664006, -0.5058135 , -0.4724881 ,\n",
              "        -0.43636793,  0.44431302,  0.5014089 ,  0.45828706, -0.5027082 ,\n",
              "        -0.49290523,  0.5020882 ,  0.44090098, -0.50843585, -0.4930278 ,\n",
              "        -0.45486313,  0.47233725,  0.46774432,  0.47521764, -0.5016898 ,\n",
              "        -0.46734387,  0.4789641 ,  0.4796718 ,  0.49860275,  0.4332084 ,\n",
              "        -0.5084745 ,  0.48158604, -0.45799506, -0.51939243, -0.4222669 ,\n",
              "        -0.482003  ,  0.47894028, -0.5083476 ,  0.509874  , -0.48892128,\n",
              "         0.46701017, -0.5279212 , -0.4858122 , -0.48403412,  0.500894  ,\n",
              "         0.44613683,  0.50713336,  0.47420654, -0.42935348,  0.48782718],\n",
              "       dtype=float32),\n",
              " 'produce': array([ 0.4602383 ,  0.4455565 , -0.4770335 , -0.4808749 , -0.52583355,\n",
              "         0.53185093,  0.49724063, -0.4741301 , -0.45950127, -0.42977208,\n",
              "        -0.51575387,  0.5190936 ,  0.49153686,  0.4536124 , -0.5204291 ,\n",
              "        -0.4756495 ,  0.4822807 ,  0.46273196, -0.47346845, -0.43853858,\n",
              "        -0.46191472,  0.4470185 ,  0.5177739 ,  0.48887807, -0.4694511 ,\n",
              "        -0.5261061 ,  0.46418497,  0.48751011,  0.47698626,  0.47154626,\n",
              "        -0.5199761 ,  0.46702194, -0.4845627 , -0.49991947, -0.5067998 ,\n",
              "        -0.48261604,  0.4523905 , -0.46399394,  0.46415135, -0.47736925,\n",
              "         0.46490252, -0.5137163 , -0.468215  , -0.50283587,  0.5126576 ,\n",
              "         0.44953334,  0.5050368 ,  0.45731777, -0.47170997,  0.4431016 ],\n",
              "       dtype=float32),\n",
              " 'competitive': array([ 0.51655954,  0.47815034, -0.4708206 , -0.48820388, -0.48979312,\n",
              "         0.5076613 ,  0.51906216, -0.4612764 , -0.50730366, -0.4562524 ,\n",
              "        -0.4741618 ,  0.43521678,  0.4539703 ,  0.5089403 , -0.5092175 ,\n",
              "        -0.47625452,  0.49545017,  0.5005175 , -0.4937355 , -0.44828153,\n",
              "        -0.4671194 ,  0.5024178 ,  0.4571934 ,  0.46883425, -0.4695252 ,\n",
              "        -0.48603868,  0.50576437,  0.53210485,  0.45572996,  0.4743833 ,\n",
              "        -0.45884472,  0.43283957, -0.46242163, -0.43699014, -0.44542482,\n",
              "        -0.48505664,  0.509036  , -0.49069682,  0.4774774 , -0.43905735,\n",
              "         0.45923597, -0.45454085, -0.45073926, -0.47695827,  0.4842959 ,\n",
              "         0.48472852,  0.46874872,  0.50756675, -0.5106913 ,  0.49315378],\n",
              "       dtype=float32),\n",
              " 'direct': array([ 0.49609402,  0.42912224, -0.46719185, -0.44230163, -0.47786078,\n",
              "         0.4920563 ,  0.5102382 , -0.5197778 , -0.51711035, -0.45046052,\n",
              "        -0.44213247,  0.52161914,  0.49919993,  0.50388914, -0.52661514,\n",
              "        -0.44217134,  0.46945637,  0.4388584 , -0.4937654 , -0.4600007 ,\n",
              "        -0.48179325,  0.43551552,  0.50422287,  0.51501524, -0.5144134 ,\n",
              "        -0.47661883,  0.4354882 ,  0.45847556,  0.45468467,  0.50837916,\n",
              "        -0.46030226,  0.4643595 , -0.50286704, -0.48290446, -0.5129556 ,\n",
              "        -0.49735856,  0.48084536, -0.51487404,  0.5084906 , -0.5240487 ,\n",
              "         0.46363026, -0.50685716, -0.475874  , -0.4929977 ,  0.4916671 ,\n",
              "         0.4916604 ,  0.43827003,  0.44097936, -0.49753782,  0.48412627],\n",
              "       dtype=float32),\n",
              " 'loan': array([ 0.5078771 ,  0.4928709 , -0.48395294, -0.49218333, -0.52463084,\n",
              "         0.45661843,  0.43551463, -0.5045211 , -0.46899903, -0.52179086,\n",
              "        -0.4497074 ,  0.4912439 ,  0.4531698 ,  0.51654315, -0.46390978,\n",
              "        -0.46453276,  0.46808112,  0.4788286 , -0.46276954, -0.5015365 ,\n",
              "        -0.4524592 ,  0.44403976,  0.46788168,  0.5186357 , -0.5168877 ,\n",
              "        -0.45563087,  0.49059212,  0.512023  ,  0.51727796,  0.49678588,\n",
              "        -0.5009766 ,  0.48182708, -0.51352143, -0.4539691 , -0.4546482 ,\n",
              "        -0.47241235,  0.4676967 , -0.446973  ,  0.52749187, -0.53366673,\n",
              "         0.4612406 , -0.44505414, -0.5063204 , -0.52301794,  0.4410732 ,\n",
              "         0.49399477,  0.5174018 ,  0.524418  , -0.49750352,  0.5148382 ],\n",
              "       dtype=float32),\n",
              " 'lipper': array([ 0.53467476,  0.4693457 , -0.5096266 , -0.44680363, -0.5118971 ,\n",
              "         0.5305223 ,  0.4340394 , -0.47948128, -0.4606693 , -0.5053936 ,\n",
              "        -0.51451135,  0.48023605,  0.4497122 ,  0.50408465, -0.55395293,\n",
              "        -0.510703  ,  0.5142402 ,  0.45745   , -0.44731596, -0.50586545,\n",
              "        -0.42807874,  0.47735733,  0.47627145,  0.51255786, -0.46606645,\n",
              "        -0.45128572,  0.45468897,  0.5062478 ,  0.45850667,  0.47681078,\n",
              "        -0.5166346 ,  0.5071852 , -0.501298  , -0.4571874 , -0.51009125,\n",
              "        -0.514444  ,  0.45435748, -0.44408065,  0.4376805 , -0.44343972,\n",
              "         0.52603567, -0.4585119 , -0.4681759 , -0.43882084,  0.48576415,\n",
              "         0.47633082,  0.5092602 ,  0.45398524, -0.50306374,  0.47264835],\n",
              "       dtype=float32),\n",
              " 'hedging': array([ 0.5188955 ,  0.47330567, -0.4474526 , -0.5281264 , -0.45856884,\n",
              "         0.49159157,  0.46451524, -0.45973998, -0.47566918, -0.437984  ,\n",
              "        -0.45685926,  0.46391377,  0.45956331,  0.48030406, -0.5172863 ,\n",
              "        -0.4986378 ,  0.4428781 ,  0.5222013 , -0.43423387, -0.4704687 ,\n",
              "        -0.47901493,  0.4734178 ,  0.46017236,  0.43740875, -0.5040673 ,\n",
              "        -0.52988964,  0.44167727,  0.50774914,  0.47054225,  0.4454819 ,\n",
              "        -0.5169369 ,  0.47577903, -0.48882833, -0.48268408, -0.44710496,\n",
              "        -0.5215999 ,  0.47981048, -0.52292705,  0.4830433 , -0.44764206,\n",
              "         0.46601564, -0.51197004, -0.49848524, -0.4914037 ,  0.51501703,\n",
              "         0.5201256 ,  0.47331282,  0.5110955 , -0.48568997,  0.46296445],\n",
              "       dtype=float32),\n",
              " 'marginal': array([ 0.48334444,  0.48469177, -0.4824539 , -0.44026572, -0.517247  ,\n",
              "         0.45924088,  0.48755917, -0.5100515 , -0.51413673, -0.45286474,\n",
              "        -0.44156384,  0.476716  ,  0.46471485,  0.49796367, -0.5247951 ,\n",
              "        -0.4541279 ,  0.5016399 ,  0.49595302, -0.48617443, -0.5057724 ,\n",
              "        -0.43804774,  0.4962626 ,  0.5282278 ,  0.47150373, -0.46711904,\n",
              "        -0.44456384,  0.4440243 ,  0.54457724,  0.4773473 ,  0.4458329 ,\n",
              "        -0.45347086,  0.49599907, -0.54084593, -0.5064445 , -0.4803452 ,\n",
              "        -0.46797892,  0.5299041 , -0.50845754,  0.48255095, -0.4650591 ,\n",
              "         0.46846688, -0.47044137, -0.5083431 , -0.43231678,  0.4752865 ,\n",
              "         0.49707252,  0.44390917,  0.4550059 , -0.5270821 ,  0.4475679 ],\n",
              "       dtype=float32),\n",
              " 'involves': array([ 0.46975142,  0.42542568, -0.44852275, -0.44376868, -0.45408046,\n",
              "         0.4724834 ,  0.44250214, -0.47246143, -0.5094    , -0.4930586 ,\n",
              "        -0.50265396,  0.44880265,  0.4782213 ,  0.48663104, -0.5485516 ,\n",
              "        -0.45129836,  0.5255295 ,  0.53281844, -0.44967964, -0.5044627 ,\n",
              "        -0.50309837,  0.4577301 ,  0.5033062 ,  0.43191287, -0.45324963,\n",
              "        -0.51363367,  0.44949624,  0.5291795 ,  0.45883784,  0.51809335,\n",
              "        -0.4917631 ,  0.4382431 , -0.5376643 , -0.51221025, -0.45852196,\n",
              "        -0.48670986,  0.49693108, -0.46158096,  0.45003706, -0.46197897,\n",
              "         0.48800427, -0.5311777 , -0.4992156 , -0.52107847,  0.45053813,\n",
              "         0.5019629 ,  0.47076258,  0.50373113, -0.47516847,  0.4439696 ],\n",
              "       dtype=float32),\n",
              " 'agreed': array([ 0.45744866,  0.40918162, -0.45780808, -0.4461988 , -0.48211765,\n",
              "         0.4544844 ,  0.5043482 , -0.5058849 , -0.49528423, -0.48566157,\n",
              "        -0.48263776,  0.42632058,  0.50681347,  0.49170205, -0.48381552,\n",
              "        -0.50053525,  0.44499183,  0.50357676, -0.43312082, -0.4513621 ,\n",
              "        -0.49051335,  0.4639859 ,  0.45013124,  0.4815746 , -0.47757682,\n",
              "        -0.44242832,  0.44472265,  0.5130256 ,  0.44875854,  0.46604627,\n",
              "        -0.49325174,  0.43396485, -0.4796745 , -0.4553617 , -0.4341203 ,\n",
              "        -0.5052699 ,  0.51016283, -0.42699987,  0.48759258, -0.46136907,\n",
              "         0.46938252, -0.4785394 , -0.47491664, -0.4382157 ,  0.51336867,\n",
              "         0.50996625,  0.5061754 ,  0.5100621 , -0.47335985,  0.49498641],\n",
              "       dtype=float32),\n",
              " 'long': array([ 0.5156163 ,  0.47913608, -0.49601883, -0.4709332 , -0.4979521 ,\n",
              "         0.49144506,  0.46906474, -0.48603317, -0.47211543, -0.46143702,\n",
              "        -0.47478223,  0.47272903,  0.4923339 ,  0.4868708 , -0.51557213,\n",
              "        -0.43460873,  0.45087877,  0.44512764, -0.43350115, -0.44017452,\n",
              "        -0.45931712,  0.51582694,  0.4942913 ,  0.44491276, -0.4667038 ,\n",
              "        -0.44470066,  0.4488451 ,  0.508761  ,  0.5112572 ,  0.5161455 ,\n",
              "        -0.4857362 ,  0.49275804, -0.47561207, -0.46955246, -0.48116034,\n",
              "        -0.4419666 ,  0.52294344, -0.4711128 ,  0.48492035, -0.49215633,\n",
              "         0.52589566, -0.49581242, -0.5011295 , -0.44426805,  0.44920728,\n",
              "         0.45095298,  0.51327825,  0.4940453 , -0.51214063,  0.49559537],\n",
              "       dtype=float32),\n",
              " 'increases': array([ 0.4996797 ,  0.5006064 , -0.5083806 , -0.51046306, -0.4605434 ,\n",
              "         0.5185602 ,  0.44991213, -0.50597703, -0.45497292, -0.4906852 ,\n",
              "        -0.5222571 ,  0.48784497,  0.4560427 ,  0.526351  , -0.50168663,\n",
              "        -0.48505455,  0.47438484,  0.4533094 , -0.46518043, -0.44663507,\n",
              "        -0.4547478 ,  0.49229592,  0.49114347,  0.44288477, -0.45278654,\n",
              "        -0.47421923,  0.4640164 ,  0.4719728 ,  0.45058802,  0.48208898,\n",
              "        -0.46983182,  0.52749205, -0.49071592, -0.50008005, -0.48739707,\n",
              "        -0.4574476 ,  0.47720456, -0.4804171 ,  0.5025672 , -0.52008855,\n",
              "         0.52880013, -0.48262483, -0.47115567, -0.47154614,  0.45710433,\n",
              "         0.4858952 ,  0.4571572 ,  0.46419412, -0.5177121 ,  0.479856  ],\n",
              "       dtype=float32),\n",
              " 'appreciation': array([ 0.5316433 ,  0.47590846, -0.4944758 , -0.4778021 , -0.5157589 ,\n",
              "         0.51428324,  0.43054074, -0.49995157, -0.45217493, -0.49278092,\n",
              "        -0.46554855,  0.4508285 ,  0.4500421 ,  0.49572384, -0.54561305,\n",
              "        -0.450403  ,  0.4452863 ,  0.5308532 , -0.47868723, -0.4548387 ,\n",
              "        -0.46857584,  0.47995326,  0.48973197,  0.4855656 , -0.43296045,\n",
              "        -0.48783028,  0.48850933,  0.50281775,  0.4328406 ,  0.47723252,\n",
              "        -0.52741903,  0.42706025, -0.5037141 , -0.52803564, -0.43090856,\n",
              "        -0.45457435,  0.5037745 , -0.5062523 ,  0.5082163 , -0.4546174 ,\n",
              "         0.49583167, -0.5292212 , -0.5068924 , -0.43052357,  0.49730957,\n",
              "         0.46354178,  0.48056212,  0.4867881 , -0.45300213,  0.51995337],\n",
              "       dtype=float32),\n",
              " 'benefit': array([ 0.5109886 ,  0.49079233, -0.50667477, -0.44785455, -0.45803934,\n",
              "         0.4921339 ,  0.43625462, -0.47046188, -0.48228282, -0.5068665 ,\n",
              "        -0.4432103 ,  0.51994294,  0.456802  ,  0.4923731 , -0.49493387,\n",
              "        -0.47788274,  0.52960145,  0.4555163 , -0.47601378, -0.503968  ,\n",
              "        -0.48386908,  0.4822911 ,  0.52704674,  0.49901825, -0.4625386 ,\n",
              "        -0.4340744 ,  0.49990678,  0.53203076,  0.5147303 ,  0.44743642,\n",
              "        -0.49230126,  0.4629467 , -0.5135051 , -0.46516627, -0.44619912,\n",
              "        -0.44553557,  0.483453  , -0.5127443 ,  0.45784262, -0.5292764 ,\n",
              "         0.44195077, -0.50739515, -0.49566245, -0.5130259 ,  0.49079043,\n",
              "         0.4414297 ,  0.46444118,  0.48991153, -0.442281  ,  0.48673922],\n",
              "       dtype=float32),\n",
              " 'possible': array([ 0.49738523,  0.44450015, -0.4465141 , -0.45342046, -0.4688563 ,\n",
              "         0.47446263,  0.4614409 , -0.5244145 , -0.46319687, -0.50175506,\n",
              "        -0.44281745,  0.45140696,  0.50764465,  0.45146567, -0.56125885,\n",
              "        -0.52077174,  0.5018448 ,  0.5157119 , -0.51543313, -0.4995948 ,\n",
              "        -0.45098805,  0.5156668 ,  0.45282787,  0.44420224, -0.4618334 ,\n",
              "        -0.52219135,  0.46404663,  0.45983586,  0.43984607,  0.5116781 ,\n",
              "        -0.45129213,  0.47201347, -0.4791792 , -0.4716947 , -0.50058866,\n",
              "        -0.51390755,  0.44882444, -0.470553  ,  0.48568133, -0.49213168,\n",
              "         0.4474294 , -0.4862451 , -0.44515955, -0.48408103,  0.5248772 ,\n",
              "         0.4840047 ,  0.48107484,  0.43871513, -0.48329404,  0.4803908 ],\n",
              "       dtype=float32),\n",
              " 'qualify': array([ 0.47982526,  0.53530645, -0.5353995 , -0.48763692, -0.48191458,\n",
              "         0.5403523 ,  0.45218578, -0.5527789 , -0.49366885, -0.45895925,\n",
              "        -0.47162026,  0.53293145,  0.5187582 ,  0.4811226 , -0.5710908 ,\n",
              "        -0.5237639 ,  0.5488189 ,  0.535262  , -0.48925114, -0.47635323,\n",
              "        -0.48741055,  0.45918506,  0.4831059 ,  0.46027184, -0.5190811 ,\n",
              "        -0.48145637,  0.4533444 ,  0.5607323 ,  0.4985816 ,  0.49363583,\n",
              "        -0.5169724 ,  0.4396515 , -0.48032713, -0.5203539 , -0.47199714,\n",
              "        -0.47729114,  0.50433224, -0.50571966,  0.4712909 , -0.46610254,\n",
              "         0.46245897, -0.54316974, -0.4588403 , -0.5370242 ,  0.5173451 ,\n",
              "         0.48609382,  0.5269531 ,  0.4866981 , -0.484971  ,  0.4538443 ],\n",
              "       dtype=float32),\n",
              " 'sometimes': array([ 0.51190907,  0.50211686, -0.49687165, -0.44420663, -0.5281919 ,\n",
              "         0.44085503,  0.43695107, -0.475815  , -0.5130798 , -0.44881517,\n",
              "        -0.51926214,  0.43382552,  0.4778089 ,  0.48659834, -0.5448604 ,\n",
              "        -0.5017898 ,  0.46012032,  0.43908957, -0.4912931 , -0.4709617 ,\n",
              "        -0.4598464 ,  0.51148444,  0.47351563,  0.42350724, -0.46303803,\n",
              "        -0.49305597,  0.4810249 ,  0.54085964,  0.49438304,  0.47896072,\n",
              "        -0.52302355,  0.4979454 , -0.5024544 , -0.4706608 , -0.4398647 ,\n",
              "        -0.47631022,  0.4959594 , -0.49146053,  0.4437279 , -0.45297286,\n",
              "         0.5015244 , -0.4991895 , -0.45905784, -0.51862085,  0.50938934,\n",
              "         0.4838816 ,  0.4889897 ,  0.42889857, -0.4899013 ,  0.47100794],\n",
              "       dtype=float32),\n",
              " 'restrictions': array([ 0.48532876,  0.4269521 , -0.46638373, -0.49710852, -0.5062603 ,\n",
              "         0.46587667,  0.44946134, -0.5148386 , -0.4700812 , -0.46322912,\n",
              "        -0.45221362,  0.4795754 ,  0.48569518,  0.49470273, -0.5217203 ,\n",
              "        -0.43782017,  0.5213174 ,  0.4725791 , -0.48106608, -0.45721537,\n",
              "        -0.51277226,  0.4783035 ,  0.43726826,  0.44699758, -0.4329049 ,\n",
              "        -0.44197735,  0.47852635,  0.52259433,  0.49940163,  0.48623243,\n",
              "        -0.45498365,  0.46492675, -0.49529165, -0.46627176, -0.48081145,\n",
              "        -0.4756513 ,  0.4361581 , -0.52114415,  0.5047978 , -0.49439728,\n",
              "         0.46633327, -0.4966872 , -0.42674187, -0.49283883,  0.49036545,\n",
              "         0.48091552,  0.49697614,  0.48913497, -0.50120854,  0.47630888],\n",
              "       dtype=float32),\n",
              " 'reits': array([ 0.46739337,  0.45451874, -0.44351533, -0.45429856, -0.49044552,\n",
              "         0.47871765,  0.46060932, -0.50511014, -0.45847347, -0.4402129 ,\n",
              "        -0.50930536,  0.4859635 ,  0.48448712,  0.4423159 , -0.5381975 ,\n",
              "        -0.45246217,  0.43715736,  0.4346693 , -0.4731787 , -0.46573105,\n",
              "        -0.4391102 ,  0.44223413,  0.48785186,  0.45444334, -0.45086396,\n",
              "        -0.46255326,  0.4401896 ,  0.49147516,  0.48239028,  0.43649814,\n",
              "        -0.4997341 ,  0.48983708, -0.45927355, -0.47774613, -0.5062378 ,\n",
              "        -0.51179   ,  0.45360145, -0.49015248,  0.46092534, -0.44267216,\n",
              "         0.504192  , -0.46324086, -0.46562934, -0.50329524,  0.5043438 ,\n",
              "         0.4981861 ,  0.49451777,  0.4276033 , -0.4509967 ,  0.5060159 ],\n",
              "       dtype=float32),\n",
              " 'made': array([ 0.46226853,  0.48862076, -0.44749504, -0.45171136, -0.52463293,\n",
              "         0.46569908,  0.5028352 , -0.48376626, -0.51372635, -0.46755913,\n",
              "        -0.5063767 ,  0.4645402 ,  0.48595434,  0.50701666, -0.48292366,\n",
              "        -0.4946224 ,  0.47925282,  0.50298595, -0.46399924, -0.46488523,\n",
              "        -0.50039655,  0.49827796,  0.4718253 ,  0.49263287, -0.4472984 ,\n",
              "        -0.45931375,  0.4709328 ,  0.49882552,  0.47343278,  0.49672487,\n",
              "        -0.4608357 ,  0.48139292, -0.5332274 , -0.4650961 , -0.4886991 ,\n",
              "        -0.51932275,  0.5123848 , -0.4749074 ,  0.4527597 , -0.5126088 ,\n",
              "         0.46338108, -0.4766404 , -0.47176188, -0.47145426,  0.510341  ,\n",
              "         0.43335295,  0.45301336,  0.48034236, -0.46201098,  0.42596903],\n",
              "       dtype=float32),\n",
              " 'limit': array([ 0.4593027 ,  0.45633578, -0.44004115, -0.48805362, -0.5297671 ,\n",
              "         0.46148705,  0.51653713, -0.44944227, -0.5083938 , -0.5150835 ,\n",
              "        -0.4671713 ,  0.49421787,  0.46821865,  0.5169808 , -0.50912106,\n",
              "        -0.50845915,  0.49468812,  0.44852936, -0.44660613, -0.45494038,\n",
              "        -0.5004021 ,  0.5157078 ,  0.45455715,  0.48161507, -0.5124131 ,\n",
              "        -0.49235624,  0.44707334,  0.4949968 ,  0.43265885,  0.5034159 ,\n",
              "        -0.5274921 ,  0.5159108 , -0.50125813, -0.5059792 , -0.44323698,\n",
              "        -0.46171945,  0.51216847, -0.49045977,  0.4572578 , -0.4417099 ,\n",
              "         0.44761035, -0.5344148 , -0.5051288 , -0.47811586,  0.5203315 ,\n",
              "         0.51626813,  0.4552377 ,  0.5083185 , -0.47623178,  0.43517464],\n",
              "       dtype=float32),\n",
              " 'henderson': array([ 0.48436996,  0.46460438, -0.4799409 , -0.4737484 , -0.48734316,\n",
              "         0.49465728,  0.48491073, -0.47065544, -0.53587765, -0.45983702,\n",
              "        -0.5015967 ,  0.46888068,  0.48530617,  0.4757301 , -0.5003335 ,\n",
              "        -0.48575976,  0.49372464,  0.43869632, -0.4801517 , -0.47639105,\n",
              "        -0.49767125,  0.5297523 ,  0.5307888 ,  0.450205  , -0.46648705,\n",
              "        -0.44396666,  0.47468722,  0.47678733,  0.4349277 ,  0.52325106,\n",
              "        -0.5205657 ,  0.52358013, -0.4901558 , -0.52166176, -0.5303307 ,\n",
              "        -0.5062468 ,  0.5117187 , -0.51434565,  0.5055715 , -0.44853735,\n",
              "         0.4712351 , -0.49388757, -0.49607047, -0.5253248 ,  0.45022014,\n",
              "         0.5392891 ,  0.47214866,  0.52226645, -0.42884988,  0.50498646],\n",
              "       dtype=float32),\n",
              " 'holding': array([ 0.5228839 ,  0.47781998, -0.4851204 , -0.47994626, -0.5358663 ,\n",
              "         0.46333432,  0.43984133, -0.5287    , -0.44205505, -0.4752677 ,\n",
              "        -0.5032075 ,  0.48188832,  0.45296922,  0.5123419 , -0.5199748 ,\n",
              "        -0.48837635,  0.44390887,  0.46049678, -0.461841  , -0.5200054 ,\n",
              "        -0.49522853,  0.49628067,  0.50389475,  0.49149865, -0.45032084,\n",
              "        -0.46424422,  0.46923018,  0.53488654,  0.44069386,  0.44240957,\n",
              "        -0.4941637 ,  0.47969702, -0.491049  , -0.49013564, -0.4957205 ,\n",
              "        -0.5264257 ,  0.45088112, -0.440755  ,  0.494834  , -0.47281292,\n",
              "         0.46961614, -0.44573224, -0.46835068, -0.46324804,  0.5025504 ,\n",
              "         0.49511978,  0.47199085,  0.4832596 , -0.45326814,  0.51181793],\n",
              "       dtype=float32),\n",
              " 'determined': array([ 0.47263023,  0.47073925, -0.52335787, -0.45963103, -0.47436348,\n",
              "         0.52117896,  0.46088398, -0.4673478 , -0.5245037 , -0.43469134,\n",
              "        -0.5170126 ,  0.5250248 ,  0.4696159 ,  0.45899564, -0.493985  ,\n",
              "        -0.4611127 ,  0.4539292 ,  0.47159284, -0.43900427, -0.4438699 ,\n",
              "        -0.5213088 ,  0.51600194,  0.50978047,  0.45778593, -0.43318897,\n",
              "        -0.49793112,  0.43731937,  0.5410333 ,  0.47554022,  0.47598308,\n",
              "        -0.47566098,  0.4876308 , -0.49312764, -0.5151737 , -0.44554237,\n",
              "        -0.4584582 ,  0.49218652, -0.45465082,  0.5074126 , -0.52161837,\n",
              "         0.52761805, -0.52499336, -0.4363201 , -0.47199017,  0.5083746 ,\n",
              "         0.472041  ,  0.44156903,  0.45111936, -0.4565673 ,  0.51214576],\n",
              "       dtype=float32),\n",
              " 'demand': array([ 0.46026957,  0.48725513, -0.51236165, -0.50321275, -0.44112247,\n",
              "         0.49306777,  0.49584228, -0.4674917 , -0.44967553, -0.44219908,\n",
              "        -0.5087381 ,  0.446397  ,  0.4705443 ,  0.435683  , -0.48813298,\n",
              "        -0.4337964 ,  0.50572574,  0.4410337 , -0.5034557 , -0.4466543 ,\n",
              "        -0.49789786,  0.4408457 ,  0.46781585,  0.49190396, -0.5020096 ,\n",
              "        -0.47773612,  0.48847908,  0.4803465 ,  0.49944025,  0.47843438,\n",
              "        -0.4719595 ,  0.43184033, -0.48488447, -0.4731289 , -0.47654146,\n",
              "        -0.46054134,  0.5198339 , -0.5111787 ,  0.47977778, -0.4804736 ,\n",
              "         0.51083064, -0.4869869 , -0.49389514, -0.47768748,  0.48900157,\n",
              "         0.48240337,  0.43052456,  0.4693932 , -0.50976443,  0.45078588],\n",
              "       dtype=float32),\n",
              " 'partner': array([ 0.54634714,  0.46538052, -0.5372277 , -0.5208665 , -0.4866915 ,\n",
              "         0.46607468,  0.4487034 , -0.46543074, -0.5049304 , -0.45253438,\n",
              "        -0.47918314,  0.54109573,  0.45975807,  0.447556  , -0.5268807 ,\n",
              "        -0.47588295,  0.45726758,  0.4463436 , -0.46300077, -0.4342719 ,\n",
              "        -0.5093827 ,  0.45505115,  0.49499977,  0.51488376, -0.4488    ,\n",
              "        -0.51226825,  0.48119605,  0.5132332 ,  0.50012183,  0.4455556 ,\n",
              "        -0.4856681 ,  0.5110626 , -0.5325892 , -0.5168794 , -0.48650196,\n",
              "        -0.5201892 ,  0.4700197 , -0.4941486 ,  0.48512325, -0.524517  ,\n",
              "         0.50680596, -0.50991434, -0.4629596 , -0.46159238,  0.50402987,\n",
              "         0.5104678 ,  0.46256077,  0.4780503 , -0.45578018,  0.45867056],\n",
              "       dtype=float32),\n",
              " 'corporate': array([ 0.46458325,  0.4917834 , -0.50008565, -0.51235914, -0.5116531 ,\n",
              "         0.47202456,  0.51195836, -0.52031875, -0.44093314, -0.47676215,\n",
              "        -0.43614614,  0.47089705,  0.50650865,  0.44360834, -0.54723233,\n",
              "        -0.52077425,  0.5008989 ,  0.4904785 , -0.51554614, -0.48708695,\n",
              "        -0.4494437 ,  0.48778772,  0.51242036,  0.44713336, -0.45946074,\n",
              "        -0.45356095,  0.4638095 ,  0.48400107,  0.49326006,  0.47766238,\n",
              "        -0.5149453 ,  0.46970007, -0.4525668 , -0.45466885, -0.48189124,\n",
              "        -0.5062759 ,  0.45824647, -0.45787436,  0.4288755 , -0.4846787 ,\n",
              "         0.45237347, -0.43781105, -0.47902116, -0.5108596 ,  0.489107  ,\n",
              "         0.46475753,  0.45138028,  0.46851212, -0.49156398,  0.47090313],\n",
              "       dtype=float32),\n",
              " 'pricing': array([ 0.45925447,  0.48439255, -0.4618794 , -0.48165378, -0.4907306 ,\n",
              "         0.5291843 ,  0.46419033, -0.51351976, -0.5082666 , -0.44575992,\n",
              "        -0.5122851 ,  0.494057  ,  0.44794685,  0.47160366, -0.53976274,\n",
              "        -0.50093526,  0.4447628 ,  0.45275602, -0.4662845 , -0.4919442 ,\n",
              "        -0.44462803,  0.43900397,  0.46942115,  0.4866314 , -0.4923398 ,\n",
              "        -0.45063537,  0.47573566,  0.48058188,  0.4722947 ,  0.5123044 ,\n",
              "        -0.45775685,  0.4890923 , -0.5098813 , -0.4739396 , -0.48510948,\n",
              "        -0.5054175 ,  0.44576174, -0.4588446 ,  0.44116044, -0.48940086,\n",
              "         0.48959798, -0.4826734 , -0.4702344 , -0.46722323,  0.47879353,\n",
              "         0.44923836,  0.4826747 ,  0.49924117, -0.5039145 ,  0.48504522],\n",
              "       dtype=float32),\n",
              " 'distributor': array([ 0.48234588,  0.4287283 , -0.49610424, -0.46950924, -0.45296523,\n",
              "         0.48258236,  0.4661964 , -0.5154846 , -0.508685  , -0.48751622,\n",
              "        -0.52090627,  0.50233096,  0.46824783,  0.49152032, -0.5095228 ,\n",
              "        -0.5407511 ,  0.49126336,  0.5210498 , -0.48908842, -0.4853694 ,\n",
              "        -0.46836942,  0.4560872 ,  0.52493405,  0.47627732, -0.46650767,\n",
              "        -0.4990997 ,  0.4927217 ,  0.5463332 ,  0.52075934,  0.49092534,\n",
              "        -0.48188442,  0.45412707, -0.53990203, -0.45407215, -0.46433783,\n",
              "        -0.5332662 ,  0.46541727, -0.53509027,  0.5098371 , -0.5078001 ,\n",
              "         0.5324398 , -0.49017745, -0.44675866, -0.520069  ,  0.52959275,\n",
              "         0.52821815,  0.48887748,  0.4550289 , -0.48018643,  0.4751858 ],\n",
              "       dtype=float32),\n",
              " 'collateral': array([ 0.52886665,  0.51517344, -0.51493937, -0.46246612, -0.4984045 ,\n",
              "         0.44391766,  0.46025285, -0.5330319 , -0.4914559 , -0.47391173,\n",
              "        -0.47833195,  0.46175802,  0.511344  ,  0.4353888 , -0.47797364,\n",
              "        -0.45901185,  0.5007801 ,  0.5110291 , -0.5239547 , -0.5207026 ,\n",
              "        -0.49264646,  0.44023156,  0.49934274,  0.43635815, -0.4271928 ,\n",
              "        -0.4976638 ,  0.508761  ,  0.5273766 ,  0.51658785,  0.4579503 ,\n",
              "        -0.49505424,  0.49640864, -0.5208509 , -0.47915357, -0.49700165,\n",
              "        -0.49104694,  0.4852836 , -0.4504606 ,  0.5041114 , -0.45079285,\n",
              "         0.49576402, -0.48655027, -0.49811026, -0.48838493,  0.49312064,\n",
              "         0.499115  ,  0.4592653 ,  0.47141996, -0.48329484,  0.5317114 ],\n",
              "       dtype=float32),\n",
              " 'obligation': array([ 0.56007475,  0.4512103 , -0.5278031 , -0.4560556 , -0.48329744,\n",
              "         0.4633954 ,  0.47355926, -0.44911385, -0.5076315 , -0.48797268,\n",
              "        -0.48892352,  0.4810206 ,  0.5183887 ,  0.47946417, -0.51284426,\n",
              "        -0.49897423,  0.48886576,  0.52943134, -0.51097167, -0.46489924,\n",
              "        -0.46579146,  0.49694124,  0.45064342,  0.4465082 , -0.44289938,\n",
              "        -0.4462151 ,  0.5116287 ,  0.46777955,  0.43618888,  0.46141338,\n",
              "        -0.4746838 ,  0.49789736, -0.50748867, -0.49527752, -0.5006502 ,\n",
              "        -0.47549984,  0.49291605, -0.47421208,  0.5118829 , -0.47204828,\n",
              "         0.5369466 , -0.4823118 , -0.4942324 , -0.4589471 ,  0.46384484,\n",
              "         0.44682005,  0.4780643 ,  0.4463396 , -0.50104856,  0.49946147],\n",
              "       dtype=float32),\n",
              " 'illustrates': array([ 0.465155  ,  0.47056064, -0.46632236, -0.52678555, -0.46782807,\n",
              "         0.53821415,  0.50202644, -0.48276854, -0.44790667, -0.50871825,\n",
              "        -0.4879309 ,  0.4669835 ,  0.46194685,  0.47995302, -0.53613114,\n",
              "        -0.44375324,  0.5093522 ,  0.5270524 , -0.5061495 , -0.5201657 ,\n",
              "        -0.4818833 ,  0.46445557,  0.4933561 ,  0.48608756, -0.48679078,\n",
              "        -0.51682943,  0.44070074,  0.48686686,  0.46472576,  0.4520893 ,\n",
              "        -0.46467903,  0.4616632 , -0.49229133, -0.5033981 , -0.4489594 ,\n",
              "        -0.4725369 ,  0.491355  , -0.4588859 ,  0.44303212, -0.5137845 ,\n",
              "         0.45275062, -0.4663675 , -0.45922846, -0.5278231 ,  0.5005056 ,\n",
              "         0.48233998,  0.49487218,  0.51584524, -0.4884438 ,  0.5163647 ],\n",
              "       dtype=float32),\n",
              " 'predecessor': array([ 0.4905607 ,  0.4783978 , -0.47100082, -0.5363178 , -0.48973382,\n",
              "         0.46640146,  0.49394217, -0.4605934 , -0.51284546, -0.49482468,\n",
              "        -0.5498735 ,  0.5246559 ,  0.47044063,  0.5446245 , -0.5251913 ,\n",
              "        -0.5433938 ,  0.53515035,  0.5125766 , -0.51477456, -0.46578383,\n",
              "        -0.50132227,  0.51089436,  0.5240791 ,  0.5011846 , -0.46938694,\n",
              "        -0.4734472 ,  0.5198666 ,  0.5270732 ,  0.46432367,  0.50048196,\n",
              "        -0.521728  ,  0.53941417, -0.48158965, -0.45228565, -0.45317256,\n",
              "        -0.46088523,  0.49557996, -0.49326596,  0.48995647, -0.49169087,\n",
              "         0.49465457, -0.55304027, -0.52962106, -0.4512331 ,  0.5194229 ,\n",
              "         0.531053  ,  0.53563964,  0.5229161 , -0.46571168,  0.48036593],\n",
              "       dtype=float32),\n",
              " 'likely': array([ 0.55024374,  0.5069815 , -0.52554715, -0.5010243 , -0.4852326 ,\n",
              "         0.4576946 ,  0.45415708, -0.48320642, -0.45983264, -0.43877017,\n",
              "        -0.45278734,  0.46232608,  0.46006745,  0.4972598 , -0.5682561 ,\n",
              "        -0.5016949 ,  0.4593505 ,  0.4668171 , -0.46540222, -0.4841789 ,\n",
              "        -0.5167281 ,  0.49834794,  0.53812027,  0.4602817 , -0.5166622 ,\n",
              "        -0.46918473,  0.50829744,  0.4726168 ,  0.4397098 ,  0.4503359 ,\n",
              "        -0.49623722,  0.44729152, -0.47366092, -0.5036365 , -0.43488997,\n",
              "        -0.52573013,  0.49001238, -0.542153  ,  0.45351467, -0.50352275,\n",
              "         0.47397554, -0.45843917, -0.48360574, -0.48847258,  0.4665981 ,\n",
              "         0.51026887,  0.52985376,  0.45002827, -0.4744746 ,  0.52160585],\n",
              "       dtype=float32),\n",
              " 'measure': array([ 0.51031446,  0.43258625, -0.44918412, -0.45916346, -0.48238173,\n",
              "         0.5277346 ,  0.4868412 , -0.45674294, -0.42931747, -0.50629914,\n",
              "        -0.4904775 ,  0.4547795 ,  0.43823954,  0.5184963 , -0.52026105,\n",
              "        -0.49881846,  0.48082614,  0.50102854, -0.43805826, -0.47423315,\n",
              "        -0.4860817 ,  0.5061782 ,  0.48972082,  0.5042024 , -0.428981  ,\n",
              "        -0.5076146 ,  0.45365277,  0.4510438 ,  0.48877096,  0.5098102 ,\n",
              "        -0.47573844,  0.4938528 , -0.48225942, -0.49825838, -0.5013941 ,\n",
              "        -0.4498359 ,  0.4965693 , -0.50405324,  0.43183967, -0.5041481 ,\n",
              "         0.44966993, -0.5006604 , -0.44140077, -0.5007407 ,  0.48570415,\n",
              "         0.49779063,  0.4735824 ,  0.4828498 , -0.48285648,  0.48908594],\n",
              "       dtype=float32),\n",
              " 'daily': array([ 0.47724876,  0.49748647, -0.4398402 , -0.4502765 , -0.49193949,\n",
              "         0.4874377 ,  0.4948412 , -0.5160844 , -0.47713408, -0.4505382 ,\n",
              "        -0.43736312,  0.47570062,  0.5178477 ,  0.4890847 , -0.4790291 ,\n",
              "        -0.45014837,  0.50445354,  0.51556844, -0.4499576 , -0.51353556,\n",
              "        -0.50350666,  0.45688182,  0.48891503,  0.4462872 , -0.45023075,\n",
              "        -0.494449  ,  0.4911092 ,  0.44837052,  0.45719844,  0.4804236 ,\n",
              "        -0.45242134,  0.45492774, -0.4953712 , -0.46231398, -0.47998637,\n",
              "        -0.47172403,  0.4605669 , -0.48786074,  0.4457684 , -0.48429704,\n",
              "         0.5050889 , -0.5157597 , -0.4652888 , -0.5161695 ,  0.46329314,\n",
              "         0.4562681 ,  0.43293196,  0.5014476 , -0.44240382,  0.49756888],\n",
              "       dtype=float32),\n",
              " 'declines': array([ 0.5232064 ,  0.5009118 , -0.46331593, -0.43476665, -0.5450319 ,\n",
              "         0.53646106,  0.4804877 , -0.514359  , -0.47385886, -0.43394345,\n",
              "        -0.4425321 ,  0.49003932,  0.45900527,  0.492157  , -0.56139433,\n",
              "        -0.46707487,  0.48440534,  0.46995807, -0.48395136, -0.42665455,\n",
              "        -0.45331156,  0.45364684,  0.5157131 ,  0.46486998, -0.5118943 ,\n",
              "        -0.44135824,  0.4928468 ,  0.51414263,  0.50361854,  0.47585648,\n",
              "        -0.4749659 ,  0.43963543, -0.51214176, -0.45469877, -0.4327147 ,\n",
              "        -0.5074223 ,  0.5110407 , -0.48794812,  0.48152927, -0.519577  ,\n",
              "         0.49723813, -0.5190909 , -0.45530272, -0.4992744 ,  0.47736248,\n",
              "         0.51165354,  0.44038847,  0.4671311 , -0.46231043,  0.4557303 ],\n",
              "       dtype=float32),\n",
              " 'fidelity®': array([ 0.48887748,  0.5089318 , -0.48355833, -0.5065961 , -0.4893132 ,\n",
              "         0.49121636,  0.49438635, -0.49801603, -0.43552282, -0.506764  ,\n",
              "        -0.46964043,  0.50143266,  0.4540435 ,  0.47070694, -0.5404967 ,\n",
              "        -0.51916355,  0.49454308,  0.45775375, -0.49498475, -0.47821808,\n",
              "        -0.48185918,  0.4309889 ,  0.4935672 ,  0.4584831 , -0.42536226,\n",
              "        -0.46375924,  0.4593279 ,  0.52724826,  0.51566267,  0.5146764 ,\n",
              "        -0.47717762,  0.46252576, -0.50994974, -0.4651745 , -0.45924154,\n",
              "        -0.43661293,  0.46062902, -0.5217996 ,  0.49548918, -0.44577667,\n",
              "         0.5152625 , -0.49926364, -0.42938486, -0.49945727,  0.44363874,\n",
              "         0.48828673,  0.4829503 ,  0.4382651 , -0.44212952,  0.45599252],\n",
              "       dtype=float32),\n",
              " 'selection': array([ 0.523536  ,  0.4946686 , -0.44765112, -0.50983864, -0.4962165 ,\n",
              "         0.44394296,  0.5055155 , -0.48866552, -0.47052476, -0.4559337 ,\n",
              "        -0.4548251 ,  0.5193489 ,  0.49976662,  0.51273245, -0.47105166,\n",
              "        -0.43151337,  0.47531408,  0.5149493 , -0.48389733, -0.43037528,\n",
              "        -0.5017234 ,  0.476076  ,  0.49805582,  0.4691692 , -0.46585894,\n",
              "        -0.4925566 ,  0.5089231 ,  0.47351298,  0.47762364,  0.4546904 ,\n",
              "        -0.49807325,  0.46279806, -0.51911587, -0.51507884, -0.46369934,\n",
              "        -0.47587645,  0.5131317 , -0.476674  ,  0.4805796 , -0.4991091 ,\n",
              "         0.48326847, -0.46389577, -0.49725157, -0.48497158,  0.505496  ,\n",
              "         0.5092037 ,  0.5021997 ,  0.4695078 , -0.441695  ,  0.51635253],\n",
              "       dtype=float32),\n",
              " 'positions': array([ 0.5054952 ,  0.45336038, -0.4595316 , -0.47440425, -0.46356907,\n",
              "         0.49296492,  0.52716017, -0.48352528, -0.44935885, -0.4965772 ,\n",
              "        -0.50175995,  0.45655212,  0.47603482,  0.4957419 , -0.47838497,\n",
              "        -0.4885988 ,  0.48570693,  0.4755133 , -0.47245264, -0.4890972 ,\n",
              "        -0.46838012,  0.44450864,  0.4964967 ,  0.4913044 , -0.48994815,\n",
              "        -0.46849224,  0.44925636,  0.54168487,  0.51305556,  0.49410003,\n",
              "        -0.5107614 ,  0.45989236, -0.49683362, -0.49542493, -0.50230396,\n",
              "        -0.51505756,  0.47571918, -0.4849546 ,  0.45556328, -0.4611209 ,\n",
              "         0.4962563 , -0.5294411 , -0.46758947, -0.49994147,  0.5287793 ,\n",
              "         0.46996686,  0.50019544,  0.45331538, -0.45108142,  0.50865674],\n",
              "       dtype=float32),\n",
              " 'payment': array([ 0.48643553,  0.41989684, -0.43715084, -0.4325372 , -0.45309782,\n",
              "         0.49647593,  0.42810115, -0.44235697, -0.4499556 , -0.48413524,\n",
              "        -0.48533314,  0.51335675,  0.42449194,  0.47962525, -0.53747857,\n",
              "        -0.47598532,  0.5079817 ,  0.44421077, -0.4665061 , -0.44236627,\n",
              "        -0.47255114,  0.4989478 ,  0.48264185,  0.43503797, -0.43180686,\n",
              "        -0.4800687 ,  0.4597996 ,  0.4640911 ,  0.42210773,  0.49432287,\n",
              "        -0.47307223,  0.42969573, -0.44444558, -0.48888552, -0.47626832,\n",
              "        -0.44447878,  0.44430867, -0.49298075,  0.4885253 , -0.4671846 ,\n",
              "         0.4582057 , -0.47179952, -0.47685528, -0.4432399 ,  0.45068273,\n",
              "         0.4549532 ,  0.49844334,  0.49085686, -0.49781895,  0.42830777],\n",
              "       dtype=float32),\n",
              " 'relatively': array([ 0.5082011 ,  0.5043324 , -0.44576803, -0.49845517, -0.50558066,\n",
              "         0.47056025,  0.43934292, -0.4587642 , -0.4818368 , -0.47796243,\n",
              "        -0.5091116 ,  0.44730178,  0.46048123,  0.44389105, -0.5530071 ,\n",
              "        -0.5103286 ,  0.51652944,  0.4919567 , -0.48691696, -0.50985634,\n",
              "        -0.44805312,  0.43183666,  0.47486007,  0.49454933, -0.47231126,\n",
              "        -0.45194408,  0.45026028,  0.48918742,  0.45514715,  0.5170757 ,\n",
              "        -0.47323304,  0.4558822 , -0.4739972 , -0.5101574 , -0.47945404,\n",
              "        -0.49302095,  0.46846747, -0.4856257 ,  0.4334051 , -0.48397794,\n",
              "         0.49026662, -0.53256553, -0.4959926 , -0.4884503 ,  0.5136291 ,\n",
              "         0.48780787,  0.4938845 ,  0.51244086, -0.46781507,  0.52224594],\n",
              "       dtype=float32),\n",
              " 'legal': array([ 0.483226  ,  0.44225496, -0.4454333 , -0.48179013, -0.5171753 ,\n",
              "         0.5148101 ,  0.4963532 , -0.49564934, -0.439958  , -0.4548377 ,\n",
              "        -0.4896404 ,  0.49546793,  0.49158406,  0.49221483, -0.47783944,\n",
              "        -0.48580855,  0.4867685 ,  0.51285607, -0.4212587 , -0.45542967,\n",
              "        -0.49004376,  0.42145872,  0.44982946,  0.50155073, -0.4327124 ,\n",
              "        -0.44997615,  0.42074916,  0.4954316 ,  0.50626886,  0.4183228 ,\n",
              "        -0.5157684 ,  0.4518196 , -0.52521294, -0.47634053, -0.47377118,\n",
              "        -0.49633494,  0.4366314 , -0.4932382 ,  0.4825373 , -0.4274895 ,\n",
              "         0.49988765, -0.47352293, -0.48882887, -0.47030154,  0.46258932,\n",
              "         0.49897715,  0.49096516,  0.49091256, -0.47632927,  0.4499828 ],\n",
              "       dtype=float32),\n",
              " 'lowest': array([ 0.49973384,  0.43707454, -0.4841285 , -0.50982165, -0.44475558,\n",
              "         0.4699192 ,  0.49720082, -0.49012417, -0.5153845 , -0.45354643,\n",
              "        -0.46814233,  0.46665135,  0.44682103,  0.4986242 , -0.47314847,\n",
              "        -0.4950006 ,  0.4784548 ,  0.45122212, -0.43282026, -0.50546086,\n",
              "        -0.45488998,  0.45608908,  0.4775797 ,  0.4311387 , -0.50080276,\n",
              "        -0.4534119 ,  0.47903082,  0.52024215,  0.47610864,  0.5175711 ,\n",
              "        -0.49394697,  0.49406803, -0.5031581 , -0.46845907, -0.45697653,\n",
              "        -0.45710015,  0.44232905, -0.495856  ,  0.4390376 , -0.4611449 ,\n",
              "         0.5198442 , -0.5005375 , -0.5079887 , -0.4887439 ,  0.47491378,\n",
              "         0.44622305,  0.45700902,  0.49695137, -0.48822775,  0.43494117],\n",
              "       dtype=float32),\n",
              " 'domestic': array([ 0.47570223,  0.5009234 , -0.4752747 , -0.4626081 , -0.4842642 ,\n",
              "         0.46793187,  0.49425173, -0.51625156, -0.46171975, -0.43617928,\n",
              "        -0.44918042,  0.49733818,  0.44084808,  0.487587  , -0.5544287 ,\n",
              "        -0.4943412 ,  0.51724917,  0.48073983, -0.52478164, -0.47674787,\n",
              "        -0.4376237 ,  0.44580543,  0.45426524,  0.46274093, -0.43709376,\n",
              "        -0.48632503,  0.46589875,  0.47264194,  0.44409293,  0.46844912,\n",
              "        -0.48153028,  0.51770896, -0.4529632 , -0.46031666, -0.48409596,\n",
              "        -0.5073744 ,  0.5028636 , -0.46971273,  0.44710556, -0.46292356,\n",
              "         0.45190507, -0.45449245, -0.4574548 , -0.49065736,  0.45225126,\n",
              "         0.46185854,  0.52313507,  0.4418304 , -0.4688261 ,  0.46660572],\n",
              "       dtype=float32),\n",
              " 'regular': array([ 0.52206135,  0.51422614, -0.53185636, -0.47586355, -0.5187721 ,\n",
              "         0.4734307 ,  0.45720297, -0.46979415, -0.49887577, -0.4939279 ,\n",
              "        -0.4497343 ,  0.51283413,  0.5242985 ,  0.49832472, -0.49273345,\n",
              "        -0.46656948,  0.5029021 ,  0.45201445, -0.47918147, -0.44627184,\n",
              "        -0.52065235,  0.4546388 ,  0.46641737,  0.52036154, -0.46818665,\n",
              "        -0.44813105,  0.46814522,  0.46734717,  0.5229735 ,  0.4472292 ,\n",
              "        -0.4944    ,  0.51877064, -0.5060691 , -0.44705236, -0.44110352,\n",
              "        -0.48687837,  0.47403947, -0.522267  ,  0.50832397, -0.5132676 ,\n",
              "         0.46107563, -0.46754622, -0.46712863, -0.44069487,  0.4632128 ,\n",
              "         0.5066301 ,  0.44262135,  0.48768124, -0.46172878,  0.4712476 ],\n",
              "       dtype=float32),\n",
              " 'tax-advantaged': array([ 0.4756391 ,  0.5060657 , -0.47461838, -0.49912238, -0.45753437,\n",
              "         0.47879267,  0.44544816, -0.47510228, -0.4474817 , -0.46651426,\n",
              "        -0.4753421 ,  0.5312676 ,  0.47248828,  0.521827  , -0.5460037 ,\n",
              "        -0.5086521 ,  0.48203868,  0.52622646, -0.5155401 , -0.46939594,\n",
              "        -0.45990756,  0.4387018 ,  0.50992924,  0.48765352, -0.45499146,\n",
              "        -0.5172024 ,  0.52348644,  0.5382042 ,  0.50147057,  0.45968986,\n",
              "        -0.48420364,  0.47811368, -0.45790055, -0.51515186, -0.5118844 ,\n",
              "        -0.4886479 ,  0.5192672 , -0.47543097,  0.5140597 , -0.45261446,\n",
              "         0.5014026 , -0.4923026 , -0.42995524, -0.43500626,  0.5021095 ,\n",
              "         0.473061  ,  0.46208262,  0.43720764, -0.49859193,  0.48002008],\n",
              "       dtype=float32),\n",
              " 'participants': array([ 0.47950473,  0.50149024, -0.50790787, -0.51612496, -0.49541467,\n",
              "         0.5271912 ,  0.42761645, -0.43828928, -0.43662527, -0.48712543,\n",
              "        -0.4563308 ,  0.47116506,  0.48395938,  0.5079303 , -0.5112398 ,\n",
              "        -0.5029658 ,  0.45697978,  0.4886678 , -0.4301676 , -0.49639022,\n",
              "        -0.43229532,  0.47428697,  0.49871585,  0.49005765, -0.4793372 ,\n",
              "        -0.48200703,  0.455477  ,  0.5167745 ,  0.45534906,  0.43000314,\n",
              "        -0.5024814 ,  0.51532906, -0.4930231 , -0.4699462 , -0.5095933 ,\n",
              "        -0.46946612,  0.49660993, -0.50837904,  0.46930492, -0.509066  ,\n",
              "         0.479604  , -0.44765753, -0.5028616 , -0.4598913 ,  0.519955  ,\n",
              "         0.437551  ,  0.45244497,  0.4960398 , -0.45358032,  0.51358426],\n",
              "       dtype=float32),\n",
              " 'correlation': array([ 0.4990301 ,  0.47036546, -0.49407017, -0.521975  , -0.53132   ,\n",
              "         0.49665105,  0.4628175 , -0.5010269 , -0.46321592, -0.44930497,\n",
              "        -0.52739686,  0.51918447,  0.5195725 ,  0.49109244, -0.56743   ,\n",
              "        -0.52174366,  0.4975962 ,  0.4848782 , -0.44270056, -0.5151888 ,\n",
              "        -0.5132752 ,  0.45378625,  0.49087703,  0.51678324, -0.5076921 ,\n",
              "        -0.45371637,  0.47072268,  0.5031011 ,  0.44459605,  0.47132775,\n",
              "        -0.48114678,  0.4450533 , -0.49502462, -0.50594354, -0.46806642,\n",
              "        -0.4812658 ,  0.46935079, -0.44926348,  0.4514026 , -0.48787796,\n",
              "         0.5196548 , -0.45588464, -0.47418314, -0.5003038 ,  0.45335808,\n",
              "         0.5116466 ,  0.52880836,  0.45752394, -0.5188944 ,  0.4555038 ],\n",
              "       dtype=float32),\n",
              " 'negatively': array([ 0.55015945,  0.5121866 , -0.49527293, -0.4695785 , -0.49196702,\n",
              "         0.5035985 ,  0.47742394, -0.5073095 , -0.47121096, -0.512734  ,\n",
              "        -0.5283028 ,  0.45827162,  0.49721432,  0.48679334, -0.5696118 ,\n",
              "        -0.5048579 ,  0.5028018 ,  0.47411546, -0.48924267, -0.5093481 ,\n",
              "        -0.51065826,  0.473275  ,  0.50106096,  0.46208772, -0.5330537 ,\n",
              "        -0.48470196,  0.49410418,  0.51559776,  0.47887263,  0.4517037 ,\n",
              "        -0.512236  ,  0.45411912, -0.5360785 , -0.4777041 , -0.53738135,\n",
              "        -0.53085715,  0.5166925 , -0.5345756 ,  0.5340592 , -0.48545617,\n",
              "         0.4817772 , -0.4834412 , -0.44982293, -0.46568325,  0.4644794 ,\n",
              "         0.46578193,  0.51873386,  0.45963708, -0.52480674,  0.48189026],\n",
              "       dtype=float32),\n",
              " 'quantitative': array([ 0.5152152 ,  0.4365272 , -0.5048275 , -0.4824674 , -0.4751044 ,\n",
              "         0.49635237,  0.530395  , -0.44856235, -0.49328738, -0.5063042 ,\n",
              "        -0.49609536,  0.48702028,  0.46363556,  0.47003165, -0.48946473,\n",
              "        -0.4475505 ,  0.50832945,  0.4751045 , -0.50540674, -0.44790497,\n",
              "        -0.45388782,  0.4292785 ,  0.4940592 ,  0.42696968, -0.44262904,\n",
              "        -0.49915022,  0.4855085 ,  0.47887284,  0.45153219,  0.43574712,\n",
              "        -0.52741444,  0.49142197, -0.47287455, -0.4880464 , -0.43495876,\n",
              "        -0.45585653,  0.43345708, -0.4262398 ,  0.49523124, -0.4764249 ,\n",
              "         0.46052918, -0.4928802 , -0.49743295, -0.47586966,  0.42088193,\n",
              "         0.4903466 ,  0.47619867,  0.47685257, -0.46964365,  0.4526061 ],\n",
              "       dtype=float32),\n",
              " 'however': array([ 0.4820222 ,  0.4986638 , -0.47741804, -0.4882465 , -0.455253  ,\n",
              "         0.49069965,  0.48318973, -0.5169683 , -0.5029863 , -0.4963718 ,\n",
              "        -0.5009395 ,  0.5260199 ,  0.47362086,  0.5032613 , -0.48491624,\n",
              "        -0.4785818 ,  0.51138717,  0.4458913 , -0.4314995 , -0.4992358 ,\n",
              "        -0.52175975,  0.43639767,  0.45430252,  0.45698738, -0.50421697,\n",
              "        -0.4439909 ,  0.46584493,  0.47265673,  0.45140582,  0.4514712 ,\n",
              "        -0.5371577 ,  0.44238883, -0.5317448 , -0.456363  , -0.4648046 ,\n",
              "        -0.46322018,  0.5305969 , -0.50356597,  0.4739079 , -0.4493073 ,\n",
              "         0.5149189 , -0.5297753 , -0.51390445, -0.45613804,  0.49608552,\n",
              "         0.4913696 ,  0.5245056 ,  0.44610706, -0.5160294 ,  0.43919602],\n",
              "       dtype=float32),\n",
              " 'iras': array([ 0.5285795 ,  0.47297856, -0.47246602, -0.48657653, -0.44288257,\n",
              "         0.4454918 ,  0.4283475 , -0.44217727, -0.4328009 , -0.48250186,\n",
              "        -0.42297834,  0.5107227 ,  0.47574028,  0.48664925, -0.5314621 ,\n",
              "        -0.43740177,  0.45110118,  0.44410977, -0.4726174 , -0.4463237 ,\n",
              "        -0.42577305,  0.50453615,  0.4745177 ,  0.49111456, -0.47171196,\n",
              "        -0.4374033 ,  0.4829281 ,  0.4595793 ,  0.47179753,  0.4256156 ,\n",
              "        -0.5169594 ,  0.5060681 , -0.4874344 , -0.48406157, -0.48532504,\n",
              "        -0.52059686,  0.49207366, -0.4510477 ,  0.45312414, -0.47772413,\n",
              "         0.4558068 , -0.49120933, -0.48805755, -0.4731444 ,  0.4720047 ,\n",
              "         0.45056832,  0.42634463,  0.47243208, -0.42757514,  0.45707285],\n",
              "       dtype=float32),\n",
              " 'contingent': array([ 0.5372855 ,  0.47692358, -0.451702  , -0.47281203, -0.46428502,\n",
              "         0.50063723,  0.51798546, -0.509264  , -0.5149239 , -0.42090702,\n",
              "        -0.5047715 ,  0.48619017,  0.4964662 ,  0.53441584, -0.56598216,\n",
              "        -0.50781834,  0.4801779 ,  0.51822084, -0.46398997, -0.5028156 ,\n",
              "        -0.5387365 ,  0.46745497,  0.44924247,  0.47055915, -0.47016454,\n",
              "        -0.50381327,  0.4471555 ,  0.5014933 ,  0.48121086,  0.49176097,\n",
              "        -0.51350164,  0.42673534, -0.5344416 , -0.44514215, -0.50704813,\n",
              "        -0.4886546 ,  0.516348  , -0.5159133 ,  0.47819388, -0.4514621 ,\n",
              "         0.51135886, -0.51136065, -0.5072426 , -0.47948945,  0.5127048 ,\n",
              "         0.5073604 ,  0.45131573,  0.52086276, -0.4670193 ,  0.43889087],\n",
              "       dtype=float32),\n",
              " 'agencies': array([ 0.498567  ,  0.4439842 , -0.4561407 , -0.49847767, -0.45664987,\n",
              "         0.5078189 ,  0.50105596, -0.5222111 , -0.45982903, -0.4294093 ,\n",
              "        -0.4187502 ,  0.45536005,  0.47088835,  0.5062811 , -0.49524963,\n",
              "        -0.46431515,  0.48394537,  0.4943394 , -0.48119536, -0.4646391 ,\n",
              "        -0.45277876,  0.44626522,  0.44660082,  0.47268796, -0.44366473,\n",
              "        -0.51575327,  0.43475094,  0.50984806,  0.438831  ,  0.5059396 ,\n",
              "        -0.45576742,  0.50772566, -0.49914324, -0.5043701 , -0.4379601 ,\n",
              "        -0.4595748 ,  0.4663554 , -0.47374377,  0.5022444 , -0.4825305 ,\n",
              "         0.45839947, -0.47526187, -0.4544384 , -0.5075871 ,  0.44263348,\n",
              "         0.46692193,  0.5209675 ,  0.4355648 , -0.48188668,  0.45118725],\n",
              "       dtype=float32),\n",
              " 'practices': array([ 0.52956134,  0.49454942, -0.4991426 , -0.4539327 , -0.48593014,\n",
              "         0.4890191 ,  0.44585454, -0.44103643, -0.49170956, -0.47183895,\n",
              "        -0.49349758,  0.436778  ,  0.4824933 ,  0.48497966, -0.52979237,\n",
              "        -0.51889557,  0.5113579 ,  0.4755045 , -0.4498865 , -0.5121875 ,\n",
              "        -0.4531026 ,  0.51634264,  0.44151074,  0.48274192, -0.50251037,\n",
              "        -0.48037836,  0.44576114,  0.5001447 ,  0.5104417 ,  0.4313159 ,\n",
              "        -0.48239887,  0.48359734, -0.5000155 , -0.47958472, -0.47985762,\n",
              "        -0.45567197,  0.5027158 , -0.4900323 ,  0.45340946, -0.5119022 ,\n",
              "         0.44025728, -0.5216846 , -0.48829994, -0.5017821 ,  0.44338396,\n",
              "         0.4747317 ,  0.46668935,  0.4621546 , -0.4418924 ,  0.5024296 ],\n",
              "       dtype=float32),\n",
              " 'resulting': array([ 0.5137983 ,  0.4673548 , -0.5194348 , -0.4857495 , -0.5356143 ,\n",
              "         0.5252873 ,  0.46013352, -0.46612167, -0.53007936, -0.46273202,\n",
              "        -0.4756397 ,  0.48864287,  0.48186278,  0.5103703 , -0.51995623,\n",
              "        -0.52112776,  0.4738129 ,  0.5230348 , -0.4403386 , -0.47384864,\n",
              "        -0.46257937,  0.45206663,  0.48717216,  0.44339135, -0.44635338,\n",
              "        -0.44287586,  0.48427567,  0.4744113 ,  0.46662536,  0.4390909 ,\n",
              "        -0.52475035,  0.47892416, -0.4876849 , -0.49288806, -0.43623406,\n",
              "        -0.52944076,  0.53370965, -0.51398695,  0.47030574, -0.45911014,\n",
              "         0.53061026, -0.49784452, -0.51378435, -0.48195735,  0.5303456 ,\n",
              "         0.53395164,  0.4747209 ,  0.50505555, -0.47441033,  0.4783679 ],\n",
              "       dtype=float32),\n",
              " 'options': array([ 0.4851158 ,  0.47747147, -0.50171614, -0.4912455 , -0.5221484 ,\n",
              "         0.50304824,  0.45587865, -0.5120752 , -0.50025916, -0.45897093,\n",
              "        -0.46537888,  0.45639044,  0.5181425 ,  0.5208202 , -0.47543788,\n",
              "        -0.43786705,  0.52161646,  0.49461722, -0.5023175 , -0.46932733,\n",
              "        -0.48473755,  0.44120294,  0.47537035,  0.46981466, -0.46104535,\n",
              "        -0.47616065,  0.5074245 ,  0.5174487 ,  0.46037787,  0.5005501 ,\n",
              "        -0.5290186 ,  0.48931783, -0.48130122, -0.4879871 , -0.49892694,\n",
              "        -0.44086015,  0.439992  , -0.5215836 ,  0.4534408 , -0.48505864,\n",
              "         0.4853431 , -0.4978751 , -0.49218017, -0.4394837 ,  0.49118352,\n",
              "         0.45487955,  0.46335292,  0.43103692, -0.49232247,  0.5123012 ],\n",
              "       dtype=float32),\n",
              " 'limitations': array([ 0.5278075 ,  0.44186544, -0.48796022, -0.46796912, -0.51741713,\n",
              "         0.51055104,  0.50837517, -0.46744663, -0.5065727 , -0.5022068 ,\n",
              "        -0.47120163,  0.47130394,  0.4757846 ,  0.48258144, -0.56399   ,\n",
              "        -0.45982444,  0.46718615,  0.527825  , -0.502411  , -0.4727425 ,\n",
              "        -0.51472664,  0.5144401 ,  0.46475533,  0.4673379 , -0.4917481 ,\n",
              "        -0.43836242,  0.43460947,  0.48384917,  0.46289483,  0.45125967,\n",
              "        -0.53935355,  0.48286107, -0.5342365 , -0.46515846, -0.52537954,\n",
              "        -0.50491416,  0.46531743, -0.5206038 ,  0.4526175 , -0.5171077 ,\n",
              "         0.43935683, -0.52348626, -0.5121668 , -0.43457535,  0.44546592,\n",
              "         0.47548878,  0.44421482,  0.48836356, -0.46188638,  0.46270365],\n",
              "       dtype=float32),\n",
              " 'broad': array([ 0.540486  ,  0.47153273, -0.4516265 , -0.47693095, -0.51237595,\n",
              "         0.4871531 ,  0.43053997, -0.50622463, -0.47519368, -0.50256217,\n",
              "        -0.49341217,  0.46074486,  0.486262  ,  0.46488595, -0.53584796,\n",
              "        -0.49681753,  0.45782036,  0.5089514 , -0.4602548 , -0.4821204 ,\n",
              "        -0.5103553 ,  0.47092015,  0.4685247 ,  0.4592745 , -0.45517114,\n",
              "        -0.46783328,  0.48969984,  0.5082476 ,  0.47079086,  0.44982055,\n",
              "        -0.49008203,  0.50314146, -0.44841582, -0.4874045 , -0.4559191 ,\n",
              "        -0.52105767,  0.5274961 , -0.46136722,  0.47994396, -0.44232997,\n",
              "         0.5156677 , -0.5216412 , -0.44869184, -0.5009271 ,  0.4912332 ,\n",
              "         0.47696787,  0.5158916 ,  0.5146476 , -0.48467594,  0.47299427],\n",
              "       dtype=float32),\n",
              " 'vanguard': array([ 0.50287527,  0.4729455 , -0.4764376 , -0.46860653, -0.47601983,\n",
              "         0.5427577 ,  0.44781706, -0.52167785, -0.49697515, -0.5048832 ,\n",
              "        -0.44569263,  0.4695834 ,  0.45083863,  0.4608526 , -0.47892886,\n",
              "        -0.52970827,  0.45844495,  0.5200159 , -0.5043067 , -0.5137715 ,\n",
              "        -0.5247067 ,  0.4787171 ,  0.480626  ,  0.44888163, -0.50983506,\n",
              "        -0.47439992,  0.44180655,  0.4683697 ,  0.52104926,  0.45257306,\n",
              "        -0.49047893,  0.46888882, -0.53841054, -0.49696305, -0.44983274,\n",
              "        -0.49726912,  0.45079428, -0.47005966,  0.47222742, -0.49742547,\n",
              "         0.53398436, -0.46035218, -0.43985435, -0.4520417 ,  0.4616749 ,\n",
              "         0.44726023,  0.44723874,  0.46726012, -0.46846405,  0.47552755],\n",
              "       dtype=float32),\n",
              " 'subadvisor': array([ 0.48802623,  0.43546483, -0.48388752, -0.45200425, -0.49105534,\n",
              "         0.4717458 ,  0.5180279 , -0.49593407, -0.5048183 , -0.48540825,\n",
              "        -0.47723317,  0.4660107 ,  0.47063336,  0.45453283, -0.5419547 ,\n",
              "        -0.48218912,  0.5108522 ,  0.46436805, -0.49583086, -0.49311262,\n",
              "        -0.442109  ,  0.44390675,  0.5168516 ,  0.4981134 , -0.46039808,\n",
              "        -0.46615493,  0.48011628,  0.5498923 ,  0.42900175,  0.48073053,\n",
              "        -0.5120577 ,  0.4949968 , -0.4749796 , -0.48437205, -0.44568282,\n",
              "        -0.52778405,  0.5171537 , -0.4445224 ,  0.51521844, -0.47372612,\n",
              "         0.44049022, -0.44958988, -0.48715445, -0.49718264,  0.51120186,\n",
              "         0.46321324,  0.4460864 ,  0.4458825 , -0.49903077,  0.45674658],\n",
              "       dtype=float32),\n",
              " 'take': array([ 0.45317042,  0.48033157, -0.49626058, -0.47604042, -0.45884618,\n",
              "         0.4771819 ,  0.48431268, -0.44929674, -0.4425918 , -0.5085929 ,\n",
              "        -0.45101407,  0.5175508 ,  0.43394938,  0.49122402, -0.5255505 ,\n",
              "        -0.46920574,  0.4499741 ,  0.43464044, -0.4802299 , -0.49612153,\n",
              "        -0.44982702,  0.43880233,  0.4826892 ,  0.50572073, -0.4965214 ,\n",
              "        -0.46292353,  0.4675866 ,  0.5317838 ,  0.45079282,  0.46295568,\n",
              "        -0.51645464,  0.451028  , -0.4605084 , -0.5191291 , -0.50908583,\n",
              "        -0.45613915,  0.52238524, -0.5105414 ,  0.4742699 , -0.47572297,\n",
              "         0.47914058, -0.43868154, -0.43046397, -0.44963992,  0.48157293,\n",
              "         0.51432025,  0.4330914 ,  0.468253  , -0.47128293,  0.429461  ],\n",
              "       dtype=float32),\n",
              " 'rising': array([ 0.52940154,  0.4906073 , -0.48459646, -0.44105718, -0.47560325,\n",
              "         0.47224042,  0.521364  , -0.4481621 , -0.45213917, -0.5074423 ,\n",
              "        -0.450157  ,  0.5088045 ,  0.46159703,  0.5037447 , -0.49528766,\n",
              "        -0.4448538 ,  0.48796296,  0.4698709 , -0.4823326 , -0.49769628,\n",
              "        -0.5147536 ,  0.46450013,  0.45293754,  0.4910725 , -0.4364945 ,\n",
              "        -0.51084036,  0.46920395,  0.46802664,  0.4590999 ,  0.5079    ,\n",
              "        -0.5002802 ,  0.44477758, -0.5151438 , -0.45211107, -0.45749035,\n",
              "        -0.48057926,  0.52647483, -0.45259485,  0.46117273, -0.49152425,\n",
              "         0.47475693, -0.50064754, -0.50048995, -0.4757948 ,  0.48660418,\n",
              "         0.45174283,  0.44320253,  0.49756604, -0.50601107,  0.51241547],\n",
              "       dtype=float32),\n",
              " 'exempt': array([ 0.5301457 ,  0.45730978, -0.45294628, -0.45011234, -0.52278364,\n",
              "         0.525626  ,  0.49300924, -0.43584245, -0.48130748, -0.43546507,\n",
              "        -0.5198284 ,  0.49843216,  0.4682142 ,  0.49834293, -0.5105841 ,\n",
              "        -0.5004964 ,  0.5065648 ,  0.49525782, -0.4220148 , -0.46170518,\n",
              "        -0.5201583 ,  0.50670743,  0.5127452 ,  0.45354787, -0.4864806 ,\n",
              "        -0.48874626,  0.52397424,  0.5447813 ,  0.47659323,  0.51314193,\n",
              "        -0.4870808 ,  0.49517924, -0.47532305, -0.47239152, -0.5155521 ,\n",
              "        -0.5180074 ,  0.52421165, -0.4510584 ,  0.4289151 , -0.52516097,\n",
              "         0.52004296, -0.48071554, -0.48027194, -0.5219214 ,  0.45182064,\n",
              "         0.4514935 ,  0.43294188,  0.46480322, -0.48671466,  0.48293185],\n",
              "       dtype=float32),\n",
              " 'resources': array([ 0.51135933,  0.5168338 , -0.4486793 , -0.4636433 , -0.46980715,\n",
              "         0.51255625,  0.49191785, -0.49599597, -0.49915597, -0.49859476,\n",
              "        -0.4650247 ,  0.5357473 ,  0.52893895,  0.48046282, -0.54052997,\n",
              "        -0.5025582 ,  0.5142885 ,  0.45531467, -0.5118874 , -0.4789527 ,\n",
              "        -0.48393968,  0.49480617,  0.52618164,  0.47298366, -0.4389326 ,\n",
              "        -0.44353795,  0.45218754,  0.48223776,  0.47386408,  0.5016271 ,\n",
              "        -0.4699055 ,  0.46549913, -0.5180529 , -0.4816942 , -0.4822111 ,\n",
              "        -0.5088835 ,  0.4521998 , -0.50097173,  0.4791556 , -0.45181817,\n",
              "         0.4771047 , -0.519863  , -0.4581139 , -0.5123289 ,  0.5195874 ,\n",
              "         0.48397556,  0.4706971 ,  0.46792975, -0.5244165 ,  0.45718578],\n",
              "       dtype=float32),\n",
              " 'offered': array([ 0.4973803 ,  0.45004654, -0.51186043, -0.5066976 , -0.4551025 ,\n",
              "         0.49789196,  0.4736338 , -0.4427184 , -0.43689916, -0.4931518 ,\n",
              "        -0.43445385,  0.4857388 ,  0.5074344 ,  0.4859097 , -0.5117112 ,\n",
              "        -0.4497651 ,  0.48721737,  0.45724037, -0.4256864 , -0.42364222,\n",
              "        -0.49778682,  0.48178613,  0.45178336,  0.48181605, -0.4591868 ,\n",
              "        -0.459896  ,  0.47398642,  0.47726816,  0.45918185,  0.45603943,\n",
              "        -0.44390154,  0.43999833, -0.4633109 , -0.4918211 , -0.5002185 ,\n",
              "        -0.5018568 ,  0.45718852, -0.45351285,  0.5101681 , -0.50726134,\n",
              "         0.44996995, -0.4621574 , -0.49993455, -0.48042655,  0.4808509 ,\n",
              "         0.45350027,  0.50093865,  0.46489617, -0.4577546 ,  0.5140212 ],\n",
              "       dtype=float32),\n",
              " 'governments': array([ 0.49966216,  0.4267656 , -0.49129015, -0.5067758 , -0.5231572 ,\n",
              "         0.5174589 ,  0.47369736, -0.520917  , -0.46107677, -0.46786243,\n",
              "        -0.46318966,  0.43797705,  0.44941035,  0.5183363 , -0.56596243,\n",
              "        -0.50242907,  0.496738  ,  0.52883375, -0.43885   , -0.511794  ,\n",
              "        -0.44412735,  0.42339087,  0.5223077 ,  0.44626427, -0.49295563,\n",
              "        -0.4679129 ,  0.43414894,  0.50169617,  0.48942375,  0.5089319 ,\n",
              "        -0.4678685 ,  0.46847782, -0.52601695, -0.52954847, -0.47558692,\n",
              "        -0.5102147 ,  0.5146911 , -0.44246495,  0.50634927, -0.5244429 ,\n",
              "         0.48068058, -0.5258877 , -0.42756712, -0.44785008,  0.44162497,\n",
              "         0.5212433 ,  0.47372803,  0.46080586, -0.48262984,  0.5100215 ],\n",
              "       dtype=float32),\n",
              " 'cdsc': array([ 0.45549336,  0.48376572, -0.4860142 , -0.49982184, -0.47389457,\n",
              "         0.45291176,  0.4389512 , -0.44741166, -0.51445615, -0.4278165 ,\n",
              "        -0.5511829 ,  0.48357198,  0.43108788,  0.5152303 , -0.52975076,\n",
              "        -0.47852087,  0.52839667,  0.48681164, -0.47919232, -0.5293419 ,\n",
              "        -0.47051322,  0.49371582,  0.45291597,  0.5188534 , -0.49404067,\n",
              "        -0.42219713,  0.43149847,  0.5294207 ,  0.5224296 ,  0.51013327,\n",
              "        -0.5123685 ,  0.47166622, -0.533212  , -0.5126965 , -0.4242096 ,\n",
              "        -0.50193185,  0.51074696, -0.5298508 ,  0.4414999 , -0.49289823,\n",
              "         0.46185437, -0.49038678, -0.4727943 , -0.5039609 ,  0.54859287,\n",
              "         0.5577066 ,  0.5002299 ,  0.47648332, -0.5391807 ,  0.5230713 ],\n",
              "       dtype=float32),\n",
              " 'glide': array([ 0.45458645,  0.4450811 , -0.5046969 , -0.4240326 , -0.50410527,\n",
              "         0.42823723,  0.45917976, -0.45741165, -0.46677476, -0.44140384,\n",
              "        -0.48853928,  0.47756958,  0.46380633,  0.47737825, -0.519691  ,\n",
              "        -0.4311757 ,  0.44282413,  0.4557562 , -0.44318447, -0.4618391 ,\n",
              "        -0.4693851 ,  0.42711192,  0.48406363,  0.46239147, -0.41447726,\n",
              "        -0.44169107,  0.48204666,  0.48654634,  0.49244258,  0.4512961 ,\n",
              "        -0.44345176,  0.4942065 , -0.48708153, -0.45615354, -0.47458732,\n",
              "        -0.45968172,  0.46940976, -0.50259084,  0.44465882, -0.46407577,\n",
              "         0.4305099 , -0.49152794, -0.44440204, -0.41298324,  0.44864175,\n",
              "         0.46771502,  0.42062545,  0.44032973, -0.4569486 ,  0.42650315],\n",
              "       dtype=float32),\n",
              " 'path': array([ 0.44836336,  0.4656161 , -0.47624758, -0.47025245, -0.46804503,\n",
              "         0.46630973,  0.4915953 , -0.43338928, -0.42069536, -0.47066057,\n",
              "        -0.43237424,  0.41468766,  0.42232493,  0.49772018, -0.47975957,\n",
              "        -0.48280537,  0.42455572,  0.46500865, -0.4636595 , -0.42652175,\n",
              "        -0.49472886,  0.45251596,  0.4276008 ,  0.47378996, -0.48708352,\n",
              "        -0.4957147 ,  0.45346993,  0.47448325,  0.4815667 ,  0.47783446,\n",
              "        -0.46142104,  0.4730311 , -0.4652507 , -0.5073299 , -0.4757041 ,\n",
              "        -0.47542164,  0.43803608, -0.47999954,  0.44675347, -0.45454365,\n",
              "         0.492202  , -0.5025321 , -0.4817192 , -0.49402   ,  0.48130342,\n",
              "         0.4883033 ,  0.4930821 ,  0.4487859 , -0.46173614,  0.47364157],\n",
              "       dtype=float32),\n",
              " 'representative': array([ 0.5131102 ,  0.48552334, -0.48120457, -0.50485265, -0.4805511 ,\n",
              "         0.4533531 ,  0.4919517 , -0.50468993, -0.46985987, -0.50944626,\n",
              "        -0.46560565,  0.45323518,  0.5074139 ,  0.47124887, -0.49684006,\n",
              "        -0.4870692 ,  0.5056021 ,  0.5257874 , -0.5113909 , -0.4514015 ,\n",
              "        -0.43383017,  0.46745557,  0.47610497,  0.49871528, -0.5133971 ,\n",
              "        -0.48427027,  0.507465  ,  0.5383223 ,  0.5011478 ,  0.4862272 ,\n",
              "        -0.45768562,  0.50157416, -0.52321225, -0.47679427, -0.46546113,\n",
              "        -0.4967927 ,  0.4887253 , -0.49489468,  0.44066435, -0.4470269 ,\n",
              "         0.47324175, -0.44628683, -0.4281422 , -0.4725656 ,  0.46237463,\n",
              "         0.5005805 ,  0.43100443,  0.49484766, -0.48307908,  0.4421488 ],\n",
              "       dtype=float32),\n",
              " 'forward': array([ 0.45648015,  0.45443764, -0.51820004, -0.4577921 , -0.46715814,\n",
              "         0.46284628,  0.5069165 , -0.50547296, -0.49652892, -0.50826544,\n",
              "        -0.4504532 ,  0.457845  ,  0.44994336,  0.4477137 , -0.54689187,\n",
              "        -0.5073848 ,  0.4750822 ,  0.52063406, -0.43572652, -0.48201668,\n",
              "        -0.43902493,  0.4941172 ,  0.47369254,  0.45410118, -0.5024297 ,\n",
              "        -0.5170022 ,  0.4701372 ,  0.45736   ,  0.4821865 ,  0.4423464 ,\n",
              "        -0.45418644,  0.50558996, -0.4908366 , -0.4960862 , -0.45737636,\n",
              "        -0.51806456,  0.46029678, -0.47652557,  0.5049654 , -0.50182486,\n",
              "         0.46509174, -0.44627246, -0.4782113 , -0.46440503,  0.5009134 ,\n",
              "         0.44255054,  0.47472474,  0.49261147, -0.4323993 ,  0.4954202 ],\n",
              "       dtype=float32),\n",
              " 'except': array([ 0.5076567 ,  0.4603591 , -0.4937151 , -0.5118744 , -0.489968  ,\n",
              "         0.49491888,  0.49891123, -0.50765896, -0.46531057, -0.466358  ,\n",
              "        -0.45370978,  0.45577514,  0.5372395 ,  0.48725584, -0.5048657 ,\n",
              "        -0.54966503,  0.5198396 ,  0.50162697, -0.4472599 , -0.46413463,\n",
              "        -0.4732316 ,  0.51630163,  0.4960261 ,  0.5069004 , -0.49935716,\n",
              "        -0.5198239 ,  0.48870376,  0.50836545,  0.46238953,  0.45695722,\n",
              "        -0.46328658,  0.45957702, -0.47210756, -0.5049851 , -0.5085484 ,\n",
              "        -0.52393174,  0.48387957, -0.5323792 ,  0.4691719 , -0.46991795,\n",
              "         0.45419273, -0.5453267 , -0.5234314 , -0.46465784,  0.4948306 ,\n",
              "         0.5166287 ,  0.4364943 ,  0.49213445, -0.50406027,  0.4510612 ],\n",
              "       dtype=float32),\n",
              " 'ira': array([ 0.466854  ,  0.44075048, -0.4690642 , -0.48060486, -0.43930766,\n",
              "         0.45996925,  0.4832334 , -0.49499485, -0.49093303, -0.44384915,\n",
              "        -0.43423936,  0.437547  ,  0.44854468,  0.46371606, -0.49668744,\n",
              "        -0.43167067,  0.4822128 ,  0.5228177 , -0.42695194, -0.43201277,\n",
              "        -0.47648296,  0.50990313,  0.49073842,  0.48901415, -0.5107876 ,\n",
              "        -0.499916  ,  0.5091986 ,  0.53220946,  0.5076431 ,  0.5090486 ,\n",
              "        -0.45105144,  0.44227907, -0.45228305, -0.48122615, -0.4987117 ,\n",
              "        -0.49805814,  0.4758948 , -0.44762045,  0.46924782, -0.44205534,\n",
              "         0.48020458, -0.46440107, -0.49261615, -0.5100708 ,  0.45888418,\n",
              "         0.47980568,  0.5006867 ,  0.44484377, -0.50613153,  0.5139704 ],\n",
              "       dtype=float32),\n",
              " 'commodity': array([ 0.52906203,  0.49993384, -0.4676745 , -0.46004575, -0.47125465,\n",
              "         0.52331537,  0.50290155, -0.48718736, -0.517357  , -0.4724513 ,\n",
              "        -0.50962317,  0.4731902 ,  0.50025016,  0.4754511 , -0.5065934 ,\n",
              "        -0.4640608 ,  0.47176433,  0.47857955, -0.43429762, -0.44565603,\n",
              "        -0.43892995,  0.49610198,  0.5213243 ,  0.4417362 , -0.51393986,\n",
              "        -0.4812671 ,  0.45368624,  0.5138757 ,  0.5020985 ,  0.46987164,\n",
              "        -0.48834157,  0.45046464, -0.54253167, -0.47214603, -0.51574844,\n",
              "        -0.4981881 ,  0.49305707, -0.4528271 ,  0.4897393 , -0.50669765,\n",
              "         0.44522846, -0.50228345, -0.49986997, -0.48306993,  0.4901065 ,\n",
              "         0.45990127,  0.499053  ,  0.5215938 , -0.4390263 ,  0.45064035],\n",
              "       dtype=float32),\n",
              " 'exchange-traded': array([ 0.53390384,  0.4663888 , -0.49314564, -0.52508825, -0.532241  ,\n",
              "         0.4944604 ,  0.44607857, -0.52288383, -0.448776  , -0.4956762 ,\n",
              "        -0.47729138,  0.4651564 ,  0.5215713 ,  0.5084603 , -0.53305405,\n",
              "        -0.4671977 ,  0.5072064 ,  0.45857713, -0.5013012 , -0.46401122,\n",
              "        -0.4404012 ,  0.45108646,  0.52388006,  0.5113902 , -0.48780212,\n",
              "        -0.50263447,  0.44014513,  0.49206182,  0.43967697,  0.46299064,\n",
              "        -0.47467214,  0.51232415, -0.4583719 , -0.46056515, -0.50192547,\n",
              "        -0.48764768,  0.4649062 , -0.45969835,  0.45654517, -0.4717313 ,\n",
              "         0.5111535 , -0.4609376 , -0.508792  , -0.47850746,  0.49300116,\n",
              "         0.47918966,  0.4478099 ,  0.4359609 , -0.48078427,  0.44436288],\n",
              "       dtype=float32),\n",
              " 'deductions': array([ 0.51784194,  0.4677358 , -0.4682091 , -0.4482414 , -0.45518863,\n",
              "         0.5004108 ,  0.4760771 , -0.48713118, -0.51580626, -0.50661755,\n",
              "        -0.5242763 ,  0.44794366,  0.45178315,  0.46816894, -0.5783081 ,\n",
              "        -0.5224166 ,  0.5352541 ,  0.52487427, -0.48274958, -0.42800716,\n",
              "        -0.5147815 ,  0.49796933,  0.46871066,  0.49305943, -0.5061785 ,\n",
              "        -0.47880968,  0.50155795,  0.5537767 ,  0.4687828 ,  0.44661406,\n",
              "        -0.4772153 ,  0.4387835 , -0.5337834 , -0.51750237, -0.43713748,\n",
              "        -0.5030838 ,  0.50356174, -0.51978964,  0.46395347, -0.50118774,\n",
              "         0.5000652 , -0.50362265, -0.4839491 , -0.4513952 ,  0.51036906,\n",
              "         0.5115404 ,  0.47857487,  0.4482419 , -0.49093592,  0.43685356],\n",
              "       dtype=float32),\n",
              " 'fail': array([ 0.527266  ,  0.4551079 , -0.5205693 , -0.51260394, -0.45652422,\n",
              "         0.48777124,  0.43464646, -0.48971653, -0.43592134, -0.45977283,\n",
              "        -0.49842763,  0.4717388 ,  0.4567577 ,  0.4709435 , -0.5246936 ,\n",
              "        -0.46584466,  0.4445946 ,  0.51444703, -0.46369186, -0.51457095,\n",
              "        -0.48095638,  0.51775414,  0.50279194,  0.4427101 , -0.48343766,\n",
              "        -0.52299654,  0.52001685,  0.4609817 ,  0.46344516,  0.44837955,\n",
              "        -0.53389716,  0.48839074, -0.469787  , -0.46534893, -0.48918533,\n",
              "        -0.5289406 ,  0.47255984, -0.44997472,  0.4891725 , -0.5197414 ,\n",
              "         0.49732658, -0.50649726, -0.44928464, -0.43822724,  0.5012646 ,\n",
              "         0.45335737,  0.45496225,  0.5069302 , -0.46622276,  0.44011313],\n",
              "       dtype=float32),\n",
              " 'entities': array([ 0.4981881 ,  0.4351193 , -0.50460875, -0.48827943, -0.4367336 ,\n",
              "         0.4840381 ,  0.5048911 , -0.50037956, -0.5035873 , -0.48067617,\n",
              "        -0.5028234 ,  0.44518423,  0.42120898,  0.4759869 , -0.5210522 ,\n",
              "        -0.4830711 ,  0.45389378,  0.5009836 , -0.48943856, -0.41621664,\n",
              "        -0.45747158,  0.4520662 ,  0.4726356 ,  0.45429397, -0.45584947,\n",
              "        -0.4810043 ,  0.44256118,  0.48375496,  0.49647394,  0.48044994,\n",
              "        -0.52173436,  0.43911695, -0.4893009 , -0.49563465, -0.43474483,\n",
              "        -0.4751151 ,  0.48880112, -0.5175605 ,  0.43522152, -0.43329823,\n",
              "         0.50087976, -0.47960153, -0.49961826, -0.4844707 ,  0.45892566,\n",
              "         0.43804565,  0.46623874,  0.5088472 , -0.46123576,  0.46633196],\n",
              "       dtype=float32),\n",
              " 'ratings': array([ 0.5110135 ,  0.42595446, -0.4496227 , -0.47978005, -0.5164556 ,\n",
              "         0.48777786,  0.49864027, -0.46299765, -0.42944822, -0.48379266,\n",
              "        -0.48577237,  0.45745587,  0.49176848,  0.4751804 , -0.5708289 ,\n",
              "        -0.48160172,  0.47806603,  0.43478987, -0.4650607 , -0.42183432,\n",
              "        -0.49345806,  0.46130422,  0.4658021 ,  0.48242196, -0.4927692 ,\n",
              "        -0.45675355,  0.4338916 ,  0.49100828,  0.47256872,  0.49820772,\n",
              "        -0.50684726,  0.4985754 , -0.5277662 , -0.45020357, -0.5065964 ,\n",
              "        -0.5063429 ,  0.46478796, -0.47340912,  0.5128557 , -0.46279466,\n",
              "         0.51110995, -0.45810273, -0.46059752, -0.51293933,  0.4851162 ,\n",
              "         0.5125232 ,  0.50843716,  0.45446008, -0.44771558,  0.4941476 ],\n",
              "       dtype=float32),\n",
              " 'usaa': array([ 0.45761314,  0.49451554, -0.49281883, -0.4978876 , -0.52835125,\n",
              "         0.47044387,  0.4580918 , -0.48341757, -0.47681576, -0.4989248 ,\n",
              "        -0.47301987,  0.467666  ,  0.4791372 ,  0.44768634, -0.51413274,\n",
              "        -0.49405104,  0.5052363 ,  0.48955968, -0.4605248 , -0.42667598,\n",
              "        -0.5168611 ,  0.5107568 ,  0.45914933,  0.48058417, -0.51380205,\n",
              "        -0.5144224 ,  0.46067947,  0.53643274,  0.4751547 ,  0.521484  ,\n",
              "        -0.4581067 ,  0.44150186, -0.4662472 , -0.50785667, -0.47837684,\n",
              "        -0.4983478 ,  0.5217782 , -0.48532808,  0.5019884 , -0.51341885,\n",
              "         0.47170895, -0.5191548 , -0.464364  , -0.46914014,  0.4990413 ,\n",
              "         0.49948567,  0.45532605,  0.49030575, -0.45755908,  0.43435743],\n",
              "       dtype=float32),\n",
              " 'substantial': array([ 0.4943558 ,  0.4325214 , -0.5172113 , -0.4407011 , -0.4631128 ,\n",
              "         0.5303726 ,  0.48010135, -0.53544563, -0.47194198, -0.4693225 ,\n",
              "        -0.48069322,  0.4518214 ,  0.52509844,  0.45559996, -0.5623288 ,\n",
              "        -0.5117034 ,  0.5212499 ,  0.5116033 , -0.4758547 , -0.45726365,\n",
              "        -0.51590693,  0.450084  ,  0.48064506,  0.49651852, -0.49792805,\n",
              "        -0.49901596,  0.4783876 ,  0.46219948,  0.47291216,  0.48368618,\n",
              "        -0.5237737 ,  0.44279128, -0.4659032 , -0.4514094 , -0.4658812 ,\n",
              "        -0.5191556 ,  0.48842254, -0.48798722,  0.44528934, -0.52727026,\n",
              "         0.5153498 , -0.48761404, -0.5186278 , -0.43504882,  0.48863143,\n",
              "         0.48837423,  0.51169425,  0.4739656 , -0.524094  ,  0.47892255],\n",
              "       dtype=float32),\n",
              " 'arrangements': array([ 0.5125276 ,  0.4902995 , -0.490502  , -0.49326012, -0.45724136,\n",
              "         0.4437116 ,  0.42593157, -0.48855668, -0.47533876, -0.46344987,\n",
              "        -0.46976542,  0.4849374 ,  0.46185642,  0.46529183, -0.47073504,\n",
              "        -0.4867211 ,  0.47291186,  0.4565505 , -0.49329206, -0.47991264,\n",
              "        -0.5133707 ,  0.4760702 ,  0.4468276 ,  0.44240665, -0.50369763,\n",
              "        -0.48217958,  0.4961658 ,  0.51753366,  0.44744438,  0.4319469 ,\n",
              "        -0.47411287,  0.49575424, -0.51153505, -0.52023494, -0.46344358,\n",
              "        -0.49080113,  0.48164243, -0.47024933,  0.42909467, -0.48091376,\n",
              "         0.5112148 , -0.45925993, -0.4459786 , -0.4717031 ,  0.5123999 ,\n",
              "         0.48480627,  0.46523836,  0.4822828 , -0.45207262,  0.4605034 ],\n",
              "       dtype=float32),\n",
              " 'managing': array([ 0.51819855,  0.46655726, -0.50121844, -0.49737817, -0.5342001 ,\n",
              "         0.51378685,  0.45211074, -0.4787543 , -0.4697108 , -0.42922115,\n",
              "        -0.48094928,  0.45504308,  0.48495454,  0.5150363 , -0.5996392 ,\n",
              "        -0.48717293,  0.50413555,  0.50839484, -0.4596268 , -0.49337634,\n",
              "        -0.45550025,  0.44235608,  0.50007993,  0.4491269 , -0.49822122,\n",
              "        -0.4384203 ,  0.50219166,  0.50413144,  0.5056792 ,  0.43785816,\n",
              "        -0.5054522 ,  0.51645875, -0.5050654 , -0.476885  , -0.43388632,\n",
              "        -0.4670391 ,  0.5096799 , -0.48802847,  0.43428117, -0.47463045,\n",
              "         0.46145457, -0.45083755, -0.49209213, -0.51111525,  0.45339784,\n",
              "         0.5109873 ,  0.45583546,  0.5100576 , -0.44172257,  0.49360043],\n",
              "       dtype=float32),\n",
              " 'next': array([ 0.4983679 ,  0.4874229 , -0.49295363, -0.46695226, -0.5023956 ,\n",
              "         0.45214802,  0.48537958, -0.49568447, -0.5035828 , -0.4419506 ,\n",
              "        -0.46899515,  0.49637428,  0.51091576,  0.454924  , -0.46164817,\n",
              "        -0.4524276 ,  0.5048632 ,  0.44359133, -0.5065796 , -0.49128708,\n",
              "        -0.42944527,  0.44464758,  0.5112315 ,  0.4279975 , -0.42608228,\n",
              "        -0.4829738 ,  0.43358248,  0.48184884,  0.43604428,  0.4933846 ,\n",
              "        -0.4745556 ,  0.51325005, -0.5043915 , -0.4733191 , -0.4433555 ,\n",
              "        -0.43641296,  0.4649667 , -0.52424866,  0.43101284, -0.47986934,\n",
              "         0.522194  , -0.49798456, -0.46863   , -0.43599546,  0.49681655,\n",
              "         0.49637538,  0.44489077,  0.4683872 , -0.45528293,  0.49854124],\n",
              "       dtype=float32),\n",
              " 'appendix': array([ 0.496076  ,  0.46950644, -0.53103733, -0.44092464, -0.4683946 ,\n",
              "         0.5172041 ,  0.4401147 , -0.45185697, -0.46106157, -0.461973  ,\n",
              "        -0.4928764 ,  0.48479632,  0.51905245,  0.4666195 , -0.5007768 ,\n",
              "        -0.4486121 ,  0.5001828 ,  0.47964424, -0.47432002, -0.45035917,\n",
              "        -0.4965123 ,  0.49933612,  0.46658686,  0.4480016 , -0.47220197,\n",
              "        -0.44288823,  0.506166  ,  0.53065073,  0.441506  ,  0.46183488,\n",
              "        -0.5075331 ,  0.43890324, -0.49329653, -0.47536945, -0.5095456 ,\n",
              "        -0.5075178 ,  0.5188358 , -0.46412396,  0.50093776, -0.44690287,\n",
              "         0.5038048 , -0.46402097, -0.4855861 , -0.48032087,  0.5044831 ,\n",
              "         0.47928077,  0.5070983 ,  0.48283225, -0.4728024 ,  0.52153355],\n",
              "       dtype=float32),\n",
              " 'susceptible': array([ 0.52795637,  0.451474  , -0.5024959 , -0.5137939 , -0.5027824 ,\n",
              "         0.50298053,  0.4523392 , -0.45941895, -0.46242055, -0.4882632 ,\n",
              "        -0.5141314 ,  0.45504034,  0.46508616,  0.51782006, -0.55519205,\n",
              "        -0.5115614 ,  0.5309162 ,  0.46127254, -0.47736996, -0.5066339 ,\n",
              "        -0.4518211 ,  0.46602315,  0.5352172 ,  0.4475832 , -0.5258188 ,\n",
              "        -0.50106996,  0.457382  ,  0.500908  ,  0.44902915,  0.4817572 ,\n",
              "        -0.49525657,  0.46129885, -0.46909767, -0.5205943 , -0.48865113,\n",
              "        -0.5213082 ,  0.52045685, -0.5077894 ,  0.4554908 , -0.51358855,\n",
              "         0.50757027, -0.53738993, -0.5114305 , -0.48973447,  0.4993385 ,\n",
              "         0.47646248,  0.52560115,  0.49158323, -0.5140558 ,  0.4540738 ],\n",
              "       dtype=float32),\n",
              " 'provided': array([ 0.50043154,  0.49804813, -0.4663437 , -0.4908119 , -0.43829212,\n",
              "         0.44927415,  0.46160716, -0.4599941 , -0.46674317, -0.47275057,\n",
              "        -0.51200354,  0.43635347,  0.47478682,  0.509148  , -0.5573447 ,\n",
              "        -0.503835  ,  0.44308925,  0.48880595, -0.44482028, -0.4500343 ,\n",
              "        -0.48312852,  0.44437835,  0.51180553,  0.43019858, -0.4579399 ,\n",
              "        -0.5140748 ,  0.5027729 ,  0.48327422,  0.44609243,  0.51394284,\n",
              "        -0.49390566,  0.4850242 , -0.4980952 , -0.4657909 , -0.469307  ,\n",
              "        -0.45173666,  0.4587798 , -0.45440745,  0.49478135, -0.43379995,\n",
              "         0.49402776, -0.51301193, -0.4930993 , -0.51404816,  0.4767855 ,\n",
              "         0.5206549 ,  0.44056445,  0.44206825, -0.48456123,  0.47279125],\n",
              "       dtype=float32),\n",
              " 'revenue': array([ 0.4660511 ,  0.47110808, -0.5217781 , -0.48772958, -0.5390375 ,\n",
              "         0.474469  ,  0.4839257 , -0.4835229 , -0.52545446, -0.44838583,\n",
              "        -0.44727632,  0.43126518,  0.4974253 ,  0.47871092, -0.5047699 ,\n",
              "        -0.43421018,  0.46433175,  0.5041734 , -0.4756494 , -0.4309058 ,\n",
              "        -0.5189255 ,  0.4283767 ,  0.50897914,  0.47510085, -0.5143759 ,\n",
              "        -0.46709004,  0.4572566 ,  0.4757166 ,  0.47567534,  0.4914009 ,\n",
              "        -0.49179524,  0.4359916 , -0.50167346, -0.4456526 , -0.4947542 ,\n",
              "        -0.47269762,  0.43454537, -0.5233083 ,  0.51189333, -0.51136875,\n",
              "         0.4464032 , -0.5283153 , -0.43276587, -0.45504427,  0.50287396,\n",
              "         0.4607898 ,  0.45836288,  0.48145694, -0.46028364,  0.51093656],\n",
              "       dtype=float32),\n",
              " 'desired': array([ 0.47024354,  0.4526392 , -0.4983718 , -0.4634999 , -0.48881927,\n",
              "         0.5197457 ,  0.49261755, -0.4792388 , -0.4362058 , -0.4319191 ,\n",
              "        -0.46818385,  0.4952206 ,  0.4572934 ,  0.4550884 , -0.49545243,\n",
              "        -0.4879199 ,  0.45737553,  0.4505425 , -0.506706  , -0.47351563,\n",
              "        -0.44611073,  0.44946182,  0.47775128,  0.45546338, -0.5012336 ,\n",
              "        -0.43928665,  0.4961946 ,  0.5269162 ,  0.4576452 ,  0.48721874,\n",
              "        -0.5006571 ,  0.50853795, -0.45546612, -0.5234101 , -0.4440966 ,\n",
              "        -0.51008326,  0.46320176, -0.5128422 ,  0.45407093, -0.45736113,\n",
              "         0.44537356, -0.47448465, -0.45955524, -0.49422508,  0.4442109 ,\n",
              "         0.48829922,  0.43907526,  0.50954175, -0.4906161 ,  0.4369386 ],\n",
              "       dtype=float32),\n",
              " 'track': array([ 0.4782332 ,  0.47772896, -0.518324  , -0.48200336, -0.46051225,\n",
              "         0.53632486,  0.4630372 , -0.47551313, -0.45262328, -0.46214136,\n",
              "        -0.46907854,  0.51115406,  0.5090581 ,  0.45055145, -0.50105023,\n",
              "        -0.5210587 ,  0.47705954,  0.44917017, -0.48943025, -0.5198802 ,\n",
              "        -0.45246154,  0.46612912,  0.4569425 ,  0.46110645, -0.5018111 ,\n",
              "        -0.47845507,  0.50121456,  0.4652919 ,  0.4770619 ,  0.44276276,\n",
              "        -0.53849083,  0.4445947 , -0.47478187, -0.4497856 , -0.509212  ,\n",
              "        -0.49618608,  0.4481033 , -0.5235978 ,  0.49199644, -0.50988317,\n",
              "         0.5285611 , -0.5227949 , -0.4934922 , -0.46015644,  0.5197945 ,\n",
              "         0.47897017,  0.472297  ,  0.5000736 , -0.47032794,  0.4375237 ],\n",
              "       dtype=float32),\n",
              " 'considered': array([ 0.5228467 ,  0.50458205, -0.48032627, -0.47075665, -0.46838847,\n",
              "         0.5158778 ,  0.5077572 , -0.45062745, -0.46898335, -0.45851335,\n",
              "        -0.47270942,  0.45291382,  0.4564095 ,  0.4638435 , -0.52375656,\n",
              "        -0.51547766,  0.44292068,  0.4684727 , -0.45216507, -0.45072764,\n",
              "        -0.5134391 ,  0.47515976,  0.4530664 ,  0.4549233 , -0.4419005 ,\n",
              "        -0.44076094,  0.43993518,  0.550483  ,  0.52980375,  0.5222108 ,\n",
              "        -0.477936  ,  0.49761158, -0.5183708 , -0.5188138 , -0.44080225,\n",
              "        -0.48410335,  0.53480345, -0.497471  ,  0.50569713, -0.5301224 ,\n",
              "         0.52957267, -0.5228343 , -0.50856066, -0.46504396,  0.53094023,\n",
              "         0.5215789 ,  0.47205192,  0.5027171 , -0.46256083,  0.48679876],\n",
              "       dtype=float32),\n",
              " 'vp': array([ 0.53397274,  0.47185156, -0.5173154 , -0.46218693, -0.51904064,\n",
              "         0.5086109 ,  0.47425556, -0.5075462 , -0.5214988 , -0.48276687,\n",
              "        -0.48190284,  0.45117137,  0.5154734 ,  0.49166995, -0.55117714,\n",
              "        -0.46078765,  0.47634304,  0.4487703 , -0.4941817 , -0.5129416 ,\n",
              "        -0.44416544,  0.48028192,  0.5290954 ,  0.51728064, -0.5083764 ,\n",
              "        -0.43410805,  0.44052705,  0.5083741 ,  0.43144378,  0.43333983,\n",
              "        -0.5014217 ,  0.46446532, -0.47148314, -0.5227668 , -0.46207237,\n",
              "        -0.537763  ,  0.5331643 , -0.44135326,  0.46213084, -0.53248876,\n",
              "         0.5147382 , -0.45137388, -0.4767937 , -0.4320838 ,  0.51390123,\n",
              "         0.53007215,  0.49499315,  0.45576242, -0.5060451 ,  0.44632548],\n",
              "       dtype=float32),\n",
              " 'maturities': array([ 0.5201912 ,  0.48962927, -0.43366525, -0.42011   , -0.45873907,\n",
              "         0.47077003,  0.44902012, -0.4313237 , -0.5026405 , -0.46173322,\n",
              "        -0.47930855,  0.4914403 ,  0.5006348 ,  0.47605598, -0.54657197,\n",
              "        -0.4358199 ,  0.5047684 ,  0.4876934 , -0.4767872 , -0.46127558,\n",
              "        -0.49069783,  0.4485665 ,  0.49833333,  0.48635694, -0.4354453 ,\n",
              "        -0.49911606,  0.491777  ,  0.5228847 ,  0.44384494,  0.45125794,\n",
              "        -0.4810982 ,  0.50098115, -0.44806722, -0.50564915, -0.4561167 ,\n",
              "        -0.43982792,  0.46361145, -0.43658477,  0.49261555, -0.43700916,\n",
              "         0.46158868, -0.5046513 , -0.46736807, -0.4735994 ,  0.48635933,\n",
              "         0.45001635,  0.47680607,  0.50170934, -0.43047246,  0.49687505],\n",
              "       dtype=float32),\n",
              " 'family': array([ 0.48389593,  0.51411694, -0.44650334, -0.4556066 , -0.48174322,\n",
              "         0.50047   ,  0.44001183, -0.50929   , -0.50581497, -0.4858048 ,\n",
              "        -0.4658157 ,  0.521808  ,  0.5017091 ,  0.52435327, -0.5685104 ,\n",
              "        -0.4792536 ,  0.52092373,  0.4415922 , -0.48948234, -0.45058754,\n",
              "        -0.49667746,  0.51536256,  0.5252791 ,  0.5175    , -0.4671171 ,\n",
              "        -0.4931193 ,  0.50967646,  0.53364944,  0.452255  ,  0.46421063,\n",
              "        -0.54085577,  0.4206655 , -0.47566202, -0.5207176 , -0.43159226,\n",
              "        -0.50727224,  0.45569876, -0.4491132 ,  0.4979974 , -0.49512762,\n",
              "         0.51992285, -0.48421875, -0.45308393, -0.50206685,  0.4744767 ,\n",
              "         0.5421945 ,  0.4725214 ,  0.52818227, -0.45123753,  0.44576573],\n",
              "       dtype=float32),\n",
              " 'secondary': array([ 0.5182075 ,  0.49808404, -0.52246094, -0.4886528 , -0.523578  ,\n",
              "         0.47137243,  0.45815745, -0.4748944 , -0.5010873 , -0.45595324,\n",
              "        -0.47131917,  0.48459268,  0.43730655,  0.49199092, -0.48006117,\n",
              "        -0.46177426,  0.455689  ,  0.4684954 , -0.49704117, -0.4589079 ,\n",
              "        -0.45438218,  0.48432642,  0.51474524,  0.44034675, -0.45272982,\n",
              "        -0.44459137,  0.49007344,  0.5282339 ,  0.48713973,  0.46546745,\n",
              "        -0.51358974,  0.43346283, -0.45783392, -0.46812987, -0.4710751 ,\n",
              "        -0.45583934,  0.49451345, -0.47269768,  0.44314858, -0.46268278,\n",
              "         0.45372227, -0.49018922, -0.4987678 , -0.49538785,  0.49723458,\n",
              "         0.48448378,  0.4365132 ,  0.43945348, -0.5068757 ,  0.4917482 ],\n",
              "       dtype=float32),\n",
              " 'agree': array([ 0.52263737,  0.44124702, -0.5102909 , -0.51239705, -0.49350637,\n",
              "         0.49004868,  0.45035902, -0.43757722, -0.48918894, -0.483964  ,\n",
              "        -0.51226306,  0.4724325 ,  0.51339376,  0.53207994, -0.5313227 ,\n",
              "        -0.5183488 ,  0.4394761 ,  0.4817008 , -0.4651846 , -0.43506414,\n",
              "        -0.4788215 ,  0.43972552,  0.44889927,  0.47139698, -0.46823376,\n",
              "        -0.5159842 ,  0.4740306 ,  0.4882679 ,  0.47224638,  0.5132114 ,\n",
              "        -0.49519885,  0.46438906, -0.5070129 , -0.50735015, -0.46152896,\n",
              "        -0.44160134,  0.44360328, -0.45416293,  0.4741976 , -0.4752104 ,\n",
              "         0.51550615, -0.49575397, -0.50823706, -0.49466926,  0.5128553 ,\n",
              "         0.4678447 ,  0.48791128,  0.49752167, -0.4961291 ,  0.5157396 ],\n",
              "       dtype=float32),\n",
              " 'exchanges': array([ 0.47796464,  0.5001352 , -0.46298245, -0.49271256, -0.44990298,\n",
              "         0.50873595,  0.46365485, -0.4962039 , -0.4378611 , -0.47542375,\n",
              "        -0.51481783,  0.496975  ,  0.51243466,  0.4486064 , -0.49050704,\n",
              "        -0.50542104,  0.52302885,  0.5072991 , -0.46870446, -0.44197685,\n",
              "        -0.486792  ,  0.48309654,  0.47263756,  0.47696328, -0.4380359 ,\n",
              "        -0.44364986,  0.4965026 ,  0.48902935,  0.45731547,  0.4475332 ,\n",
              "        -0.49448997,  0.4472173 , -0.53498995, -0.53127694, -0.4843627 ,\n",
              "        -0.5096518 ,  0.4987239 , -0.5244489 ,  0.5109131 , -0.4465889 ,\n",
              "         0.4977832 , -0.4732597 , -0.52156436, -0.47833425,  0.5127902 ,\n",
              "         0.5112697 ,  0.4738505 ,  0.5097538 , -0.4454963 ,  0.43983787],\n",
              "       dtype=float32),\n",
              " 'perceived': array([ 0.510544  ,  0.42645562, -0.49167442, -0.48704922, -0.5076079 ,\n",
              "         0.45658824,  0.48209968, -0.53167456, -0.52147317, -0.49453974,\n",
              "        -0.5176203 ,  0.44973487,  0.47704434,  0.46635896, -0.5112537 ,\n",
              "        -0.47561374,  0.44527078,  0.5222415 , -0.49674937, -0.43683553,\n",
              "        -0.48614496,  0.50179625,  0.52791387,  0.5131753 , -0.48814908,\n",
              "        -0.513222  ,  0.43942872,  0.4578106 ,  0.5069689 ,  0.5063451 ,\n",
              "        -0.5054738 ,  0.4943538 , -0.5187049 , -0.5358049 , -0.45292336,\n",
              "        -0.513422  ,  0.5118943 , -0.4523149 ,  0.49955487, -0.4480425 ,\n",
              "         0.4671466 , -0.51030004, -0.49051315, -0.5183966 ,  0.49800384,\n",
              "         0.48203975,  0.5167983 ,  0.46555218, -0.461636  ,  0.476732  ],\n",
              "       dtype=float32),\n",
              " 'march': array([ 0.5144619 ,  0.45701212, -0.457782  , -0.4835886 , -0.5255616 ,\n",
              "         0.48904997,  0.426696  , -0.47186115, -0.46295467, -0.4364125 ,\n",
              "        -0.45149162,  0.49551672,  0.4395329 ,  0.4762783 , -0.5145141 ,\n",
              "        -0.5069294 ,  0.4371268 ,  0.51928395, -0.4249776 , -0.4707742 ,\n",
              "        -0.47355288,  0.46131796,  0.46702412,  0.44914603, -0.4619062 ,\n",
              "        -0.51544803,  0.4357191 ,  0.5032216 ,  0.4962629 ,  0.46046048,\n",
              "        -0.48884267,  0.46846816, -0.53484374, -0.44845274, -0.4687812 ,\n",
              "        -0.51186603,  0.451976  , -0.48086527,  0.45909387, -0.4414144 ,\n",
              "         0.48355395, -0.49492735, -0.42551553, -0.50258315,  0.49235958,\n",
              "         0.45504862,  0.43564567,  0.4770593 , -0.5108675 ,  0.49804884],\n",
              "       dtype=float32),\n",
              " 'poor': array([ 0.5200228 ,  0.4549031 , -0.49166593, -0.5043116 , -0.4618986 ,\n",
              "         0.46078968,  0.5293269 , -0.45135903, -0.4724025 , -0.5143176 ,\n",
              "        -0.44683328,  0.5284465 ,  0.5125537 ,  0.44605765, -0.48151904,\n",
              "        -0.46779567,  0.49423575,  0.53115046, -0.5100424 , -0.43945858,\n",
              "        -0.4476662 ,  0.50300384,  0.5104428 ,  0.4824504 , -0.49793276,\n",
              "        -0.51823086,  0.45333275,  0.48719668,  0.4776256 ,  0.46984002,\n",
              "        -0.48370245,  0.50642323, -0.47237465, -0.44795635, -0.49667075,\n",
              "        -0.46968335,  0.52828515, -0.49603778,  0.46950597, -0.48514587,\n",
              "         0.46150577, -0.4770051 , -0.47759748, -0.50205064,  0.44245845,\n",
              "         0.50733846,  0.50486106,  0.4936121 , -0.48754084,  0.5073367 ],\n",
              "       dtype=float32),\n",
              " 'approximately': array([ 0.4560396 ,  0.49859333, -0.43521875, -0.50940806, -0.48370022,\n",
              "         0.44644472,  0.48652586, -0.45193163, -0.44395292, -0.4628005 ,\n",
              "        -0.45763832,  0.4792174 ,  0.50502324,  0.5106966 , -0.482641  ,\n",
              "        -0.5010434 ,  0.42635745,  0.4751842 , -0.47457668, -0.49971747,\n",
              "        -0.50322694,  0.43705744,  0.47987932,  0.48900107, -0.43452778,\n",
              "        -0.5057775 ,  0.48531678,  0.5285082 ,  0.4472505 ,  0.4449424 ,\n",
              "        -0.49965554,  0.4338508 , -0.50928855, -0.5049549 , -0.44293204,\n",
              "        -0.4949695 ,  0.5072063 , -0.5013611 ,  0.48798352, -0.4255766 ,\n",
              "         0.5006778 , -0.49157444, -0.4404548 , -0.5087814 ,  0.42718193,\n",
              "         0.45562148,  0.45150024,  0.4507453 , -0.5035332 ,  0.4751792 ],\n",
              "       dtype=float32),\n",
              " 'junk': array([ 0.5261225 ,  0.48105374, -0.4873774 , -0.4559547 , -0.5270617 ,\n",
              "         0.48525277,  0.4688207 , -0.4463605 , -0.45070198, -0.5024365 ,\n",
              "        -0.4575704 ,  0.51249343,  0.5005831 ,  0.44016838, -0.5622354 ,\n",
              "        -0.44732326,  0.47152954,  0.5091647 , -0.5194573 , -0.481453  ,\n",
              "        -0.46682307,  0.46141535,  0.49011037,  0.45181176, -0.48639485,\n",
              "        -0.44982895,  0.43816915,  0.54587483,  0.44273654,  0.4565269 ,\n",
              "        -0.5078384 ,  0.47008362, -0.5047138 , -0.45533946, -0.4767619 ,\n",
              "        -0.4607321 ,  0.45366356, -0.44538948,  0.44483942, -0.46199447,\n",
              "         0.4816465 , -0.5111686 , -0.5160329 , -0.457614  ,  0.5240642 ,\n",
              "         0.4574572 ,  0.47168988,  0.5009007 , -0.52208227,  0.45913172],\n",
              "       dtype=float32),\n",
              " 'variety': array([ 0.53791857,  0.44654658, -0.5124779 , -0.49433425, -0.52162516,\n",
              "         0.45396286,  0.45379126, -0.51888347, -0.44824073, -0.4993219 ,\n",
              "        -0.45074865,  0.4480978 ,  0.44044185,  0.4592083 , -0.49865413,\n",
              "        -0.51543534,  0.5084939 ,  0.49976563, -0.49881652, -0.47097868,\n",
              "        -0.43854994,  0.5147663 ,  0.44249195,  0.44232517, -0.46884972,\n",
              "        -0.5052259 ,  0.5133187 ,  0.5444831 ,  0.5198984 ,  0.48451608,\n",
              "        -0.5108234 ,  0.517104  , -0.4766548 , -0.50799966, -0.48771125,\n",
              "        -0.4965179 ,  0.51140094, -0.4740178 ,  0.45504668, -0.52879494,\n",
              "         0.53465813, -0.47592545, -0.4935518 , -0.48419225,  0.49051994,\n",
              "         0.50607926,  0.49932066,  0.4465297 , -0.4413729 ,  0.5160122 ],\n",
              "       dtype=float32),\n",
              " 'california': array([ 0.48362732,  0.51873076, -0.4956368 , -0.46452597, -0.5344598 ,\n",
              "         0.51852417,  0.5352976 , -0.4586201 , -0.47114444, -0.46408033,\n",
              "        -0.52732456,  0.5267541 ,  0.45897198,  0.5168129 , -0.50697047,\n",
              "        -0.47086295,  0.4615371 ,  0.50973356, -0.5276581 , -0.48843095,\n",
              "        -0.47742105,  0.4865589 ,  0.5089457 ,  0.46693018, -0.50237316,\n",
              "        -0.46004203,  0.46358812,  0.5333776 ,  0.47994825,  0.4449183 ,\n",
              "        -0.52927595,  0.46364203, -0.50796527, -0.5412391 , -0.45242745,\n",
              "        -0.48637757,  0.44980952, -0.46401194,  0.4598646 , -0.48738003,\n",
              "         0.53640306, -0.4769832 , -0.520012  , -0.48005545,  0.5010339 ,\n",
              "         0.47705057,  0.48533583,  0.46244037, -0.48365986,  0.48451194],\n",
              "       dtype=float32),\n",
              " 'actively': array([ 0.53830713,  0.42930982, -0.5177722 , -0.4821698 , -0.4558615 ,\n",
              "         0.51796544,  0.4894548 , -0.52160805, -0.48253766, -0.5081471 ,\n",
              "        -0.44465658,  0.460409  ,  0.5042846 ,  0.47627172, -0.5542222 ,\n",
              "        -0.49084806,  0.4720731 ,  0.47149324, -0.47346207, -0.45999402,\n",
              "        -0.45205554,  0.48668045,  0.47687197,  0.44552633, -0.47730872,\n",
              "        -0.4411861 ,  0.44237077,  0.5100752 ,  0.48507693,  0.43921357,\n",
              "        -0.45356533,  0.4800964 , -0.51961845, -0.49281353, -0.5067582 ,\n",
              "        -0.44794163,  0.4709095 , -0.467791  ,  0.48013818, -0.46836686,\n",
              "         0.5095381 , -0.494248  , -0.43770128, -0.5204505 ,  0.52201915,\n",
              "         0.52488893,  0.5067557 ,  0.49354824, -0.5199597 ,  0.48476997],\n",
              "       dtype=float32),\n",
              " 'withholding': array([ 0.5082679 ,  0.514176  , -0.5038215 , -0.43361172, -0.47924495,\n",
              "         0.45948204,  0.4514624 , -0.44376412, -0.49778932, -0.50916445,\n",
              "        -0.46298194,  0.45151573,  0.43708962,  0.46314895, -0.54563373,\n",
              "        -0.5269608 ,  0.46265286,  0.51458555, -0.4808476 , -0.5072824 ,\n",
              "        -0.4901827 ,  0.49120936,  0.5137112 ,  0.4716946 , -0.5050524 ,\n",
              "        -0.5141511 ,  0.43610376,  0.53184915,  0.4732409 ,  0.51748604,\n",
              "        -0.44699827,  0.52198243, -0.5173862 , -0.47908875, -0.49943745,\n",
              "        -0.50766367,  0.5084671 , -0.5047053 ,  0.52215064, -0.52294517,\n",
              "         0.48713142, -0.5152907 , -0.44356275, -0.44818556,  0.48869565,\n",
              "         0.4848677 ,  0.4371349 ,  0.50211537, -0.48006725,  0.51569146],\n",
              "       dtype=float32),\n",
              " 'best': array([ 0.4911149 ,  0.43204528, -0.44699743, -0.45069414, -0.5275786 ,\n",
              "         0.4971469 ,  0.50952554, -0.5164334 , -0.43789452, -0.42675507,\n",
              "        -0.43548116,  0.47859356,  0.4740641 ,  0.4933155 , -0.5504429 ,\n",
              "        -0.44214645,  0.4367195 ,  0.48501036, -0.43054444, -0.44308546,\n",
              "        -0.46525502,  0.45050594,  0.44315478,  0.45841187, -0.43439212,\n",
              "        -0.50532603,  0.43444517,  0.51470333,  0.5149152 ,  0.5020251 ,\n",
              "        -0.51000214,  0.4465032 , -0.5060535 , -0.5090518 , -0.43095323,\n",
              "        -0.5196984 ,  0.5169885 , -0.4521927 ,  0.4688485 , -0.43069762,\n",
              "         0.51928943, -0.47755274, -0.45066938, -0.46164536,  0.49777362,\n",
              "         0.48335844,  0.48524916,  0.48073044, -0.50339216,  0.4972148 ],\n",
              "       dtype=float32),\n",
              " 'products': array([ 0.54373026,  0.46966362, -0.5178027 , -0.47921318, -0.46200553,\n",
              "         0.4896442 ,  0.49328843, -0.48025125, -0.50259995, -0.47499523,\n",
              "        -0.49050912,  0.47855392,  0.49788424,  0.5240113 , -0.5729675 ,\n",
              "        -0.46046707,  0.48298988,  0.5362558 , -0.51073885, -0.4296603 ,\n",
              "        -0.5142664 ,  0.46890566,  0.47230864,  0.44274965, -0.43282387,\n",
              "        -0.4476731 ,  0.43404588,  0.5110211 ,  0.4490834 ,  0.47086877,\n",
              "        -0.5094622 ,  0.45534396, -0.5036771 , -0.5228353 , -0.5179489 ,\n",
              "        -0.5180517 ,  0.50271523, -0.5139178 ,  0.43765637, -0.46466202,\n",
              "         0.51089007, -0.51197684, -0.44721255, -0.48180985,  0.46116388,\n",
              "         0.48813438,  0.47382528,  0.49260956, -0.4407254 ,  0.5331785 ],\n",
              "       dtype=float32),\n",
              " 'mortgage-backed': array([ 0.49192947,  0.5141865 , -0.46948078, -0.51582503, -0.44491887,\n",
              "         0.46117458,  0.4674958 , -0.48004374, -0.54330957, -0.4391496 ,\n",
              "        -0.45474598,  0.47728515,  0.45884427,  0.49452114, -0.54161066,\n",
              "        -0.5244154 ,  0.48389012,  0.55683047, -0.5100872 , -0.4839017 ,\n",
              "        -0.5098944 ,  0.5040407 ,  0.46493396,  0.5341838 , -0.47571796,\n",
              "        -0.4786855 ,  0.46658215,  0.53482276,  0.5373488 ,  0.51393735,\n",
              "        -0.53595906,  0.4834687 , -0.5249048 , -0.52308065, -0.5215102 ,\n",
              "        -0.46213308,  0.503098  , -0.46769497,  0.45265538, -0.5449825 ,\n",
              "         0.52926457, -0.51293397, -0.4709218 , -0.45685652,  0.5388085 ,\n",
              "         0.5264244 ,  0.46192878,  0.47686297, -0.52067536,  0.49370116],\n",
              "       dtype=float32),\n",
              " 'depending': array([ 0.46875504,  0.45940053, -0.49191964, -0.51363385, -0.48995927,\n",
              "         0.44266576,  0.46356773, -0.4686818 , -0.4755379 , -0.4313989 ,\n",
              "        -0.47651595,  0.49875674,  0.5165096 ,  0.4813314 , -0.47861615,\n",
              "        -0.46565792,  0.5284732 ,  0.46171048, -0.44616812, -0.42865008,\n",
              "        -0.52204466,  0.4462477 ,  0.5265181 ,  0.44140685, -0.51553637,\n",
              "        -0.48575178,  0.47935015,  0.510352  ,  0.51402795,  0.4709853 ,\n",
              "        -0.47048718,  0.50282574, -0.46987325, -0.48028663, -0.4725901 ,\n",
              "        -0.5160688 ,  0.44470733, -0.4674689 ,  0.4783038 , -0.43703797,\n",
              "         0.46099094, -0.50859976, -0.42983788, -0.47130477,  0.4523473 ,\n",
              "         0.48621276,  0.5138066 ,  0.5042352 , -0.46839282,  0.49766225],\n",
              "       dtype=float32),\n",
              " 'e.g.': array([ 0.52166176,  0.44719622, -0.46533105, -0.4686274 , -0.4591289 ,\n",
              "         0.45978016,  0.5289491 , -0.46760187, -0.5064745 , -0.46744344,\n",
              "        -0.5041136 ,  0.45364395,  0.50565577,  0.48587933, -0.4863048 ,\n",
              "        -0.51194453,  0.5138954 ,  0.473036  , -0.51853645, -0.47003648,\n",
              "        -0.49720192,  0.51450753,  0.458794  ,  0.46028417, -0.44005024,\n",
              "        -0.5164084 ,  0.47285214,  0.47174102,  0.4860364 ,  0.47167316,\n",
              "        -0.50746953,  0.45677102, -0.46413693, -0.46860373, -0.52509636,\n",
              "        -0.51199234,  0.454586  , -0.4523386 ,  0.5190514 , -0.5073078 ,\n",
              "         0.4687945 , -0.4741726 , -0.46516407, -0.48587275,  0.4815446 ,\n",
              "         0.50088763,  0.51321745,  0.47378245, -0.49163413,  0.49792898],\n",
              "       dtype=float32),\n",
              " 'reductions': array([ 0.4423294 ,  0.47608396, -0.5352539 , -0.44196644, -0.508304  ,\n",
              "         0.5212899 ,  0.4574894 , -0.46266818, -0.4864007 , -0.43964988,\n",
              "        -0.4961542 ,  0.512865  ,  0.4387589 ,  0.53223586, -0.5411895 ,\n",
              "        -0.48224247,  0.49704736,  0.44292933, -0.51058775, -0.4391318 ,\n",
              "        -0.4590188 ,  0.49723572,  0.46024406,  0.48096868, -0.520525  ,\n",
              "        -0.4754316 ,  0.5195767 ,  0.48966327,  0.46430135,  0.4787107 ,\n",
              "        -0.47929698,  0.47053713, -0.5070418 , -0.48188058, -0.5091614 ,\n",
              "        -0.5011415 ,  0.45713016, -0.4448323 ,  0.45357844, -0.44712046,\n",
              "         0.44892603, -0.51218724, -0.47546357, -0.46044153,  0.45300606,\n",
              "         0.53411067,  0.49244007,  0.4410009 , -0.5097492 ,  0.4376622 ],\n",
              "       dtype=float32),\n",
              " 'part': array([ 0.5356427 ,  0.47818744, -0.49269554, -0.5066487 , -0.45462865,\n",
              "         0.49854085,  0.48306626, -0.4816417 , -0.4531545 , -0.44317538,\n",
              "        -0.4965142 ,  0.46099675,  0.44962186,  0.5124539 , -0.5041301 ,\n",
              "        -0.50301677,  0.45605206,  0.49780115, -0.47166044, -0.43940318,\n",
              "        -0.44778478,  0.42873296,  0.48991197,  0.44196945, -0.44515955,\n",
              "        -0.5008421 ,  0.4613345 ,  0.49032772,  0.4393322 ,  0.47676837,\n",
              "        -0.5176587 ,  0.44595477, -0.51133007, -0.44371217, -0.42616427,\n",
              "        -0.44789973,  0.45258945, -0.51092803,  0.4977941 , -0.48080778,\n",
              "         0.47217083, -0.47757643, -0.43975106, -0.46946913,  0.4691815 ,\n",
              "         0.52082896,  0.5175544 ,  0.49842608, -0.48191455,  0.5009947 ],\n",
              "       dtype=float32),\n",
              " 'aggregate': array([ 0.477049  ,  0.4724463 , -0.44514292, -0.46862176, -0.5215708 ,\n",
              "         0.5187548 ,  0.4911713 , -0.49821717, -0.47612178, -0.48515445,\n",
              "        -0.48476928,  0.4518094 ,  0.4502703 ,  0.46727586, -0.56352735,\n",
              "        -0.499131  ,  0.49427116,  0.48091343, -0.4337326 , -0.48333326,\n",
              "        -0.47054395,  0.42477915,  0.46504685,  0.49288595, -0.49789932,\n",
              "        -0.44138756,  0.42689514,  0.49200603,  0.49361292,  0.5089105 ,\n",
              "        -0.5064131 ,  0.43526962, -0.5271735 , -0.46065265, -0.45462647,\n",
              "        -0.4538747 ,  0.5209412 , -0.45090872,  0.48180795, -0.51241165,\n",
              "         0.50218654, -0.49188074, -0.43700972, -0.51522887,  0.4511236 ,\n",
              "         0.4719753 ,  0.4456789 ,  0.44291207, -0.4451545 ,  0.49372187],\n",
              "       dtype=float32),\n",
              " 'substantially': array([ 0.5146906 ,  0.50775266, -0.46121198, -0.4750513 , -0.4801436 ,\n",
              "         0.51769406,  0.52356595, -0.4766046 , -0.48738027, -0.47174537,\n",
              "        -0.4706193 ,  0.45641866,  0.52439475,  0.5213574 , -0.5528213 ,\n",
              "        -0.4674738 ,  0.4750501 ,  0.46549678, -0.47608435, -0.4652707 ,\n",
              "        -0.5182077 ,  0.5044941 ,  0.48874658,  0.4684797 , -0.48591754,\n",
              "        -0.49651653,  0.43862402,  0.46368757,  0.45346317,  0.5100352 ,\n",
              "        -0.44799834,  0.5227265 , -0.54657304, -0.4879489 , -0.49692562,\n",
              "        -0.51977706,  0.46525902, -0.4836415 ,  0.51595384, -0.4755325 ,\n",
              "         0.4771208 , -0.5128576 , -0.4640228 , -0.46801287,  0.50340486,\n",
              "         0.45198548,  0.48502773,  0.5204455 , -0.52628285,  0.44480032],\n",
              "       dtype=float32),\n",
              " 'prepayment': array([ 0.45129898,  0.47393975, -0.48629063, -0.44915822, -0.4767066 ,\n",
              "         0.5100083 ,  0.45790923, -0.5107086 , -0.47079644, -0.46398228,\n",
              "        -0.43414587,  0.49685538,  0.47096956,  0.506894  , -0.5259022 ,\n",
              "        -0.47776076,  0.4715105 ,  0.50992835, -0.4687579 , -0.4423231 ,\n",
              "        -0.49163795,  0.4553573 ,  0.43987978,  0.44530702, -0.46531165,\n",
              "        -0.4637158 ,  0.5126656 ,  0.45173925,  0.50026464,  0.488298  ,\n",
              "        -0.47739187,  0.49040723, -0.5210533 , -0.49843857, -0.47462702,\n",
              "        -0.46641958,  0.48233092, -0.4618973 ,  0.43010533, -0.5043308 ,\n",
              "         0.5210165 , -0.4536337 , -0.46781725, -0.43909672,  0.46781498,\n",
              "         0.47824606,  0.44154936,  0.4744139 , -0.5146142 ,  0.4743848 ],\n",
              "       dtype=float32),\n",
              " 'subadviser': array([ 0.52375215,  0.50104016, -0.50403416, -0.4906125 , -0.47917125,\n",
              "         0.4985042 ,  0.46364546, -0.46149945, -0.5182886 , -0.48915985,\n",
              "        -0.5225369 ,  0.48678923,  0.524617  ,  0.5350145 , -0.52159655,\n",
              "        -0.48795733,  0.45312357,  0.4884998 , -0.51007324, -0.47337693,\n",
              "        -0.5131544 ,  0.46390426,  0.45767704,  0.5211092 , -0.5186795 ,\n",
              "        -0.47249073,  0.48860073,  0.4744475 ,  0.4661377 ,  0.44706076,\n",
              "        -0.48707968,  0.4869673 , -0.5026815 , -0.4610486 , -0.51677793,\n",
              "        -0.46514907,  0.54990727, -0.51036745,  0.5319892 , -0.5315354 ,\n",
              "         0.46266258, -0.5336526 , -0.47336772, -0.4499335 ,  0.50011075,\n",
              "         0.5105407 ,  0.4565859 ,  0.48239827, -0.4667644 ,  0.44901818],\n",
              "       dtype=float32),\n",
              " 'underperform': array([ 0.48234946,  0.43709832, -0.52364004, -0.48637334, -0.54082084,\n",
              "         0.48080474,  0.48331162, -0.45959038, -0.5076796 , -0.49410227,\n",
              "        -0.51547265,  0.45437264,  0.4666619 ,  0.4592774 , -0.50978   ,\n",
              "        -0.4559567 ,  0.4690333 ,  0.4595651 , -0.45925757, -0.4694539 ,\n",
              "        -0.44363698,  0.521915  ,  0.48640943,  0.4610925 , -0.4655831 ,\n",
              "        -0.48342744,  0.5162159 ,  0.47236317,  0.5258924 ,  0.4813576 ,\n",
              "        -0.48503533,  0.45981383, -0.5070793 , -0.52593195, -0.44147193,\n",
              "        -0.52217454,  0.5093308 , -0.51195943,  0.5202474 , -0.4886891 ,\n",
              "         0.5337097 , -0.5261495 , -0.5025395 , -0.5328447 ,  0.46579015,\n",
              "         0.49122036,  0.47117144,  0.5118033 , -0.53544986,  0.5286324 ],\n",
              "       dtype=float32),\n",
              " 'calling': array([ 0.487054  ,  0.432598  , -0.4640728 , -0.48976022, -0.507787  ,\n",
              "         0.48734003,  0.43390456, -0.47865757, -0.50869155, -0.46455425,\n",
              "        -0.46524394,  0.5149326 ,  0.51636964,  0.48268372, -0.5504351 ,\n",
              "        -0.4748456 ,  0.44354293,  0.4564895 , -0.45418912, -0.45814216,\n",
              "        -0.4324821 ,  0.44647983,  0.4721848 ,  0.5003502 , -0.47218588,\n",
              "        -0.5104945 ,  0.43523747,  0.53622353,  0.4996177 ,  0.44977742,\n",
              "        -0.49014592,  0.5149507 , -0.47763503, -0.4890635 , -0.44685394,\n",
              "        -0.50012994,  0.49537978, -0.4589924 ,  0.47432202, -0.44016534,\n",
              "         0.51238734, -0.4530242 , -0.4773792 , -0.4970749 ,  0.4429419 ,\n",
              "         0.4814329 ,  0.504408  ,  0.43083313, -0.4601829 ,  0.49065423],\n",
              "       dtype=float32),\n",
              " 'telephone': array([ 0.5054122 ,  0.4832614 , -0.45944068, -0.46561742, -0.4412605 ,\n",
              "         0.4927544 ,  0.4813527 , -0.49053666, -0.47539476, -0.4640967 ,\n",
              "        -0.46874756,  0.438407  ,  0.4756014 ,  0.4727495 , -0.46953717,\n",
              "        -0.5101907 ,  0.44066846,  0.42901316, -0.45051637, -0.48050335,\n",
              "        -0.46111387,  0.43797803,  0.5097755 ,  0.44005203, -0.4457035 ,\n",
              "        -0.5045529 ,  0.430418  ,  0.5286083 ,  0.47360703,  0.47703868,\n",
              "        -0.48278284,  0.43234518, -0.48164707, -0.4676725 , -0.43241087,\n",
              "        -0.5098041 ,  0.5212257 , -0.49839664,  0.5132669 , -0.49182996,\n",
              "         0.460649  , -0.43437594, -0.46727398, -0.4811653 ,  0.45757252,\n",
              "         0.45088497,  0.4593495 ,  0.43378857, -0.49263144,  0.46198827],\n",
              "       dtype=float32),\n",
              " 'distributors': array([ 0.47745603,  0.4952699 , -0.48956174, -0.51279354, -0.4598396 ,\n",
              "         0.44690764,  0.44987842, -0.45859337, -0.48137727, -0.49664828,\n",
              "        -0.5226717 ,  0.5052115 ,  0.4557808 ,  0.45368093, -0.52665955,\n",
              "        -0.44859427,  0.45193323,  0.4955017 , -0.47444454, -0.457985  ,\n",
              "        -0.47418046,  0.49319986,  0.46809062,  0.45108438, -0.51934886,\n",
              "        -0.5043945 ,  0.46006945,  0.4803395 ,  0.5102898 ,  0.51342535,\n",
              "        -0.47996515,  0.50330687, -0.5404356 , -0.4534658 , -0.4740771 ,\n",
              "        -0.52617663,  0.48397258, -0.5044384 ,  0.46784616, -0.5105981 ,\n",
              "         0.49119318, -0.4558422 , -0.459856  , -0.45380792,  0.48615998,\n",
              "         0.45683613,  0.46208206,  0.5014969 , -0.5107317 ,  0.4880089 ],\n",
              "       dtype=float32),\n",
              " 'even': array([ 0.5202164 ,  0.47251326, -0.5022651 , -0.5026172 , -0.45880684,\n",
              "         0.52020836,  0.48773754, -0.45141017, -0.42746082, -0.47018808,\n",
              "        -0.4668587 ,  0.4423127 ,  0.49961126,  0.49268213, -0.50862646,\n",
              "        -0.44617152,  0.4337422 ,  0.47674724, -0.4911316 , -0.42564085,\n",
              "        -0.4354106 ,  0.4972191 ,  0.4604462 ,  0.49846298, -0.49093485,\n",
              "        -0.4530282 ,  0.5109236 ,  0.5015845 ,  0.5060964 ,  0.48481375,\n",
              "        -0.46893463,  0.5001156 , -0.4887315 , -0.52380145, -0.5019653 ,\n",
              "        -0.46502253,  0.47468165, -0.4466072 ,  0.49043417, -0.44670725,\n",
              "         0.47616938, -0.50038254, -0.45632887, -0.4367361 ,  0.44405192,\n",
              "         0.4770468 ,  0.43820757,  0.48041433, -0.4610325 ,  0.47903833],\n",
              "       dtype=float32),\n",
              " 'waive': array([ 0.45666134,  0.4362137 , -0.46514696, -0.4743959 , -0.512479  ,\n",
              "         0.47702688,  0.48085478, -0.511703  , -0.5060672 , -0.44272178,\n",
              "        -0.49618113,  0.43782994,  0.4656793 ,  0.450077  , -0.55328137,\n",
              "        -0.47474393,  0.4421404 ,  0.45634642, -0.42244962, -0.45561022,\n",
              "        -0.5037507 ,  0.43927106,  0.4792062 ,  0.47011083, -0.45355386,\n",
              "        -0.4793486 ,  0.47524968,  0.49633533,  0.5017769 ,  0.4868349 ,\n",
              "        -0.50023246,  0.4412978 , -0.4686742 , -0.48999536, -0.46830022,\n",
              "        -0.46336365,  0.50707626, -0.5051323 ,  0.45930663, -0.4327501 ,\n",
              "         0.47518507, -0.49080655, -0.4226579 , -0.4459541 ,  0.5001928 ,\n",
              "         0.51619047,  0.50316286,  0.4455045 , -0.43940568,  0.49443072],\n",
              "       dtype=float32),\n",
              " 'compares': array([ 0.48798397,  0.44553053, -0.4720672 , -0.5294746 , -0.47169602,\n",
              "         0.5212846 ,  0.50467235, -0.5375313 , -0.5071127 , -0.46289223,\n",
              "        -0.539302  ,  0.52400064,  0.46199197,  0.5037856 , -0.5180843 ,\n",
              "        -0.4804272 ,  0.4761684 ,  0.45607752, -0.50361943, -0.4449577 ,\n",
              "        -0.4779629 ,  0.49680394,  0.5109122 ,  0.52027875, -0.4725841 ,\n",
              "        -0.51565874,  0.52836335,  0.55000544,  0.4771812 ,  0.45797688,\n",
              "        -0.52675676,  0.520428  , -0.5176294 , -0.4569119 , -0.52524245,\n",
              "        -0.5373098 ,  0.48053408, -0.5369322 ,  0.49283826, -0.537707  ,\n",
              "         0.48665252, -0.4774683 , -0.46013346, -0.49126908,  0.48457316,\n",
              "         0.48661265,  0.5260796 ,  0.45653877, -0.453326  ,  0.4807396 ],\n",
              "       dtype=float32),\n",
              " 'regarding': array([ 0.47810715,  0.42741793, -0.47207972, -0.43383312, -0.48112088,\n",
              "         0.44621745,  0.49987292, -0.44583142, -0.45281738, -0.45201573,\n",
              "        -0.47471243,  0.507411  ,  0.48028523,  0.48447552, -0.49637344,\n",
              "        -0.50684977,  0.45645106,  0.50002086, -0.46081007, -0.48170036,\n",
              "        -0.52150345,  0.49702203,  0.47871917,  0.4790029 , -0.50018525,\n",
              "        -0.49655578,  0.50918734,  0.50457484,  0.4915325 ,  0.5181651 ,\n",
              "        -0.48017794,  0.4954499 , -0.46508136, -0.4708774 , -0.5014639 ,\n",
              "        -0.4508754 ,  0.46252495, -0.4480806 ,  0.49785227, -0.5030284 ,\n",
              "         0.47502336, -0.4698269 , -0.4530935 , -0.5239882 ,  0.48833114,\n",
              "         0.49745846,  0.48315713,  0.47991455, -0.47087425,  0.43504536],\n",
              "       dtype=float32),\n",
              " 'settlement': array([ 0.5135337 ,  0.45317793, -0.5127638 , -0.45359868, -0.51460236,\n",
              "         0.45856842,  0.47050968, -0.45711625, -0.47867528, -0.4997195 ,\n",
              "        -0.45065126,  0.45705935,  0.44462878,  0.45823568, -0.55022204,\n",
              "        -0.45948806,  0.45003662,  0.4704135 , -0.48983523, -0.4868492 ,\n",
              "        -0.4778723 ,  0.46844497,  0.48402885,  0.5204811 , -0.52663   ,\n",
              "        -0.49385902,  0.49183744,  0.48062682,  0.4415936 ,  0.47599518,\n",
              "        -0.5029991 ,  0.4758252 , -0.5427444 , -0.507301  , -0.47754356,\n",
              "        -0.50746477,  0.54402965, -0.5210434 ,  0.44003037, -0.51785445,\n",
              "         0.52617025, -0.49215722, -0.4722854 , -0.4425221 ,  0.47310677,\n",
              "         0.525126  ,  0.44719335,  0.48245487, -0.4711931 ,  0.49172702],\n",
              "       dtype=float32),\n",
              " 'range': array([ 0.53429556,  0.44608   , -0.4475378 , -0.4811178 , -0.53054464,\n",
              "         0.45860526,  0.44433674, -0.49763966, -0.44901398, -0.519014  ,\n",
              "        -0.44155976,  0.48774824,  0.48214695,  0.527437  , -0.53426087,\n",
              "        -0.52433324,  0.51203686,  0.498298  , -0.4592855 , -0.44039163,\n",
              "        -0.51355547,  0.44833246,  0.53404284,  0.47339445, -0.44325635,\n",
              "        -0.44828305,  0.51164514,  0.54480076,  0.47197345,  0.47368118,\n",
              "        -0.4864216 ,  0.47588316, -0.50140494, -0.44554898, -0.44410068,\n",
              "        -0.5205852 ,  0.46620813, -0.4488016 ,  0.51679087, -0.4587407 ,\n",
              "         0.50965095, -0.49395102, -0.47214085, -0.48980153,  0.45064455,\n",
              "         0.5185472 ,  0.47544804,  0.44113237, -0.505376  ,  0.45316198],\n",
              "       dtype=float32),\n",
              " 'annuity': array([ 0.46101692,  0.476528  , -0.45607185, -0.44600138, -0.43950126,\n",
              "         0.45138007,  0.49179435, -0.467372  , -0.47694963, -0.45004305,\n",
              "        -0.48975998,  0.5058821 ,  0.45506445,  0.46325192, -0.5268684 ,\n",
              "        -0.46916085,  0.438188  ,  0.4937823 , -0.44189152, -0.4490517 ,\n",
              "        -0.4507816 ,  0.48073485,  0.44786087,  0.4410135 , -0.47648853,\n",
              "        -0.4729457 ,  0.48029515,  0.49607438,  0.50479454,  0.48292676,\n",
              "        -0.46871224,  0.4278837 , -0.4883981 , -0.4699566 , -0.4866499 ,\n",
              "        -0.43290633,  0.5064365 , -0.5141873 ,  0.5042322 , -0.5013388 ,\n",
              "         0.45960325, -0.51115173, -0.41726536, -0.48767385,  0.50272536,\n",
              "         0.52278596,  0.49482095,  0.43082732, -0.4372091 ,  0.43849853],\n",
              "       dtype=float32),\n",
              " 'whose': array([ 0.4867059 ,  0.430326  , -0.4392836 , -0.47359353, -0.46180308,\n",
              "         0.46997938,  0.48957154, -0.4781048 , -0.4779996 , -0.5109495 ,\n",
              "        -0.46579027,  0.5107431 ,  0.43858844,  0.51587385, -0.53125936,\n",
              "        -0.50626063,  0.47727633,  0.49021593, -0.50986534, -0.4593546 ,\n",
              "        -0.48443645,  0.4747949 ,  0.46109435,  0.4468432 , -0.437622  ,\n",
              "        -0.51688915,  0.50791234,  0.5157027 ,  0.45422217,  0.49713114,\n",
              "        -0.53169376,  0.5109595 , -0.5030084 , -0.48806962, -0.5066648 ,\n",
              "        -0.50680566,  0.45012355, -0.44378096,  0.51405466, -0.5022597 ,\n",
              "         0.45153692, -0.47879   , -0.4336052 , -0.44120616,  0.48892426,\n",
              "         0.46738568,  0.44921562,  0.51851827, -0.47591257,  0.51099646],\n",
              "       dtype=float32),\n",
              " 'director': array([ 0.58302605,  0.49684808, -0.45392442, -0.4804836 , -0.45507127,\n",
              "         0.48422116,  0.45601314, -0.47167262, -0.44446692, -0.5084334 ,\n",
              "        -0.44095695,  0.44496873,  0.44081214,  0.44692603, -0.54374135,\n",
              "        -0.4587438 ,  0.45570284,  0.47830218, -0.44851953, -0.49460018,\n",
              "        -0.50070155,  0.43469015,  0.5137529 ,  0.47885507, -0.51994264,\n",
              "        -0.44465762,  0.4930491 ,  0.57422185,  0.47102368,  0.4667924 ,\n",
              "        -0.47501066,  0.49735108, -0.56151575, -0.4595539 , -0.46883744,\n",
              "        -0.44909737,  0.47393647, -0.4612424 ,  0.50792474, -0.5168643 ,\n",
              "         0.44636625, -0.45923826, -0.4958332 , -0.522428  ,  0.5127331 ,\n",
              "         0.51140285,  0.5219286 ,  0.43767372, -0.511795  ,  0.47454396],\n",
              "       dtype=float32),\n",
              " 'techniques': array([ 0.51941574,  0.4518623 , -0.5016717 , -0.486302  , -0.5136881 ,\n",
              "         0.4989035 ,  0.4598026 , -0.43999276, -0.47350603, -0.49633384,\n",
              "        -0.48184624,  0.49209315,  0.46273682,  0.45637628, -0.496584  ,\n",
              "        -0.50090456,  0.47951376,  0.442661  , -0.5034255 , -0.458731  ,\n",
              "        -0.44813347,  0.4350082 ,  0.4838675 ,  0.44052696, -0.47518235,\n",
              "        -0.4617301 ,  0.47659415,  0.5268187 ,  0.5064508 ,  0.46433827,\n",
              "        -0.48025277,  0.4362141 , -0.49511734, -0.52340865, -0.47105765,\n",
              "        -0.4583301 ,  0.48997962, -0.44382054,  0.450398  , -0.4425497 ,\n",
              "         0.4901811 , -0.50784147, -0.431474  , -0.4454705 ,  0.49443159,\n",
              "         0.4793891 ,  0.47328284,  0.43559524, -0.5133468 ,  0.44615915],\n",
              "       dtype=float32),\n",
              " 'program': array([ 0.5047541 ,  0.4884994 , -0.49005225, -0.48379868, -0.4675017 ,\n",
              "         0.47980335,  0.45738348, -0.44648582, -0.44622475, -0.4549901 ,\n",
              "        -0.5152459 ,  0.44173002,  0.4736519 ,  0.50894886, -0.5226946 ,\n",
              "        -0.49649185,  0.4691597 ,  0.47074398, -0.5093272 , -0.49947745,\n",
              "        -0.4610373 ,  0.4844518 ,  0.48686665,  0.47046724, -0.5173843 ,\n",
              "        -0.44305435,  0.44276708,  0.54605633,  0.44062465,  0.4709836 ,\n",
              "        -0.49972364,  0.4546317 , -0.5073915 , -0.48301405, -0.4891784 ,\n",
              "        -0.464871  ,  0.45274654, -0.4943916 ,  0.4679635 , -0.53259075,\n",
              "         0.5169023 , -0.44728217, -0.4318156 , -0.47533265,  0.49059153,\n",
              "         0.5215633 ,  0.49134418,  0.4588412 , -0.452812  ,  0.5190107 ],\n",
              "       dtype=float32),\n",
              " 'option': array([ 0.50185734,  0.469408  , -0.45984662, -0.48226804, -0.45845076,\n",
              "         0.4551867 ,  0.44686127, -0.5110657 , -0.4492599 , -0.5095804 ,\n",
              "        -0.5163473 ,  0.4490915 ,  0.52788395,  0.5346252 , -0.5188163 ,\n",
              "        -0.48076963,  0.49572837,  0.4509413 , -0.5176883 , -0.476809  ,\n",
              "        -0.4915831 ,  0.44654608,  0.47183028,  0.52425104, -0.5240772 ,\n",
              "        -0.51876855,  0.46927133,  0.46955574,  0.49508324,  0.52047145,\n",
              "        -0.4597963 ,  0.5034839 , -0.5495717 , -0.5299161 , -0.51336104,\n",
              "        -0.45636135,  0.5480885 , -0.48849383,  0.44009545, -0.46160483,\n",
              "         0.47808582, -0.4762571 , -0.438327  , -0.48019165,  0.5048606 ,\n",
              "         0.4760598 ,  0.46877444,  0.506448  , -0.46231097,  0.487679  ],\n",
              "       dtype=float32),\n",
              " 'understand': array([ 0.53952754,  0.47705013, -0.51556754, -0.4436664 , -0.49201024,\n",
              "         0.53847575,  0.51357055, -0.44483674, -0.48970965, -0.49979898,\n",
              "        -0.44108412,  0.4723971 ,  0.50714046,  0.46496898, -0.54275167,\n",
              "        -0.46894965,  0.4722098 ,  0.47640803, -0.49106008, -0.44435608,\n",
              "        -0.44199032,  0.4603683 ,  0.48252746,  0.44217128, -0.5143174 ,\n",
              "        -0.44693166,  0.47303554,  0.49518597,  0.4371115 ,  0.44301197,\n",
              "        -0.46388412,  0.45941493, -0.48452902, -0.5095354 , -0.4658065 ,\n",
              "        -0.48058754,  0.46637866, -0.53021663,  0.47888574, -0.46490642,\n",
              "         0.51250774, -0.4856811 , -0.44112718, -0.45568046,  0.51920366,\n",
              "         0.48658141,  0.46841148,  0.5239609 , -0.44165492,  0.5128466 ],\n",
              "       dtype=float32),\n",
              " 'type': array([ 0.4749602 ,  0.4987788 , -0.48661408, -0.4469703 , -0.51731944,\n",
              "         0.4487667 ,  0.48066288, -0.43563247, -0.49806288, -0.44625437,\n",
              "        -0.5021376 ,  0.4720528 ,  0.45881552,  0.48390585, -0.48641977,\n",
              "        -0.44398502,  0.49725875,  0.48446313, -0.4667967 , -0.4168004 ,\n",
              "        -0.45263457,  0.5043686 ,  0.48171812,  0.4906872 , -0.44043073,\n",
              "        -0.45816192,  0.5023581 ,  0.452684  ,  0.4233893 ,  0.50297505,\n",
              "        -0.5245116 ,  0.4783695 , -0.44579098, -0.50728595, -0.49060175,\n",
              "        -0.4629423 ,  0.51074   , -0.5139003 ,  0.42318907, -0.44515938,\n",
              "         0.46492237, -0.4789282 , -0.49731764, -0.42183906,  0.47877467,\n",
              "         0.4901343 ,  0.50742495,  0.44316727, -0.4904701 ,  0.5118543 ],\n",
              "       dtype=float32),\n",
              " 'convertible': array([ 0.5013166 ,  0.50210845, -0.5202628 , -0.50681806, -0.540636  ,\n",
              "         0.4845633 ,  0.46161824, -0.513822  , -0.44723305, -0.46220744,\n",
              "        -0.4690048 ,  0.49220318,  0.45432463,  0.48072532, -0.5810796 ,\n",
              "        -0.47511607,  0.46815202,  0.5272593 , -0.46906593, -0.46811426,\n",
              "        -0.48733732,  0.44577694,  0.4568361 ,  0.5024311 , -0.511771  ,\n",
              "        -0.49422547,  0.47522095,  0.5602865 ,  0.48568696,  0.4501804 ,\n",
              "        -0.45828938,  0.51066685, -0.5368804 , -0.51588845, -0.46994203,\n",
              "        -0.5049025 ,  0.49386927, -0.4991071 ,  0.46049967, -0.535423  ,\n",
              "         0.47368568, -0.45820838, -0.47500747, -0.50367355,  0.47343716,\n",
              "         0.48824012,  0.48960036,  0.46080205, -0.4777643 ,  0.4754633 ],\n",
              "       dtype=float32),\n",
              " 'position': array([ 0.54261947,  0.45175886, -0.48315644, -0.5293921 , -0.45317534,\n",
              "         0.5018648 ,  0.4422959 , -0.48432386, -0.48204458, -0.45889395,\n",
              "        -0.46616176,  0.53091997,  0.4627793 ,  0.47983927, -0.50455093,\n",
              "        -0.45953906,  0.4933464 ,  0.44818008, -0.49151844, -0.51774347,\n",
              "        -0.45215833,  0.5142703 ,  0.50363404,  0.4396392 , -0.52324075,\n",
              "        -0.4440989 ,  0.49039087,  0.4825716 ,  0.5153848 ,  0.47005337,\n",
              "        -0.46315283,  0.4461156 , -0.48657802, -0.4777089 , -0.51756096,\n",
              "        -0.46288595,  0.464456  , -0.44917637,  0.5166109 , -0.47155368,\n",
              "         0.4712345 , -0.49603662, -0.51908094, -0.4791342 ,  0.44593713,\n",
              "         0.47273898,  0.5295004 ,  0.50707513, -0.5268244 ,  0.52226764],\n",
              "       dtype=float32),\n",
              " 'affiliated': array([ 0.4954142 ,  0.48009053, -0.47434548, -0.47412127, -0.44420448,\n",
              "         0.484962  ,  0.41755953, -0.49802738, -0.42985803, -0.45027912,\n",
              "        -0.47505748,  0.45754668,  0.4862449 ,  0.4379403 , -0.5251885 ,\n",
              "        -0.45828602,  0.42960966,  0.4921816 , -0.41623718, -0.4561973 ,\n",
              "        -0.4234305 ,  0.46044916,  0.49445787,  0.41284603, -0.47561488,\n",
              "        -0.43901467,  0.41017178,  0.48520407,  0.42951626,  0.42718375,\n",
              "        -0.41940472,  0.464895  , -0.44571674, -0.43591598, -0.45578384,\n",
              "        -0.44286197,  0.41783103, -0.4600479 ,  0.42653522, -0.43409073,\n",
              "         0.48067433, -0.4364311 , -0.418684  , -0.4423092 ,  0.41480199,\n",
              "         0.4508566 ,  0.43169454,  0.43498886, -0.45529097,  0.4855995 ],\n",
              "       dtype=float32),\n",
              " 'banks': array([ 0.52148795,  0.5106743 , -0.4608675 , -0.5179652 , -0.47062585,\n",
              "         0.4548553 ,  0.50926393, -0.5023402 , -0.46824592, -0.4451752 ,\n",
              "        -0.4883075 ,  0.4724691 ,  0.49895155,  0.4609645 , -0.51320696,\n",
              "        -0.47459754,  0.44201034,  0.51007175, -0.4324472 , -0.47334173,\n",
              "        -0.46518117,  0.5084231 ,  0.46953648,  0.48858437, -0.5052872 ,\n",
              "        -0.51509017,  0.4368469 ,  0.5227347 ,  0.50615144,  0.43684107,\n",
              "        -0.45003942,  0.47793627, -0.48738414, -0.51527643, -0.44984418,\n",
              "        -0.53130394,  0.47866684, -0.4787476 ,  0.47599208, -0.5198766 ,\n",
              "         0.51509064, -0.50216454, -0.5122324 , -0.49245942,  0.4530897 ,\n",
              "         0.44365013,  0.44149193,  0.43928593, -0.45458108,  0.43465856],\n",
              "       dtype=float32),\n",
              " 'discount': array([ 0.48190507,  0.4614036 , -0.471138  , -0.48450857, -0.53519255,\n",
              "         0.4891019 ,  0.44854167, -0.45017466, -0.5313293 , -0.46921906,\n",
              "        -0.46372354,  0.46834645,  0.52303725,  0.5153424 , -0.57697296,\n",
              "        -0.45211864,  0.5200531 ,  0.5021704 , -0.46306324, -0.4435029 ,\n",
              "        -0.48565838,  0.45069718,  0.45210925,  0.49782535, -0.47713658,\n",
              "        -0.44326317,  0.45627177,  0.4973476 ,  0.508247  ,  0.5203784 ,\n",
              "        -0.5022142 ,  0.51092255, -0.50845116, -0.4849092 , -0.49308124,\n",
              "        -0.47341415,  0.50812626, -0.46806657,  0.52988744, -0.49599987,\n",
              "         0.48462152, -0.47186777, -0.50301194, -0.45956686,  0.52070105,\n",
              "         0.48678184,  0.50191104,  0.4848913 , -0.5085856 ,  0.45227888],\n",
              "       dtype=float32),\n",
              " 'traditional': array([ 0.5076333 ,  0.43907562, -0.5167182 , -0.46534193, -0.52261347,\n",
              "         0.45698124,  0.5106561 , -0.509836  , -0.504751  , -0.43468282,\n",
              "        -0.4944219 ,  0.43694788,  0.5040678 ,  0.46208346, -0.51987827,\n",
              "        -0.44550896,  0.5144783 ,  0.49339595, -0.51192033, -0.42879087,\n",
              "        -0.43413526,  0.48451054,  0.49797395,  0.50361514, -0.5027924 ,\n",
              "        -0.5031396 ,  0.4870436 ,  0.45613834,  0.4831228 ,  0.46696374,\n",
              "        -0.43532294,  0.48558   , -0.449626  , -0.5224049 , -0.47028214,\n",
              "        -0.4951516 ,  0.48698297, -0.44013774,  0.4873351 , -0.45405933,\n",
              "         0.4772425 , -0.47609138, -0.44685632, -0.4422808 ,  0.50062656,\n",
              "         0.47064823,  0.45142886,  0.46185663, -0.4840342 ,  0.49252102],\n",
              "       dtype=float32),\n",
              " 'gain': array([ 0.50722253,  0.46278083, -0.47061116, -0.5254284 , -0.5101664 ,\n",
              "         0.50227964,  0.48594785, -0.48819488, -0.48531505, -0.45137832,\n",
              "        -0.51314163,  0.48319462,  0.45599735,  0.4562152 , -0.5523356 ,\n",
              "        -0.43999434,  0.45804718,  0.49129653, -0.4319271 , -0.4576348 ,\n",
              "        -0.48807684,  0.4662993 ,  0.46172047,  0.46280828, -0.44423062,\n",
              "        -0.4484917 ,  0.51812726,  0.5229775 ,  0.49623764,  0.4853276 ,\n",
              "        -0.5146677 ,  0.44617015, -0.45586938, -0.461453  , -0.473521  ,\n",
              "        -0.46740314,  0.5232094 , -0.48583835,  0.5157737 , -0.4842655 ,\n",
              "         0.508269  , -0.51921177, -0.48901528, -0.44954604,  0.50334436,\n",
              "         0.43805858,  0.44429633,  0.4809651 , -0.47304228,  0.5114243 ],\n",
              "       dtype=float32),\n",
              " 'inst': array([ 0.48933345,  0.48160437, -0.4585431 , -0.4990242 , -0.5193125 ,\n",
              "         0.4947176 ,  0.46416998, -0.48971367, -0.4493462 , -0.41745645,\n",
              "        -0.51493096,  0.51319605,  0.47517636,  0.48610508, -0.4967862 ,\n",
              "        -0.5082269 ,  0.5103762 ,  0.47614455, -0.4635583 , -0.4950764 ,\n",
              "        -0.4947075 ,  0.503901  ,  0.5056059 ,  0.47333694, -0.4747419 ,\n",
              "        -0.5068172 ,  0.5075833 ,  0.45112386,  0.46988854,  0.4847169 ,\n",
              "        -0.5092534 ,  0.45079777, -0.45029005, -0.48162434, -0.46984088,\n",
              "        -0.46065673,  0.5067171 , -0.46719933,  0.46597216, -0.48641038,\n",
              "         0.45006013, -0.46571034, -0.44493768, -0.50103253,  0.4516659 ,\n",
              "         0.47293037,  0.43701202,  0.47199348, -0.43739405,  0.48030674],\n",
              "       dtype=float32),\n",
              " 'economy': array([ 0.5477034 ,  0.4736121 , -0.46967363, -0.52238375, -0.45818695,\n",
              "         0.46901783,  0.45956904, -0.46187332, -0.49701008, -0.4705156 ,\n",
              "        -0.51729435,  0.46063852,  0.51645815,  0.4990839 , -0.52788115,\n",
              "        -0.49773428,  0.5093894 ,  0.4423418 , -0.5105521 , -0.48019373,\n",
              "        -0.47858104,  0.49666813,  0.53244865,  0.49200153, -0.42146212,\n",
              "        -0.4560454 ,  0.4945044 ,  0.5000651 ,  0.5008188 ,  0.47280824,\n",
              "        -0.46072036,  0.44078127, -0.4752176 , -0.48773497, -0.5134584 ,\n",
              "        -0.453449  ,  0.4719873 , -0.4872375 ,  0.5067657 , -0.46636552,\n",
              "         0.5012953 , -0.43901324, -0.45402274, -0.4926055 ,  0.4862128 ,\n",
              "         0.49387598,  0.45778608,  0.51014614, -0.5021311 ,  0.44402972],\n",
              "       dtype=float32),\n",
              " 'terminated': array([ 0.5103048 ,  0.43165132, -0.50621873, -0.42154396, -0.44632158,\n",
              "         0.43796274,  0.4326577 , -0.44088092, -0.46921834, -0.4418185 ,\n",
              "        -0.5079044 ,  0.4540671 ,  0.45276397,  0.43825674, -0.5310825 ,\n",
              "        -0.49057353,  0.42783797,  0.4998078 , -0.4282283 , -0.4944989 ,\n",
              "        -0.42249647,  0.49122062,  0.48255676,  0.48023447, -0.48662543,\n",
              "        -0.48667827,  0.50600046,  0.47482502,  0.4830707 ,  0.46409976,\n",
              "        -0.50383013,  0.41620603, -0.47068456, -0.50731134, -0.42545703,\n",
              "        -0.46951923,  0.4859728 , -0.48909354,  0.48116067, -0.42356062,\n",
              "         0.48719606, -0.4879296 , -0.47768903, -0.4629574 ,  0.45103234,\n",
              "         0.49592805,  0.45955706,  0.42833433, -0.45296043,  0.49000108],\n",
              "       dtype=float32),\n",
              " 'fundamental': array([ 0.5113204 ,  0.44933662, -0.47516254, -0.470287  , -0.5161917 ,\n",
              "         0.50861776,  0.49351218, -0.5017383 , -0.42486286, -0.48675945,\n",
              "        -0.42674986,  0.43165046,  0.50132865,  0.5173221 , -0.4963397 ,\n",
              "        -0.43516257,  0.503489  ,  0.49359357, -0.4980898 , -0.45128435,\n",
              "        -0.45763624,  0.4878799 ,  0.4344327 ,  0.42564127, -0.4668999 ,\n",
              "        -0.43893716,  0.50909805,  0.5327621 ,  0.45367292,  0.47277746,\n",
              "        -0.5017816 ,  0.46348813, -0.5063119 , -0.51490295, -0.43198603,\n",
              "        -0.49949673,  0.47063756, -0.4947256 ,  0.4426131 , -0.45186138,\n",
              "         0.43575364, -0.47561753, -0.43240893, -0.44048294,  0.4333301 ,\n",
              "         0.4922193 ,  0.507912  ,  0.42824608, -0.4637652 ,  0.43155795],\n",
              "       dtype=float32),\n",
              " 'rapidly': array([ 0.5074543 ,  0.4179635 , -0.50168633, -0.5045203 , -0.45658302,\n",
              "         0.4412314 ,  0.43343538, -0.5128114 , -0.4298319 , -0.45456532,\n",
              "        -0.4372035 ,  0.46801874,  0.45512718,  0.4859326 , -0.53425187,\n",
              "        -0.4596898 ,  0.47808686,  0.49697852, -0.4231031 , -0.43986538,\n",
              "        -0.5090155 ,  0.51313466,  0.46428528,  0.45912707, -0.4805703 ,\n",
              "        -0.5164676 ,  0.505649  ,  0.5116471 ,  0.43856066,  0.49568087,\n",
              "        -0.5031609 ,  0.44398245, -0.48333746, -0.5180791 , -0.46092892,\n",
              "        -0.4443543 ,  0.48698455, -0.5070474 ,  0.4500108 , -0.43544513,\n",
              "         0.49583903, -0.51156014, -0.50601935, -0.45957392,  0.4363688 ,\n",
              "         0.47219414,  0.44499245,  0.4867611 , -0.4309001 ,  0.4230964 ],\n",
              "       dtype=float32),\n",
              " 'creation': array([ 0.50541437,  0.4960103 , -0.49956182, -0.4587644 , -0.50133145,\n",
              "         0.48333877,  0.45693597, -0.45214352, -0.46935982, -0.47902393,\n",
              "        -0.48624974,  0.46318236,  0.5008072 ,  0.44647115, -0.5517581 ,\n",
              "        -0.48881492,  0.5292982 ,  0.5093355 , -0.4768937 , -0.5034636 ,\n",
              "        -0.4553506 ,  0.480802  ,  0.4772164 ,  0.4673787 , -0.4466068 ,\n",
              "        -0.49148   ,  0.50235224,  0.5449399 ,  0.5026384 ,  0.44511384,\n",
              "        -0.49874866,  0.47099906, -0.53186095, -0.46574798, -0.4737315 ,\n",
              "        -0.48307008,  0.4516954 , -0.49203604,  0.45606023, -0.4698506 ,\n",
              "         0.4494895 , -0.50993025, -0.46996167, -0.46859363,  0.46302032,\n",
              "         0.4572262 ,  0.45511723,  0.5126154 , -0.4816544 ,  0.50191045],\n",
              "       dtype=float32),\n",
              " 'buying': array([ 0.5230367 ,  0.48857868, -0.49684042, -0.4296162 , -0.49709496,\n",
              "         0.46144325,  0.4963999 , -0.4318181 , -0.5003182 , -0.4338579 ,\n",
              "        -0.42492896,  0.50084597,  0.49219134,  0.43356594, -0.46615854,\n",
              "        -0.48529395,  0.5009357 ,  0.4924703 , -0.48157373, -0.43772033,\n",
              "        -0.44469142,  0.5078182 ,  0.4630478 ,  0.5000212 , -0.48618394,\n",
              "        -0.43824622,  0.5093735 ,  0.49315053,  0.42991576,  0.4373146 ,\n",
              "        -0.4948798 ,  0.44687012, -0.49763617, -0.5156122 , -0.46087334,\n",
              "        -0.4900861 ,  0.5098074 , -0.4786023 ,  0.46675283, -0.4542277 ,\n",
              "         0.49135482, -0.49024272, -0.48868778, -0.44184273,  0.46332648,\n",
              "         0.45300567,  0.46419016,  0.4933173 , -0.51505923,  0.50312257],\n",
              "       dtype=float32),\n",
              " 'apply': array([ 0.5196784 ,  0.50294375, -0.47680926, -0.4737895 , -0.5165207 ,\n",
              "         0.47697875,  0.50088334, -0.47524664, -0.46921468, -0.43928307,\n",
              "        -0.48901707,  0.452846  ,  0.45872504,  0.43510625, -0.4754341 ,\n",
              "        -0.505857  ,  0.4835652 ,  0.43968523, -0.46173546, -0.48063785,\n",
              "        -0.47435543,  0.45140016,  0.4487243 ,  0.45987883, -0.45507812,\n",
              "        -0.50072324,  0.5113919 ,  0.5319371 ,  0.4993589 ,  0.51186264,\n",
              "        -0.506533  ,  0.47333616, -0.48119658, -0.5118747 , -0.47800317,\n",
              "        -0.4479804 ,  0.45967638, -0.46796983,  0.51433456, -0.49675047,\n",
              "         0.5150992 , -0.49221432, -0.46939933, -0.44251397,  0.502201  ,\n",
              "         0.51192355,  0.43048677,  0.4790727 , -0.47606754,  0.48348257],\n",
              "       dtype=float32),\n",
              " 'mortgage': array([ 0.48846117,  0.48549277, -0.46656105, -0.51610285, -0.48429605,\n",
              "         0.5014258 ,  0.42551783, -0.4816654 , -0.4441849 , -0.5114069 ,\n",
              "        -0.49800062,  0.45127994,  0.45361024,  0.45236444, -0.48694357,\n",
              "        -0.43634075,  0.47542334,  0.47140723, -0.4620012 , -0.47856548,\n",
              "        -0.49308848,  0.44926882,  0.46126652,  0.4367846 , -0.4881042 ,\n",
              "        -0.42412317,  0.41895536,  0.51197356,  0.5058259 ,  0.43904397,\n",
              "        -0.5037787 ,  0.44557518, -0.5159173 , -0.51398116, -0.45854422,\n",
              "        -0.46548736,  0.44119787, -0.48525473,  0.47681683, -0.4846264 ,\n",
              "         0.49359035, -0.5031711 , -0.44356117, -0.44201002,  0.49037302,\n",
              "         0.4506592 ,  0.4771143 ,  0.5105122 , -0.51113814,  0.4760222 ],\n",
              "       dtype=float32),\n",
              " 'title': array([ 0.46865556,  0.4478864 , -0.5468324 , -0.4984817 , -0.49250048,\n",
              "         0.48732185,  0.4936269 , -0.5112747 , -0.52458847, -0.5055884 ,\n",
              "        -0.4461186 ,  0.46758497,  0.4616796 ,  0.4624559 , -0.4872086 ,\n",
              "        -0.49264565,  0.46382624,  0.5331948 , -0.4496329 , -0.5108433 ,\n",
              "        -0.46451357,  0.49037856,  0.4669968 ,  0.5065707 , -0.48234084,\n",
              "        -0.5073704 ,  0.44707558,  0.47554454,  0.45334092,  0.45722276,\n",
              "        -0.49904522,  0.492456  , -0.53938943, -0.50889045, -0.5002822 ,\n",
              "        -0.5118283 ,  0.5427146 , -0.51822823,  0.53359795, -0.5071304 ,\n",
              "         0.45591307, -0.44494233, -0.45177615, -0.5204467 ,  0.52622974,\n",
              "         0.4762552 ,  0.5122666 ,  0.4991451 , -0.4571231 ,  0.5148903 ],\n",
              "       dtype=float32),\n",
              " 'levels': array([ 0.51027423,  0.43085888, -0.47590604, -0.51221997, -0.52615523,\n",
              "         0.46429777,  0.44925457, -0.5306906 , -0.5320995 , -0.45970035,\n",
              "        -0.45830992,  0.52606755,  0.46104488,  0.48185447, -0.49377406,\n",
              "        -0.49782148,  0.4722836 ,  0.4914745 , -0.45939368, -0.50056547,\n",
              "        -0.44156167,  0.53149915,  0.4860075 ,  0.4453704 , -0.45502347,\n",
              "        -0.5046803 ,  0.46240464,  0.51232725,  0.4829232 ,  0.44446668,\n",
              "        -0.532807  ,  0.4900759 , -0.5198236 , -0.52256113, -0.43520072,\n",
              "        -0.46130165,  0.54389185, -0.5298428 ,  0.5185933 , -0.49207875,\n",
              "         0.5189357 , -0.45382857, -0.43753138, -0.52838904,  0.51715624,\n",
              "         0.44910443,  0.4499971 ,  0.484285  , -0.51058006,  0.52296233],\n",
              "       dtype=float32),\n",
              " 'selected': array([ 0.51443493,  0.47621822, -0.50481117, -0.51131904, -0.47822648,\n",
              "         0.4985184 ,  0.49493513, -0.46665576, -0.44716504, -0.47421443,\n",
              "        -0.47591084,  0.5167823 ,  0.45919967,  0.45224863, -0.55153376,\n",
              "        -0.5166469 ,  0.50111914,  0.47602242, -0.4563525 , -0.42931056,\n",
              "        -0.4596349 ,  0.43313527,  0.47238815,  0.5050134 , -0.4534629 ,\n",
              "        -0.5149173 ,  0.5090205 ,  0.45764405,  0.4970382 ,  0.45930755,\n",
              "        -0.4400318 ,  0.5125103 , -0.47150758, -0.48728684, -0.46869138,\n",
              "        -0.5116513 ,  0.51333195, -0.5030127 ,  0.4442358 , -0.5259032 ,\n",
              "         0.47530526, -0.52217555, -0.46931595, -0.50146383,  0.4607155 ,\n",
              "         0.43983603,  0.46650255,  0.48024783, -0.46435767,  0.4745768 ],\n",
              "       dtype=float32),\n",
              " 'product': array([ 0.5093699 ,  0.4210602 , -0.48314747, -0.48671588, -0.50011235,\n",
              "         0.50830495,  0.51669747, -0.4579482 , -0.4499556 , -0.4659099 ,\n",
              "        -0.5009888 ,  0.46312815,  0.4632145 ,  0.47604626, -0.53164953,\n",
              "        -0.44251424,  0.463265  ,  0.48737884, -0.43405613, -0.454549  ,\n",
              "        -0.50769967,  0.4393719 ,  0.5096773 ,  0.5023094 , -0.45020792,\n",
              "        -0.5004764 ,  0.4730878 ,  0.5081595 ,  0.44410765,  0.4871041 ,\n",
              "        -0.4912898 ,  0.45577002, -0.49153662, -0.51941127, -0.4932607 ,\n",
              "        -0.44800934,  0.43304867, -0.4826086 ,  0.5070975 , -0.49884665,\n",
              "         0.50890553, -0.45415163, -0.4157457 , -0.49121884,  0.4880922 ,\n",
              "         0.45439282,  0.46596366,  0.43710417, -0.4439748 ,  0.49282068],\n",
              "       dtype=float32),\n",
              " 'provides': array([ 0.48257324,  0.4980533 , -0.5024803 , -0.472832  , -0.5004692 ,\n",
              "         0.49877042,  0.5168606 , -0.46150002, -0.4856137 , -0.49850821,\n",
              "        -0.5133479 ,  0.46758243,  0.49334073,  0.44209543, -0.49334145,\n",
              "        -0.44065037,  0.4611638 ,  0.5039109 , -0.5128404 , -0.4854708 ,\n",
              "        -0.48559603,  0.45232782,  0.44319823,  0.49854892, -0.49855912,\n",
              "        -0.500271  ,  0.47502965,  0.47764784,  0.45624697,  0.44045725,\n",
              "        -0.49732748,  0.4248802 , -0.53433484, -0.46177685, -0.4335543 ,\n",
              "        -0.45680106,  0.4652152 , -0.5121619 ,  0.5058054 , -0.44590202,\n",
              "         0.52691835, -0.47279635, -0.4581452 , -0.4681092 ,  0.46105286,\n",
              "         0.4621396 ,  0.45374042,  0.49991655, -0.4742137 ,  0.4300281 ],\n",
              "       dtype=float32),\n",
              " 'transfer': array([ 0.52796066,  0.44643375, -0.4401337 , -0.49438077, -0.5249012 ,\n",
              "         0.50129884,  0.49164727, -0.43855146, -0.512128  , -0.46518272,\n",
              "        -0.47311535,  0.5224831 ,  0.47894272,  0.5155963 , -0.502891  ,\n",
              "        -0.5187812 ,  0.4602546 ,  0.49428415, -0.44597924, -0.43966907,\n",
              "        -0.4979812 ,  0.4640649 ,  0.52392215,  0.42951366, -0.4422081 ,\n",
              "        -0.5215427 ,  0.5092421 ,  0.45169622,  0.47374186,  0.43980613,\n",
              "        -0.512867  ,  0.44488782, -0.4735203 , -0.4750995 , -0.45118278,\n",
              "        -0.5275919 ,  0.4905873 , -0.5146697 ,  0.5222782 , -0.4942952 ,\n",
              "         0.464728  , -0.5319461 , -0.5151451 , -0.4680181 ,  0.5186236 ,\n",
              "         0.4642129 ,  0.5086814 ,  0.45610446, -0.44590864,  0.4783317 ],\n",
              "       dtype=float32),\n",
              " 'team': array([ 0.52814436,  0.50435495, -0.5039757 , -0.467012  , -0.4479075 ,\n",
              "         0.45879692,  0.502234  , -0.49760914, -0.46228546, -0.49803123,\n",
              "        -0.4645731 ,  0.5124206 ,  0.46296948,  0.44309646, -0.4737232 ,\n",
              "        -0.5067018 ,  0.49637437,  0.5332236 , -0.5216415 , -0.47724625,\n",
              "        -0.47379136,  0.5037183 ,  0.4618771 ,  0.45712757, -0.5176584 ,\n",
              "        -0.4471099 ,  0.43770856,  0.472608  ,  0.44607562,  0.51466554,\n",
              "        -0.5098278 ,  0.4904682 , -0.46410218, -0.5286225 , -0.5232076 ,\n",
              "        -0.45946315,  0.5056869 , -0.47053266,  0.4374198 , -0.49799645,\n",
              "         0.4721835 , -0.5093411 , -0.5013646 , -0.43752584,  0.49477395,\n",
              "         0.52645576,  0.5127743 ,  0.5085587 , -0.43975297,  0.5239161 ],\n",
              "       dtype=float32),\n",
              " 'preferred': array([ 0.51620847,  0.4486748 , -0.5213526 , -0.52886647, -0.523297  ,\n",
              "         0.47530118,  0.49879903, -0.51590055, -0.5106577 , -0.47389996,\n",
              "        -0.45880133,  0.46294695,  0.44716817,  0.5366395 , -0.5391321 ,\n",
              "        -0.5153858 ,  0.46103376,  0.53868043, -0.51932   , -0.45928913,\n",
              "        -0.45471662,  0.5189499 ,  0.5334145 ,  0.4391674 , -0.47623008,\n",
              "        -0.45374113,  0.49695894,  0.4838525 ,  0.5320814 ,  0.49228382,\n",
              "        -0.47621357,  0.49620122, -0.5374062 , -0.50878775, -0.48796114,\n",
              "        -0.5066252 ,  0.5270404 , -0.45670924,  0.43502995, -0.45054808,\n",
              "         0.50758654, -0.49169046, -0.4910524 , -0.5204935 ,  0.5274725 ,\n",
              "         0.466876  ,  0.5002861 ,  0.5269565 , -0.5091547 ,  0.46829337],\n",
              "       dtype=float32),\n",
              " 'add': array([ 0.5025238 ,  0.45455596, -0.5243371 , -0.4656192 , -0.50370103,\n",
              "         0.5223107 ,  0.48129296, -0.5059826 , -0.4688409 , -0.49226058,\n",
              "        -0.5103182 ,  0.48816195,  0.43996552,  0.50020844, -0.47273436,\n",
              "        -0.46269938,  0.494521  ,  0.4511711 , -0.46767548, -0.46663952,\n",
              "        -0.5007391 ,  0.46734357,  0.46473038,  0.44260323, -0.47595617,\n",
              "        -0.44344065,  0.44367552,  0.49453375,  0.48903504,  0.5177447 ,\n",
              "        -0.48214215,  0.4886519 , -0.48492518, -0.45656407, -0.46497262,\n",
              "        -0.4368401 ,  0.46530083, -0.44978622,  0.50797904, -0.46524698,\n",
              "         0.49959782, -0.45526424, -0.46147147, -0.47782224,  0.4645757 ,\n",
              "         0.45440063,  0.49283075,  0.452213  , -0.51043075,  0.51239645],\n",
              "       dtype=float32),\n",
              " 'assuming': array([ 0.4536743 ,  0.49255258, -0.5026422 , -0.49630308, -0.4482463 ,\n",
              "         0.52354527,  0.48311794, -0.43660042, -0.4427216 , -0.50108665,\n",
              "        -0.45111507,  0.43623295,  0.48343885,  0.4926944 , -0.4862905 ,\n",
              "        -0.51890755,  0.4780211 ,  0.5236395 , -0.44035497, -0.51335126,\n",
              "        -0.46269068,  0.50955904,  0.43870118,  0.47017807, -0.45533714,\n",
              "        -0.46912453,  0.49661916,  0.51561034,  0.4526631 ,  0.43236163,\n",
              "        -0.5111206 ,  0.48755604, -0.5195676 , -0.52191865, -0.50828576,\n",
              "        -0.49099392,  0.48319966, -0.48598263,  0.50949216, -0.43591744,\n",
              "         0.5203446 , -0.47918704, -0.4840823 , -0.50502235,  0.4809762 ,\n",
              "         0.44494402,  0.48000544,  0.4910677 , -0.47572073,  0.4343243 ],\n",
              "       dtype=float32),\n",
              " 'heightened': array([ 0.4657788 ,  0.48945135, -0.43886203, -0.5124761 , -0.4678644 ,\n",
              "         0.44538844,  0.45554525, -0.49463907, -0.5046973 , -0.4767239 ,\n",
              "        -0.51558715,  0.4420247 ,  0.48570418,  0.4841971 , -0.5606464 ,\n",
              "        -0.49631113,  0.47776657,  0.48541096, -0.483252  , -0.51286167,\n",
              "        -0.5022343 ,  0.47253355,  0.46624213,  0.45732287, -0.4318509 ,\n",
              "        -0.43833092,  0.45941076,  0.5168899 ,  0.48182234,  0.4555901 ,\n",
              "        -0.49699104,  0.46129248, -0.50745314, -0.48938388, -0.45690054,\n",
              "        -0.4892209 ,  0.50011885, -0.48299596,  0.51467526, -0.47950265,\n",
              "         0.44872355, -0.5202036 , -0.47619095, -0.45573646,  0.447392  ,\n",
              "         0.44187197,  0.44746408,  0.51992035, -0.4951338 ,  0.4466373 ],\n",
              "       dtype=float32),\n",
              " 'original': array([ 0.5274127 ,  0.43641126, -0.50604737, -0.49295476, -0.52950275,\n",
              "         0.5026337 ,  0.52201056, -0.47978324, -0.44204074, -0.4626272 ,\n",
              "        -0.43993285,  0.44383475,  0.46942353,  0.46925914, -0.4793497 ,\n",
              "        -0.4984812 ,  0.45703673,  0.521302  , -0.5112603 , -0.45234296,\n",
              "        -0.4699006 ,  0.47637647,  0.4451254 ,  0.49338862, -0.43589982,\n",
              "        -0.48992535,  0.4521793 ,  0.539179  ,  0.44786087,  0.51881063,\n",
              "        -0.4614898 ,  0.43054503, -0.47536445, -0.4833707 , -0.43033892,\n",
              "        -0.48234057,  0.4795804 , -0.48579186,  0.44450203, -0.48327774,\n",
              "         0.45125496, -0.5285261 , -0.45540094, -0.49656594,  0.52597994,\n",
              "         0.44588786,  0.46002856,  0.4936025 , -0.44436666,  0.46243843],\n",
              "       dtype=float32),\n",
              " 'become': array([ 0.48359844,  0.46057636, -0.5008378 , -0.4762485 , -0.46579385,\n",
              "         0.50934744,  0.48625728, -0.43139958, -0.4967487 , -0.46552435,\n",
              "        -0.4577971 ,  0.43671274,  0.4480725 ,  0.48468208, -0.51417893,\n",
              "        -0.45311555,  0.45976964,  0.47332558, -0.501771  , -0.45548007,\n",
              "        -0.48866957,  0.47551495,  0.4936574 ,  0.4918456 , -0.4629883 ,\n",
              "        -0.46513978,  0.4833909 ,  0.5312671 ,  0.46742368,  0.4911821 ,\n",
              "        -0.4769707 ,  0.4617237 , -0.5216658 , -0.43800506, -0.45674592,\n",
              "        -0.4304186 ,  0.49761903, -0.48771304,  0.482456  , -0.45226687,\n",
              "         0.44563556, -0.49610394, -0.48042512, -0.50463367,  0.43622488,\n",
              "         0.48590606,  0.43747848,  0.45393828, -0.43749493,  0.45127973],\n",
              "       dtype=float32),\n",
              " 'seeking': array([ 0.5077347 ,  0.4943137 , -0.4838919 , -0.49641806, -0.53200054,\n",
              "         0.4769683 ,  0.52405727, -0.5287464 , -0.4695978 , -0.45913818,\n",
              "        -0.43712264,  0.50083464,  0.5216174 ,  0.48813766, -0.5420507 ,\n",
              "        -0.4764335 ,  0.49151558,  0.51906145, -0.46778446, -0.47899547,\n",
              "        -0.52872616,  0.5159917 ,  0.48446506,  0.44562942, -0.44676635,\n",
              "        -0.46946353,  0.45019522,  0.5004764 ,  0.48653743,  0.45228162,\n",
              "        -0.50651985,  0.47928175, -0.48460954, -0.48151803, -0.47569856,\n",
              "        -0.5021858 ,  0.46217632, -0.5114335 ,  0.4797159 , -0.4572686 ,\n",
              "         0.53066343, -0.47719586, -0.4844412 , -0.44550267,  0.44737637,\n",
              "         0.4824077 ,  0.49784568,  0.4469278 , -0.45385695,  0.51789796],\n",
              "       dtype=float32),\n",
              " 'attractive': array([ 0.541306  ,  0.45085108, -0.50161165, -0.50066537, -0.4564581 ,\n",
              "         0.51807475,  0.48297966, -0.49000472, -0.47311923, -0.43238005,\n",
              "        -0.500029  ,  0.49598286,  0.47008416,  0.4741826 , -0.5144419 ,\n",
              "        -0.48229796,  0.44137126,  0.4495616 , -0.4612347 , -0.51058626,\n",
              "        -0.4670359 ,  0.46480426,  0.49784225,  0.4735059 , -0.46443623,\n",
              "        -0.5171385 ,  0.4471214 ,  0.50579363,  0.44038987,  0.50599205,\n",
              "        -0.48161286,  0.43862468, -0.5322373 , -0.5249894 , -0.4557402 ,\n",
              "        -0.44437933,  0.4641006 , -0.50491613,  0.43254608, -0.51894623,\n",
              "         0.5249434 , -0.43014827, -0.504827  , -0.4583533 ,  0.5014629 ,\n",
              "         0.4381286 ,  0.4873947 ,  0.49867812, -0.46268266,  0.50260395],\n",
              "       dtype=float32),\n",
              " 'ivy': array([ 0.540221  ,  0.428801  , -0.53929335, -0.44042853, -0.5202459 ,\n",
              "         0.5105418 ,  0.45628047, -0.46274716, -0.5206646 , -0.46755004,\n",
              "        -0.48850814,  0.5366866 ,  0.5188363 ,  0.5184096 , -0.5383769 ,\n",
              "        -0.5395163 ,  0.48195583,  0.5373132 , -0.45541131, -0.4854915 ,\n",
              "        -0.45224917,  0.521439  ,  0.5028381 ,  0.44400537, -0.5352832 ,\n",
              "        -0.4641424 ,  0.4931622 ,  0.49249983,  0.4994247 ,  0.4617417 ,\n",
              "        -0.5116591 ,  0.46360835, -0.5192179 , -0.47012624, -0.45085984,\n",
              "        -0.46911576,  0.53136545, -0.45759317,  0.47591674, -0.44233924,\n",
              "         0.47522122, -0.51421994, -0.49499878, -0.49649018,  0.5040897 ,\n",
              "         0.4919271 ,  0.49734408,  0.48856062, -0.50944185,  0.46791884],\n",
              "       dtype=float32),\n",
              " 'contractual': array([ 0.5006605 ,  0.43972054, -0.47237626, -0.49030906, -0.5166112 ,\n",
              "         0.45620748,  0.44174704, -0.4444162 , -0.42758948, -0.48471326,\n",
              "        -0.50001067,  0.44583064,  0.457366  ,  0.45199364, -0.5118149 ,\n",
              "        -0.4989138 ,  0.4944144 ,  0.49546394, -0.45755777, -0.43541476,\n",
              "        -0.48301822,  0.4218956 ,  0.46964118,  0.47714123, -0.49349907,\n",
              "        -0.46560314,  0.48332208,  0.4839581 ,  0.4753256 ,  0.4986537 ,\n",
              "        -0.5058255 ,  0.42866322, -0.47546214, -0.4431095 , -0.44717222,\n",
              "        -0.44392565,  0.5015091 , -0.4544363 ,  0.4684943 , -0.4848346 ,\n",
              "         0.43378532, -0.49262467, -0.49560896, -0.44077185,  0.49673766,\n",
              "         0.43774605,  0.44579372,  0.49964094, -0.45619956,  0.45523182],\n",
              "       dtype=float32),\n",
              " 'rights': array([ 0.47175786,  0.4816273 , -0.44367433, -0.46084967, -0.51506275,\n",
              "         0.4659513 ,  0.4307244 , -0.45193282, -0.49342486, -0.41764447,\n",
              "        -0.508927  ,  0.42968   ,  0.51023775,  0.44101623, -0.5229347 ,\n",
              "        -0.48053083,  0.4755383 ,  0.4753704 , -0.48624814, -0.49188933,\n",
              "        -0.48272347,  0.5031439 ,  0.5005079 ,  0.4453743 , -0.42316523,\n",
              "        -0.48695403,  0.4211659 ,  0.45366457,  0.5130105 ,  0.47905266,\n",
              "        -0.46903926,  0.4491161 , -0.5101856 , -0.44711202, -0.43725118,\n",
              "        -0.5018368 ,  0.49582276, -0.47734156,  0.49043566, -0.46611708,\n",
              "         0.5093446 , -0.45742327, -0.41962832, -0.5093487 ,  0.4399743 ,\n",
              "         0.4438259 ,  0.43157265,  0.5093145 , -0.47578278,  0.43644625],\n",
              "       dtype=float32),\n",
              " 'size': array([ 0.522787  ,  0.50298405, -0.47861868, -0.48815173, -0.5048858 ,\n",
              "         0.45596713,  0.52607626, -0.49850073, -0.4795543 , -0.50190735,\n",
              "        -0.44960967,  0.47494817,  0.46979687,  0.5298092 , -0.49114475,\n",
              "        -0.49328598,  0.5224855 ,  0.53255355, -0.4626632 , -0.5221032 ,\n",
              "        -0.51566786,  0.500972  ,  0.4976198 ,  0.47985858, -0.4403167 ,\n",
              "        -0.4705174 ,  0.51002204,  0.51458365,  0.44403028,  0.4694833 ,\n",
              "        -0.4916829 ,  0.45284012, -0.5006517 , -0.45957592, -0.46016943,\n",
              "        -0.5086151 ,  0.48677385, -0.44590044,  0.5047212 , -0.47818345,\n",
              "         0.53018844, -0.45667124, -0.50873387, -0.46274847,  0.47491038,\n",
              "         0.45562774,  0.51685226,  0.46977532, -0.45225942,  0.5228893 ],\n",
              "       dtype=float32),\n",
              " 'agent': array([ 0.5295613 ,  0.50523007, -0.4456989 , -0.5219649 , -0.51596355,\n",
              "         0.5264947 ,  0.44530264, -0.45912856, -0.4826294 , -0.4723843 ,\n",
              "        -0.45124277,  0.5078839 ,  0.46528575,  0.43776554, -0.4657071 ,\n",
              "        -0.43642133,  0.43925816,  0.46054596, -0.42554837, -0.46971822,\n",
              "        -0.46359435,  0.5146887 ,  0.52233946,  0.47339302, -0.47844112,\n",
              "        -0.49042046,  0.49406797,  0.45959356,  0.47442886,  0.5127658 ,\n",
              "        -0.48979354,  0.48342872, -0.5170345 , -0.44715145, -0.5143509 ,\n",
              "        -0.47454095,  0.4912975 , -0.4716986 ,  0.47431353, -0.47874784,\n",
              "         0.50143635, -0.5053193 , -0.50806344, -0.50657284,  0.51771677,\n",
              "         0.44078612,  0.46043673,  0.49940065, -0.49776703,  0.4993084 ],\n",
              "       dtype=float32),\n",
              " 'favor': array([ 0.51594   ,  0.4835407 , -0.5105289 , -0.51146185, -0.48036754,\n",
              "         0.48890078,  0.48539802, -0.52696836, -0.43454042, -0.48868817,\n",
              "        -0.47404185,  0.5093391 ,  0.4510749 ,  0.4731945 , -0.5205951 ,\n",
              "        -0.4648121 ,  0.48348346,  0.4794686 , -0.506166  , -0.5089957 ,\n",
              "        -0.51913655,  0.5233092 ,  0.4421776 ,  0.43690318, -0.4653241 ,\n",
              "        -0.47326773,  0.47240925,  0.49120986,  0.4611356 ,  0.4827913 ,\n",
              "        -0.46616998,  0.45144182, -0.5109537 , -0.4967944 , -0.46162438,\n",
              "        -0.46095794,  0.4513613 , -0.49546987,  0.45783064, -0.49005973,\n",
              "         0.4957863 , -0.47229752, -0.45948926, -0.47577232,  0.5082895 ,\n",
              "         0.5246818 ,  0.49897352,  0.50670856, -0.44334072,  0.4400265 ],\n",
              "       dtype=float32),\n",
              " 'select': array([ 0.56054056,  0.47571474, -0.4451398 , -0.4623855 , -0.4720672 ,\n",
              "         0.45691848,  0.49068502, -0.50567394, -0.50531083, -0.4294574 ,\n",
              "        -0.5175755 ,  0.4812698 ,  0.4581955 ,  0.45899057, -0.5627334 ,\n",
              "        -0.4428418 ,  0.4873309 ,  0.4516284 , -0.4947842 , -0.46078616,\n",
              "        -0.52445203,  0.45076746,  0.52281797,  0.50699496, -0.48886275,\n",
              "        -0.4547643 ,  0.48332435,  0.49499786,  0.5258981 ,  0.43536738,\n",
              "        -0.51789236,  0.46525583, -0.5045258 , -0.47470963, -0.44666553,\n",
              "        -0.46074954,  0.5009681 , -0.52700186,  0.49517027, -0.5222389 ,\n",
              "         0.52790797, -0.51087415, -0.46290988, -0.5238205 ,  0.5024524 ,\n",
              "         0.526674  ,  0.5018922 ,  0.5176648 , -0.45812488,  0.51721686],\n",
              "       dtype=float32),\n",
              " 'incurred': array([ 0.48173535,  0.47965354, -0.5128444 , -0.5129684 , -0.49178916,\n",
              "         0.480593  ,  0.4523089 , -0.50362295, -0.4894907 , -0.45011955,\n",
              "        -0.5012895 ,  0.4586594 ,  0.47033647,  0.48446918, -0.55759424,\n",
              "        -0.514569  ,  0.48850554,  0.4450323 , -0.44032523, -0.47509757,\n",
              "        -0.45558193,  0.4764749 ,  0.4694302 ,  0.49285835, -0.44147384,\n",
              "        -0.49029493,  0.44780833,  0.5239865 ,  0.44830784,  0.45158982,\n",
              "        -0.51388234,  0.45285183, -0.46570677, -0.5134424 , -0.4223973 ,\n",
              "        -0.4969458 ,  0.48997614, -0.5094384 ,  0.50213313, -0.4794318 ,\n",
              "         0.5076705 , -0.44023678, -0.43097016, -0.4675106 ,  0.4781986 ,\n",
              "         0.46763206,  0.47729927,  0.4725069 , -0.4693285 ,  0.45963088],\n",
              "       dtype=float32),\n",
              " 'proper': array([ 0.4757358 ,  0.4360489 , -0.44232818, -0.48987976, -0.50556815,\n",
              "         0.52178717,  0.48986322, -0.46823812, -0.47655872, -0.4770903 ,\n",
              "        -0.44317308,  0.47970098,  0.49465364,  0.48664403, -0.52170765,\n",
              "        -0.4681219 ,  0.471042  ,  0.46403438, -0.443556  , -0.4824887 ,\n",
              "        -0.43725446,  0.50762844,  0.44635665,  0.4696212 , -0.4378036 ,\n",
              "        -0.50001544,  0.42441   ,  0.48758563,  0.47031105,  0.48812127,\n",
              "        -0.44578406,  0.5017542 , -0.46069914, -0.50889987, -0.4727    ,\n",
              "        -0.43402073,  0.45037413, -0.50620335,  0.45480603, -0.47639933,\n",
              "         0.50545716, -0.47273695, -0.46008566, -0.47191244,  0.51006794,\n",
              "         0.4675012 ,  0.4864922 ,  0.4449053 , -0.45777553,  0.50639975],\n",
              "       dtype=float32),\n",
              " 'employer-sponsored': array([ 0.46397856,  0.50007963, -0.5251239 , -0.48558798, -0.5211603 ,\n",
              "         0.45004615,  0.46380517, -0.46919805, -0.43684778, -0.4653353 ,\n",
              "        -0.4529885 ,  0.523866  ,  0.44731775,  0.52680343, -0.54191834,\n",
              "        -0.50387764,  0.4845148 ,  0.4670928 , -0.51741236, -0.44415176,\n",
              "        -0.47969475,  0.44834173,  0.45140097,  0.4798738 , -0.43068662,\n",
              "        -0.47445077,  0.4500528 ,  0.47455275,  0.44809103,  0.4608777 ,\n",
              "        -0.523804  ,  0.46338782, -0.5092281 , -0.47165757, -0.4795203 ,\n",
              "        -0.4840318 ,  0.5199301 , -0.51152   ,  0.49038795, -0.46294197,\n",
              "         0.48821592, -0.5266165 , -0.47126305, -0.47223508,  0.44312704,\n",
              "         0.44350103,  0.50653845,  0.5118937 , -0.5188068 ,  0.5063115 ],\n",
              "       dtype=float32),\n",
              " 'timely': array([ 0.5174851 ,  0.4902403 , -0.4750263 , -0.5148224 , -0.4865123 ,\n",
              "         0.5307466 ,  0.43585306, -0.49192587, -0.45420516, -0.49681157,\n",
              "        -0.47896537,  0.52254987,  0.4618367 ,  0.5260593 , -0.51883614,\n",
              "        -0.45665234,  0.48450738,  0.49485102, -0.4386749 , -0.4514854 ,\n",
              "        -0.4733787 ,  0.50749415,  0.4708064 ,  0.43273643, -0.49781454,\n",
              "        -0.43484747,  0.50879097,  0.4806058 ,  0.43282485,  0.47339934,\n",
              "        -0.45445085,  0.45895186, -0.5196286 , -0.46800277, -0.49485844,\n",
              "        -0.52763313,  0.51733625, -0.5238264 ,  0.4754077 , -0.50590676,\n",
              "         0.47312212, -0.49999708, -0.4666432 , -0.4601433 ,  0.49863207,\n",
              "         0.4960407 ,  0.45402408,  0.47028846, -0.491514  ,  0.45619363],\n",
              "       dtype=float32),\n",
              " 'degree': array([ 0.46672106,  0.45121208, -0.45390514, -0.4750207 , -0.52960914,\n",
              "         0.49693015,  0.47736728, -0.4626227 , -0.4816377 , -0.45238668,\n",
              "        -0.5046505 ,  0.44085413,  0.439966  ,  0.44341666, -0.5561542 ,\n",
              "        -0.51969206,  0.51084757,  0.51066315, -0.43334195, -0.5030916 ,\n",
              "        -0.4410282 ,  0.4418349 ,  0.45976526,  0.42249262, -0.50061774,\n",
              "        -0.45534962,  0.4572221 ,  0.53153694,  0.46717542,  0.497839  ,\n",
              "        -0.4785236 ,  0.49205   , -0.49006444, -0.4460901 , -0.45453468,\n",
              "        -0.44755965,  0.49135995, -0.48172644,  0.4902863 , -0.51108634,\n",
              "         0.44574273, -0.5063505 , -0.45808256, -0.4727999 ,  0.45397767,\n",
              "         0.50720066,  0.48499244,  0.44281206, -0.46855915,  0.44215095],\n",
              "       dtype=float32),\n",
              " 'either': array([ 0.52221566,  0.44654638, -0.46166784, -0.4606914 , -0.52456474,\n",
              "         0.4941128 ,  0.45325473, -0.51225704, -0.47918528, -0.4526808 ,\n",
              "        -0.5265788 ,  0.48230946,  0.45211154,  0.4853635 , -0.5097241 ,\n",
              "        -0.45926166,  0.521474  ,  0.44555134, -0.48368478, -0.44805378,\n",
              "        -0.49906006,  0.46360952,  0.46545854,  0.5160503 , -0.47720796,\n",
              "        -0.5149671 ,  0.4622907 ,  0.51520467,  0.4620962 ,  0.435884  ,\n",
              "        -0.45646167,  0.4539465 , -0.4902967 , -0.4825204 , -0.48930347,\n",
              "        -0.47443128,  0.5212492 , -0.48206326,  0.5266903 , -0.5316194 ,\n",
              "         0.53124076, -0.5217413 , -0.4562258 , -0.4554972 ,  0.4717316 ,\n",
              "         0.49100238,  0.47059783,  0.5158967 , -0.46993732,  0.5190695 ],\n",
              "       dtype=float32),\n",
              " 'natural': array([ 0.4619704 ,  0.44569075, -0.47302723, -0.44216576, -0.48545337,\n",
              "         0.45703277,  0.4335007 , -0.5234802 , -0.4853785 , -0.46688747,\n",
              "        -0.49766955,  0.47366333,  0.51435065,  0.501619  , -0.48634055,\n",
              "        -0.5117469 ,  0.45650023,  0.47530022, -0.4452135 , -0.49269065,\n",
              "        -0.5055756 ,  0.51328224,  0.50367665,  0.5211725 , -0.46153018,\n",
              "        -0.4622338 ,  0.4486451 ,  0.46943045,  0.4431532 ,  0.48988682,\n",
              "        -0.5090072 ,  0.44675514, -0.48696044, -0.485918  , -0.51613826,\n",
              "        -0.5306777 ,  0.50218934, -0.46362737,  0.46135858, -0.48363414,\n",
              "         0.48864317, -0.49766958, -0.48425704, -0.4687945 ,  0.5167737 ,\n",
              "         0.5080854 ,  0.5064689 ,  0.5039083 , -0.46385515,  0.47650233],\n",
              "       dtype=float32),\n",
              " 'billion': array([ 0.48012164,  0.44909796, -0.4478523 , -0.51523614, -0.4652844 ,\n",
              "         0.48184252,  0.43538338, -0.43815744, -0.44513458, -0.47848937,\n",
              "        -0.50593686,  0.4466116 ,  0.45724237,  0.4535574 , -0.522028  ,\n",
              "        -0.45960614,  0.4540549 ,  0.50220996, -0.46622702, -0.46924382,\n",
              "        -0.5050455 ,  0.5094874 ,  0.5124145 ,  0.4753939 , -0.48649895,\n",
              "        -0.5186785 ,  0.4737643 ,  0.5046491 ,  0.4826637 ,  0.48602277,\n",
              "        -0.51142186,  0.50303185, -0.5110908 , -0.49567834, -0.48498428,\n",
              "        -0.48901343,  0.5158461 , -0.46466827,  0.49199843, -0.49069345,\n",
              "         0.4963106 , -0.5266186 , -0.50317717, -0.47097054,  0.46357056,\n",
              "         0.52271754,  0.51256454,  0.46168098, -0.44694257,  0.47792304],\n",
              "       dtype=float32),\n",
              " 'often': array([ 0.53143626,  0.44075224, -0.4522159 , -0.45871753, -0.45199573,\n",
              "         0.48164886,  0.47460008, -0.4732028 , -0.4352972 , -0.49584422,\n",
              "        -0.44875282,  0.47506765,  0.44841516,  0.4849046 , -0.54574883,\n",
              "        -0.49106896,  0.50830495,  0.50580156, -0.51232225, -0.4604941 ,\n",
              "        -0.512655  ,  0.46094447,  0.52144974,  0.43884292, -0.50038624,\n",
              "        -0.45453125,  0.47073877,  0.5376723 ,  0.5191328 ,  0.4384658 ,\n",
              "        -0.52394193,  0.44891414, -0.4793366 , -0.52123636, -0.4530066 ,\n",
              "        -0.5259744 ,  0.51670694, -0.51580954,  0.47953972, -0.46466506,\n",
              "         0.4989366 , -0.5058333 , -0.4567425 , -0.49785638,  0.5074344 ,\n",
              "         0.46544218,  0.48816717,  0.48214948, -0.49126908,  0.5176328 ],\n",
              "       dtype=float32),\n",
              " 'varied': array([ 0.51283836,  0.50476086, -0.47737408, -0.52910477, -0.50605714,\n",
              "         0.5115527 ,  0.46323326, -0.44070223, -0.51392645, -0.45876253,\n",
              "        -0.51978207,  0.4655604 ,  0.4976703 ,  0.52193815, -0.5562439 ,\n",
              "        -0.45267662,  0.47915497,  0.4503293 , -0.49947715, -0.49598253,\n",
              "        -0.5086414 ,  0.4996385 ,  0.4783217 ,  0.47847384, -0.5175607 ,\n",
              "        -0.52965474,  0.47167072,  0.4772265 ,  0.50101906,  0.49865097,\n",
              "        -0.4638407 ,  0.45481825, -0.5078966 , -0.53568655, -0.44269276,\n",
              "        -0.54033864,  0.5067286 , -0.528414  ,  0.5150421 , -0.5153606 ,\n",
              "         0.45963502, -0.48459888, -0.47659588, -0.5162963 ,  0.46883604,\n",
              "         0.50546306,  0.46114755,  0.52708375, -0.5311289 ,  0.4974588 ],\n",
              "       dtype=float32),\n",
              " 'includes': array([ 0.48184562,  0.47413126, -0.45070952, -0.46745384, -0.44493195,\n",
              "         0.49888352,  0.4842283 , -0.47945088, -0.47516856, -0.47776556,\n",
              "        -0.504513  ,  0.5082611 ,  0.44323123,  0.52401   , -0.4887921 ,\n",
              "        -0.44271487,  0.5197762 ,  0.45277044, -0.47966662, -0.4748072 ,\n",
              "        -0.46247363,  0.50358105,  0.50218457,  0.46091244, -0.4639    ,\n",
              "        -0.45579147,  0.4353956 ,  0.46357256,  0.47182715,  0.5090659 ,\n",
              "        -0.50812346,  0.4724751 , -0.52092695, -0.52882653, -0.48557132,\n",
              "        -0.4512589 ,  0.48053533, -0.46175715,  0.45157757, -0.4863987 ,\n",
              "         0.4560442 , -0.4832468 , -0.43397227, -0.4788302 ,  0.46256787,\n",
              "         0.49574178,  0.51327634,  0.51056147, -0.47024822,  0.4779449 ],\n",
              "       dtype=float32),\n",
              " 'combination': array([ 0.52177095,  0.4684065 , -0.50901103, -0.49660608, -0.444154  ,\n",
              "         0.51884   ,  0.4751321 , -0.44978023, -0.44406918, -0.46046212,\n",
              "        -0.5166298 ,  0.4338623 ,  0.5013209 ,  0.46605465, -0.5582255 ,\n",
              "        -0.46689126,  0.4496203 ,  0.45230353, -0.49742782, -0.47522384,\n",
              "        -0.4593224 ,  0.44988692,  0.4992261 ,  0.4331298 , -0.4872238 ,\n",
              "        -0.44161072,  0.42499307,  0.5320028 ,  0.42490128,  0.45938644,\n",
              "        -0.43617153,  0.49584937, -0.5087144 , -0.46719423, -0.45017073,\n",
              "        -0.4454807 ,  0.4913281 , -0.46707   ,  0.47513533, -0.45956528,\n",
              "         0.44155693, -0.51493555, -0.5004914 , -0.4992259 ,  0.4777406 ,\n",
              "         0.515092  ,  0.46755874,  0.4846567 , -0.51878285,  0.46198595],\n",
              "       dtype=float32),\n",
              " 'term': array([ 0.50572824,  0.46589753, -0.4707052 , -0.44684428, -0.5082811 ,\n",
              "         0.4891716 ,  0.45872682, -0.4365878 , -0.45741466, -0.4452372 ,\n",
              "        -0.5084145 ,  0.47662473,  0.44655767,  0.5063651 , -0.54737973,\n",
              "        -0.5014225 ,  0.4715024 ,  0.4768369 , -0.45347378, -0.48733354,\n",
              "        -0.42577997,  0.46868306,  0.4549452 ,  0.47534642, -0.46146464,\n",
              "        -0.43313673,  0.44370174,  0.49995393,  0.48790193,  0.50412405,\n",
              "        -0.46613106,  0.43290234, -0.45317522, -0.48344195, -0.41966653,\n",
              "        -0.43319544,  0.48549938, -0.48956066,  0.5070506 , -0.4647068 ,\n",
              "         0.4291111 , -0.48915002, -0.4920284 , -0.49085876,  0.42808324,\n",
              "         0.43058074,  0.50087965,  0.44724992, -0.50069386,  0.48774278],\n",
              "       dtype=float32),\n",
              " 'universal': array([ 0.48272935,  0.48514473, -0.46972787, -0.43667045, -0.44216192,\n",
              "         0.5167403 ,  0.4785334 , -0.45044914, -0.4410241 , -0.4157626 ,\n",
              "        -0.46838236,  0.49187657,  0.454512  ,  0.49962777, -0.54816914,\n",
              "        -0.4858023 ,  0.50154275,  0.5037589 , -0.48188174, -0.43707788,\n",
              "        -0.48370624,  0.43172973,  0.45317918,  0.45371512, -0.49990505,\n",
              "        -0.49534997,  0.49831784,  0.46283573,  0.4808781 ,  0.48433876,\n",
              "        -0.49862555,  0.43909758, -0.52766794, -0.45752728, -0.41676483,\n",
              "        -0.4977092 ,  0.45164534, -0.4487562 ,  0.48267254, -0.4706135 ,\n",
              "         0.5096296 , -0.4464311 , -0.48751205, -0.4671309 ,  0.47269616,\n",
              "         0.5056502 ,  0.46721238,  0.45472193, -0.48755267,  0.45773447],\n",
              "       dtype=float32),\n",
              " 'traded': array([ 0.5348984 ,  0.4956299 , -0.528338  , -0.47814018, -0.5358896 ,\n",
              "         0.45637253,  0.49163556, -0.5160219 , -0.47796366, -0.4639589 ,\n",
              "        -0.45429125,  0.44025174,  0.44812396,  0.45852438, -0.47954097,\n",
              "        -0.44030613,  0.44240695,  0.47028542, -0.4860474 , -0.4883303 ,\n",
              "        -0.5077822 ,  0.4450003 ,  0.47072324,  0.52164894, -0.4701653 ,\n",
              "        -0.49989662,  0.46279362,  0.46746927,  0.46837083,  0.5060982 ,\n",
              "        -0.49183118,  0.46542087, -0.5398814 , -0.49800712, -0.50060594,\n",
              "        -0.5181495 ,  0.52100784, -0.45401144,  0.46015197, -0.4470781 ,\n",
              "         0.50480884, -0.45064247, -0.48023844, -0.5084378 ,  0.50441647,\n",
              "         0.49220717,  0.5209776 ,  0.47125334, -0.46841538,  0.5184757 ],\n",
              "       dtype=float32),\n",
              " 'making': array([ 0.5320922 ,  0.46223018, -0.4304483 , -0.4801719 , -0.44486147,\n",
              "         0.44149688,  0.46085578, -0.48521543, -0.47264963, -0.44036356,\n",
              "        -0.5111577 ,  0.47536874,  0.4719494 ,  0.50421643, -0.475184  ,\n",
              "        -0.50445163,  0.47831234,  0.45690966, -0.45513678, -0.4684765 ,\n",
              "        -0.4387262 ,  0.49099624,  0.44999212,  0.44030267, -0.49021423,\n",
              "        -0.45587838,  0.46880704,  0.48369494,  0.46708488,  0.46555024,\n",
              "        -0.45941955,  0.48543245, -0.5063804 , -0.504471  , -0.43226156,\n",
              "        -0.49903965,  0.51295817, -0.4780377 ,  0.46388662, -0.4886812 ,\n",
              "         0.5230101 , -0.4985784 , -0.4801786 , -0.50539553,  0.43789324,\n",
              "         0.48375088,  0.46670863,  0.46314153, -0.473292  ,  0.45405337],\n",
              "       dtype=float32),\n",
              " 'assurance': array([ 0.5005825 ,  0.503369  , -0.4795602 , -0.45305622, -0.46477246,\n",
              "         0.46958712,  0.51161987, -0.45127153, -0.5168473 , -0.43559796,\n",
              "        -0.5016617 ,  0.43828583,  0.4377544 ,  0.51970536, -0.5306408 ,\n",
              "        -0.43599904,  0.47522938,  0.49001157, -0.4290955 , -0.46908697,\n",
              "        -0.46748185,  0.45916554,  0.488846  ,  0.4593789 , -0.4600742 ,\n",
              "        -0.46005595,  0.46691817,  0.46580535,  0.5183833 ,  0.48376036,\n",
              "        -0.49936768,  0.47684616, -0.454122  , -0.48667142, -0.5003003 ,\n",
              "        -0.48798174,  0.5166574 , -0.4357575 ,  0.46810284, -0.4401877 ,\n",
              "         0.48960817, -0.51888055, -0.5074142 , -0.49345016,  0.48516136,\n",
              "         0.51431346,  0.44223282,  0.45941466, -0.4883454 ,  0.44218183],\n",
              "       dtype=float32),\n",
              " 'realized': array([ 0.46527717,  0.45241538, -0.44527724, -0.52068895, -0.4895094 ,\n",
              "         0.5310893 ,  0.48267794, -0.5226252 , -0.45314133, -0.4405091 ,\n",
              "        -0.496388  ,  0.44017547,  0.48875254,  0.44809553, -0.5479562 ,\n",
              "        -0.4790245 ,  0.4950055 ,  0.5112695 , -0.43020013, -0.4620133 ,\n",
              "        -0.4829387 ,  0.47270346,  0.50666016,  0.49219695, -0.4932686 ,\n",
              "        -0.46512082,  0.5170238 ,  0.5339385 ,  0.47812292,  0.46928638,\n",
              "        -0.49957106,  0.50316495, -0.5017101 , -0.4713953 , -0.4856249 ,\n",
              "        -0.48939002,  0.46303827, -0.4877305 ,  0.49552128, -0.5260157 ,\n",
              "         0.47887087, -0.45067632, -0.43106198, -0.4658097 ,  0.5025333 ,\n",
              "         0.4987085 ,  0.5172579 ,  0.47133318, -0.43447807,  0.4317963 ],\n",
              "       dtype=float32),\n",
              " 'trustees': array([ 0.5069624 ,  0.4675549 , -0.48385963, -0.4325321 , -0.45485303,\n",
              "         0.44544148,  0.42374098, -0.42885655, -0.49129924, -0.47959828,\n",
              "        -0.5000162 ,  0.51730824,  0.4876476 ,  0.4379075 , -0.4629053 ,\n",
              "        -0.4977928 ,  0.47199988,  0.4400902 , -0.50361943, -0.48790067,\n",
              "        -0.45498246,  0.45722085,  0.4515993 ,  0.4392698 , -0.4371252 ,\n",
              "        -0.43713716,  0.49244642,  0.4972677 ,  0.44040108,  0.46028924,\n",
              "        -0.46220133,  0.49437827, -0.4647896 , -0.5018599 , -0.4550549 ,\n",
              "        -0.48943946,  0.48619637, -0.47093377,  0.45392787, -0.4979763 ,\n",
              "         0.4838977 , -0.50916123, -0.5034517 , -0.42360756,  0.5149573 ,\n",
              "         0.51384103,  0.42231515,  0.4840356 , -0.5112282 ,  0.49752158],\n",
              "       dtype=float32),\n",
              " 'e': array([ 0.49287343,  0.52482754, -0.46305156, -0.45134667, -0.5288234 ,\n",
              "         0.4931833 ,  0.44533497, -0.46930188, -0.51767904, -0.4606454 ,\n",
              "        -0.5481442 ,  0.4988205 ,  0.47899798,  0.53359354, -0.5111046 ,\n",
              "        -0.5163682 ,  0.47552624,  0.47533682, -0.49745882, -0.4688696 ,\n",
              "        -0.48965496,  0.5085529 ,  0.5023781 ,  0.4930036 , -0.52341473,\n",
              "        -0.44232646,  0.493662  ,  0.5236481 ,  0.47171664,  0.4744617 ,\n",
              "        -0.5319967 ,  0.5102056 , -0.5446024 , -0.5047618 , -0.5148577 ,\n",
              "        -0.47804365,  0.51683813, -0.5107918 ,  0.50223637, -0.49565518,\n",
              "         0.5381567 , -0.47870728, -0.4887645 , -0.44697532,  0.51218146,\n",
              "         0.4745056 ,  0.45543543,  0.4882732 , -0.4807641 ,  0.5147734 ],\n",
              "       dtype=float32),\n",
              " 'unrated': array([ 0.53075236,  0.49701253, -0.4347766 , -0.43590438, -0.4361685 ,\n",
              "         0.5070366 ,  0.457017  , -0.42887518, -0.4444908 , -0.45663232,\n",
              "        -0.47506043,  0.5083094 ,  0.42788774,  0.44460565, -0.47566444,\n",
              "        -0.5049705 ,  0.5203208 ,  0.43049842, -0.43331113, -0.4726688 ,\n",
              "        -0.43171456,  0.45555267,  0.5011129 ,  0.449726  , -0.50653875,\n",
              "        -0.42757052,  0.5131389 ,  0.48974457,  0.4999281 ,  0.51169354,\n",
              "        -0.51938045,  0.48498833, -0.53007424, -0.47457817, -0.48599643,\n",
              "        -0.5071254 ,  0.49188152, -0.44126895,  0.45171067, -0.4760527 ,\n",
              "         0.46344247, -0.44005382, -0.49716794, -0.4890336 ,  0.4760052 ,\n",
              "         0.44866464,  0.4722066 ,  0.44416386, -0.4899338 ,  0.43783677],\n",
              "       dtype=float32),\n",
              " 'compared': array([ 0.4872915 ,  0.45773765, -0.51624835, -0.50441384, -0.49108234,\n",
              "         0.45856836,  0.42997885, -0.48368618, -0.46626756, -0.4823437 ,\n",
              "        -0.48331267,  0.46352208,  0.4851175 ,  0.46601057, -0.5375401 ,\n",
              "        -0.49400163,  0.4757087 ,  0.4426321 , -0.48239294, -0.49623296,\n",
              "        -0.49446464,  0.47175783,  0.4898595 ,  0.47304282, -0.46800357,\n",
              "        -0.4857806 ,  0.4525258 ,  0.46937728,  0.45340067,  0.4289384 ,\n",
              "        -0.4752065 ,  0.5055637 , -0.46760187, -0.4928684 , -0.5100511 ,\n",
              "        -0.44843778,  0.4425373 , -0.43596077,  0.502113  , -0.46329233,\n",
              "         0.49404183, -0.46924388, -0.482458  , -0.51420665,  0.48182863,\n",
              "         0.44025204,  0.44974327,  0.47430956, -0.46158993,  0.47776034],\n",
              "       dtype=float32),\n",
              " 'standards': array([ 0.47255147,  0.47227913, -0.48090202, -0.5078827 , -0.47909397,\n",
              "         0.53000385,  0.4919306 , -0.4504556 , -0.48036   , -0.47143236,\n",
              "        -0.513056  ,  0.45664075,  0.471904  ,  0.4727387 , -0.5224608 ,\n",
              "        -0.46319014,  0.45368513,  0.5037947 , -0.49606073, -0.43917054,\n",
              "        -0.4425629 ,  0.43812683,  0.4692137 ,  0.502939  , -0.47701055,\n",
              "        -0.50392085,  0.4629629 ,  0.5526002 ,  0.5225652 ,  0.51773375,\n",
              "        -0.46249786,  0.49047905, -0.5043282 , -0.51709545, -0.47719964,\n",
              "        -0.49539664,  0.47043657, -0.46725136,  0.4629979 , -0.4764663 ,\n",
              "         0.4433359 , -0.49367553, -0.4621015 , -0.47281715,  0.45074096,\n",
              "         0.45373097,  0.48798904,  0.43964148, -0.5153444 ,  0.4582009 ],\n",
              "       dtype=float32),\n",
              " 'follows': array([ 0.4882712 ,  0.46483058, -0.4466352 , -0.5122688 , -0.51117104,\n",
              "         0.52307594,  0.45310277, -0.4749552 , -0.4641926 , -0.44436824,\n",
              "        -0.5293097 ,  0.4829515 ,  0.44943687,  0.47465664, -0.4880858 ,\n",
              "        -0.5270083 ,  0.48774675,  0.50634456, -0.47133324, -0.5213356 ,\n",
              "        -0.4602338 ,  0.512731  ,  0.46289426,  0.5047751 , -0.4940681 ,\n",
              "        -0.45575312,  0.48374397,  0.5083398 ,  0.46995074,  0.5291317 ,\n",
              "        -0.50469726,  0.50380766, -0.5287759 , -0.48841512, -0.49020073,\n",
              "        -0.52065086,  0.51854885, -0.4929405 ,  0.4468189 , -0.51417947,\n",
              "         0.5177055 , -0.45490205, -0.5008188 , -0.46948844,  0.49453023,\n",
              "         0.50693697,  0.47072896,  0.45586655, -0.47456795,  0.50186116],\n",
              "       dtype=float32),\n",
              " 'especially': array([ 0.46113008,  0.47881967, -0.50303715, -0.48349443, -0.46602637,\n",
              "         0.466633  ,  0.46158287, -0.45023945, -0.443829  , -0.4677618 ,\n",
              "        -0.49430513,  0.4545739 ,  0.43813172,  0.46711934, -0.53297246,\n",
              "        -0.5073484 ,  0.4684835 ,  0.47790304, -0.48256952, -0.4553463 ,\n",
              "        -0.45916897,  0.49747357,  0.44375125,  0.5022807 , -0.48416126,\n",
              "        -0.49628967,  0.50602895,  0.48355335,  0.43722388,  0.4275121 ,\n",
              "        -0.52233714,  0.443211  , -0.525719  , -0.49298197, -0.4556632 ,\n",
              "        -0.4583726 ,  0.4476295 , -0.5062541 ,  0.46004695, -0.5034953 ,\n",
              "         0.49728763, -0.49797451, -0.46745583, -0.46692023,  0.5172451 ,\n",
              "         0.51549137,  0.51456714,  0.48573896, -0.48401108,  0.4551333 ],\n",
              "       dtype=float32),\n",
              " 'indexes': array([ 0.48799604,  0.44883606, -0.44791847, -0.49053523, -0.47122115,\n",
              "         0.525023  ,  0.46322894, -0.5133099 , -0.45033854, -0.45372984,\n",
              "        -0.4704457 ,  0.46111938,  0.47016767,  0.43463063, -0.47300032,\n",
              "        -0.48332626,  0.5052052 ,  0.4382938 , -0.48833075, -0.45417005,\n",
              "        -0.43605196,  0.4669566 ,  0.44911927,  0.4837623 , -0.42051563,\n",
              "        -0.47865522,  0.47657806,  0.46632957,  0.5025877 ,  0.424553  ,\n",
              "        -0.4851863 ,  0.4304076 , -0.51992756, -0.47689047, -0.46089387,\n",
              "        -0.49407727,  0.4805392 , -0.4784501 ,  0.5032894 , -0.4475692 ,\n",
              "         0.48212716, -0.44185734, -0.50262153, -0.48367286,  0.5131774 ,\n",
              "         0.47656593,  0.5056546 ,  0.50535005, -0.49872273,  0.4677459 ],\n",
              "       dtype=float32),\n",
              " 'contractually': array([ 0.5232624 ,  0.4240593 , -0.4750059 , -0.42910686, -0.49105573,\n",
              "         0.51377785,  0.46202874, -0.4549292 , -0.451725  , -0.46757406,\n",
              "        -0.4939647 ,  0.444834  ,  0.43383226,  0.42922094, -0.46384132,\n",
              "        -0.50578624,  0.44098073,  0.4866934 , -0.48074442, -0.46286392,\n",
              "        -0.48509133,  0.4142587 ,  0.47753286,  0.48762232, -0.46675897,\n",
              "        -0.46214005,  0.5045669 ,  0.44690955,  0.41520026,  0.41846618,\n",
              "        -0.45350897,  0.4143055 , -0.5039958 , -0.4725627 , -0.46648616,\n",
              "        -0.4538488 ,  0.507855  , -0.45837682,  0.47413376, -0.5048247 ,\n",
              "         0.51427644, -0.4561788 , -0.4433445 , -0.47700563,  0.4403473 ,\n",
              "         0.47737616,  0.4552586 ,  0.43238515, -0.49733418,  0.49357817],\n",
              "       dtype=float32),\n",
              " 'uses': array([ 0.4571007 ,  0.4379774 , -0.4865403 , -0.43456832, -0.45765787,\n",
              "         0.47310132,  0.46672967, -0.5131505 , -0.4409274 , -0.47779793,\n",
              "        -0.47492433,  0.5159175 ,  0.50132173,  0.4333349 , -0.4905165 ,\n",
              "        -0.4660768 ,  0.47208372,  0.45005822, -0.4971038 , -0.49230477,\n",
              "        -0.4599577 ,  0.4460111 ,  0.44104704,  0.44172227, -0.48424572,\n",
              "        -0.5014666 ,  0.4655944 ,  0.47543854,  0.4732199 ,  0.50531834,\n",
              "        -0.43668967,  0.4752129 , -0.48363873, -0.46723557, -0.4650921 ,\n",
              "        -0.51889807,  0.50025374, -0.48351976,  0.42645416, -0.45693618,\n",
              "         0.4366197 , -0.4446717 , -0.49249035, -0.44780695,  0.43810517,\n",
              "         0.456786  ,  0.4592677 ,  0.44734016, -0.43567294,  0.45087317],\n",
              "       dtype=float32),\n",
              " 'regulation': array([ 0.47359213,  0.47969708, -0.47606346, -0.44670677, -0.5156933 ,\n",
              "         0.48726135,  0.43211603, -0.47238344, -0.51614094, -0.45109245,\n",
              "        -0.45190373,  0.4518201 ,  0.49706748,  0.44331625, -0.478321  ,\n",
              "        -0.43000042,  0.46759194,  0.45978165, -0.50690603, -0.4472291 ,\n",
              "        -0.5004662 ,  0.44443506,  0.50037956,  0.45703375, -0.4494188 ,\n",
              "        -0.4807203 ,  0.50914085,  0.49609852,  0.4657486 ,  0.44287384,\n",
              "        -0.52484643,  0.45235327, -0.45995334, -0.51394635, -0.44697335,\n",
              "        -0.44311327,  0.50258124, -0.4764687 ,  0.44036865, -0.49307725,\n",
              "         0.45517278, -0.4814216 , -0.48466417, -0.49085397,  0.46364966,\n",
              "         0.49500725,  0.47117084,  0.47481313, -0.5072272 ,  0.49510923],\n",
              "       dtype=float32),\n",
              " 'regions': array([ 0.49761048,  0.44971213, -0.46371487, -0.44125453, -0.5066692 ,\n",
              "         0.43950406,  0.48661146, -0.44315875, -0.43497643, -0.4308262 ,\n",
              "        -0.5132441 ,  0.49052346,  0.48353887,  0.52104497, -0.5503111 ,\n",
              "        -0.44002968,  0.5145179 ,  0.5026222 , -0.46386194, -0.44875056,\n",
              "        -0.50590134,  0.46029007,  0.49814498,  0.5105679 , -0.46379316,\n",
              "        -0.5206635 ,  0.43709448,  0.523544  ,  0.46529907,  0.4442185 ,\n",
              "        -0.47640404,  0.45185012, -0.53004193, -0.46599525, -0.523077  ,\n",
              "        -0.48776335,  0.47057012, -0.48057443,  0.43935326, -0.45796138,\n",
              "         0.4716069 , -0.47473425, -0.50538003, -0.46307456,  0.5233254 ,\n",
              "         0.49689254,  0.48959312,  0.45329756, -0.49897122,  0.5061746 ],\n",
              "       dtype=float32),\n",
              " 'entity': array([ 0.49810672,  0.5238831 , -0.4431064 , -0.498857  , -0.48840907,\n",
              "         0.5241226 ,  0.4606019 , -0.4743994 , -0.47762156, -0.47487992,\n",
              "        -0.48021704,  0.4741477 ,  0.5018983 ,  0.5125593 , -0.53784555,\n",
              "        -0.50548685,  0.46958542,  0.48274836, -0.44452524, -0.4464923 ,\n",
              "        -0.5070734 ,  0.49197617,  0.44129387,  0.4514254 , -0.4623412 ,\n",
              "        -0.48249853,  0.42572945,  0.5262981 ,  0.43769723,  0.43343827,\n",
              "        -0.48133594,  0.4998456 , -0.48209596, -0.49979   , -0.4558908 ,\n",
              "        -0.46804857,  0.51230234, -0.48591095,  0.44957218, -0.4662792 ,\n",
              "         0.50860703, -0.4716675 , -0.49656478, -0.4579019 ,  0.5089587 ,\n",
              "         0.45390773,  0.45965737,  0.50848085, -0.46108148,  0.45098212],\n",
              "       dtype=float32),\n",
              " 'backed': array([ 0.4787399 ,  0.42541122, -0.44924164, -0.43197334, -0.46447307,\n",
              "         0.47302026,  0.506822  , -0.46696314, -0.49346188, -0.49532497,\n",
              "        -0.44921052,  0.48441732,  0.48978764,  0.46619603, -0.5356047 ,\n",
              "        -0.4417312 ,  0.44467792,  0.499616  , -0.49295098, -0.4266647 ,\n",
              "        -0.45749268,  0.49694178,  0.43564576,  0.47229418, -0.50501657,\n",
              "        -0.45535973,  0.44800287,  0.52780074,  0.43085042,  0.4838727 ,\n",
              "        -0.452098  ,  0.4355711 , -0.48402062, -0.43350735, -0.4656238 ,\n",
              "        -0.49611777,  0.49909267, -0.4936481 ,  0.47487593, -0.51465666,\n",
              "         0.45670786, -0.4476003 , -0.46622854, -0.45645338,  0.4806267 ,\n",
              "         0.43688068,  0.4825499 ,  0.44370154, -0.5109949 ,  0.45560563],\n",
              "       dtype=float32),\n",
              " 'floating': array([ 0.51908624,  0.46935162, -0.4857493 , -0.4394862 , -0.4906546 ,\n",
              "         0.5268094 ,  0.4643896 , -0.52270746, -0.49722153, -0.45305744,\n",
              "        -0.43233076,  0.4969757 ,  0.43899062,  0.5194187 , -0.5502791 ,\n",
              "        -0.5065022 ,  0.507236  ,  0.44848788, -0.50250345, -0.49845538,\n",
              "        -0.48053405,  0.4463913 ,  0.5065993 ,  0.45912358, -0.43655252,\n",
              "        -0.48625433,  0.43117142,  0.49153888,  0.48061714,  0.46700665,\n",
              "        -0.4875734 ,  0.46192658, -0.5072358 , -0.44352847, -0.49784973,\n",
              "        -0.44365183,  0.5262624 , -0.510178  ,  0.51295006, -0.48659462,\n",
              "         0.50960755, -0.4976265 , -0.4333755 , -0.4657501 ,  0.45302227,\n",
              "         0.46849942,  0.4602109 ,  0.5176655 , -0.47728494,  0.5113439 ],\n",
              "       dtype=float32),\n",
              " 'adjusted': array([ 0.5285667 ,  0.47587848, -0.5021131 , -0.46077868, -0.46880904,\n",
              "         0.47453555,  0.4773086 , -0.45219326, -0.44584864, -0.46209645,\n",
              "        -0.45952567,  0.5289991 ,  0.44402963,  0.45855328, -0.4991668 ,\n",
              "        -0.4816855 ,  0.44925368,  0.51362973, -0.48421478, -0.44135728,\n",
              "        -0.50991595,  0.45148766,  0.51933926,  0.46865314, -0.44725868,\n",
              "        -0.44304997,  0.4726608 ,  0.49349687,  0.52137053,  0.4437367 ,\n",
              "        -0.5089256 ,  0.43174312, -0.49719194, -0.51906407, -0.49892896,\n",
              "        -0.51084197,  0.48287982, -0.4603552 ,  0.44521534, -0.48807678,\n",
              "         0.4718935 , -0.46156728, -0.50464284, -0.50339633,  0.52509654,\n",
              "         0.48101074,  0.5159974 ,  0.4567593 , -0.5111321 ,  0.447589  ],\n",
              "       dtype=float32),\n",
              " 'effective': array([ 0.5069488 ,  0.44801116, -0.50377905, -0.46735355, -0.4485571 ,\n",
              "         0.47384855,  0.4836848 , -0.46256086, -0.4500885 , -0.49226132,\n",
              "        -0.45549518,  0.51294255,  0.43877274,  0.5162533 , -0.5326445 ,\n",
              "        -0.5044282 ,  0.46591508,  0.4592304 , -0.50168115, -0.42953235,\n",
              "        -0.48870337,  0.483953  ,  0.5086117 ,  0.48334607, -0.45168725,\n",
              "        -0.47523445,  0.44408527,  0.5370393 ,  0.49364096,  0.47696868,\n",
              "        -0.5037983 ,  0.46855712, -0.4559426 , -0.44626418, -0.47731876,\n",
              "        -0.46773273,  0.46453074, -0.473116  ,  0.47663316, -0.5243165 ,\n",
              "         0.51347065, -0.48461175, -0.44429028, -0.45436546,  0.5029119 ,\n",
              "         0.4915651 ,  0.5151008 ,  0.46615916, -0.50895363,  0.47710973],\n",
              "       dtype=float32),\n",
              " 'participant': array([ 0.47265762,  0.4967708 , -0.5175881 , -0.4779026 , -0.5047915 ,\n",
              "         0.5010411 ,  0.4433178 , -0.46035254, -0.466433  , -0.43339327,\n",
              "        -0.443352  ,  0.4744659 ,  0.48709485,  0.48148894, -0.4878765 ,\n",
              "        -0.47947964,  0.44317394,  0.47685352, -0.47874057, -0.44630358,\n",
              "        -0.45897576,  0.47280705,  0.4446911 ,  0.44174364, -0.43162093,\n",
              "        -0.43563446,  0.51999193,  0.4693152 ,  0.4796431 ,  0.47688404,\n",
              "        -0.46443138,  0.48376346, -0.48565263, -0.5271469 , -0.49744886,\n",
              "        -0.44141456,  0.49772605, -0.48222673,  0.5195397 , -0.4392784 ,\n",
              "         0.50417435, -0.44422355, -0.50820744, -0.5065993 ,  0.49078238,\n",
              "         0.51530004,  0.50035465,  0.43587703, -0.5027803 ,  0.4466675 ],\n",
              "       dtype=float32),\n",
              " 'wisc': array([ 0.452057  ,  0.47443634, -0.43775305, -0.44079962, -0.5232787 ,\n",
              "         0.48665527,  0.49498603, -0.5030916 , -0.48704612, -0.44331703,\n",
              "        -0.4347217 ,  0.48233828,  0.47255188,  0.51402   , -0.5279203 ,\n",
              "        -0.51509756,  0.52696884,  0.4746569 , -0.49172306, -0.45474502,\n",
              "        -0.4192936 ,  0.47069845,  0.4414555 ,  0.4934959 , -0.497367  ,\n",
              "        -0.46549797,  0.46180397,  0.4972519 ,  0.51138824,  0.49647233,\n",
              "        -0.4770456 ,  0.47446162, -0.48810658, -0.48395142, -0.5168448 ,\n",
              "        -0.4900608 ,  0.5120499 , -0.4986956 ,  0.4358202 , -0.42129758,\n",
              "         0.49919963, -0.45135638, -0.4793743 , -0.47819364,  0.45591545,\n",
              "         0.48292547,  0.4544887 ,  0.45757964, -0.5057345 ,  0.5039951 ],\n",
              "       dtype=float32),\n",
              " 'referred': array([ 0.51092976,  0.5055522 , -0.5157511 , -0.5091301 , -0.45754462,\n",
              "         0.5091305 ,  0.45107684, -0.48182726, -0.42965212, -0.4503211 ,\n",
              "        -0.44864354,  0.44671783,  0.42906952,  0.5204083 , -0.4753717 ,\n",
              "        -0.5121697 ,  0.5027883 ,  0.4772402 , -0.4727943 , -0.4348334 ,\n",
              "        -0.5044232 ,  0.44602236,  0.43733057,  0.45834467, -0.42496336,\n",
              "        -0.44834587,  0.51439166,  0.5076393 ,  0.47506186,  0.4422256 ,\n",
              "        -0.5101434 ,  0.4822318 , -0.52840406, -0.48310065, -0.45238298,\n",
              "        -0.47743458,  0.46301332, -0.51117307,  0.5066657 , -0.5192674 ,\n",
              "         0.47847503, -0.44857448, -0.510076  , -0.4536597 ,  0.46811235,\n",
              "         0.44668904,  0.44134977,  0.45712093, -0.5007061 ,  0.472686  ],\n",
              "       dtype=float32),\n",
              " 'possibility': array([ 0.5332023 ,  0.48602307, -0.46652886, -0.4596831 , -0.50810444,\n",
              "         0.50526375,  0.5065742 , -0.48393756, -0.46304756, -0.42396525,\n",
              "        -0.42935315,  0.4983342 ,  0.49563438,  0.4361039 , -0.51710504,\n",
              "        -0.45663518,  0.47356743,  0.44017476, -0.46720728, -0.42075327,\n",
              "        -0.47638535,  0.44747534,  0.43999586,  0.4275695 , -0.44827995,\n",
              "        -0.451213  ,  0.43357754,  0.50000554,  0.43108013,  0.45802122,\n",
              "        -0.45808563,  0.43255842, -0.49669796, -0.4521068 , -0.43399763,\n",
              "        -0.47900167,  0.48740596, -0.5031342 ,  0.45412195, -0.50827277,\n",
              "         0.5164479 , -0.49935097, -0.47865894, -0.4731672 ,  0.4701678 ,\n",
              "         0.50412476,  0.42646793,  0.48656058, -0.48536152,  0.45185515],\n",
              "       dtype=float32),\n",
              " 'premium': array([ 0.54211426,  0.49284092, -0.47294813, -0.52498597, -0.46472   ,\n",
              "         0.47079673,  0.4510081 , -0.46028113, -0.46131918, -0.455998  ,\n",
              "        -0.47595665,  0.47346586,  0.4502804 ,  0.45739853, -0.5555698 ,\n",
              "        -0.47398847,  0.4809822 ,  0.49945948, -0.50603324, -0.4476921 ,\n",
              "        -0.50012857,  0.50528616,  0.5283072 ,  0.46195284, -0.434273  ,\n",
              "        -0.512327  ,  0.52336687,  0.4910718 ,  0.4648478 ,  0.50317985,\n",
              "        -0.5008852 ,  0.49196413, -0.49083662, -0.54345906, -0.48479486,\n",
              "        -0.5078615 ,  0.49745387, -0.47184798,  0.44226563, -0.47844055,\n",
              "         0.5098473 , -0.5300855 , -0.46487576, -0.48067674,  0.46548343,\n",
              "         0.45776692,  0.46952793,  0.47519994, -0.4593277 ,  0.46165448],\n",
              "       dtype=float32),\n",
              " 'co-managed': array([ 0.45305312,  0.4322232 , -0.4299127 , -0.4705112 , -0.5019517 ,\n",
              "         0.5135434 ,  0.4470701 , -0.48609644, -0.48068082, -0.48824558,\n",
              "        -0.45640224,  0.47238067,  0.4462369 ,  0.48592412, -0.516283  ,\n",
              "        -0.5096889 ,  0.5047767 ,  0.4437002 , -0.43595302, -0.4360899 ,\n",
              "        -0.4825254 ,  0.4408526 ,  0.44701076,  0.48982835, -0.46596336,\n",
              "        -0.46124715,  0.50528103,  0.45424873,  0.43289158,  0.41590706,\n",
              "        -0.51008105,  0.4757969 , -0.49855947, -0.4618203 , -0.4168231 ,\n",
              "        -0.42652613,  0.51199293, -0.42680302,  0.504297  , -0.48703793,\n",
              "         0.51146424, -0.50285155, -0.4920624 , -0.47212535,  0.42881063,\n",
              "         0.43943608,  0.50389755,  0.48013416, -0.4596976 ,  0.48143506],\n",
              "       dtype=float32),\n",
              " 'inflation': array([ 0.49306548,  0.45989698, -0.49381638, -0.49208856, -0.4264557 ,\n",
              "         0.49547017,  0.47117898, -0.42229033, -0.41980994, -0.44667208,\n",
              "        -0.47046983,  0.47054696,  0.44916376,  0.45710778, -0.49409464,\n",
              "        -0.4628474 ,  0.48157588,  0.50528026, -0.4987931 , -0.46008217,\n",
              "        -0.48589483,  0.44398102,  0.45098385,  0.41498163, -0.48738813,\n",
              "        -0.48888955,  0.4410332 ,  0.45612463,  0.41717982,  0.475479  ,\n",
              "        -0.47015628,  0.45650768, -0.475073  , -0.46158034, -0.48702365,\n",
              "        -0.49300763,  0.47787908, -0.43103984,  0.43248194, -0.48121578,\n",
              "         0.5065113 , -0.4774247 , -0.4357117 , -0.4226375 ,  0.48828405,\n",
              "         0.50428927,  0.43893382,  0.43430692, -0.45394275,  0.44441208],\n",
              "       dtype=float32),\n",
              " 'agreement': array([ 0.53799427,  0.500574  , -0.51400506, -0.48816997, -0.53681225,\n",
              "         0.5072392 ,  0.5276625 , -0.446391  , -0.46667767, -0.47925645,\n",
              "        -0.52284324,  0.4790591 ,  0.49858892,  0.5088614 , -0.48384288,\n",
              "        -0.47253475,  0.4704864 ,  0.44276887, -0.4358763 , -0.51658165,\n",
              "        -0.4467289 ,  0.48150292,  0.46062595,  0.43706223, -0.47529992,\n",
              "        -0.45633852,  0.43788067,  0.4674399 ,  0.47615397,  0.45046034,\n",
              "        -0.4941198 ,  0.4779997 , -0.4660521 , -0.5162521 , -0.44896904,\n",
              "        -0.51071423,  0.51450866, -0.50293714,  0.44654632, -0.52127415,\n",
              "         0.48246375, -0.5057611 , -0.50562227, -0.44049078,  0.5143547 ,\n",
              "         0.47901863,  0.5124932 ,  0.44968918, -0.5201862 ,  0.4743955 ],\n",
              "       dtype=float32),\n",
              " 'act': array([ 0.5106124 ,  0.4996845 , -0.5285832 , -0.49048287, -0.44776174,\n",
              "         0.4957431 ,  0.43870237, -0.44650057, -0.47747457, -0.47786757,\n",
              "        -0.5031899 ,  0.44047663,  0.45457414,  0.47887394, -0.4782925 ,\n",
              "        -0.42893904,  0.48303977,  0.48110884, -0.45993927, -0.43074524,\n",
              "        -0.46563733,  0.5125586 ,  0.462359  ,  0.50554633, -0.5129336 ,\n",
              "        -0.5105363 ,  0.4508339 ,  0.5206838 ,  0.44973698,  0.46279427,\n",
              "        -0.52585983,  0.4742832 , -0.54063296, -0.4400117 , -0.47340456,\n",
              "        -0.5213993 ,  0.518715  , -0.4561278 ,  0.46286616, -0.5306592 ,\n",
              "         0.4800219 , -0.47448698, -0.44549564, -0.493056  ,  0.43463418,\n",
              "         0.44705617,  0.519178  ,  0.47098786, -0.44869715,  0.51111865],\n",
              "       dtype=float32),\n",
              " 'broad-based': array([ 0.45950142,  0.4221019 , -0.45487085, -0.51263875, -0.5290373 ,\n",
              "         0.49576354,  0.49531722, -0.48573026, -0.46304876, -0.49338737,\n",
              "        -0.49734798,  0.4688115 ,  0.51708704,  0.4910881 , -0.53649974,\n",
              "        -0.4996165 ,  0.44531545,  0.5103356 , -0.43438476, -0.50883776,\n",
              "        -0.481047  ,  0.5030878 ,  0.45241457,  0.45319432, -0.4599071 ,\n",
              "        -0.45069975,  0.48386464,  0.53821325,  0.46597826,  0.454936  ,\n",
              "        -0.44356838,  0.512742  , -0.5342072 , -0.46605337, -0.4871717 ,\n",
              "        -0.47056404,  0.51341105, -0.4494877 ,  0.46919695, -0.44214556,\n",
              "         0.47380668, -0.5181308 , -0.4893741 , -0.43816632,  0.48491356,\n",
              "         0.51714873,  0.45505145,  0.44140092, -0.51285493,  0.46019167],\n",
              "       dtype=float32),\n",
              " 'savings': array([ 0.47196674,  0.43113616, -0.4696683 , -0.45365846, -0.47570425,\n",
              "         0.48196587,  0.45283714, -0.4339676 , -0.4517367 , -0.5015241 ,\n",
              "        -0.49309388,  0.48143685,  0.51286364,  0.44477844, -0.48831168,\n",
              "        -0.4780268 ,  0.4630478 ,  0.48019668, -0.44971177, -0.42736655,\n",
              "        -0.43627274,  0.44973356,  0.4795857 ,  0.50049895, -0.43092158,\n",
              "        -0.45434904,  0.43696913,  0.48385525,  0.47142553,  0.45666167,\n",
              "        -0.5234953 ,  0.4784754 , -0.49639314, -0.44395304, -0.42781124,\n",
              "        -0.4424945 ,  0.4870917 , -0.4856062 ,  0.4634428 , -0.5113585 ,\n",
              "         0.44794628, -0.5039676 , -0.43122986, -0.47633323,  0.4631893 ,\n",
              "         0.48560113,  0.47916085,  0.4521681 , -0.47099462,  0.49155533],\n",
              "       dtype=float32),\n",
              " 'per': array([ 0.53075886,  0.47314593, -0.497815  , -0.52532303, -0.5317727 ,\n",
              "         0.47375408,  0.47945294, -0.44370645, -0.43727934, -0.51792383,\n",
              "        -0.43695843,  0.5048508 ,  0.45247102,  0.46529716, -0.51508415,\n",
              "        -0.49157766,  0.51646036,  0.51252484, -0.4647702 , -0.47714072,\n",
              "        -0.507655  ,  0.44423717,  0.47635758,  0.4456735 , -0.5186603 ,\n",
              "        -0.47169474,  0.46900752,  0.49400327,  0.4583099 ,  0.52330923,\n",
              "        -0.48363575,  0.4743282 , -0.48035085, -0.48550946, -0.4728413 ,\n",
              "        -0.49921483,  0.48352054, -0.462623  ,  0.49715728, -0.44033957,\n",
              "         0.45612788, -0.47001094, -0.4797132 , -0.47373357,  0.45734242,\n",
              "         0.4714416 ,  0.45082644,  0.5183393 , -0.4571759 ,  0.47899497],\n",
              "       dtype=float32),\n",
              " 'important': array([ 0.47029415,  0.46888006, -0.5072421 , -0.5328298 , -0.45692518,\n",
              "         0.5381007 ,  0.51822996, -0.44283256, -0.47267127, -0.50681984,\n",
              "        -0.49799603,  0.47731367,  0.52746516,  0.483947  , -0.5674967 ,\n",
              "        -0.4921969 ,  0.4920168 ,  0.53557175, -0.4647202 , -0.5043794 ,\n",
              "        -0.46844858,  0.5165971 ,  0.5120946 ,  0.49550512, -0.46023783,\n",
              "        -0.52052766,  0.4847631 ,  0.48108727,  0.51772773,  0.45937365,\n",
              "        -0.5162647 ,  0.45552945, -0.50006473, -0.46177542, -0.48626572,\n",
              "        -0.4539974 ,  0.46596164, -0.527578  ,  0.4899073 , -0.45760685,\n",
              "         0.45604396, -0.4940592 , -0.5180085 , -0.44408625,  0.47037187,\n",
              "         0.52406454,  0.5026001 ,  0.464018  , -0.5191121 ,  0.5204393 ],\n",
              "       dtype=float32),\n",
              " 'eaton': array([ 0.51598996,  0.4445542 , -0.4566868 , -0.5027202 , -0.49522966,\n",
              "         0.53217477,  0.46245307, -0.45720267, -0.43818492, -0.49263453,\n",
              "        -0.45313525,  0.47849104,  0.44874802,  0.5009077 , -0.5591632 ,\n",
              "        -0.50975466,  0.4708535 ,  0.43431675, -0.52198225, -0.5122018 ,\n",
              "        -0.5091289 ,  0.5004533 ,  0.44559604,  0.48738265, -0.5120389 ,\n",
              "        -0.47827375,  0.4958367 ,  0.53118896,  0.5044821 ,  0.5065085 ,\n",
              "        -0.4820122 ,  0.5037309 , -0.46667525, -0.5390732 , -0.5271909 ,\n",
              "        -0.47159356,  0.48102915, -0.50498354,  0.52200305, -0.46102953,\n",
              "         0.4987349 , -0.44611707, -0.49216133, -0.4533238 ,  0.48427668,\n",
              "         0.5197233 ,  0.4675276 ,  0.46476427, -0.52080286,  0.4892748 ],\n",
              "       dtype=float32),\n",
              " 'vance': array([ 0.48565987,  0.50984555, -0.5105655 , -0.47867015, -0.5057564 ,\n",
              "         0.50808537,  0.4912299 , -0.46772546, -0.492917  , -0.45558703,\n",
              "        -0.52560484,  0.47021455,  0.5326841 ,  0.52300435, -0.5031588 ,\n",
              "        -0.50281054,  0.5237812 ,  0.5260888 , -0.5244386 , -0.4836773 ,\n",
              "        -0.5133179 ,  0.46730733,  0.537454  ,  0.4725224 , -0.47123253,\n",
              "        -0.4995053 ,  0.44641787,  0.5489336 ,  0.48851216,  0.4765393 ,\n",
              "        -0.54068905,  0.5261245 , -0.4991569 , -0.4635179 , -0.4529224 ,\n",
              "        -0.45463884,  0.47684264, -0.44641584,  0.508981  , -0.48662335,\n",
              "         0.5130804 , -0.5426961 , -0.44243664, -0.48323926,  0.46125802,\n",
              "         0.45604908,  0.46604258,  0.50086516, -0.51441306,  0.46956527],\n",
              "       dtype=float32),\n",
              " 'basis': array([ 0.5124326 ,  0.48610198, -0.44982028, -0.4635385 , -0.44407213,\n",
              "         0.52410644,  0.46979338, -0.4830073 , -0.49174106, -0.44558465,\n",
              "        -0.48293424,  0.5101994 ,  0.46102434,  0.46634507, -0.48555088,\n",
              "        -0.47210518,  0.5154422 ,  0.50584805, -0.5116616 , -0.42960066,\n",
              "        -0.4921676 ,  0.47976494,  0.4684777 ,  0.45251498, -0.4656514 ,\n",
              "        -0.5091393 ,  0.5021247 ,  0.4704041 ,  0.51864415,  0.45749825,\n",
              "        -0.49660155,  0.4725491 , -0.49739674, -0.51208556, -0.50253075,\n",
              "        -0.5007062 ,  0.4528761 , -0.444994  ,  0.51568526, -0.45413232,\n",
              "         0.5169006 , -0.5187867 , -0.49723116, -0.52015924,  0.44033456,\n",
              "         0.45615268,  0.4836983 ,  0.4394126 , -0.48927358,  0.5131903 ],\n",
              "       dtype=float32),\n",
              " 'accounting': array([ 0.4999566 ,  0.43647152, -0.4668878 , -0.4223342 , -0.5168483 ,\n",
              "         0.44291082,  0.42633036, -0.5093205 , -0.4630852 , -0.48887837,\n",
              "        -0.46927682,  0.46457437,  0.4411888 ,  0.48922542, -0.47267863,\n",
              "        -0.5090362 ,  0.45990074,  0.42343083, -0.45203674, -0.43602645,\n",
              "        -0.42243356,  0.43310666,  0.50181854,  0.46459684, -0.4605743 ,\n",
              "        -0.4846233 ,  0.49224216,  0.51408136,  0.46757543,  0.4265126 ,\n",
              "        -0.4437457 ,  0.4961387 , -0.51699954, -0.44487745, -0.46757397,\n",
              "        -0.44513962,  0.50433916, -0.5087981 ,  0.44800285, -0.44844908,\n",
              "         0.48913983, -0.45030493, -0.47728756, -0.50343835,  0.4586339 ,\n",
              "         0.42788312,  0.48208338,  0.5064849 , -0.5044735 ,  0.45005232],\n",
              "       dtype=float32),\n",
              " 'agreements': array([ 0.47085506,  0.48632628, -0.49864596, -0.5127307 , -0.50985813,\n",
              "         0.49861947,  0.4352789 , -0.44828102, -0.47863492, -0.4286933 ,\n",
              "        -0.45776337,  0.47794968,  0.43747592,  0.45185128, -0.5363213 ,\n",
              "        -0.46825495,  0.47663888,  0.49516514, -0.43157145, -0.4276589 ,\n",
              "        -0.489907  ,  0.45764044,  0.45234942,  0.4597785 , -0.4309345 ,\n",
              "        -0.46057555,  0.44467187,  0.5107067 ,  0.48761714,  0.432089  ,\n",
              "        -0.45117894,  0.43567273, -0.52949363, -0.48506457, -0.44604483,\n",
              "        -0.4635959 ,  0.46439523, -0.51181376,  0.49822813, -0.4739343 ,\n",
              "         0.47959027, -0.47671086, -0.42981625, -0.49512443,  0.5012582 ,\n",
              "         0.44355893,  0.43560323,  0.44818667, -0.4765711 ,  0.4624902 ],\n",
              "       dtype=float32),\n",
              " 'limitation': array([ 0.49575594,  0.44086185, -0.46438712, -0.47350812, -0.44985852,\n",
              "         0.48443002,  0.51242834, -0.524023  , -0.5203257 , -0.49374935,\n",
              "        -0.46973345,  0.52235687,  0.49816447,  0.5035559 , -0.5197301 ,\n",
              "        -0.46749085,  0.5356602 ,  0.5482861 , -0.46044102, -0.4938472 ,\n",
              "        -0.5005498 ,  0.48472333,  0.46697336,  0.50404507, -0.49585044,\n",
              "        -0.44591096,  0.4738075 ,  0.4934985 ,  0.52598464,  0.45029092,\n",
              "        -0.5492668 ,  0.4571998 , -0.48605573, -0.49700034, -0.53488535,\n",
              "        -0.5470483 ,  0.4907768 , -0.44924158,  0.45454153, -0.47606367,\n",
              "         0.54800844, -0.48698378, -0.45424774, -0.5415735 ,  0.4440512 ,\n",
              "         0.4506302 ,  0.5176646 ,  0.45064452, -0.5152485 ,  0.51527643],\n",
              "       dtype=float32),\n",
              " 'amounts': array([ 0.45234534,  0.4384835 , -0.4365012 , -0.50670797, -0.49951285,\n",
              "         0.4486664 ,  0.47044057, -0.46606836, -0.5087212 , -0.5038533 ,\n",
              "        -0.47489956,  0.51101226,  0.44506687,  0.5143026 , -0.55309254,\n",
              "        -0.4695995 ,  0.49636513,  0.44339502, -0.5075074 , -0.48154607,\n",
              "        -0.44121015,  0.4638132 ,  0.483883  ,  0.50039214, -0.43952906,\n",
              "        -0.43659973,  0.4292408 ,  0.5335246 ,  0.4291034 ,  0.4507023 ,\n",
              "        -0.4464085 ,  0.48906675, -0.5188276 , -0.4318727 , -0.47215486,\n",
              "        -0.5056031 ,  0.46030053, -0.49358138,  0.44262624, -0.42793408,\n",
              "         0.5181363 , -0.44988874, -0.45507225, -0.48555395,  0.50957376,\n",
              "         0.4718355 ,  0.4526015 ,  0.48443127, -0.50441974,  0.4263413 ],\n",
              "       dtype=float32),\n",
              " 'delivery': array([ 0.53791004,  0.50588924, -0.4323521 , -0.47926006, -0.47437087,\n",
              "         0.44670394,  0.5079664 , -0.4425407 , -0.48061314, -0.49408814,\n",
              "        -0.48843575,  0.47075137,  0.5033022 ,  0.44885138, -0.48621264,\n",
              "        -0.47155955,  0.51043415,  0.43802407, -0.4258109 , -0.45866656,\n",
              "        -0.45638093,  0.49041784,  0.44922364,  0.46419477, -0.46200472,\n",
              "        -0.4740451 ,  0.46889353,  0.47811484,  0.4551814 ,  0.43113014,\n",
              "        -0.51300037,  0.43223026, -0.48990667, -0.5198201 , -0.44270542,\n",
              "        -0.45082542,  0.5030441 , -0.5125682 ,  0.4497168 , -0.45529273,\n",
              "         0.5184668 , -0.4926247 , -0.49756703, -0.43677786,  0.47000062,\n",
              "         0.51954406,  0.4991812 ,  0.44229564, -0.4597933 ,  0.45477226],\n",
              "       dtype=float32),\n",
              " 'treasury': array([ 0.526462  ,  0.5140868 , -0.48610187, -0.45662656, -0.53900766,\n",
              "         0.48303753,  0.5184972 , -0.48456323, -0.4660474 , -0.45022646,\n",
              "        -0.48436764,  0.49063742,  0.51725554,  0.45706013, -0.48201114,\n",
              "        -0.45874202,  0.53348196,  0.492772  , -0.49614653, -0.5095546 ,\n",
              "        -0.48755097,  0.46267974,  0.48490357,  0.48731494, -0.4380586 ,\n",
              "        -0.5195591 ,  0.45242053,  0.5209487 ,  0.50530136,  0.42802978,\n",
              "        -0.5267699 ,  0.53367114, -0.48910657, -0.47223267, -0.4871259 ,\n",
              "        -0.4513485 ,  0.4987278 , -0.51398987,  0.48177212, -0.52002496,\n",
              "         0.441098  , -0.4643462 , -0.44986093, -0.5164383 ,  0.48605812,\n",
              "         0.46074674,  0.49508822,  0.50232005, -0.44612426,  0.44832546],\n",
              "       dtype=float32),\n",
              " 'third': array([ 0.5494412 ,  0.47166023, -0.4426807 , -0.45706415, -0.5163497 ,\n",
              "         0.5055592 ,  0.45393783, -0.4820425 , -0.5188065 , -0.4396857 ,\n",
              "        -0.4563772 ,  0.46531323,  0.46803084,  0.4340032 , -0.5388576 ,\n",
              "        -0.47639018,  0.4568859 ,  0.4911372 , -0.43720195, -0.43576545,\n",
              "        -0.44562396,  0.51211977,  0.52430224,  0.47075105, -0.47502333,\n",
              "        -0.4513254 ,  0.52403164,  0.50937736,  0.44410044,  0.46660203,\n",
              "        -0.476659  ,  0.45700854, -0.5051477 , -0.4784067 , -0.48301917,\n",
              "        -0.49399716,  0.43563592, -0.44242138,  0.4393667 , -0.45262736,\n",
              "         0.46158653, -0.47528538, -0.46944577, -0.48981178,  0.46087188,\n",
              "         0.49633178,  0.4885257 ,  0.5135784 , -0.4762707 ,  0.4301057 ],\n",
              "       dtype=float32),\n",
              " 'party': array([ 0.5001048 ,  0.45192537, -0.44546497, -0.4935521 , -0.4827605 ,\n",
              "         0.51182765,  0.5304596 , -0.45799348, -0.46549773, -0.45831072,\n",
              "        -0.5100848 ,  0.47794968,  0.4276079 ,  0.48803627, -0.48176473,\n",
              "        -0.50107634,  0.48681885,  0.45230663, -0.46683437, -0.4989367 ,\n",
              "        -0.5039403 ,  0.5129446 ,  0.44962907,  0.43642485, -0.4431048 ,\n",
              "        -0.48228526,  0.48668292,  0.45259687,  0.43659654,  0.4852152 ,\n",
              "        -0.52780765,  0.46921396, -0.49840152, -0.45195043, -0.44748816,\n",
              "        -0.5097603 ,  0.4670699 , -0.4499941 ,  0.47802985, -0.43753827,\n",
              "         0.47404116, -0.46477243, -0.42851543, -0.48753932,  0.47695664,\n",
              "         0.47433218,  0.50433725,  0.4870809 , -0.42389688,  0.4351938 ],\n",
              "       dtype=float32),\n",
              " 'primary': array([ 0.50522435,  0.43469203, -0.504725  , -0.5202101 , -0.52527726,\n",
              "         0.49169943,  0.45563942, -0.5160773 , -0.44193286, -0.4701657 ,\n",
              "        -0.5132768 ,  0.47153795,  0.5058419 ,  0.50615185, -0.52163404,\n",
              "        -0.5021139 ,  0.5194107 ,  0.4695517 , -0.522714  , -0.4941246 ,\n",
              "        -0.460546  ,  0.4704603 ,  0.5473874 ,  0.50991225, -0.46034253,\n",
              "        -0.45445466,  0.44699097,  0.58202547,  0.5026559 ,  0.46846366,\n",
              "        -0.5500722 ,  0.44974062, -0.48681322, -0.47951907, -0.4490437 ,\n",
              "        -0.5475219 ,  0.53640646, -0.45316276,  0.48274451, -0.51957107,\n",
              "         0.4866184 , -0.46944606, -0.4929706 , -0.44784537,  0.48242822,\n",
              "         0.4799199 ,  0.5071829 ,  0.45428193, -0.51978034,  0.49066293],\n",
              "       dtype=float32),\n",
              " 'currently': array([ 0.5305721 ,  0.41572943, -0.47168615, -0.44768754, -0.503912  ,\n",
              "         0.5141947 ,  0.47836837, -0.4432177 , -0.50149393, -0.44244492,\n",
              "        -0.45714796,  0.46017873,  0.44442618,  0.45281565, -0.48589444,\n",
              "        -0.4350368 ,  0.46364555,  0.5101647 , -0.49666023, -0.46026355,\n",
              "        -0.47139233,  0.44939184,  0.5020506 ,  0.4945564 , -0.48386294,\n",
              "        -0.4424314 ,  0.45046523,  0.5162051 ,  0.4570718 ,  0.46146607,\n",
              "        -0.49580052,  0.4645939 , -0.51103866, -0.5102521 , -0.43844628,\n",
              "        -0.5084586 ,  0.5056244 , -0.48334783,  0.4303358 , -0.43272105,\n",
              "         0.5098829 , -0.47328046, -0.50933284, -0.43382952,  0.45290643,\n",
              "         0.45118967,  0.46394068,  0.44781402, -0.4703057 ,  0.4974537 ],\n",
              "       dtype=float32),\n",
              " 'broadly': array([ 0.46285492,  0.44513112, -0.48320168, -0.4898213 , -0.47865686,\n",
              "         0.46285474,  0.43988946, -0.43694338, -0.4874403 , -0.46625617,\n",
              "        -0.51550245,  0.4422825 ,  0.4981411 ,  0.49274406, -0.5015825 ,\n",
              "        -0.47769505,  0.5230745 ,  0.4991297 , -0.51158816, -0.41972858,\n",
              "        -0.46785367,  0.48789504,  0.52323407,  0.4911016 , -0.46066704,\n",
              "        -0.44956478,  0.44561118,  0.45547464,  0.43993437,  0.46834874,\n",
              "        -0.4761594 ,  0.50466514, -0.467235  , -0.50988954, -0.47503975,\n",
              "        -0.45579308,  0.5130093 , -0.4989336 ,  0.45558524, -0.48997903,\n",
              "         0.4774936 , -0.5176497 , -0.49718595, -0.48229623,  0.4315217 ,\n",
              "         0.44936374,  0.4756008 ,  0.5010724 , -0.48122868,  0.47118428],\n",
              "       dtype=float32),\n",
              " 'additionally': array([ 0.5221175 ,  0.43509957, -0.4762275 , -0.49785465, -0.4655569 ,\n",
              "         0.48171037,  0.5221841 , -0.48619807, -0.4453291 , -0.5070975 ,\n",
              "        -0.50529474,  0.45826322,  0.50633806,  0.49786615, -0.5245549 ,\n",
              "        -0.4562778 ,  0.51989967,  0.5193419 , -0.5077055 , -0.44189072,\n",
              "        -0.49270448,  0.45999593,  0.5255864 ,  0.43607828, -0.46214032,\n",
              "        -0.4398149 ,  0.46837643,  0.48839307,  0.4911325 ,  0.4583446 ,\n",
              "        -0.48121345,  0.515528  , -0.51874787, -0.44340605, -0.43248773,\n",
              "        -0.466957  ,  0.48439726, -0.46694088,  0.4452772 , -0.4792482 ,\n",
              "         0.47242922, -0.45842007, -0.5159574 , -0.46711135,  0.47313434,\n",
              "         0.46694684,  0.48339996,  0.51001406, -0.43643156,  0.47265366],\n",
              "       dtype=float32),\n",
              " 'public': array([ 0.5194527 ,  0.41593072, -0.4523821 , -0.4425377 , -0.4793598 ,\n",
              "         0.47932547,  0.48559016, -0.5056117 , -0.46996558, -0.45783982,\n",
              "        -0.4682627 ,  0.44095963,  0.43875974,  0.4322231 , -0.52931595,\n",
              "        -0.5124085 ,  0.5011276 ,  0.49251083, -0.46011052, -0.48581678,\n",
              "        -0.44568756,  0.4192381 ,  0.49102986,  0.49096432, -0.48958272,\n",
              "        -0.4302261 ,  0.47796386,  0.4481416 ,  0.46268573,  0.48457393,\n",
              "        -0.46608245,  0.46137854, -0.45910984, -0.47683215, -0.457069  ,\n",
              "        -0.42981985,  0.49908698, -0.49069786,  0.44461134, -0.4289201 ,\n",
              "         0.47612667, -0.46955624, -0.46760246, -0.4491245 ,  0.51424897,\n",
              "         0.4623807 ,  0.46443352,  0.43491173, -0.4367297 ,  0.48738122],\n",
              "       dtype=float32),\n",
              " 'months': array([ 0.4497433 ,  0.43942353, -0.4219954 , -0.45375252, -0.4552448 ,\n",
              "         0.4565873 ,  0.48518026, -0.5034674 , -0.44917995, -0.48405233,\n",
              "        -0.4693671 ,  0.49096635,  0.433067  ,  0.49626672, -0.5400884 ,\n",
              "        -0.50829583,  0.4916067 ,  0.4657808 , -0.42682362, -0.4727782 ,\n",
              "        -0.4552504 ,  0.46683893,  0.45867807,  0.4259553 , -0.42293766,\n",
              "        -0.43335715,  0.4461072 ,  0.43660393,  0.47367215,  0.4889934 ,\n",
              "        -0.43080097,  0.42320204, -0.5146983 , -0.47236395, -0.4212365 ,\n",
              "        -0.49947813,  0.49066788, -0.50182414,  0.47787294, -0.5067681 ,\n",
              "         0.46614772, -0.5111466 , -0.49845177, -0.4831935 ,  0.44351223,\n",
              "         0.4786682 ,  0.4674876 ,  0.4940753 , -0.44000265,  0.48588514],\n",
              "       dtype=float32),\n",
              " 'extended': array([ 0.5239725 ,  0.46224636, -0.46713418, -0.45898613, -0.48690873,\n",
              "         0.52167416,  0.45726782, -0.4505183 , -0.43662128, -0.4621611 ,\n",
              "        -0.49931616,  0.47514883,  0.50078356,  0.48628414, -0.5023526 ,\n",
              "        -0.4933212 ,  0.51128924,  0.47019342, -0.46020162, -0.4335223 ,\n",
              "        -0.43732876,  0.43872157,  0.45308682,  0.5091213 , -0.4304565 ,\n",
              "        -0.44607252,  0.5023969 ,  0.49494576,  0.48416752,  0.50235254,\n",
              "        -0.4808727 ,  0.43537158, -0.4594564 , -0.48313338, -0.43520778,\n",
              "        -0.44923174,  0.51706105, -0.46141493,  0.42378643, -0.4599713 ,\n",
              "         0.44906124, -0.51027435, -0.47674862, -0.4749577 ,  0.47906628,\n",
              "         0.4991641 ,  0.50887096,  0.505567  , -0.4561983 ,  0.49374092],\n",
              "       dtype=float32),\n",
              " 'fmr': array([ 0.5222313 ,  0.4198346 , -0.4805423 , -0.48926958, -0.5244003 ,\n",
              "         0.45991167,  0.5047741 , -0.5146085 , -0.45206562, -0.44055924,\n",
              "        -0.5021896 ,  0.52141225,  0.42726827,  0.49124712, -0.47202736,\n",
              "        -0.4317161 ,  0.43649986,  0.52152973, -0.4979657 , -0.445164  ,\n",
              "        -0.4878437 ,  0.43527424,  0.51216084,  0.47336468, -0.50716305,\n",
              "        -0.49142152,  0.49967548,  0.49162427,  0.46075124,  0.514118  ,\n",
              "        -0.48390007,  0.48717445, -0.49288854, -0.49634543, -0.5105159 ,\n",
              "        -0.5241822 ,  0.49403128, -0.4543783 ,  0.46368092, -0.433984  ,\n",
              "         0.44006765, -0.52117425, -0.45676303, -0.4432117 ,  0.47301903,\n",
              "         0.44441178,  0.4545142 ,  0.5181851 , -0.43028468,  0.47748458],\n",
              "       dtype=float32),\n",
              " 'low': array([ 0.5482046 ,  0.43598795, -0.48755574, -0.5240706 , -0.4532623 ,\n",
              "         0.45626476,  0.5179296 , -0.47660547, -0.4870777 , -0.5121593 ,\n",
              "        -0.4882402 ,  0.4491386 ,  0.49912423,  0.46280593, -0.48491013,\n",
              "        -0.47550166,  0.48684862,  0.45078993, -0.48281813, -0.4853407 ,\n",
              "        -0.5182627 ,  0.44678345,  0.46783096,  0.4878323 , -0.4514643 ,\n",
              "        -0.49745858,  0.48426735,  0.45727322,  0.4358912 ,  0.49473304,\n",
              "        -0.46712205,  0.46855044, -0.53485954, -0.47234654, -0.4619587 ,\n",
              "        -0.46276885,  0.51276475, -0.46023607,  0.5045788 , -0.49449366,\n",
              "         0.49340057, -0.5058623 , -0.51264524, -0.44683218,  0.47707176,\n",
              "         0.5167316 ,  0.47110605,  0.46180463, -0.44532335,  0.4582314 ],\n",
              "       dtype=float32),\n",
              " 'trust': array([ 0.52972513,  0.4430546 , -0.52217   , -0.44907287, -0.4544355 ,\n",
              "         0.5106714 ,  0.43376544, -0.5179611 , -0.4578915 , -0.48970497,\n",
              "        -0.50297505,  0.49489987,  0.43462077,  0.51174176, -0.50104785,\n",
              "        -0.45225617,  0.5122144 ,  0.52220064, -0.5132368 , -0.44217378,\n",
              "        -0.5078415 ,  0.45821106,  0.45507956,  0.50757456, -0.48618358,\n",
              "        -0.49092624,  0.4461459 ,  0.4757967 ,  0.514243  ,  0.45911202,\n",
              "        -0.50847566,  0.44468486, -0.49235833, -0.50909483, -0.44537643,\n",
              "        -0.5167669 ,  0.44088495, -0.48797762,  0.45147967, -0.4807588 ,\n",
              "         0.5258677 , -0.4416667 , -0.4970558 , -0.44970605,  0.4494414 ,\n",
              "         0.4883797 ,  0.46110487,  0.490891  , -0.52136254,  0.5026582 ],\n",
              "       dtype=float32),\n",
              " 'i.e.': array([ 0.49842373,  0.46625763, -0.47959304, -0.5188445 , -0.52119327,\n",
              "         0.4604779 ,  0.4394551 , -0.52539307, -0.48384044, -0.45603088,\n",
              "        -0.46520713,  0.4573975 ,  0.44820505,  0.52301687, -0.5584116 ,\n",
              "        -0.4441931 ,  0.498153  ,  0.4560914 , -0.46291083, -0.49246654,\n",
              "        -0.48269883,  0.4677571 ,  0.5004023 ,  0.50466484, -0.44225436,\n",
              "        -0.43954536,  0.44557983,  0.4891461 ,  0.48769718,  0.5022998 ,\n",
              "        -0.4680791 ,  0.48387045, -0.46807742, -0.49216834, -0.4756853 ,\n",
              "        -0.49703813,  0.5332296 , -0.4587033 ,  0.5239617 , -0.5326112 ,\n",
              "         0.5069317 , -0.5197447 , -0.44123966, -0.5204677 ,  0.52266556,\n",
              "         0.4475013 ,  0.52350366,  0.51575255, -0.4482387 ,  0.51287216],\n",
              "       dtype=float32),\n",
              " 'measures': array([ 0.48671412,  0.45609716, -0.455717  , -0.5046589 , -0.47862142,\n",
              "         0.5185058 ,  0.46163407, -0.44983992, -0.44081652, -0.4301942 ,\n",
              "        -0.43777865,  0.51138747,  0.4419295 ,  0.48095816, -0.5211681 ,\n",
              "        -0.5104581 ,  0.5304686 ,  0.4766561 , -0.47837883, -0.44049233,\n",
              "        -0.46456268,  0.50492686,  0.50334847,  0.492659  , -0.4334145 ,\n",
              "        -0.49476084,  0.46496662,  0.516327  ,  0.49498823,  0.5001241 ,\n",
              "        -0.4881221 ,  0.43518764, -0.5156001 , -0.50466305, -0.50849986,\n",
              "        -0.5288313 ,  0.51191515, -0.4682125 ,  0.49435687, -0.5223379 ,\n",
              "         0.49585688, -0.48107392, -0.5033495 , -0.45159653,  0.48425996,\n",
              "         0.49049434,  0.48640445,  0.4748273 , -0.49353573,  0.43898055],\n",
              "       dtype=float32),\n",
              " 'holds': array([ 0.47027242,  0.43847057, -0.49051732, -0.48167878, -0.47603497,\n",
              "         0.5162166 ,  0.46090874, -0.45321256, -0.4486526 , -0.49576682,\n",
              "        -0.44499424,  0.5197831 ,  0.4921453 ,  0.5054943 , -0.54022676,\n",
              "        -0.4703401 ,  0.44731528,  0.4662806 , -0.48872116, -0.47681776,\n",
              "        -0.47813255,  0.49942806,  0.5183849 ,  0.4296086 , -0.4922117 ,\n",
              "        -0.46074438,  0.4456439 ,  0.5361576 ,  0.46177763,  0.45925325,\n",
              "        -0.46979582,  0.4596986 , -0.4853442 , -0.52020794, -0.5080756 ,\n",
              "        -0.4621018 ,  0.44051495, -0.5120981 ,  0.48937842, -0.48579115,\n",
              "         0.50923723, -0.4601516 , -0.50585026, -0.46877006,  0.47669974,\n",
              "         0.46201444,  0.4503505 ,  0.5139779 , -0.4404226 ,  0.46999013],\n",
              "       dtype=float32),\n",
              " 'existing': array([ 0.46770924,  0.43042856, -0.4695585 , -0.46691048, -0.47099704,\n",
              "         0.4994153 ,  0.43607423, -0.47897476, -0.47981682, -0.5187362 ,\n",
              "        -0.48265615,  0.48016554,  0.5061735 ,  0.52024364, -0.47520426,\n",
              "        -0.46148574,  0.5097189 ,  0.46157742, -0.51496863, -0.49681938,\n",
              "        -0.45755923,  0.4662247 ,  0.4621683 ,  0.47056615, -0.48381087,\n",
              "        -0.46101916,  0.5021475 ,  0.52060103,  0.44823265,  0.44643196,\n",
              "        -0.4580273 ,  0.4874239 , -0.51810205, -0.45018625, -0.4695496 ,\n",
              "        -0.4988952 ,  0.46941152, -0.4855313 ,  0.5049638 , -0.5085403 ,\n",
              "         0.49969143, -0.45330727, -0.46753106, -0.48319218,  0.44393897,\n",
              "         0.5080722 ,  0.508852  ,  0.47415096, -0.5009221 ,  0.50653   ],\n",
              "       dtype=float32),\n",
              " 'delays': array([ 0.4955152 ,  0.45278555, -0.5122911 , -0.4548319 , -0.4785956 ,\n",
              "         0.4765164 ,  0.47036576, -0.5089108 , -0.5057633 , -0.48673832,\n",
              "        -0.46661723,  0.50777566,  0.47658038,  0.5251037 , -0.55483073,\n",
              "        -0.43183586,  0.46506906,  0.46787345, -0.5235059 , -0.4942425 ,\n",
              "        -0.45442164,  0.5114601 ,  0.4884572 ,  0.49627584, -0.5215939 ,\n",
              "        -0.49325854,  0.49357194,  0.4942987 ,  0.4931717 ,  0.48463023,\n",
              "        -0.53406954,  0.48885012, -0.4841502 , -0.47859314, -0.5064696 ,\n",
              "        -0.4606722 ,  0.51665103, -0.4652247 ,  0.44912168, -0.50116694,\n",
              "         0.4969644 , -0.50263774, -0.48746336, -0.48815057,  0.4448064 ,\n",
              "         0.4987861 ,  0.49688226,  0.4426576 , -0.4843199 ,  0.48967552],\n",
              "       dtype=float32),\n",
              " 'represent': array([ 0.4754682 ,  0.43525407, -0.5285557 , -0.48967633, -0.48954096,\n",
              "         0.4591377 ,  0.4945359 , -0.4742528 , -0.4954875 , -0.44489703,\n",
              "        -0.4961257 ,  0.44903427,  0.4897058 ,  0.4669641 , -0.55369014,\n",
              "        -0.53361905,  0.46103683,  0.47649032, -0.43929946, -0.49300867,\n",
              "        -0.45894098,  0.4784881 ,  0.47017467,  0.44279864, -0.4474968 ,\n",
              "        -0.48638612,  0.49889928,  0.4712309 ,  0.44682753,  0.47621176,\n",
              "        -0.45656186,  0.4930111 , -0.46676347, -0.48981825, -0.48184538,\n",
              "        -0.45155928,  0.5173392 , -0.53463614,  0.5275339 , -0.49241278,\n",
              "         0.4524789 , -0.5276213 , -0.44904232, -0.49359113,  0.4837282 ,\n",
              "         0.50911593,  0.4460441 ,  0.5104317 , -0.5173137 ,  0.5214037 ],\n",
              "       dtype=float32),\n",
              " 'worst': array([ 0.53249604,  0.49692118, -0.48399344, -0.44297555, -0.51512736,\n",
              "         0.46254846,  0.4788513 , -0.51463866, -0.44558042, -0.4587363 ,\n",
              "        -0.44457927,  0.4584275 ,  0.5012642 ,  0.4458917 , -0.49563536,\n",
              "        -0.43424723,  0.4375992 ,  0.50742054, -0.4588239 , -0.45746508,\n",
              "        -0.48956013,  0.46552426,  0.48440504,  0.46863735, -0.4366854 ,\n",
              "        -0.49474517,  0.4303642 ,  0.4958103 ,  0.46520337,  0.48864153,\n",
              "        -0.44344848,  0.4922206 , -0.46447796, -0.47799355, -0.46011344,\n",
              "        -0.5124931 ,  0.5081178 , -0.43499616,  0.44215715, -0.5208013 ,\n",
              "         0.44974932, -0.5148839 , -0.43696043, -0.45899227,  0.4840983 ,\n",
              "         0.46331847,  0.514686  ,  0.49002647, -0.43958497,  0.51008713],\n",
              "       dtype=float32),\n",
              " 'means': array([ 0.4924936 ,  0.5061009 , -0.47843733, -0.50645614, -0.53056926,\n",
              "         0.44609132,  0.4928509 , -0.47979742, -0.44076157, -0.50080615,\n",
              "        -0.45026502,  0.47357315,  0.4691065 ,  0.45286515, -0.49481565,\n",
              "        -0.4658197 ,  0.44295958,  0.44470397, -0.46467695, -0.49568364,\n",
              "        -0.51293933,  0.48684204,  0.46158347,  0.48035014, -0.43413436,\n",
              "        -0.45379466,  0.47520465,  0.47124794,  0.4694469 ,  0.4517454 ,\n",
              "        -0.45611712,  0.4259584 , -0.5027365 , -0.48491994, -0.49337006,\n",
              "        -0.43867657,  0.5006105 , -0.47760016,  0.4903994 , -0.44885623,\n",
              "         0.4586422 , -0.49632543, -0.5073835 , -0.49434736,  0.52153987,\n",
              "         0.49442768,  0.4336733 ,  0.48661712, -0.5100251 ,  0.4492298 ],\n",
              "       dtype=float32),\n",
              " 'dependent': array([ 0.4632159 ,  0.47636408, -0.4593163 , -0.444103  , -0.4653318 ,\n",
              "         0.48740968,  0.43098992, -0.43517664, -0.49341267, -0.44713548,\n",
              "        -0.48843914,  0.42630088,  0.43723992,  0.46576324, -0.46867883,\n",
              "        -0.5031513 ,  0.50342107,  0.50899583, -0.48130623, -0.45810175,\n",
              "        -0.44227874,  0.4758303 ,  0.5098099 ,  0.46202707, -0.4509129 ,\n",
              "        -0.44767457,  0.4858193 ,  0.5266684 ,  0.45763496,  0.48375466,\n",
              "        -0.48370758,  0.46030053, -0.4993864 , -0.4613043 , -0.47458366,\n",
              "        -0.50371444,  0.5146244 , -0.47089887,  0.5058094 , -0.43223768,\n",
              "         0.50245196, -0.43734297, -0.47317648, -0.49584076,  0.46351284,\n",
              "         0.47021312,  0.46291316,  0.47709322, -0.50755584,  0.48490703],\n",
              "       dtype=float32),\n",
              " 'administrative': array([ 0.46680897,  0.45108375, -0.49350783, -0.46481964, -0.52568007,\n",
              "         0.5170119 ,  0.44744074, -0.4876957 , -0.45403996, -0.45780817,\n",
              "        -0.4684872 ,  0.44098425,  0.44323894,  0.4423076 , -0.49184033,\n",
              "        -0.5094907 ,  0.45178303,  0.51463354, -0.49335274, -0.4593089 ,\n",
              "        -0.4300574 ,  0.4411603 ,  0.47175512,  0.429124  , -0.4658713 ,\n",
              "        -0.4342249 ,  0.46893135,  0.5053994 ,  0.46648243,  0.45104387,\n",
              "        -0.5046561 ,  0.44678777, -0.50870466, -0.4649661 , -0.43945605,\n",
              "        -0.44358814,  0.4867973 , -0.4510429 ,  0.42756805, -0.5135728 ,\n",
              "         0.48644355, -0.5241575 , -0.44995558, -0.49086285,  0.4876475 ,\n",
              "         0.47948384,  0.46009654,  0.4406464 , -0.4608909 ,  0.4307329 ],\n",
              "       dtype=float32),\n",
              " 'focus': array([ 0.4972189 ,  0.41740632, -0.47266567, -0.43772438, -0.49524227,\n",
              "         0.52464604,  0.45866913, -0.4492626 , -0.44737098, -0.48404443,\n",
              "        -0.44110876,  0.5148719 ,  0.51888317,  0.4762882 , -0.5470648 ,\n",
              "        -0.45601335,  0.4585909 ,  0.49333075, -0.5087774 , -0.4938721 ,\n",
              "        -0.4887088 ,  0.42733672,  0.45978606,  0.49047765, -0.44109744,\n",
              "        -0.47692683,  0.44261244,  0.51762813,  0.5036713 ,  0.4301497 ,\n",
              "        -0.499139  ,  0.49613184, -0.4859024 , -0.45144823, -0.494366  ,\n",
              "        -0.43749475,  0.4490045 , -0.4774361 ,  0.48421612, -0.47847024,\n",
              "         0.46255994, -0.51717514, -0.42161086, -0.4732529 ,  0.49631408,\n",
              "         0.49293706,  0.4302397 ,  0.43148834, -0.5113535 ,  0.47159386],\n",
              "       dtype=float32),\n",
              " 'event': array([ 0.48348406,  0.4362884 , -0.46587944, -0.44247007, -0.454047  ,\n",
              "         0.45576102,  0.46975973, -0.43463874, -0.4283037 , -0.46480832,\n",
              "        -0.43943778,  0.50413185,  0.5032292 ,  0.45763782, -0.5117521 ,\n",
              "        -0.45864695,  0.49479616,  0.47420156, -0.44070736, -0.42423826,\n",
              "        -0.510731  ,  0.46454608,  0.4361477 ,  0.5028557 , -0.47968376,\n",
              "        -0.51012015,  0.4641411 ,  0.49684706,  0.5143228 ,  0.4393445 ,\n",
              "        -0.44209653,  0.4959069 , -0.45795313, -0.49566427, -0.4578175 ,\n",
              "        -0.471267  ,  0.4686859 , -0.48994812,  0.43600935, -0.46186396,\n",
              "         0.43526846, -0.48426843, -0.4938845 , -0.42801043,  0.47274667,\n",
              "         0.4644067 ,  0.45862854,  0.4767142 , -0.4545974 ,  0.4501128 ],\n",
              "       dtype=float32),\n",
              " 'support': array([ 0.52611405,  0.45075628, -0.48113889, -0.43718746, -0.5135617 ,\n",
              "         0.45595947,  0.5083997 , -0.51577055, -0.45653287, -0.4310174 ,\n",
              "        -0.44246733,  0.44721836,  0.4349883 ,  0.4572137 , -0.5261033 ,\n",
              "        -0.47058386,  0.50499034,  0.46500218, -0.49861053, -0.50120896,\n",
              "        -0.5071751 ,  0.4434246 ,  0.5131997 ,  0.48016232, -0.43504208,\n",
              "        -0.42858836,  0.42957303,  0.49920845,  0.42583913,  0.42274538,\n",
              "        -0.46618783,  0.4624546 , -0.50289285, -0.4959636 , -0.5058112 ,\n",
              "        -0.4919209 ,  0.43264082, -0.43366042,  0.5040708 , -0.518892  ,\n",
              "         0.51957214, -0.46486786, -0.43953732, -0.43430832,  0.500093  ,\n",
              "         0.500153  ,  0.45098865,  0.43789348, -0.4311221 ,  0.4369018 ],\n",
              "       dtype=float32),\n",
              " 'statement': array([ 0.5074942 ,  0.4408917 , -0.47012538, -0.51664656, -0.51068175,\n",
              "         0.5179755 ,  0.51709306, -0.5148436 , -0.4822904 , -0.4439227 ,\n",
              "        -0.4493497 ,  0.4589954 ,  0.44676685,  0.5010181 , -0.50542885,\n",
              "        -0.53114164,  0.45423463,  0.5020643 , -0.45269537, -0.4623996 ,\n",
              "        -0.5188551 ,  0.4684076 ,  0.4915341 ,  0.43098918, -0.4677417 ,\n",
              "        -0.48849997,  0.4459749 ,  0.47144258,  0.523621  ,  0.52036434,\n",
              "        -0.4762991 ,  0.5059066 , -0.50810975, -0.5217987 , -0.45027533,\n",
              "        -0.5061424 ,  0.49917266, -0.4487991 ,  0.5169002 , -0.5065538 ,\n",
              "         0.4951243 , -0.5193174 , -0.45384198, -0.5074354 ,  0.5108463 ,\n",
              "         0.51883477,  0.44316807,  0.4569426 , -0.46288013,  0.4807143 ],\n",
              "       dtype=float32),\n",
              " 'borrowings': array([ 0.4864446 ,  0.44589818, -0.5369261 , -0.5049212 , -0.46932214,\n",
              "         0.51559365,  0.5056161 , -0.47325984, -0.4706423 , -0.43973768,\n",
              "        -0.45273542,  0.52559775,  0.5139916 ,  0.4757289 , -0.521572  ,\n",
              "        -0.49984622,  0.45464435,  0.5395116 , -0.4365691 , -0.4590742 ,\n",
              "        -0.45624572,  0.48306262,  0.51284826,  0.4540674 , -0.47630435,\n",
              "        -0.52459204,  0.45597517,  0.5338071 ,  0.45796287,  0.47633833,\n",
              "        -0.531105  ,  0.48641163, -0.53587884, -0.45170096, -0.496857  ,\n",
              "        -0.49613842,  0.47650704, -0.5219873 ,  0.45298976, -0.48719195,\n",
              "         0.47533613, -0.48116654, -0.4443589 , -0.45942846,  0.5318137 ,\n",
              "         0.489509  ,  0.5142548 ,  0.49378556, -0.48364162,  0.47508687],\n",
              "       dtype=float32),\n",
              " 'participating': array([ 0.50112695,  0.45820013, -0.4508321 , -0.45361662, -0.48386383,\n",
              "         0.45912674,  0.5086496 , -0.5062935 , -0.48738006, -0.42392835,\n",
              "        -0.445477  ,  0.515894  ,  0.45258   ,  0.4403081 , -0.52928674,\n",
              "        -0.48086327,  0.4349561 ,  0.47465578, -0.48057806, -0.47846228,\n",
              "        -0.5127228 ,  0.48992872,  0.45199347,  0.42308715, -0.48258752,\n",
              "        -0.4875946 ,  0.49501377,  0.4870359 ,  0.5112223 ,  0.5118912 ,\n",
              "        -0.5252789 ,  0.42997262, -0.45144784, -0.45845866, -0.50221777,\n",
              "        -0.45934394,  0.5115581 , -0.47928917,  0.50701183, -0.5084331 ,\n",
              "         0.44277424, -0.47705635, -0.5069422 , -0.4509942 ,  0.46985352,\n",
              "         0.47507232,  0.51568455,  0.48243633, -0.4485806 ,  0.5089172 ],\n",
              "       dtype=float32),\n",
              " 'shorter': array([ 0.5418779 ,  0.44131127, -0.502873  , -0.5138407 , -0.5201248 ,\n",
              "         0.5134158 ,  0.4785791 , -0.47411737, -0.49543062, -0.44675404,\n",
              "        -0.43633342,  0.4450685 ,  0.48520905,  0.4904549 , -0.547444  ,\n",
              "        -0.46240783,  0.48004672,  0.47648132, -0.50680876, -0.49279082,\n",
              "        -0.4356874 ,  0.49589816,  0.52474815,  0.5121507 , -0.49052656,\n",
              "        -0.44604123,  0.4564743 ,  0.48731837,  0.46706954,  0.51211846,\n",
              "        -0.45941517,  0.44903582, -0.47954613, -0.5281485 , -0.46622974,\n",
              "        -0.49798822,  0.46297726, -0.47888243,  0.44067883, -0.5163888 ,\n",
              "         0.4399555 , -0.52741224, -0.48396185, -0.5034587 ,  0.43976805,\n",
              "         0.47408494,  0.44202268,  0.47367793, -0.50991774,  0.45744163],\n",
              "       dtype=float32),\n",
              " 'unpredictably': array([ 0.503653  ,  0.46344358, -0.43172526, -0.47456068, -0.4615199 ,\n",
              "         0.5144674 ,  0.49422675, -0.43444183, -0.4700202 , -0.46328124,\n",
              "        -0.49214506,  0.51041704,  0.49949545,  0.4546154 , -0.46669132,\n",
              "        -0.43969288,  0.4478619 ,  0.4931507 , -0.43009642, -0.4997515 ,\n",
              "        -0.5027678 ,  0.50783765,  0.4536179 ,  0.4667481 , -0.4183068 ,\n",
              "        -0.48842233,  0.47623357,  0.5156509 ,  0.4400982 ,  0.43860668,\n",
              "        -0.44568536,  0.420725  , -0.49978235, -0.4983857 , -0.44203433,\n",
              "        -0.49888316,  0.4708329 , -0.43852866,  0.46607375, -0.49052104,\n",
              "         0.453497  , -0.4848438 , -0.43318564, -0.47091737,  0.48140243,\n",
              "         0.48499227,  0.47502053,  0.43647277, -0.45128492,  0.4417734 ],\n",
              "       dtype=float32),\n",
              " 'commodities': array([ 0.53171194,  0.49981695, -0.43117625, -0.4400055 , -0.49578395,\n",
              "         0.45838407,  0.4766222 , -0.43219122, -0.44043696, -0.48874876,\n",
              "        -0.43245268,  0.50115377,  0.47177637,  0.45108643, -0.49346435,\n",
              "        -0.51010275,  0.48619223,  0.4283504 , -0.48548955, -0.47769752,\n",
              "        -0.45977554,  0.45000094,  0.46187356,  0.469973  , -0.4235914 ,\n",
              "        -0.43967736,  0.422342  ,  0.4648568 ,  0.43398705,  0.49948478,\n",
              "        -0.46144253,  0.46017182, -0.45631936, -0.49897903, -0.48283005,\n",
              "        -0.4440063 ,  0.46477816, -0.44602254,  0.4767965 , -0.5083133 ,\n",
              "         0.49670553, -0.4382295 , -0.50682193, -0.45443833,  0.49033082,\n",
              "         0.4617942 ,  0.49689248,  0.43409947, -0.46603325,  0.48776162],\n",
              "       dtype=float32),\n",
              " 'maintain': array([ 0.49111995,  0.50712764, -0.481     , -0.44310778, -0.46292055,\n",
              "         0.4570567 ,  0.50142616, -0.50792116, -0.43784916, -0.5010651 ,\n",
              "        -0.448616  ,  0.51991475,  0.48523512,  0.46780825, -0.51365525,\n",
              "        -0.44654685,  0.49457863,  0.46476337, -0.48647875, -0.45369166,\n",
              "        -0.49486148,  0.4379911 ,  0.50304425,  0.44934094, -0.45366526,\n",
              "        -0.4787986 ,  0.49205887,  0.51793456,  0.45098856,  0.51652074,\n",
              "        -0.5036566 ,  0.4600452 , -0.47077882, -0.4998372 , -0.4692154 ,\n",
              "        -0.46455005,  0.48719004, -0.482165  ,  0.52329254, -0.5227039 ,\n",
              "         0.47878146, -0.52774346, -0.45413628, -0.46751228,  0.49485135,\n",
              "         0.4955027 ,  0.48509145,  0.4734111 , -0.45567656,  0.52060163],\n",
              "       dtype=float32),\n",
              " 'data': array([ 0.5111407 ,  0.4411724 , -0.5187074 , -0.45170754, -0.50089073,\n",
              "         0.47001606,  0.4411118 , -0.49090612, -0.49127778, -0.43692124,\n",
              "        -0.45214003,  0.44990987,  0.4879982 ,  0.50511664, -0.5093177 ,\n",
              "        -0.45680332,  0.47667757,  0.5040786 , -0.5081321 , -0.48291007,\n",
              "        -0.46084496,  0.4967095 ,  0.4960373 ,  0.5044802 , -0.45386147,\n",
              "        -0.4769745 ,  0.4232192 ,  0.53222185,  0.5119873 ,  0.44669783,\n",
              "        -0.51057595,  0.4441957 , -0.47685882, -0.44673988, -0.42257395,\n",
              "        -0.4745053 ,  0.5262867 , -0.5112573 ,  0.45370093, -0.5086994 ,\n",
              "         0.4979691 , -0.47531453, -0.4543255 , -0.4982735 ,  0.4612681 ,\n",
              "         0.4524209 ,  0.43536407,  0.4918097 , -0.50011796,  0.47703564],\n",
              "       dtype=float32),\n",
              " 'ratio': array([ 0.50832045,  0.46885052, -0.44555703, -0.4195253 , -0.45173874,\n",
              "         0.49867105,  0.49424273, -0.45850876, -0.4888162 , -0.42795393,\n",
              "        -0.46797222,  0.44060233,  0.5047764 ,  0.43656027, -0.5149344 ,\n",
              "        -0.46166843,  0.48439524,  0.4474281 , -0.42956257, -0.4934164 ,\n",
              "        -0.4809621 ,  0.46308863,  0.48021707,  0.42222714, -0.42582166,\n",
              "        -0.4774565 ,  0.48963606,  0.4979984 ,  0.4986537 ,  0.48437408,\n",
              "        -0.5115144 ,  0.43222412, -0.4636726 , -0.49507105, -0.45293975,\n",
              "        -0.46388602,  0.5139121 , -0.42619053,  0.44695023, -0.44217244,\n",
              "         0.45990148, -0.47542053, -0.46851653, -0.43532234,  0.48582405,\n",
              "         0.4393709 ,  0.49205533,  0.43576986, -0.47853222,  0.429308  ],\n",
              "       dtype=float32),\n",
              " 'movements': array([ 0.5226906 ,  0.4495932 , -0.4734684 , -0.43813646, -0.4494921 ,\n",
              "         0.46511632,  0.4268293 , -0.4374562 , -0.49744523, -0.45366678,\n",
              "        -0.4893541 ,  0.46481448,  0.49754634,  0.48095486, -0.49180844,\n",
              "        -0.4533577 ,  0.47696027,  0.48235488, -0.4730817 , -0.49000803,\n",
              "        -0.49051744,  0.42138737,  0.48467278,  0.4331817 , -0.44269463,\n",
              "        -0.44621763,  0.42559636,  0.5039102 ,  0.45719108,  0.43871722,\n",
              "        -0.51399326,  0.47503147, -0.52418715, -0.47750863, -0.47197583,\n",
              "        -0.44657496,  0.43693712, -0.52098155,  0.5109555 , -0.4835232 ,\n",
              "         0.49211583, -0.52289355, -0.4969582 , -0.47139287,  0.45858994,\n",
              "         0.5070514 ,  0.46051326,  0.48140845, -0.4464572 ,  0.43552652],\n",
              "       dtype=float32),\n",
              " 'consider': array([ 0.48761562,  0.44264635, -0.48908502, -0.45697698, -0.44859967,\n",
              "         0.5217568 ,  0.43498176, -0.43450862, -0.4679213 , -0.41580436,\n",
              "        -0.49874336,  0.49799228,  0.5099861 ,  0.4403843 , -0.4630682 ,\n",
              "        -0.48356116,  0.4436079 ,  0.48877123, -0.44170725, -0.4381274 ,\n",
              "        -0.47073552,  0.47541204,  0.49716625,  0.44621202, -0.430317  ,\n",
              "        -0.42341393,  0.4288358 ,  0.46643347,  0.47680458,  0.50893396,\n",
              "        -0.5024545 ,  0.45743787, -0.50343275, -0.4991404 , -0.4249933 ,\n",
              "        -0.5025043 ,  0.45996356, -0.50400156,  0.46931714, -0.5096804 ,\n",
              "         0.45124018, -0.45769155, -0.43415168, -0.5008239 ,  0.45743528,\n",
              "         0.5020482 ,  0.47276676,  0.48293698, -0.5038842 ,  0.4977941 ],\n",
              "       dtype=float32),\n",
              " 'considers': array([ 0.54907745,  0.46906006, -0.4782224 , -0.49136078, -0.52509284,\n",
              "         0.5124965 ,  0.44276708, -0.5246331 , -0.45980942, -0.47511113,\n",
              "        -0.4559339 ,  0.5123007 ,  0.44182235,  0.4733448 , -0.5546998 ,\n",
              "        -0.44592574,  0.4935684 ,  0.4591321 , -0.46946102, -0.4622053 ,\n",
              "        -0.5184781 ,  0.4651455 ,  0.48034176,  0.44958702, -0.44131598,\n",
              "        -0.4622757 ,  0.5176462 ,  0.48613307,  0.49571982,  0.52300304,\n",
              "        -0.48449317,  0.4480725 , -0.5014151 , -0.4531907 , -0.46881363,\n",
              "        -0.5309603 ,  0.45577505, -0.5394476 ,  0.50532454, -0.45502916,\n",
              "         0.51064444, -0.5344644 , -0.48128217, -0.465062  ,  0.4576799 ,\n",
              "         0.5170158 ,  0.48046216,  0.49419075, -0.4810628 ,  0.48420775],\n",
              "       dtype=float32),\n",
              " 'reinvest': array([ 0.54878974,  0.48308045, -0.53014636, -0.48333126, -0.4801797 ,\n",
              "         0.52903736,  0.4876072 , -0.5231049 , -0.50855154, -0.49810404,\n",
              "        -0.46552604,  0.48095873,  0.5043316 ,  0.46703908, -0.49440077,\n",
              "        -0.4815937 ,  0.47338906,  0.51877284, -0.4722193 , -0.46345425,\n",
              "        -0.46887398,  0.4867049 ,  0.5259781 ,  0.45493314, -0.4458502 ,\n",
              "        -0.5269155 ,  0.5112862 ,  0.5040482 ,  0.50729525,  0.51509744,\n",
              "        -0.512849  ,  0.45852172, -0.54715604, -0.4634432 , -0.50981575,\n",
              "        -0.4783964 ,  0.53369975, -0.45513156,  0.45941293, -0.4738232 ,\n",
              "         0.45271483, -0.47124407, -0.4856639 , -0.44345087,  0.49687168,\n",
              "         0.44362423,  0.5271994 ,  0.505364  , -0.5270175 ,  0.4998762 ],\n",
              "       dtype=float32),\n",
              " 'located': array([ 0.46164608,  0.512502  , -0.5129874 , -0.4806478 , -0.45499167,\n",
              "         0.5016935 ,  0.5229982 , -0.46676284, -0.4366074 , -0.49669325,\n",
              "        -0.4273518 ,  0.5101756 ,  0.49565032,  0.46931246, -0.53820515,\n",
              "        -0.46240884,  0.5112121 ,  0.5275744 , -0.5009024 , -0.4878503 ,\n",
              "        -0.502883  ,  0.47567198,  0.46120465,  0.47305036, -0.48590004,\n",
              "        -0.43860653,  0.43722427,  0.4797976 ,  0.48038265,  0.48446494,\n",
              "        -0.44331065,  0.4791438 , -0.4886727 , -0.49943852, -0.50346786,\n",
              "        -0.48124337,  0.50561804, -0.45830777,  0.51038   , -0.466518  ,\n",
              "         0.48672003, -0.44412422, -0.4709062 , -0.51606274,  0.4767485 ,\n",
              "         0.47396973,  0.4789434 ,  0.5142101 , -0.5251969 ,  0.505329  ],\n",
              "       dtype=float32),\n",
              " 'listed': array([ 0.50305486,  0.44351637, -0.5169892 , -0.5097836 , -0.44737265,\n",
              "         0.455617  ,  0.49523845, -0.48556903, -0.51300937, -0.51064396,\n",
              "        -0.5158658 ,  0.49705577,  0.49932045,  0.4353178 , -0.51688105,\n",
              "        -0.46569306,  0.45969093,  0.45596877, -0.4704023 , -0.43354398,\n",
              "        -0.4426247 ,  0.46829978,  0.45139647,  0.4952001 , -0.47791797,\n",
              "        -0.43736   ,  0.49883136,  0.5110319 ,  0.43463176,  0.45841002,\n",
              "        -0.50144154,  0.47059843, -0.51429737, -0.4911593 , -0.42402664,\n",
              "        -0.47404838,  0.4606275 , -0.44707504,  0.44165862, -0.50512666,\n",
              "         0.5192209 , -0.5241566 , -0.43345395, -0.4230319 ,  0.47087178,\n",
              "         0.47300178,  0.45442376,  0.43549275, -0.51667506,  0.5113541 ],\n",
              "       dtype=float32),\n",
              " 'contact': array([ 0.5089673 ,  0.45334154, -0.46881074, -0.44758832, -0.5142491 ,\n",
              "         0.4757011 ,  0.46212167, -0.4445965 , -0.4622819 , -0.44589996,\n",
              "        -0.4428555 ,  0.49001405,  0.4345761 ,  0.43916237, -0.5473138 ,\n",
              "        -0.4336886 ,  0.47414967,  0.5182855 , -0.46254325, -0.47479248,\n",
              "        -0.5066208 ,  0.46622032,  0.4390901 ,  0.5082539 , -0.44353244,\n",
              "        -0.51759374,  0.45317113,  0.46037656,  0.43098533,  0.49237478,\n",
              "        -0.44679427,  0.48971084, -0.45873028, -0.5038515 , -0.4999112 ,\n",
              "        -0.49823314,  0.48731828, -0.47835726,  0.50599676, -0.5022412 ,\n",
              "         0.5041589 , -0.45143983, -0.47012123, -0.4453084 ,  0.49024105,\n",
              "         0.5007242 ,  0.46677643,  0.50095546, -0.485484  ,  0.50646394],\n",
              "       dtype=float32),\n",
              " 'revenues': array([ 0.5439733 ,  0.45028862, -0.50111055, -0.45692602, -0.4886583 ,\n",
              "         0.44508034,  0.48654258, -0.454334  , -0.48377454, -0.49889442,\n",
              "        -0.44517758,  0.520524  ,  0.4803915 ,  0.44909358, -0.4962399 ,\n",
              "        -0.4367311 ,  0.47921702,  0.5013819 , -0.43807623, -0.4959907 ,\n",
              "        -0.47235066,  0.4497261 ,  0.46558282,  0.44545466, -0.5064685 ,\n",
              "        -0.46053913,  0.44675323,  0.47794515,  0.51588875,  0.43953073,\n",
              "        -0.44321114,  0.5245333 , -0.49869004, -0.51984954, -0.5155492 ,\n",
              "        -0.48359564,  0.50208265, -0.51603115,  0.51942766, -0.50417525,\n",
              "         0.48420763, -0.51850486, -0.4571928 , -0.46275976,  0.4616051 ,\n",
              "         0.4709524 ,  0.5166982 ,  0.48213485, -0.47503892,  0.5045782 ],\n",
              "       dtype=float32),\n",
              " 'controls': array([ 0.5148254 ,  0.5149423 , -0.43716267, -0.5017887 , -0.47735295,\n",
              "         0.49773318,  0.49544555, -0.46874604, -0.49971634, -0.45432884,\n",
              "        -0.5041976 ,  0.5306562 ,  0.510614  ,  0.44231382, -0.47933638,\n",
              "        -0.4646054 ,  0.52260554,  0.47848892, -0.4982676 , -0.455909  ,\n",
              "        -0.4797817 ,  0.52005124,  0.4596815 ,  0.43116432, -0.45449507,\n",
              "        -0.44286883,  0.49517   ,  0.4993988 ,  0.4686341 ,  0.52418756,\n",
              "        -0.47518682,  0.49129456, -0.4868013 , -0.45741796, -0.48711863,\n",
              "        -0.45477584,  0.5078766 , -0.45562303,  0.4586332 , -0.48964208,\n",
              "         0.4912501 , -0.49003726, -0.4304408 , -0.49297944,  0.5029584 ,\n",
              "         0.43411478,  0.46643627,  0.4931619 , -0.43757904,  0.5144943 ],\n",
              "       dtype=float32),\n",
              " 'component': array([ 0.46813607,  0.49425262, -0.50114834, -0.51475275, -0.49842298,\n",
              "         0.5254287 ,  0.4758273 , -0.4992866 , -0.4348654 , -0.42994016,\n",
              "        -0.4639504 ,  0.46100345,  0.43738273,  0.5147904 , -0.5043622 ,\n",
              "        -0.44339067,  0.4561072 ,  0.49166483, -0.48662806, -0.43832925,\n",
              "        -0.5015139 ,  0.4342797 ,  0.49237093,  0.46379286, -0.43289906,\n",
              "        -0.48203036,  0.51139754,  0.45714122,  0.5017563 ,  0.5110396 ,\n",
              "        -0.45071468,  0.4381633 , -0.5159588 , -0.52468145, -0.45715946,\n",
              "        -0.5109161 ,  0.44432765, -0.5164909 ,  0.4377812 , -0.4590453 ,\n",
              "         0.460471  , -0.52950627, -0.4403001 , -0.4691705 ,  0.5168341 ,\n",
              "         0.50750303,  0.47096124,  0.52486074, -0.4412321 ,  0.4644371 ],\n",
              "       dtype=float32),\n",
              " 'connection': array([ 0.4756164 ,  0.4873935 , -0.4684035 , -0.5232887 , -0.50074416,\n",
              "         0.46541995,  0.514278  , -0.53651077, -0.46142477, -0.5057931 ,\n",
              "        -0.51387835,  0.45295835,  0.51267016,  0.47028273, -0.54507786,\n",
              "        -0.44260773,  0.46293855,  0.51580274, -0.50794685, -0.47270918,\n",
              "        -0.45611322,  0.4715379 ,  0.4825635 ,  0.45382547, -0.5069693 ,\n",
              "        -0.48056754,  0.49394417,  0.4815504 ,  0.4421677 ,  0.49103746,\n",
              "        -0.5049435 ,  0.5326328 , -0.54333675, -0.49080077, -0.51134664,\n",
              "        -0.5447636 ,  0.47378862, -0.51432544,  0.52132   , -0.4623777 ,\n",
              "         0.48951092, -0.50628936, -0.45854625, -0.47152734,  0.45047885,\n",
              "         0.45953703,  0.45102572,  0.5042675 , -0.51393104,  0.4836801 ],\n",
              "       dtype=float32),\n",
              " 'mid': array([ 0.520648  ,  0.44496614, -0.52361757, -0.5204331 , -0.5425489 ,\n",
              "         0.48311648,  0.5091594 , -0.54371667, -0.4876635 , -0.48538658,\n",
              "        -0.51343155,  0.5236455 ,  0.45616484,  0.47201368, -0.5225531 ,\n",
              "        -0.48950577,  0.48257944,  0.4586869 , -0.5268431 , -0.48866087,\n",
              "        -0.5008321 ,  0.48124754,  0.5417602 ,  0.4572528 , -0.52215886,\n",
              "        -0.4610483 ,  0.5063179 ,  0.56758034,  0.4760903 ,  0.5241412 ,\n",
              "        -0.50877714,  0.5331266 , -0.53222156, -0.4659611 , -0.45549938,\n",
              "        -0.46737584,  0.5182675 , -0.4582502 ,  0.53312594, -0.5185117 ,\n",
              "         0.4952917 , -0.51572466, -0.512431  , -0.50388825,  0.54359746,\n",
              "         0.5191408 ,  0.52776617,  0.4621982 , -0.49784675,  0.51784956],\n",
              "       dtype=float32),\n",
              " 'hedge': array([ 0.48337612,  0.42292836, -0.4859814 , -0.4986047 , -0.49250132,\n",
              "         0.4677241 ,  0.48823866, -0.44575885, -0.49389845, -0.45451564,\n",
              "        -0.44267118,  0.45980448,  0.48646042,  0.5176246 , -0.53271466,\n",
              "        -0.49168235,  0.46226805,  0.5231769 , -0.49492645, -0.48772028,\n",
              "        -0.48441002,  0.47071087,  0.49018365,  0.44125947, -0.44173893,\n",
              "        -0.4294004 ,  0.4610701 ,  0.46634158,  0.4698207 ,  0.48826805,\n",
              "        -0.4582388 ,  0.48279864, -0.45367843, -0.49468058, -0.46713632,\n",
              "        -0.45825663,  0.45030448, -0.5073518 ,  0.49038142, -0.48710606,\n",
              "         0.5114696 , -0.4397551 , -0.45792037, -0.48779684,  0.5226309 ,\n",
              "         0.509359  ,  0.5074286 ,  0.5028742 , -0.51770884,  0.47566548],\n",
              "       dtype=float32),\n",
              " 'structure': array([ 0.47987676,  0.49246815, -0.44737738, -0.4315757 , -0.51470244,\n",
              "         0.44545144,  0.43626347, -0.50045884, -0.48052883, -0.4488225 ,\n",
              "        -0.49574113,  0.4959176 ,  0.50240284,  0.44775257, -0.48490614,\n",
              "        -0.4893046 ,  0.51164067,  0.44524774, -0.464643  , -0.5010148 ,\n",
              "        -0.46092802,  0.44440937,  0.44268423,  0.4532178 , -0.43623537,\n",
              "        -0.42540374,  0.46212918,  0.5098364 ,  0.47201872,  0.5094809 ,\n",
              "        -0.49494237,  0.42681634, -0.5139844 , -0.46977562, -0.5005956 ,\n",
              "        -0.4645613 ,  0.50380206, -0.43341836,  0.48511544, -0.48578995,\n",
              "         0.51067644, -0.4847907 , -0.5086887 , -0.4625481 ,  0.50990784,\n",
              "         0.48466337,  0.5063952 ,  0.46071607, -0.4963126 ,  0.4380794 ],\n",
              "       dtype=float32),\n",
              " 'special': array([ 0.49760377,  0.5045321 , -0.4931015 , -0.4932316 , -0.5252489 ,\n",
              "         0.49755746,  0.4497189 , -0.5154948 , -0.51463646, -0.51413774,\n",
              "        -0.4859653 ,  0.4904742 ,  0.5054912 ,  0.48032856, -0.53502893,\n",
              "        -0.52138513,  0.45851505,  0.45382813, -0.4991808 , -0.44547808,\n",
              "        -0.49554157,  0.49880672,  0.5105164 ,  0.48512596, -0.45136032,\n",
              "        -0.4892948 ,  0.46710074,  0.4963764 ,  0.4508434 ,  0.4770619 ,\n",
              "        -0.5211147 ,  0.43559888, -0.47444445, -0.46324396, -0.50455546,\n",
              "        -0.45055613,  0.529654  , -0.50854677,  0.46199176, -0.4893776 ,\n",
              "         0.4722027 , -0.53042674, -0.48077637, -0.47902688,  0.43962455,\n",
              "         0.5209835 ,  0.49894112,  0.4945162 , -0.5208395 ,  0.45226187],\n",
              "       dtype=float32),\n",
              " 'actions': array([ 0.51235896,  0.45920908, -0.541503  , -0.44979027, -0.52021956,\n",
              "         0.468214  ,  0.46021324, -0.48171014, -0.45596626, -0.46636963,\n",
              "        -0.44137257,  0.47740877,  0.48407584,  0.46775025, -0.56628555,\n",
              "        -0.44747505,  0.51654315,  0.47570074, -0.44973293, -0.52326775,\n",
              "        -0.45632368,  0.4827806 ,  0.4614113 ,  0.44362152, -0.50692564,\n",
              "        -0.46952313,  0.4496821 ,  0.47696882,  0.5119968 ,  0.5221429 ,\n",
              "        -0.523065  ,  0.47820887, -0.5479004 , -0.4604665 , -0.5210192 ,\n",
              "        -0.4646062 ,  0.5305816 , -0.4972376 ,  0.47620928, -0.4746515 ,\n",
              "         0.53105366, -0.47162607, -0.5163328 , -0.4943315 ,  0.46855757,\n",
              "         0.5188475 ,  0.48486567,  0.51492566, -0.52424514,  0.4481894 ],\n",
              "       dtype=float32),\n",
              " 'borrower': array([ 0.4657025 ,  0.48150787, -0.45025933, -0.4891898 , -0.4389082 ,\n",
              "         0.50896186,  0.48333868, -0.5060626 , -0.48986652, -0.46694756,\n",
              "        -0.42730567,  0.4512097 ,  0.48407724,  0.47277716, -0.4938795 ,\n",
              "        -0.4968456 ,  0.51569766,  0.5107804 , -0.5168    , -0.48119044,\n",
              "        -0.45569083,  0.46514505,  0.49317712,  0.48529872, -0.43574658,\n",
              "        -0.49775395,  0.4644522 ,  0.5429045 ,  0.49228477,  0.45532215,\n",
              "        -0.45460063,  0.508212  , -0.4858318 , -0.46744585, -0.5328199 ,\n",
              "        -0.48367947,  0.5213391 , -0.45372534,  0.4442363 , -0.4485347 ,\n",
              "         0.48712757, -0.45038167, -0.48898977, -0.45591268,  0.47746015,\n",
              "         0.4510244 ,  0.51643467,  0.4182778 , -0.45720464,  0.5009591 ],\n",
              "       dtype=float32),\n",
              " 'reinvestment': array([ 0.4782024 ,  0.514002  , -0.52326876, -0.44981605, -0.46934706,\n",
              "         0.53027433,  0.501298  , -0.49223205, -0.44457582, -0.4739303 ,\n",
              "        -0.4997127 ,  0.48470795,  0.44716454,  0.49106923, -0.49061003,\n",
              "        -0.43444443,  0.5077421 ,  0.48905477, -0.47860926, -0.4781847 ,\n",
              "        -0.47123128,  0.50549793,  0.51825684,  0.44380382, -0.47895545,\n",
              "        -0.4224812 ,  0.46583298,  0.4657558 ,  0.5176271 ,  0.47215435,\n",
              "        -0.52830285,  0.4751716 , -0.46536794, -0.5008975 , -0.43334615,\n",
              "        -0.49276417,  0.46302167, -0.49073747,  0.4876941 , -0.52953213,\n",
              "         0.52612835, -0.4997684 , -0.49255112, -0.42940074,  0.515664  ,\n",
              "         0.46823123,  0.48072094,  0.50026536, -0.4652943 ,  0.46900305],\n",
              "       dtype=float32),\n",
              " 'given': array([ 0.51349723,  0.49043545, -0.50601596, -0.43029422, -0.49859565,\n",
              "         0.502464  ,  0.47473353, -0.44024628, -0.5085095 , -0.5036516 ,\n",
              "        -0.49488467,  0.4574379 ,  0.50804794,  0.47670478, -0.54470074,\n",
              "        -0.5149152 ,  0.5198911 ,  0.48906645, -0.44899762, -0.48846278,\n",
              "        -0.4993587 ,  0.44756025,  0.47481182,  0.48334453, -0.4231741 ,\n",
              "        -0.46440902,  0.48462382,  0.5286389 ,  0.4692683 ,  0.4427516 ,\n",
              "        -0.47847885,  0.48946485, -0.49289572, -0.49447998, -0.4228897 ,\n",
              "        -0.4736489 ,  0.4705471 , -0.47181326,  0.47758433, -0.47654796,\n",
              "         0.4428224 , -0.51859623, -0.49223518, -0.5029453 ,  0.5140116 ,\n",
              "         0.5165456 ,  0.4832353 ,  0.46851385, -0.48468798,  0.43203145],\n",
              "       dtype=float32),\n",
              " 'comparable': array([ 0.50800604,  0.43211442, -0.46582174, -0.4745782 , -0.45042154,\n",
              "         0.49203247,  0.48636723, -0.484525  , -0.4948305 , -0.42497763,\n",
              "        -0.5013523 ,  0.4888472 ,  0.41895607,  0.4897753 , -0.5371316 ,\n",
              "        -0.43747127,  0.46151274,  0.43104652, -0.49255568, -0.4896353 ,\n",
              "        -0.41700363,  0.43704423,  0.49328828,  0.49148095, -0.42256597,\n",
              "        -0.4935233 ,  0.44450203,  0.50727516,  0.4638094 ,  0.4993001 ,\n",
              "        -0.50497735,  0.48083696, -0.48603746, -0.43085498, -0.41423762,\n",
              "        -0.49332687,  0.506064  , -0.47864294,  0.47132283, -0.4792947 ,\n",
              "         0.4593371 , -0.44516465, -0.49341187, -0.454783  ,  0.48674846,\n",
              "         0.45644486,  0.44883564,  0.45090252, -0.4620379 ,  0.49660918],\n",
              "       dtype=float32),\n",
              " 'japan': array([ 0.5116304 ,  0.49194157, -0.4662674 , -0.49026263, -0.4814912 ,\n",
              "         0.48333284,  0.501232  , -0.50385207, -0.48697755, -0.49830598,\n",
              "        -0.5327364 ,  0.45149493,  0.45446503,  0.54200274, -0.4928318 ,\n",
              "        -0.5310749 ,  0.5424215 ,  0.49657613, -0.4781195 , -0.48713937,\n",
              "        -0.4617039 ,  0.53018343,  0.46334377,  0.4956869 , -0.5316645 ,\n",
              "        -0.5369453 ,  0.49527687,  0.47765425,  0.46062535,  0.47000414,\n",
              "        -0.5462758 ,  0.4957064 , -0.5397285 , -0.4773727 , -0.4502916 ,\n",
              "        -0.5154117 ,  0.53024423, -0.49782997,  0.4850432 , -0.535333  ,\n",
              "         0.54341304, -0.46297604, -0.45635283, -0.5213833 ,  0.47449875,\n",
              "         0.52446735,  0.51170146,  0.48353678, -0.53082013,  0.46786454],\n",
              "       dtype=float32),\n",
              " 'sensitivity': array([ 0.54186785,  0.49805588, -0.49995667, -0.48416358, -0.52383745,\n",
              "         0.5176657 ,  0.4744966 , -0.49728566, -0.49131358, -0.49456793,\n",
              "        -0.4769463 ,  0.42946818,  0.50900817,  0.48614872, -0.54651123,\n",
              "        -0.43608823,  0.4603    ,  0.51481485, -0.50406474, -0.49181834,\n",
              "        -0.4752801 ,  0.499377  ,  0.43447313,  0.47499356, -0.43712106,\n",
              "        -0.4967154 ,  0.4551463 ,  0.4938732 ,  0.44892776,  0.49258235,\n",
              "        -0.47486478,  0.44246763, -0.4721267 , -0.52325135, -0.46618715,\n",
              "        -0.4857734 ,  0.47383812, -0.48613498,  0.45264173, -0.48119974,\n",
              "         0.48905924, -0.5019085 , -0.48736084, -0.44973028,  0.5070624 ,\n",
              "         0.44399372,  0.447757  ,  0.47705373, -0.46871904,  0.4472589 ],\n",
              "       dtype=float32),\n",
              " 'administrator': array([ 0.49988976,  0.42162117, -0.4896883 , -0.43569037, -0.4716412 ,\n",
              "         0.47775257,  0.4614926 , -0.45246878, -0.43388325, -0.45370266,\n",
              "        -0.49689347,  0.44874716,  0.43424815,  0.52551216, -0.49565825,\n",
              "        -0.5074142 ,  0.48019505,  0.46291387, -0.45246464, -0.46127594,\n",
              "        -0.50934035,  0.48588812,  0.44287872,  0.4367716 , -0.50205475,\n",
              "        -0.45965898,  0.4520362 ,  0.46025592,  0.5020288 ,  0.50318885,\n",
              "        -0.46689448,  0.4302939 , -0.5334349 , -0.49528426, -0.51032174,\n",
              "        -0.49898583,  0.4467088 , -0.5089715 ,  0.4529885 , -0.5271887 ,\n",
              "         0.49038124, -0.49540067, -0.49644354, -0.49568924,  0.4894947 ,\n",
              "         0.48930433,  0.4556331 ,  0.48922098, -0.46229044,  0.47754925],\n",
              "       dtype=float32),\n",
              " 'condition': array([ 0.5294124 ,  0.47548705, -0.5033228 , -0.48735625, -0.5025135 ,\n",
              "         0.53091097,  0.51135314, -0.46366027, -0.50561506, -0.44958058,\n",
              "        -0.49957418,  0.4563352 ,  0.50644106,  0.5105446 , -0.4724664 ,\n",
              "        -0.43302315,  0.46247128,  0.45175996, -0.48724118, -0.47461122,\n",
              "        -0.50834763,  0.5083262 ,  0.44732228,  0.4766888 , -0.48837766,\n",
              "        -0.45865205,  0.4387219 ,  0.53514475,  0.4375684 ,  0.46396184,\n",
              "        -0.49687994,  0.46971142, -0.52398664, -0.49869815, -0.50234675,\n",
              "        -0.49451485,  0.5196137 , -0.47707802,  0.5080068 , -0.50662446,\n",
              "         0.49037483, -0.4893253 , -0.5061304 , -0.45664024,  0.45018625,\n",
              "         0.5254978 ,  0.46188885,  0.45316902, -0.45747167,  0.46945196],\n",
              "       dtype=float32),\n",
              " 'instability': array([ 0.5369897 ,  0.48195428, -0.42673036, -0.5125119 , -0.44352287,\n",
              "         0.50131047,  0.48419562, -0.4809912 , -0.47395238, -0.4685788 ,\n",
              "        -0.49707168,  0.43906525,  0.5119226 ,  0.5097679 , -0.548427  ,\n",
              "        -0.5054419 ,  0.43909103,  0.5106752 , -0.49662775, -0.43648157,\n",
              "        -0.5182113 ,  0.5085736 ,  0.50922096,  0.4400068 , -0.4352585 ,\n",
              "        -0.5212629 ,  0.44354713,  0.50519234,  0.44497362,  0.43791455,\n",
              "        -0.5218233 ,  0.46702924, -0.48017836, -0.461474  , -0.5001407 ,\n",
              "        -0.5276362 ,  0.4606636 , -0.44060513,  0.50619876, -0.46401122,\n",
              "         0.5033703 , -0.48155758, -0.5005281 , -0.48959664,  0.51520723,\n",
              "         0.463857  ,  0.46866786,  0.43244398, -0.44345707,  0.47180742],\n",
              "       dtype=float32),\n",
              " 'fmrc': array([ 0.44229093,  0.4831741 , -0.46242636, -0.49393338, -0.457662  ,\n",
              "         0.48336703,  0.42897186, -0.4529267 , -0.4226973 , -0.44427857,\n",
              "        -0.4146499 ,  0.4447636 ,  0.48508257,  0.48975402, -0.49165836,\n",
              "        -0.5037808 ,  0.49697325,  0.41894677, -0.44404352, -0.46767431,\n",
              "        -0.41780314,  0.4410465 ,  0.47518617,  0.47568762, -0.44990462,\n",
              "        -0.45643428,  0.49465835,  0.47447306,  0.4846561 ,  0.4506491 ,\n",
              "        -0.5070457 ,  0.46855962, -0.5199226 , -0.4544184 , -0.49693054,\n",
              "        -0.45927817,  0.47094083, -0.48593882,  0.47147447, -0.48230135,\n",
              "         0.46044654, -0.5093952 , -0.4138646 , -0.4619734 ,  0.4547064 ,\n",
              "         0.46707985,  0.48268482,  0.43123913, -0.49364102,  0.4927971 ],\n",
              "       dtype=float32),\n",
              " 'react': array([ 0.52702904,  0.42408702, -0.44384804, -0.51522416, -0.46970424,\n",
              "         0.44877243,  0.46520853, -0.49606264, -0.43484664, -0.4959779 ,\n",
              "        -0.49053597,  0.46514845,  0.43482032,  0.5207425 , -0.5129577 ,\n",
              "        -0.49944568,  0.4647122 ,  0.44329166, -0.44742948, -0.44195274,\n",
              "        -0.4394331 ,  0.4896314 ,  0.48367235,  0.49545634, -0.44201577,\n",
              "        -0.4376123 ,  0.48198938,  0.4720338 ,  0.46181464,  0.4397948 ,\n",
              "        -0.49766544,  0.4531096 , -0.5327264 , -0.5137018 , -0.5093252 ,\n",
              "        -0.47344857,  0.49159104, -0.4852365 ,  0.50825393, -0.52629924,\n",
              "         0.48225957, -0.45018628, -0.43573868, -0.49125013,  0.44703954,\n",
              "         0.5012485 ,  0.5126035 ,  0.49717867, -0.45920616,  0.47509313],\n",
              "       dtype=float32),\n",
              " 'capitalizations': array([ 0.47019142,  0.45337003, -0.4741961 , -0.46799335, -0.4695267 ,\n",
              "         0.45952696,  0.48761964, -0.52610976, -0.4570991 , -0.5093474 ,\n",
              "        -0.4814879 ,  0.47288147,  0.47406068,  0.5205636 , -0.49862698,\n",
              "        -0.49652457,  0.49970233,  0.49531406, -0.5133952 , -0.5179572 ,\n",
              "        -0.4405156 ,  0.49000695,  0.5131516 ,  0.4939767 , -0.45679495,\n",
              "        -0.45930538,  0.45685023,  0.47302055,  0.48277232,  0.51630306,\n",
              "        -0.45788398,  0.4805774 , -0.55252475, -0.49817848, -0.51340944,\n",
              "        -0.4944578 ,  0.5264311 , -0.4843396 ,  0.5118326 , -0.49697703,\n",
              "         0.4573764 , -0.4537074 , -0.5021377 , -0.47431332,  0.5192893 ,\n",
              "         0.5030984 ,  0.48414978,  0.53450954, -0.51104295,  0.46782646],\n",
              "       dtype=float32),\n",
              " 'prospects': array([ 0.4737714 ,  0.43225706, -0.47854257, -0.5173691 , -0.51317596,\n",
              "         0.5172134 ,  0.48567793, -0.49694926, -0.49191546, -0.51120996,\n",
              "        -0.49917918,  0.43903893,  0.49610072,  0.48029616, -0.49967766,\n",
              "        -0.48023987,  0.4665529 ,  0.4394706 , -0.50538087, -0.46491843,\n",
              "        -0.46426386,  0.48613456,  0.49738243,  0.44754243, -0.5112449 ,\n",
              "        -0.49969846,  0.47976455,  0.5294002 ,  0.4638496 ,  0.445086  ,\n",
              "        -0.47072032,  0.4336324 , -0.5361006 , -0.5043249 , -0.4585286 ,\n",
              "        -0.5123398 ,  0.46963423, -0.45444578,  0.51311904, -0.45984432,\n",
              "         0.51615345, -0.45229384, -0.42130005, -0.513682  ,  0.49756548,\n",
              "         0.5251377 ,  0.49119195,  0.4485452 , -0.46414635,  0.5023881 ],\n",
              "       dtype=float32),\n",
              " 'negative': array([ 0.46042648,  0.434282  , -0.4991258 , -0.43392587, -0.47693214,\n",
              "         0.47823977,  0.49946094, -0.51027435, -0.5188506 , -0.45881447,\n",
              "        -0.4724759 ,  0.44895077,  0.47093233,  0.4777943 , -0.49652874,\n",
              "        -0.4915725 ,  0.4499731 ,  0.49377728, -0.49731618, -0.48257506,\n",
              "        -0.4704462 ,  0.4662612 ,  0.52928174,  0.5037364 , -0.49870527,\n",
              "        -0.44722867,  0.47576714,  0.54108614,  0.48990893,  0.45249858,\n",
              "        -0.5205554 ,  0.47862628, -0.5280933 , -0.4456369 , -0.45061803,\n",
              "        -0.52986014,  0.51510125, -0.48332304,  0.45495784, -0.44405624,\n",
              "         0.46701363, -0.5275339 , -0.4941439 , -0.4936573 ,  0.50924885,\n",
              "         0.50763226,  0.4417862 ,  0.49158958, -0.49140716,  0.45866817],\n",
              "       dtype=float32),\n",
              " 'specified': array([ 0.5024433 ,  0.4485589 , -0.4736629 , -0.46459934, -0.440671  ,\n",
              "         0.48567763,  0.5102579 , -0.4297693 , -0.48410338, -0.4511547 ,\n",
              "        -0.44415605,  0.5117435 ,  0.4460475 ,  0.506265  , -0.46121687,\n",
              "        -0.42923662,  0.46486914,  0.4479858 , -0.4517284 , -0.42496875,\n",
              "        -0.42237976,  0.4410476 ,  0.43473363,  0.4693251 , -0.49844655,\n",
              "        -0.4542386 ,  0.49536407,  0.5093559 ,  0.44298425,  0.4793595 ,\n",
              "        -0.5180033 ,  0.45511243, -0.48279315, -0.5145277 , -0.43772274,\n",
              "        -0.46224827,  0.48054147, -0.4648571 ,  0.48549038, -0.50870717,\n",
              "         0.46854952, -0.4781021 , -0.46670142, -0.4212049 ,  0.5077241 ,\n",
              "         0.5057825 ,  0.42994234,  0.4229258 , -0.48886916,  0.49282283],\n",
              "       dtype=float32),\n",
              " 'repay': array([ 0.48058578,  0.47073552, -0.46637076, -0.4908276 , -0.51241195,\n",
              "         0.49190098,  0.42809802, -0.5039243 , -0.43934724, -0.43396991,\n",
              "        -0.45815554,  0.45287508,  0.50512105,  0.49261576, -0.5370248 ,\n",
              "        -0.5024199 ,  0.4518301 ,  0.4620733 , -0.48606113, -0.5065531 ,\n",
              "        -0.50352806,  0.50452715,  0.43317094,  0.4946522 , -0.47386485,\n",
              "        -0.50695974,  0.46361223,  0.48373997,  0.46889415,  0.4930541 ,\n",
              "        -0.52127206,  0.42693034, -0.51912856, -0.48270515, -0.48498303,\n",
              "        -0.46396407,  0.4914716 , -0.4691736 ,  0.43368822, -0.4398985 ,\n",
              "         0.5162363 , -0.48400104, -0.46037242, -0.42120564,  0.43752977,\n",
              "         0.46474266,  0.42659175,  0.4888837 , -0.48985222,  0.48178977],\n",
              "       dtype=float32),\n",
              " 'neutral': array([ 0.48908105,  0.43304455, -0.4742199 , -0.509566  , -0.48356047,\n",
              "         0.51744527,  0.4952187 , -0.447142  , -0.46156812, -0.5010311 ,\n",
              "        -0.46759403,  0.4313912 ,  0.47434294,  0.46478358, -0.54501927,\n",
              "        -0.4298213 ,  0.49538738,  0.4878077 , -0.44170293, -0.4545477 ,\n",
              "        -0.485313  ,  0.50612813,  0.4422438 ,  0.42728028, -0.49027017,\n",
              "        -0.4915918 ,  0.43275428,  0.4622136 ,  0.47563234,  0.41993788,\n",
              "        -0.44075787,  0.45050955, -0.44182867, -0.4467431 , -0.46135703,\n",
              "        -0.47242182,  0.46298745, -0.5063238 ,  0.49130657, -0.5066097 ,\n",
              "         0.46246064, -0.50181764, -0.49899176, -0.42923504,  0.4587798 ,\n",
              "         0.48535094,  0.50140214,  0.4421348 , -0.49043655,  0.4744749 ],\n",
              "       dtype=float32),\n",
              " 'sufficient': array([ 0.47849226,  0.45649174, -0.4472877 , -0.49816838, -0.45115295,\n",
              "         0.47377393,  0.43063417, -0.5086241 , -0.45856202, -0.44174623,\n",
              "        -0.4372478 ,  0.46930623,  0.49716708,  0.4438483 , -0.49747398,\n",
              "        -0.4533491 ,  0.5207471 ,  0.46140286, -0.5004915 , -0.46862352,\n",
              "        -0.42806354,  0.44016653,  0.45954525,  0.44262725, -0.4724191 ,\n",
              "        -0.50844586,  0.4779472 ,  0.5090071 ,  0.47600022,  0.5074738 ,\n",
              "        -0.52516276,  0.4778217 , -0.5313375 , -0.45695668, -0.4651978 ,\n",
              "        -0.5216508 ,  0.46017843, -0.45791203,  0.47676802, -0.43217742,\n",
              "         0.511242  , -0.503453  , -0.46990332, -0.48711044,  0.4442101 ,\n",
              "         0.50987303,  0.47124046,  0.5204105 , -0.51648015,  0.43717408],\n",
              "       dtype=float32),\n",
              " 'anticipated': array([ 0.5059163 ,  0.47092927, -0.47952327, -0.5007847 , -0.4888715 ,\n",
              "         0.46771842,  0.47815022, -0.48613966, -0.48811835, -0.43440786,\n",
              "        -0.5142218 ,  0.5213068 ,  0.44435945,  0.51687545, -0.46939024,\n",
              "        -0.505201  ,  0.4889585 ,  0.47599798, -0.47023693, -0.48394716,\n",
              "        -0.46943334,  0.5140724 ,  0.5072993 ,  0.513441  , -0.5115484 ,\n",
              "        -0.5042558 ,  0.51001966,  0.50681293,  0.45607093,  0.48474327,\n",
              "        -0.50831926,  0.44796374, -0.49168128, -0.48270285, -0.46958283,\n",
              "        -0.4990098 ,  0.44650522, -0.5100746 ,  0.4559484 , -0.4381472 ,\n",
              "         0.47054225, -0.52591616, -0.45800787, -0.43680176,  0.4928655 ,\n",
              "         0.5130483 ,  0.4477411 ,  0.5019512 , -0.4707324 ,  0.50622076],\n",
              "       dtype=float32),\n",
              " 'tracking': array([ 0.47108057,  0.42681572, -0.4924407 , -0.4316244 , -0.49815845,\n",
              "         0.44400322,  0.48930702, -0.523525  , -0.5136099 , -0.4340046 ,\n",
              "        -0.45234573,  0.50834924,  0.49256238,  0.48088136, -0.5239464 ,\n",
              "        -0.44804153,  0.48176324,  0.46708864, -0.49614552, -0.49067587,\n",
              "        -0.4866564 ,  0.46680948,  0.49851906,  0.5155339 , -0.4451556 ,\n",
              "        -0.45332783,  0.5071382 ,  0.53770614,  0.45105553,  0.50790024,\n",
              "        -0.46715903,  0.43579444, -0.491616  , -0.43980232, -0.4440971 ,\n",
              "        -0.4968267 ,  0.49328718, -0.4743435 ,  0.50212   , -0.45099154,\n",
              "         0.52272755, -0.44079468, -0.51198065, -0.47515163,  0.49860317,\n",
              "         0.48634154,  0.49547333,  0.4457508 , -0.47815824,  0.42701143],\n",
              "       dtype=float32),\n",
              " 'rather': array([ 0.5020504 ,  0.48996663, -0.46680188, -0.49777132, -0.45495284,\n",
              "         0.47277215,  0.47291884, -0.5018228 , -0.48881146, -0.47137588,\n",
              "        -0.44039142,  0.5067282 ,  0.46288523,  0.48655933, -0.491991  ,\n",
              "        -0.44284207,  0.51732033,  0.51139784, -0.5087618 , -0.42996034,\n",
              "        -0.505281  ,  0.45595825,  0.5099497 ,  0.47299248, -0.44641778,\n",
              "        -0.5210847 ,  0.45460343,  0.48476076,  0.50291777,  0.481149  ,\n",
              "        -0.525002  ,  0.43306684, -0.5350017 , -0.48982775, -0.46237877,\n",
              "        -0.47771668,  0.48432863, -0.5121738 ,  0.46388605, -0.44931242,\n",
              "         0.44238496, -0.44549388, -0.4350424 , -0.45279038,  0.5082865 ,\n",
              "         0.5134138 ,  0.4343938 ,  0.4514401 , -0.43932265,  0.48951086],\n",
              "       dtype=float32),\n",
              " 'see': array([ 0.5528157 ,  0.44813624, -0.43598935, -0.4519521 , -0.49348682,\n",
              "         0.51686853,  0.4348105 , -0.47897515, -0.5224334 , -0.47597724,\n",
              "        -0.44670853,  0.47025675,  0.45370692,  0.49852997, -0.53404254,\n",
              "        -0.5135552 ,  0.52908754,  0.43649495, -0.4427102 , -0.4883616 ,\n",
              "        -0.47037974,  0.43795857,  0.50423217,  0.5082096 , -0.47072572,\n",
              "        -0.47561738,  0.5095685 ,  0.5292974 ,  0.43481   ,  0.4666083 ,\n",
              "        -0.45541394,  0.49819946, -0.45522562, -0.48616448, -0.50379455,\n",
              "        -0.51731646,  0.50340104, -0.51668173,  0.49772143, -0.489021  ,\n",
              "         0.4930957 , -0.4689471 , -0.49111867, -0.4640119 ,  0.50212574,\n",
              "         0.4910935 ,  0.48610696,  0.51880103, -0.46026134,  0.48117727],\n",
              "       dtype=float32),\n",
              " 'undervalued': array([ 0.47818992,  0.5006093 , -0.51886475, -0.4922008 , -0.52895784,\n",
              "         0.4830926 ,  0.48464435, -0.4984393 , -0.49460328, -0.48882395,\n",
              "        -0.43261895,  0.47019646,  0.48266208,  0.49725023, -0.48850515,\n",
              "        -0.45611852,  0.52186775,  0.46281493, -0.50221944, -0.44912007,\n",
              "        -0.46867803,  0.51250625,  0.5183188 ,  0.5046515 , -0.5164938 ,\n",
              "        -0.47910643,  0.45125893,  0.48307163,  0.51623225,  0.48107538,\n",
              "        -0.46592093,  0.43950757, -0.5201876 , -0.46657285, -0.43351075,\n",
              "        -0.52170664,  0.4974161 , -0.45896754,  0.4311519 , -0.5122237 ,\n",
              "         0.50469613, -0.50079113, -0.43792844, -0.5004791 ,  0.4423838 ,\n",
              "         0.45065972,  0.4473773 ,  0.45365173, -0.43606025,  0.4984375 ],\n",
              "       dtype=float32),\n",
              " 'respectively': array([ 0.5072814 ,  0.44796312, -0.5134376 , -0.5069376 , -0.45561603,\n",
              "         0.4859194 ,  0.48294675, -0.49722266, -0.4610293 , -0.5030141 ,\n",
              "        -0.4639285 ,  0.47032642,  0.4681613 ,  0.4583814 , -0.50825286,\n",
              "        -0.5037053 ,  0.5020606 ,  0.4419393 , -0.4639703 , -0.44026476,\n",
              "        -0.48924148,  0.4448774 ,  0.5230286 ,  0.5290416 , -0.4611526 ,\n",
              "        -0.5057855 ,  0.50229436,  0.51767105,  0.48067763,  0.52777755,\n",
              "        -0.5148256 ,  0.49894625, -0.52178645, -0.48390007, -0.501707  ,\n",
              "        -0.50097376,  0.54602104, -0.5213926 ,  0.51390827, -0.47944683,\n",
              "         0.4611963 , -0.5358125 , -0.4424767 , -0.49272078,  0.4553164 ,\n",
              "         0.46297812,  0.5084251 ,  0.48544842, -0.49450082,  0.44764853],\n",
              "       dtype=float32),\n",
              " 'place': array([ 0.5504548 ,  0.51029074, -0.5026331 , -0.47049764, -0.54492456,\n",
              "         0.46276027,  0.47317246, -0.45986223, -0.4303137 , -0.47499755,\n",
              "        -0.47029552,  0.51771986,  0.477139  ,  0.52126247, -0.4934365 ,\n",
              "        -0.44402394,  0.4663658 ,  0.44315735, -0.4562058 , -0.5223858 ,\n",
              "        -0.48386526,  0.5146053 ,  0.45721066,  0.4814866 , -0.50538373,\n",
              "        -0.46661943,  0.5022088 ,  0.50451225,  0.5214009 ,  0.46784103,\n",
              "        -0.46206227,  0.4746578 , -0.54325885, -0.5079599 , -0.44207242,\n",
              "        -0.53464186,  0.50647277, -0.4706165 ,  0.44546694, -0.453425  ,\n",
              "         0.43870848, -0.52853614, -0.48000616, -0.4830576 ,  0.510071  ,\n",
              "         0.4501834 ,  0.5225309 ,  0.4692184 , -0.4840547 ,  0.48083934],\n",
              "       dtype=float32),\n",
              " 'alternative': array([ 0.5148734 ,  0.4738109 , -0.47559088, -0.48310006, -0.48179597,\n",
              "         0.49585563,  0.44393986, -0.49013463, -0.5035851 , -0.481597  ,\n",
              "        -0.5078993 ,  0.5275621 ,  0.49426755,  0.45571998, -0.55671424,\n",
              "        -0.49522227,  0.4939189 ,  0.45775628, -0.45245457, -0.51237315,\n",
              "        -0.46337613,  0.46168166,  0.5360399 ,  0.5299071 , -0.47114894,\n",
              "        -0.48974967,  0.45747003,  0.55412376,  0.4821236 ,  0.53083724,\n",
              "        -0.51089925,  0.5245601 , -0.49559933, -0.48451823, -0.5258075 ,\n",
              "        -0.49159005,  0.5326671 , -0.5244454 ,  0.4897997 , -0.5038903 ,\n",
              "         0.5259073 , -0.46564835, -0.44078946, -0.49724218,  0.52340204,\n",
              "         0.45829302,  0.46841887,  0.48817697, -0.4720113 ,  0.5125096 ],\n",
              "       dtype=float32),\n",
              " 'energy': array([ 0.5295306 ,  0.44458804, -0.45671344, -0.46558416, -0.44335946,\n",
              "         0.5282983 ,  0.50230324, -0.454231  , -0.4601545 , -0.47124642,\n",
              "        -0.51729906,  0.45119956,  0.48283696,  0.49371785, -0.48840138,\n",
              "        -0.44616297,  0.523542  ,  0.5095553 , -0.4662729 , -0.4587895 ,\n",
              "        -0.5151741 ,  0.50183094,  0.50403506,  0.48187894, -0.44564822,\n",
              "        -0.45257002,  0.5071747 ,  0.52256095,  0.46738538,  0.4346368 ,\n",
              "        -0.48413998,  0.4924993 , -0.5188881 , -0.47433564, -0.5180624 ,\n",
              "        -0.5014958 ,  0.5244753 , -0.4669164 ,  0.5250472 , -0.52311325,\n",
              "         0.50942504, -0.51365435, -0.4670617 , -0.47509027,  0.44300297,\n",
              "         0.44972318,  0.4934194 ,  0.444279  , -0.5082886 ,  0.50570315],\n",
              "       dtype=float32),\n",
              " 'later': array([ 0.5143382 ,  0.43012086, -0.50819826, -0.48352608, -0.4592412 ,\n",
              "         0.5117309 ,  0.43104613, -0.4917519 , -0.44623214, -0.48943472,\n",
              "        -0.42955267,  0.42149788,  0.41688925,  0.4495276 , -0.49637634,\n",
              "        -0.42550695,  0.42882496,  0.4473099 , -0.47869328, -0.42275432,\n",
              "        -0.46402732,  0.46535003,  0.4380874 ,  0.41394287, -0.49082837,\n",
              "        -0.43417603,  0.48248512,  0.5135599 ,  0.44731644,  0.49498713,\n",
              "        -0.4424356 ,  0.43912065, -0.5065104 , -0.5066404 , -0.41436657,\n",
              "        -0.48825336,  0.4826679 , -0.50672305,  0.43127218, -0.44619337,\n",
              "         0.46689245, -0.48637483, -0.46863163, -0.49460843,  0.45301756,\n",
              "         0.48544997,  0.47685412,  0.4996098 , -0.49100837,  0.47723702],\n",
              "       dtype=float32),\n",
              " 'examples': array([ 0.53329796,  0.44826952, -0.4519327 , -0.46625322, -0.49966747,\n",
              "         0.5225646 ,  0.508866  , -0.49124572, -0.490567  , -0.43285125,\n",
              "        -0.45181167,  0.5134038 ,  0.5124754 ,  0.4909728 , -0.48441353,\n",
              "        -0.51969016,  0.47333816,  0.43911418, -0.5056014 , -0.44302705,\n",
              "        -0.45782995,  0.44834667,  0.48421395,  0.44293988, -0.4839987 ,\n",
              "        -0.44698378,  0.47892666,  0.538596  ,  0.4890703 ,  0.4327899 ,\n",
              "        -0.52477497,  0.46631488, -0.492885  , -0.5132402 , -0.42818838,\n",
              "        -0.51787037,  0.4820111 , -0.46857178,  0.48613453, -0.45978403,\n",
              "         0.47481734, -0.47868553, -0.46388906, -0.5070188 ,  0.46947098,\n",
              "         0.44302472,  0.44691902,  0.51244557, -0.466565  ,  0.45950657],\n",
              "       dtype=float32),\n",
              " 'balance': array([ 0.4901412 ,  0.42356318, -0.4918664 , -0.51889426, -0.47759932,\n",
              "         0.53019494,  0.47373846, -0.5240351 , -0.46962196, -0.44608822,\n",
              "        -0.44111592,  0.44498783,  0.5057731 ,  0.4457256 , -0.5258044 ,\n",
              "        -0.4669749 ,  0.49259543,  0.5034956 , -0.5056247 , -0.43186897,\n",
              "        -0.5230834 ,  0.47495246,  0.45985547,  0.497397  , -0.44757786,\n",
              "        -0.4914635 ,  0.43807316,  0.5344223 ,  0.4587946 ,  0.46753192,\n",
              "        -0.47862267,  0.49075675, -0.50690234, -0.46775967, -0.47132802,\n",
              "        -0.50501597,  0.5314305 , -0.45122522,  0.44919077, -0.4399527 ,\n",
              "         0.50632215, -0.45022908, -0.4286057 , -0.4952176 ,  0.47103104,\n",
              "         0.45682266,  0.506621  ,  0.44892618, -0.49195346,  0.49751818],\n",
              "       dtype=float32),\n",
              " 'figures': array([ 0.53291655,  0.5091816 , -0.46352404, -0.43790329, -0.47185665,\n",
              "         0.4922261 ,  0.46856236, -0.4479831 , -0.45002633, -0.5109858 ,\n",
              "        -0.47990668,  0.51922715,  0.48588505,  0.48652065, -0.4754056 ,\n",
              "        -0.44831154,  0.503648  ,  0.44129956, -0.4791622 , -0.45744368,\n",
              "        -0.5115995 ,  0.47663403,  0.52584887,  0.47397086, -0.46217045,\n",
              "        -0.43947577,  0.4737379 ,  0.45508116,  0.5075289 ,  0.4739803 ,\n",
              "        -0.47326085,  0.46509042, -0.4656848 , -0.46978304, -0.48712158,\n",
              "        -0.51677746,  0.48846385, -0.49405068,  0.4416914 , -0.44784752,\n",
              "         0.4631767 , -0.518257  , -0.5010691 , -0.4867573 ,  0.48473427,\n",
              "         0.46976122,  0.48767877,  0.5282366 , -0.48048046,  0.4635864 ],\n",
              "       dtype=float32),\n",
              " 'showing': array([ 0.5357832 ,  0.45342624, -0.5182923 , -0.51348966, -0.5460259 ,\n",
              "         0.5131467 ,  0.47704124, -0.4749489 , -0.52336705, -0.4677204 ,\n",
              "        -0.5564375 ,  0.49758705,  0.5494937 ,  0.55997735, -0.554297  ,\n",
              "        -0.49936268,  0.47113   ,  0.4883659 , -0.5445212 , -0.4421307 ,\n",
              "        -0.5201069 ,  0.49071783,  0.5165545 ,  0.5019896 , -0.48992693,\n",
              "        -0.46364415,  0.4508885 ,  0.50709975,  0.5339387 ,  0.51580703,\n",
              "        -0.5155442 ,  0.49657378, -0.5300114 , -0.4950898 , -0.5427262 ,\n",
              "        -0.5497774 ,  0.5635006 , -0.45701164,  0.5132608 , -0.48097485,\n",
              "         0.52896076, -0.4809252 , -0.4891164 , -0.47030726,  0.4537793 ,\n",
              "         0.53616107,  0.5393727 ,  0.49899164, -0.488924  ,  0.48814493],\n",
              "       dtype=float32),\n",
              " 'methodology': array([ 0.51451474,  0.49607748, -0.50784624, -0.4816262 , -0.52763927,\n",
              "         0.50977564,  0.5244261 , -0.44505474, -0.47645614, -0.4854688 ,\n",
              "        -0.47037286,  0.4995321 ,  0.46662185,  0.5147066 , -0.47493944,\n",
              "        -0.4453969 ,  0.5068791 ,  0.45395762, -0.46840867, -0.5045154 ,\n",
              "        -0.47696   ,  0.44257587,  0.47989237,  0.44410005, -0.51697165,\n",
              "        -0.47694397,  0.4604646 ,  0.4691437 ,  0.4633109 ,  0.47498432,\n",
              "        -0.46964794,  0.4292426 , -0.46810216, -0.4555337 , -0.49213588,\n",
              "        -0.50292   ,  0.5302251 , -0.47353804,  0.48094633, -0.51565176,\n",
              "         0.5034974 , -0.47299314, -0.47337425, -0.5194684 ,  0.44124988,\n",
              "         0.455148  ,  0.4445973 ,  0.466723  , -0.4583366 ,  0.505113  ],\n",
              "       dtype=float32),\n",
              " 'error': array([ 0.4928223 ,  0.42910537, -0.49535555, -0.47850287, -0.47656703,\n",
              "         0.51464766,  0.51221603, -0.44957376, -0.48894405, -0.49822176,\n",
              "        -0.50355023,  0.50708234,  0.47375056,  0.49546733, -0.517536  ,\n",
              "        -0.49033335,  0.45626122,  0.46370015, -0.51242614, -0.4859366 ,\n",
              "        -0.5137796 ,  0.48930383,  0.44566217,  0.48980564, -0.46320334,\n",
              "        -0.5211739 ,  0.50980973,  0.52739966,  0.44552493,  0.47248274,\n",
              "        -0.46324116,  0.43462178, -0.5267595 , -0.44945407, -0.4690235 ,\n",
              "        -0.46751764,  0.48836863, -0.5131826 ,  0.48465723, -0.49115777,\n",
              "         0.47524998, -0.49761593, -0.46951804, -0.521094  ,  0.44993752,\n",
              "         0.45259264,  0.4723778 ,  0.49222013, -0.47732604,  0.44804597],\n",
              "       dtype=float32),\n",
              " 'generated': array([ 0.4834041 ,  0.46381786, -0.49288133, -0.5143397 , -0.53991055,\n",
              "         0.4667737 ,  0.462604  , -0.4697658 , -0.47967854, -0.4522239 ,\n",
              "        -0.45143718,  0.5253426 ,  0.5078148 ,  0.53302735, -0.5631708 ,\n",
              "        -0.4831373 ,  0.4722903 ,  0.5183259 , -0.45201322, -0.48466223,\n",
              "        -0.49140394,  0.45758775,  0.47490066,  0.5313835 , -0.44186553,\n",
              "        -0.45274812,  0.5019983 ,  0.54522234,  0.496325  ,  0.48033708,\n",
              "        -0.44236374,  0.544892  , -0.53153914, -0.52079225, -0.49592003,\n",
              "        -0.4782874 ,  0.46859917, -0.5172534 ,  0.49512333, -0.46071628,\n",
              "         0.5476303 , -0.50103563, -0.4961906 , -0.47366244,  0.47750852,\n",
              "         0.5030331 ,  0.47551772,  0.4949029 , -0.47233307,  0.51327133],\n",
              "       dtype=float32),\n",
              " 'effects': array([ 0.5201956 ,  0.49051684, -0.46704453, -0.4232344 , -0.44534346,\n",
              "         0.452159  ,  0.4519331 , -0.5041821 , -0.45250586, -0.49014616,\n",
              "        -0.47016224,  0.4416155 ,  0.48345828,  0.44901788, -0.53516823,\n",
              "        -0.47676057,  0.46604398,  0.5057116 , -0.4903493 , -0.43447563,\n",
              "        -0.47091183,  0.4416644 ,  0.5192149 ,  0.44348738, -0.46579078,\n",
              "        -0.5007582 ,  0.4176071 ,  0.49573982,  0.44180152,  0.49993336,\n",
              "        -0.4943695 ,  0.4212742 , -0.48237228, -0.4636272 , -0.48003614,\n",
              "        -0.43397632,  0.47142714, -0.49422798,  0.47818768, -0.45595655,\n",
              "         0.4421548 , -0.52012897, -0.46326372, -0.47359276,  0.4840907 ,\n",
              "         0.43006104,  0.5075182 ,  0.44637805, -0.48497713,  0.50405836],\n",
              "       dtype=float32),\n",
              " 'changing': array([ 0.48571056,  0.46854   , -0.5125216 , -0.45248276, -0.48734736,\n",
              "         0.44091666,  0.4593891 , -0.463412  , -0.4709985 , -0.47316533,\n",
              "        -0.4824702 ,  0.48818743,  0.4377476 ,  0.5008133 , -0.46724835,\n",
              "        -0.46516785,  0.47980446,  0.47014335, -0.4625954 , -0.45108962,\n",
              "        -0.4250331 ,  0.43892017,  0.52141225,  0.42258015, -0.49028563,\n",
              "        -0.4559219 ,  0.4633579 ,  0.5044742 ,  0.50841945,  0.48776516,\n",
              "        -0.46893156,  0.4750432 , -0.49238482, -0.45484227, -0.46772227,\n",
              "        -0.43111938,  0.5235911 , -0.51371217,  0.48803562, -0.49634853,\n",
              "         0.44381592, -0.505759  , -0.44201648, -0.48439363,  0.4725543 ,\n",
              "         0.4806599 ,  0.44852877,  0.47427806, -0.50728923,  0.50324905],\n",
              "       dtype=float32),\n",
              " 'relating': array([ 0.5572646 ,  0.4506267 , -0.43776816, -0.4883312 , -0.45136714,\n",
              "         0.45666957,  0.45361796, -0.43519145, -0.5012314 , -0.49082845,\n",
              "        -0.50004756,  0.49273023,  0.47824162,  0.46850204, -0.5064913 ,\n",
              "        -0.49392062,  0.48416528,  0.4768572 , -0.48137158, -0.50630915,\n",
              "        -0.48458585,  0.4265274 ,  0.51702464,  0.46424887, -0.49257857,\n",
              "        -0.46341202,  0.47086695,  0.49833074,  0.45885247,  0.44709688,\n",
              "        -0.52403724,  0.43789735, -0.5108653 , -0.49786782, -0.42550138,\n",
              "        -0.4454603 ,  0.48988864, -0.44655806,  0.48130265, -0.46920934,\n",
              "         0.44546998, -0.44912055, -0.5055511 , -0.49072757,  0.4758532 ,\n",
              "         0.44022003,  0.45993993,  0.48044202, -0.42901057,  0.4649044 ],\n",
              "       dtype=float32),\n",
              " 'denominated': array([ 0.4511782 ,  0.44949   , -0.42991433, -0.44938174, -0.5067569 ,\n",
              "         0.50162554,  0.46532643, -0.5072725 , -0.49626034, -0.4818998 ,\n",
              "        -0.4744165 ,  0.42749882,  0.43293598,  0.47197407, -0.48981062,\n",
              "        -0.48774472,  0.4381817 ,  0.4632923 , -0.4561369 , -0.4181239 ,\n",
              "        -0.46622187,  0.4298777 ,  0.52060974,  0.45117876, -0.49850473,\n",
              "        -0.4463758 ,  0.49989173,  0.45664325,  0.45318404,  0.44168046,\n",
              "        -0.50587595,  0.4949847 , -0.5207387 , -0.47691992, -0.43690372,\n",
              "        -0.49711815,  0.48829305, -0.45691156,  0.4386844 , -0.51312125,\n",
              "         0.48327497, -0.47793362, -0.494921  , -0.4507961 ,  0.48761344,\n",
              "         0.4741712 ,  0.43267843,  0.51376075, -0.48607415,  0.46128052],\n",
              "       dtype=float32),\n",
              " 'exposures': array([ 0.45722833,  0.4536854 , -0.4266596 , -0.4818655 , -0.42838523,\n",
              "         0.49941325,  0.41403204, -0.46406356, -0.49632043, -0.4333816 ,\n",
              "        -0.4758474 ,  0.42893288,  0.46890524,  0.44767886, -0.46604753,\n",
              "        -0.42366448,  0.43680742,  0.48627502, -0.4721913 , -0.4433707 ,\n",
              "        -0.45294344,  0.4509791 ,  0.50130683,  0.4415632 , -0.43472126,\n",
              "        -0.4277675 ,  0.4536508 ,  0.47056964,  0.47091717,  0.4199867 ,\n",
              "        -0.45683742,  0.43796977, -0.4836979 , -0.4204292 , -0.4620994 ,\n",
              "        -0.44691157,  0.49616417, -0.48460644,  0.49263954, -0.45314813,\n",
              "         0.43757302, -0.41820592, -0.44772834, -0.4499588 ,  0.44214892,\n",
              "         0.4794109 ,  0.47676748,  0.43212557, -0.46107757,  0.46862912],\n",
              "       dtype=float32),\n",
              " 'selecting': array([ 0.4727016 ,  0.46555817, -0.45743027, -0.51425856, -0.49787846,\n",
              "         0.4805138 ,  0.46593797, -0.4806313 , -0.49216998, -0.48003477,\n",
              "        -0.49403295,  0.5094538 ,  0.43601376,  0.47037965, -0.48590276,\n",
              "        -0.49829134,  0.47054645,  0.48102716, -0.5246248 , -0.5075361 ,\n",
              "        -0.51627845,  0.44257042,  0.45268813,  0.4680121 , -0.4829641 ,\n",
              "        -0.46254146,  0.5102409 ,  0.50803787,  0.5011221 ,  0.47024322,\n",
              "        -0.5196976 ,  0.4938653 , -0.5321825 , -0.47908545, -0.4542608 ,\n",
              "        -0.51963115,  0.52144474, -0.52968276,  0.4840839 , -0.49953204,\n",
              "         0.47419637, -0.4804599 , -0.46659902, -0.48083892,  0.45254368,\n",
              "         0.5168396 ,  0.51367635,  0.4984569 , -0.45053917,  0.48701948],\n",
              "       dtype=float32),\n",
              " 'small-': array([ 0.45249653,  0.48719224, -0.43914738, -0.51759225, -0.50787354,\n",
              "         0.48939955,  0.47408226, -0.44238785, -0.43751475, -0.48121357,\n",
              "        -0.51078707,  0.44086987,  0.47116303,  0.5119362 , -0.5300203 ,\n",
              "        -0.43799904,  0.43995064,  0.47008774, -0.4660144 , -0.4655912 ,\n",
              "        -0.49205494,  0.44062504,  0.46438634,  0.4979694 , -0.44282728,\n",
              "        -0.5166073 ,  0.46238324,  0.45960715,  0.48870715,  0.5021395 ,\n",
              "        -0.49734756,  0.51300555, -0.46058267, -0.45776147, -0.44973323,\n",
              "        -0.4789855 ,  0.5188621 , -0.5042519 ,  0.4929374 , -0.51173425,\n",
              "         0.4950906 , -0.5099356 , -0.45433068, -0.50185835,  0.47323498,\n",
              "         0.48158118,  0.45932564,  0.45178965, -0.45531136,  0.43752688],\n",
              "       dtype=float32),\n",
              " 'depends': array([ 0.49902925,  0.5005624 , -0.45656595, -0.48888662, -0.44617572,\n",
              "         0.4772817 ,  0.5115376 , -0.5225274 , -0.4295473 , -0.4669574 ,\n",
              "        -0.5043812 ,  0.44809082,  0.44434446,  0.47472852, -0.53899556,\n",
              "        -0.49319193,  0.49685907,  0.4759062 , -0.50002956, -0.45335484,\n",
              "        -0.51147145,  0.4318758 ,  0.47458774,  0.48862207, -0.48085874,\n",
              "        -0.4350197 ,  0.4314605 ,  0.46071467,  0.49173993,  0.4423451 ,\n",
              "        -0.4801141 ,  0.44197342, -0.5209757 , -0.46776235, -0.45148167,\n",
              "        -0.51721597,  0.44803724, -0.46757674,  0.45754552, -0.4992175 ,\n",
              "         0.46756473, -0.48350638, -0.5019175 , -0.50727355,  0.51935935,\n",
              "         0.44382596,  0.514953  ,  0.4909444 , -0.50594175,  0.47287938],\n",
              "       dtype=float32),\n",
              " 'refer': array([ 0.4793244 ,  0.43915087, -0.478267  , -0.46838635, -0.5152868 ,\n",
              "         0.52279776,  0.46986234, -0.513836  , -0.53164184, -0.4756056 ,\n",
              "        -0.44320148,  0.5091063 ,  0.45780855,  0.5127508 , -0.5459642 ,\n",
              "        -0.49300796,  0.5335927 ,  0.4764194 , -0.45847443, -0.43379918,\n",
              "        -0.45074078,  0.48375925,  0.53254485,  0.43990198, -0.5002647 ,\n",
              "        -0.45912784,  0.4860434 ,  0.4904082 ,  0.51211596,  0.47868514,\n",
              "        -0.4940479 ,  0.44974032, -0.47850478, -0.48039895, -0.48090255,\n",
              "        -0.48068422,  0.45779836, -0.48184678,  0.50701463, -0.45849794,\n",
              "         0.47873852, -0.5010139 , -0.45701477, -0.44594985,  0.47399616,\n",
              "         0.4873083 ,  0.49691314,  0.5274433 , -0.46479255,  0.45850146],\n",
              "       dtype=float32),\n",
              " 'stable': array([ 0.4919788 ,  0.4598935 , -0.49212623, -0.4654829 , -0.5173123 ,\n",
              "         0.45995325,  0.5032563 , -0.42884177, -0.50547045, -0.47394022,\n",
              "        -0.42525303,  0.49360383,  0.49754924,  0.44970798, -0.46856466,\n",
              "        -0.44780996,  0.499739  ,  0.45960397, -0.45027196, -0.4747021 ,\n",
              "        -0.46099585,  0.46240968,  0.4586917 ,  0.48669538, -0.46194297,\n",
              "        -0.45542675,  0.47998914,  0.5229368 ,  0.49553686,  0.41710892,\n",
              "        -0.4554334 ,  0.44426763, -0.46191853, -0.5191389 , -0.4235365 ,\n",
              "        -0.453246  ,  0.4417453 , -0.47263256,  0.41933975, -0.47619134,\n",
              "         0.48412293, -0.49463087, -0.42410666, -0.47077668,  0.43274328,\n",
              "         0.46400604,  0.4689818 ,  0.49460313, -0.4566864 ,  0.42250046],\n",
              "       dtype=float32),\n",
              " 'requests': array([ 0.46798036,  0.50791425, -0.4922691 , -0.5061945 , -0.4961633 ,\n",
              "         0.48822153,  0.44310057, -0.4591167 , -0.45093727, -0.47038978,\n",
              "        -0.51036245,  0.52036417,  0.471057  ,  0.49297422, -0.4898414 ,\n",
              "        -0.49570963,  0.48915175,  0.48125347, -0.46323833, -0.5199689 ,\n",
              "        -0.49424738,  0.4754928 ,  0.45864123,  0.45736837, -0.51125723,\n",
              "        -0.44118625,  0.48338434,  0.51046795,  0.49248174,  0.5100068 ,\n",
              "        -0.52903813,  0.44200322, -0.48735338, -0.47456294, -0.44656226,\n",
              "        -0.4694666 ,  0.48302656, -0.46535483,  0.46173513, -0.48681444,\n",
              "         0.51802385, -0.48129332, -0.46287644, -0.44691446,  0.50908124,\n",
              "         0.5372417 ,  0.44584423,  0.46441528, -0.52764   ,  0.46042714],\n",
              "       dtype=float32),\n",
              " 'institutions': array([ 0.45968997,  0.45055157, -0.47531512, -0.5241834 , -0.48599747,\n",
              "         0.47667027,  0.4572992 , -0.4523398 , -0.51777613, -0.43184346,\n",
              "        -0.5042993 ,  0.44118088,  0.5134743 ,  0.4931592 , -0.5137357 ,\n",
              "        -0.44752356,  0.43808132,  0.51918036, -0.49373576, -0.5204241 ,\n",
              "        -0.5106395 ,  0.48075932,  0.47233897,  0.4735848 , -0.52297014,\n",
              "        -0.49333188,  0.46317777,  0.4990179 ,  0.47348234,  0.52438855,\n",
              "        -0.46819162,  0.4449532 , -0.5286797 , -0.4880258 , -0.47687003,\n",
              "        -0.4655969 ,  0.46886984, -0.5241964 ,  0.49538636, -0.5273338 ,\n",
              "         0.4822184 , -0.4631685 , -0.46862143, -0.50545174,  0.48833784,\n",
              "         0.52970797,  0.46856087,  0.48049444, -0.4632844 ,  0.47367895],\n",
              "       dtype=float32),\n",
              " 'generate': array([ 0.50824285,  0.4420299 , -0.4448252 , -0.48901466, -0.5229453 ,\n",
              "         0.4865483 ,  0.46520042, -0.4786427 , -0.5229738 , -0.485379  ,\n",
              "        -0.5089914 ,  0.46531892,  0.458196  ,  0.5128982 , -0.5617837 ,\n",
              "        -0.4735694 ,  0.48474184,  0.5307776 , -0.5173632 , -0.46379182,\n",
              "        -0.5230757 ,  0.4327495 ,  0.50179917,  0.49343944, -0.520228  ,\n",
              "        -0.48113048,  0.4372245 ,  0.47137514,  0.43500194,  0.47520012,\n",
              "        -0.5160289 ,  0.5235655 , -0.5474854 , -0.5240948 , -0.48098928,\n",
              "        -0.48798352,  0.512217  , -0.46704972,  0.49791175, -0.54344356,\n",
              "         0.45167804, -0.5272232 , -0.5001322 , -0.43846524,  0.49345627,\n",
              "         0.4435701 ,  0.47248298,  0.4527586 , -0.4947042 ,  0.44893038],\n",
              "       dtype=float32),\n",
              " 'august': array([ 0.53094864,  0.41757104, -0.4634103 , -0.45390117, -0.5019239 ,\n",
              "         0.5206837 ,  0.5005041 , -0.51248366, -0.4359423 , -0.42057136,\n",
              "        -0.48012578,  0.46433738,  0.4622029 ,  0.42820132, -0.5411926 ,\n",
              "        -0.51057136,  0.46193135,  0.49074313, -0.50996643, -0.48274988,\n",
              "        -0.50990343,  0.5020414 ,  0.49843898,  0.44780207, -0.44258723,\n",
              "        -0.48529106,  0.50220644,  0.4561411 ,  0.49731693,  0.44591343,\n",
              "        -0.5062085 ,  0.44448814, -0.450666  , -0.45696992, -0.4670289 ,\n",
              "        -0.5048742 ,  0.5137217 , -0.51781297,  0.45737407, -0.435002  ,\n",
              "         0.51890254, -0.5092636 , -0.4473978 , -0.42346218,  0.5111276 ,\n",
              "         0.5147307 ,  0.49703598,  0.44814622, -0.51632226,  0.4604365 ],\n",
              "       dtype=float32),\n",
              " 'reserve': array([ 0.448363  ,  0.41638902, -0.44760954, -0.45377186, -0.47064653,\n",
              "         0.44931516,  0.45316657, -0.4288148 , -0.47731724, -0.42391804,\n",
              "        -0.4704792 ,  0.5095647 ,  0.46897498,  0.44530287, -0.502354  ,\n",
              "        -0.43694144,  0.50706124,  0.48895428, -0.49400374, -0.4611169 ,\n",
              "        -0.5055303 ,  0.46496314,  0.49898797,  0.42774007, -0.5009715 ,\n",
              "        -0.44280317,  0.43148696,  0.5004844 ,  0.49777913,  0.46870452,\n",
              "        -0.45813155,  0.47946194, -0.48674148, -0.5156102 , -0.48406267,\n",
              "        -0.43119463,  0.4415226 , -0.4814896 ,  0.4298553 , -0.4967496 ,\n",
              "         0.4775176 , -0.5150554 , -0.4511841 , -0.49350277,  0.4371783 ,\n",
              "         0.47008768,  0.46318474,  0.44256586, -0.4653954 ,  0.4512679 ],\n",
              "       dtype=float32),\n",
              " 'first': array([ 0.53512496,  0.45291787, -0.5220281 , -0.4326168 , -0.45247602,\n",
              "         0.49092197,  0.4729499 , -0.46035755, -0.46661404, -0.5051991 ,\n",
              "        -0.5158726 ,  0.45160988,  0.4607354 ,  0.5038503 , -0.52152073,\n",
              "        -0.50131124,  0.48224887,  0.47413942, -0.49145678, -0.4888769 ,\n",
              "        -0.5223255 ,  0.45018572,  0.49089485,  0.51177853, -0.46628943,\n",
              "        -0.4591533 ,  0.51544553,  0.47803193,  0.47303876,  0.4852081 ,\n",
              "        -0.47166806,  0.47089863, -0.48609093, -0.5140275 , -0.5110455 ,\n",
              "        -0.44217333,  0.5235902 , -0.50966907,  0.50219285, -0.4721898 ,\n",
              "         0.4681384 , -0.45588523, -0.45449236, -0.45776236,  0.46806008,\n",
              "         0.51070076,  0.46845144,  0.49173087, -0.47396138,  0.44608763],\n",
              "       dtype=float32),\n",
              " 'reference': array([ 0.46675712,  0.4310489 , -0.5030752 , -0.5220237 , -0.45901302,\n",
              "         0.4541659 ,  0.4759535 , -0.472619  , -0.5127254 , -0.45286897,\n",
              "        -0.50684303,  0.5228717 ,  0.440399  ,  0.4776978 , -0.48210055,\n",
              "        -0.4944885 ,  0.4805469 ,  0.44953787, -0.4948039 , -0.4461705 ,\n",
              "        -0.45877475,  0.44455042,  0.47040126,  0.46535093, -0.46076912,\n",
              "        -0.47462487,  0.47039548,  0.5428169 ,  0.5240274 ,  0.44999927,\n",
              "        -0.5099355 ,  0.45140788, -0.54307216, -0.46261564, -0.47955665,\n",
              "        -0.46872598,  0.5148989 , -0.5243671 ,  0.48219994, -0.51414585,\n",
              "         0.48001912, -0.53315437, -0.48923987, -0.4541484 ,  0.5043552 ,\n",
              "         0.45329368,  0.49796993,  0.44154853, -0.44987845,  0.43790132],\n",
              "       dtype=float32),\n",
              " 'excluding': array([ 0.47354102,  0.43630162, -0.4895227 , -0.45709813, -0.47471994,\n",
              "         0.46828064,  0.445777  , -0.5093858 , -0.4241795 , -0.4349694 ,\n",
              "        -0.44501323,  0.4827966 ,  0.44541463,  0.44774106, -0.48793665,\n",
              "        -0.47215545,  0.5159698 ,  0.514513  , -0.48959634, -0.45311472,\n",
              "        -0.49253508,  0.4375392 ,  0.42946884,  0.4901491 , -0.4303902 ,\n",
              "        -0.50803787,  0.49577153,  0.4535974 ,  0.4523714 ,  0.451107  ,\n",
              "        -0.437493  ,  0.48393834, -0.51575017, -0.49487165, -0.44087023,\n",
              "        -0.46510997,  0.5013826 , -0.48274362,  0.47349265, -0.48593986,\n",
              "         0.42929322, -0.51290536, -0.49787056, -0.44210762,  0.45944196,\n",
              "         0.48065364,  0.42071965,  0.50713825, -0.4435665 ,  0.46165463],\n",
              "       dtype=float32),\n",
              " 'manage': array([ 0.45080602,  0.41769218, -0.49363077, -0.4545517 , -0.5177039 ,\n",
              "         0.46604317,  0.46452066, -0.44604418, -0.5071449 , -0.43960208,\n",
              "        -0.45503068,  0.48410177,  0.49999857,  0.4406907 , -0.4901617 ,\n",
              "        -0.4832443 ,  0.5025675 ,  0.4796045 , -0.4785232 , -0.50380707,\n",
              "        -0.44772914,  0.46564853,  0.44362602,  0.48140582, -0.47095966,\n",
              "        -0.501418  ,  0.49393073,  0.5326677 ,  0.43405977,  0.46030024,\n",
              "        -0.44060066,  0.4804045 , -0.47077304, -0.52025795, -0.42262813,\n",
              "        -0.5003618 ,  0.52400595, -0.4647991 ,  0.49559224, -0.44077706,\n",
              "         0.49087992, -0.49286693, -0.49508232, -0.42759192,  0.43322265,\n",
              "         0.45630893,  0.4526377 ,  0.4885695 , -0.5158289 ,  0.4780007 ],\n",
              "       dtype=float32),\n",
              " 'systematic': array([ 0.47901937,  0.45224923, -0.4730745 , -0.50448006, -0.45246664,\n",
              "         0.52707493,  0.43579534, -0.50510573, -0.50064546, -0.45590067,\n",
              "        -0.44167304,  0.4978536 ,  0.513245  ,  0.5126245 , -0.49515066,\n",
              "        -0.48493814,  0.4718209 ,  0.51398456, -0.44416395, -0.44881922,\n",
              "        -0.46490508,  0.4837528 ,  0.4755996 ,  0.49790856, -0.43999386,\n",
              "        -0.4556755 ,  0.5078232 ,  0.46484697,  0.44754344,  0.4718575 ,\n",
              "        -0.46397075,  0.4671561 , -0.54393303, -0.5104004 , -0.4630224 ,\n",
              "        -0.49836934,  0.48477095, -0.47004545,  0.44866365, -0.4412921 ,\n",
              "         0.533533  , -0.48645058, -0.477557  , -0.43235233,  0.44009623,\n",
              "         0.4858898 ,  0.45900926,  0.47399518, -0.45476383,  0.50262314],\n",
              "       dtype=float32),\n",
              " 'domiciled': array([ 0.52159274,  0.50827056, -0.4369756 , -0.45158648, -0.52023226,\n",
              "         0.5438573 ,  0.46240163, -0.5322931 , -0.4587196 , -0.44450843,\n",
              "        -0.49819708,  0.47457802,  0.46376038,  0.47988442, -0.55185074,\n",
              "        -0.45791793,  0.5153337 ,  0.5327535 , -0.4972285 , -0.48691356,\n",
              "        -0.4682845 ,  0.4962441 ,  0.5225984 ,  0.4555466 , -0.47017315,\n",
              "        -0.47063342,  0.5098867 ,  0.5387641 ,  0.44276318,  0.45289853,\n",
              "        -0.4524916 ,  0.51125014, -0.5236448 , -0.45955753, -0.5085022 ,\n",
              "        -0.44495597,  0.48028272, -0.5087545 ,  0.44236785, -0.49481848,\n",
              "         0.4554506 , -0.4666073 , -0.44651937, -0.4743074 ,  0.47086775,\n",
              "         0.47823343,  0.5153992 ,  0.45462248, -0.5025155 ,  0.49679035],\n",
              "       dtype=float32),\n",
              " 'closely': array([ 0.48025894,  0.43835258, -0.4989898 , -0.50583583, -0.46684125,\n",
              "         0.5352108 ,  0.5287164 , -0.45350152, -0.479333  , -0.46175414,\n",
              "        -0.5270291 ,  0.463904  ,  0.47520304,  0.50441825, -0.53447783,\n",
              "        -0.4916057 ,  0.459923  ,  0.47948885, -0.48934346, -0.49228424,\n",
              "        -0.49060565,  0.46506253,  0.45711324,  0.4771899 , -0.52456015,\n",
              "        -0.48970282,  0.5296623 ,  0.5030823 ,  0.43952736,  0.43583512,\n",
              "        -0.4748344 ,  0.5169092 , -0.51766163, -0.518754  , -0.5185351 ,\n",
              "        -0.48818046,  0.5252148 , -0.51676476,  0.4422938 , -0.48707512,\n",
              "         0.48992622, -0.51846075, -0.52141255, -0.50877   ,  0.46863365,\n",
              "         0.45685092,  0.48695126,  0.48478815, -0.50163585,  0.4895872 ],\n",
              "       dtype=float32),\n",
              " 'regulations': array([ 0.48236826,  0.49222058, -0.4340418 , -0.465791  , -0.4830129 ,\n",
              "         0.51352686,  0.4347044 , -0.46235678, -0.44351253, -0.50873977,\n",
              "        -0.43674824,  0.482831  ,  0.50943655,  0.44864812, -0.48521346,\n",
              "        -0.44384897,  0.50768363,  0.47994155, -0.42362568, -0.47638503,\n",
              "        -0.44155264,  0.50438976,  0.46214053,  0.45141912, -0.47655705,\n",
              "        -0.51416636,  0.44929773,  0.46275204,  0.4595232 ,  0.43494892,\n",
              "        -0.46457684,  0.45930326, -0.46670485, -0.49180862, -0.49136555,\n",
              "        -0.49515024,  0.47219837, -0.5141767 ,  0.4331026 , -0.48597017,\n",
              "         0.4915388 , -0.525502  , -0.4765819 , -0.43937424,  0.45280507,\n",
              "         0.4395361 ,  0.4542692 ,  0.45115545, -0.43664494,  0.45577866],\n",
              "       dtype=float32),\n",
              " 'stated': array([ 0.51861995,  0.44721228, -0.44718248, -0.4746074 , -0.44507256,\n",
              "         0.4500125 ,  0.50614923, -0.4893393 , -0.5114998 , -0.45567116,\n",
              "        -0.47273952,  0.44873387,  0.48440105,  0.49365526, -0.4675627 ,\n",
              "        -0.48148212,  0.51941705,  0.48763806, -0.4289937 , -0.4629205 ,\n",
              "        -0.4378921 ,  0.48064184,  0.43189326,  0.43164247, -0.4599875 ,\n",
              "        -0.514515  ,  0.4319647 ,  0.49680984,  0.5037747 ,  0.50961053,\n",
              "        -0.5016588 ,  0.4281189 , -0.51768976, -0.45748416, -0.4214645 ,\n",
              "        -0.4565909 ,  0.49161977, -0.4563895 ,  0.5001981 , -0.49108535,\n",
              "         0.44405335, -0.496301  , -0.45286077, -0.43136647,  0.4458516 ,\n",
              "         0.4331632 ,  0.45858043,  0.49407387, -0.49951354,  0.47290954],\n",
              "       dtype=float32),\n",
              " 'indices': array([ 0.4844139 ,  0.50226575, -0.48160353, -0.49271017, -0.5005319 ,\n",
              "         0.51417494,  0.4586061 , -0.5124215 , -0.4459815 , -0.4646225 ,\n",
              "        -0.48021087,  0.477084  ,  0.49108478,  0.45123395, -0.5350953 ,\n",
              "        -0.46233594,  0.4617561 ,  0.5180743 , -0.45157102, -0.46965608,\n",
              "        -0.47760892,  0.49602634,  0.4473373 ,  0.47052306, -0.5137405 ,\n",
              "        -0.45476538,  0.5020908 ,  0.53885174,  0.47357574,  0.5230893 ,\n",
              "        -0.5151404 ,  0.45935768, -0.46259272, -0.53216225, -0.4682193 ,\n",
              "        -0.4872645 ,  0.4956572 , -0.47770804,  0.48518068, -0.5178819 ,\n",
              "         0.4540511 , -0.4734422 , -0.4914495 , -0.45006448,  0.48366725,\n",
              "         0.45172387,  0.47739008,  0.46624732, -0.51935834,  0.46449286],\n",
              "       dtype=float32),\n",
              " 'broker': array([ 0.4776547 ,  0.48226523, -0.45853657, -0.4744176 , -0.50756526,\n",
              "         0.50268483,  0.5024719 , -0.4367455 , -0.4581241 , -0.43335482,\n",
              "        -0.4727637 ,  0.4632674 ,  0.4612087 ,  0.44206452, -0.5292796 ,\n",
              "        -0.4446845 ,  0.504735  ,  0.4988188 , -0.4993806 , -0.4577207 ,\n",
              "        -0.5039814 ,  0.4656427 ,  0.44831565,  0.44640464, -0.49757743,\n",
              "        -0.49539262,  0.5083582 ,  0.50720054,  0.42131677,  0.46735442,\n",
              "        -0.48401278,  0.5157536 , -0.4496909 , -0.50321436, -0.43757728,\n",
              "        -0.45867985,  0.4612025 , -0.49991563,  0.43791655, -0.49009705,\n",
              "         0.5299816 , -0.4798152 , -0.4800309 , -0.43927953,  0.4588467 ,\n",
              "         0.44297513,  0.4815581 ,  0.48750654, -0.4395703 ,  0.46634448],\n",
              "       dtype=float32),\n",
              " 'unlike': array([ 0.46729404,  0.47660047, -0.5123513 , -0.49891132, -0.48818654,\n",
              "         0.5097599 ,  0.51144   , -0.44342232, -0.47997493, -0.44118166,\n",
              "        -0.47804594,  0.5056316 ,  0.4913355 ,  0.4908883 , -0.4914367 ,\n",
              "        -0.48704338,  0.49396616,  0.44438303, -0.44427007, -0.48698428,\n",
              "        -0.44328988,  0.42319307,  0.45135704,  0.4460522 , -0.44045836,\n",
              "        -0.48924926,  0.49409333,  0.52330846,  0.49908036,  0.46083772,\n",
              "        -0.47302794,  0.48094434, -0.45392898, -0.43468255, -0.5057065 ,\n",
              "        -0.46767685,  0.4351421 , -0.4560281 ,  0.47907338, -0.43153134,\n",
              "         0.4420175 , -0.44367802, -0.42147112, -0.47021246,  0.5150385 ,\n",
              "         0.47268948,  0.4429488 ,  0.46849534, -0.49233177,  0.49909687],\n",
              "       dtype=float32),\n",
              " 'favorable': array([ 0.49407178,  0.4819582 , -0.49797592, -0.44346827, -0.5230359 ,\n",
              "         0.4764888 ,  0.44072866, -0.49690986, -0.4784962 , -0.42023945,\n",
              "        -0.43881977,  0.43928087,  0.42622614,  0.48954362, -0.54020476,\n",
              "        -0.44214222,  0.49502367,  0.42601955, -0.47472432, -0.46847516,\n",
              "        -0.44740397,  0.43863752,  0.43160608,  0.5021842 , -0.44198963,\n",
              "        -0.47558615,  0.46087644,  0.4990188 ,  0.45462012,  0.48850504,\n",
              "        -0.5050082 ,  0.44065085, -0.4720972 , -0.47501066, -0.43298805,\n",
              "        -0.4814228 ,  0.5072652 , -0.47214642,  0.48245972, -0.50936687,\n",
              "         0.43122688, -0.48194587, -0.44736055, -0.47101682,  0.51211464,\n",
              "         0.4576944 ,  0.49659768,  0.49425858, -0.49384552,  0.47790736],\n",
              "       dtype=float32),\n",
              " 'fdc': array([ 0.53551275,  0.43212008, -0.4843412 , -0.46408996, -0.46099705,\n",
              "         0.51488155,  0.43694395, -0.4393825 , -0.47225797, -0.46825552,\n",
              "        -0.43537533,  0.52070886,  0.4730422 ,  0.49534628, -0.5090753 ,\n",
              "        -0.51968706,  0.49352694,  0.49368185, -0.50788563, -0.49679542,\n",
              "        -0.4794769 ,  0.4466055 ,  0.49198455,  0.50521964, -0.49581283,\n",
              "        -0.4442763 ,  0.5098767 ,  0.5236326 ,  0.5162224 ,  0.4630416 ,\n",
              "        -0.49726453,  0.48292524, -0.5119586 , -0.46406206, -0.51205623,\n",
              "        -0.4886778 ,  0.49035624, -0.44489503,  0.45867658, -0.47279182,\n",
              "         0.43687263, -0.4825188 , -0.4927405 , -0.48536685,  0.5204541 ,\n",
              "         0.4715506 ,  0.44636682,  0.45473084, -0.5054211 ,  0.46964714],\n",
              "       dtype=float32),\n",
              " 'sampling': array([ 0.47459248,  0.46776402, -0.511549  , -0.42436787, -0.46092227,\n",
              "         0.5005765 ,  0.48994407, -0.43529034, -0.4553364 , -0.44710982,\n",
              "        -0.42513615,  0.4313363 ,  0.4474776 ,  0.4794234 , -0.46612597,\n",
              "        -0.46238205,  0.44309592,  0.438983  , -0.48075694, -0.42514238,\n",
              "        -0.44419035,  0.44515255,  0.50217164,  0.44883206, -0.4916116 ,\n",
              "        -0.48516926,  0.49101594,  0.49704516,  0.42991984,  0.49368846,\n",
              "        -0.4375198 ,  0.4924547 , -0.46777052, -0.46392345, -0.4640461 ,\n",
              "        -0.47820213,  0.5095161 , -0.49641252,  0.46454275, -0.49719182,\n",
              "         0.47644207, -0.49382114, -0.4432644 , -0.4886166 ,  0.42932218,\n",
              "         0.43117064,  0.49149388,  0.51322675, -0.43785957,  0.48725414],\n",
              "       dtype=float32),\n",
              " 'must': array([ 0.5196056 ,  0.49458683, -0.5175664 , -0.5046953 , -0.47384515,\n",
              "         0.4588456 ,  0.51039934, -0.4436285 , -0.4700456 , -0.49960738,\n",
              "        -0.5020904 ,  0.4716559 ,  0.47772145,  0.4925618 , -0.5003504 ,\n",
              "        -0.50493115,  0.5080282 ,  0.52422297, -0.43183574, -0.48120195,\n",
              "        -0.51362115,  0.48661768,  0.49989226,  0.43591964, -0.44430986,\n",
              "        -0.47282597,  0.46968243,  0.46413887,  0.45448998,  0.42662048,\n",
              "        -0.49710295,  0.4563286 , -0.51921886, -0.5194266 , -0.50729495,\n",
              "        -0.49194843,  0.45460284, -0.49562237,  0.45234376, -0.4815237 ,\n",
              "         0.50931096, -0.5258769 , -0.4793313 , -0.44199738,  0.46212485,\n",
              "         0.5206207 ,  0.45966616,  0.49017704, -0.4584455 ,  0.45391864],\n",
              "       dtype=float32),\n",
              " 'repurchase': array([ 0.4873138 ,  0.46543908, -0.46383262, -0.44828838, -0.49505803,\n",
              "         0.46304166,  0.47302458, -0.48197412, -0.48040166, -0.48713776,\n",
              "        -0.47707146,  0.46546856,  0.4834708 ,  0.47582766, -0.5574419 ,\n",
              "        -0.489156  ,  0.4410438 ,  0.47048274, -0.4924961 , -0.50147545,\n",
              "        -0.48915496,  0.49832305,  0.4898002 ,  0.5170326 , -0.4862944 ,\n",
              "        -0.46860054,  0.48035324,  0.47173077,  0.49159354,  0.45789245,\n",
              "        -0.4823979 ,  0.51726615, -0.47874257, -0.47271517, -0.43899947,\n",
              "        -0.52881134,  0.4442284 , -0.50386876,  0.46729368, -0.47786394,\n",
              "         0.5159207 , -0.4862578 , -0.43828735, -0.49373376,  0.46530643,\n",
              "         0.52128154,  0.49919736,  0.47837573, -0.4951525 ,  0.49500066],\n",
              "       dtype=float32),\n",
              " 'indirectly': array([ 0.4768414 ,  0.4775949 , -0.4593775 , -0.43916488, -0.42783165,\n",
              "         0.43459818,  0.41536728, -0.49751455, -0.42804825, -0.4595734 ,\n",
              "        -0.4466576 ,  0.44246262,  0.443846  ,  0.4364929 , -0.54696876,\n",
              "        -0.4941465 ,  0.43309647,  0.433076  , -0.4291553 , -0.4257052 ,\n",
              "        -0.49227688,  0.4850461 ,  0.49022242,  0.49156746, -0.4292574 ,\n",
              "        -0.5040799 ,  0.46293634,  0.48728752,  0.48137727,  0.45393884,\n",
              "        -0.45202854,  0.46767062, -0.5215246 , -0.45514768, -0.48133507,\n",
              "        -0.42984408,  0.50169843, -0.432162  ,  0.4983944 , -0.48969296,\n",
              "         0.51297015, -0.48520926, -0.4557813 , -0.47969434,  0.42589214,\n",
              "         0.47541985,  0.46602118,  0.48035362, -0.42538273,  0.46601206],\n",
              "       dtype=float32),\n",
              " 'servicing': array([ 0.52650225,  0.4304218 , -0.42990583, -0.49309656, -0.5168778 ,\n",
              "         0.47824082,  0.4741948 , -0.43993428, -0.49798992, -0.4244765 ,\n",
              "        -0.46014413,  0.45914474,  0.50454766,  0.4252028 , -0.46690235,\n",
              "        -0.49941465,  0.4889775 ,  0.46687874, -0.44925034, -0.42735496,\n",
              "        -0.4602266 ,  0.47226787,  0.50488585,  0.45465675, -0.5048001 ,\n",
              "        -0.44055173,  0.5085447 ,  0.47469008,  0.44564202,  0.49619988,\n",
              "        -0.5157007 ,  0.4777728 , -0.46520323, -0.51491416, -0.49662763,\n",
              "        -0.5014059 ,  0.45891273, -0.50624466,  0.4209645 , -0.501314  ,\n",
              "         0.5071316 , -0.45821428, -0.46493864, -0.46831188,  0.45150784,\n",
              "         0.4577832 ,  0.4422248 ,  0.43894824, -0.45797113,  0.46544957],\n",
              "       dtype=float32),\n",
              " 'interests': array([ 0.48132882,  0.4813169 , -0.42297518, -0.5075738 , -0.42869452,\n",
              "         0.51047873,  0.50696814, -0.42558235, -0.45432517, -0.47790986,\n",
              "        -0.44697016,  0.45257443,  0.505421  ,  0.48866984, -0.5150062 ,\n",
              "        -0.43427008,  0.47545186,  0.4777781 , -0.43390787, -0.48213267,\n",
              "        -0.44553027,  0.4522178 ,  0.42660177,  0.4497568 , -0.42619938,\n",
              "        -0.44650742,  0.4919349 ,  0.47980478,  0.4245693 ,  0.47371146,\n",
              "        -0.4831044 ,  0.4859162 , -0.48503175, -0.4447222 , -0.48544168,\n",
              "        -0.49134517,  0.45060444, -0.49323425,  0.5042681 , -0.50226057,\n",
              "         0.46465263, -0.5099254 , -0.48082805, -0.45636302,  0.46796823,\n",
              "         0.4732218 ,  0.47919744,  0.45925543, -0.49604124,  0.4818624 ],\n",
              "       dtype=float32),\n",
              " 'october': array([ 0.47156537,  0.5050033 , -0.5153527 , -0.47390574, -0.47983673,\n",
              "         0.5303492 ,  0.44423023, -0.51711786, -0.49434313, -0.5214979 ,\n",
              "        -0.49311653,  0.45398217,  0.44974682,  0.46184948, -0.53141975,\n",
              "        -0.472564  ,  0.44687968,  0.5056302 , -0.48131898, -0.46235973,\n",
              "        -0.4420916 ,  0.43562502,  0.4833615 ,  0.51778   , -0.5145628 ,\n",
              "        -0.5152287 ,  0.45357245,  0.5369887 ,  0.50938547,  0.44473916,\n",
              "        -0.45528278,  0.48957428, -0.49087524, -0.46459183, -0.50311583,\n",
              "        -0.44793785,  0.48758757, -0.5003873 ,  0.48259568, -0.45106003,\n",
              "         0.4959941 , -0.46162462, -0.47347668, -0.50602835,  0.48371103,\n",
              "         0.47835723,  0.4530958 ,  0.4838515 , -0.46957535,  0.4917267 ],\n",
              "       dtype=float32),\n",
              " 'institution': array([ 0.5345931 ,  0.47441572, -0.49362984, -0.4634322 , -0.50199705,\n",
              "         0.49309665,  0.49944898, -0.5046096 , -0.4753617 , -0.43238878,\n",
              "        -0.47655678,  0.43927988,  0.46241102,  0.4930402 , -0.55706227,\n",
              "        -0.47519973,  0.45509696,  0.4585567 , -0.42645198, -0.49614492,\n",
              "        -0.43030304,  0.43689665,  0.43803003,  0.49271056, -0.5108809 ,\n",
              "        -0.5083349 ,  0.4589219 ,  0.48232907,  0.50953776,  0.42690402,\n",
              "        -0.47210744,  0.44391704, -0.44873998, -0.466835  , -0.45749235,\n",
              "        -0.48869592,  0.4501601 , -0.4451408 ,  0.45462424, -0.50981075,\n",
              "         0.46699402, -0.4712195 , -0.44695476, -0.45098937,  0.4577437 ,\n",
              "         0.4378206 ,  0.45234075,  0.4333008 , -0.5071627 ,  0.51148313],\n",
              "       dtype=float32),\n",
              " 'delayed': array([ 0.44812807,  0.45366338, -0.4666639 , -0.5093974 , -0.48914653,\n",
              "         0.47458047,  0.5048683 , -0.502228  , -0.45357692, -0.46159548,\n",
              "        -0.50839704,  0.47984603,  0.51274395,  0.44215947, -0.55337375,\n",
              "        -0.4687113 ,  0.4494056 ,  0.4431659 , -0.4865185 , -0.43250448,\n",
              "        -0.5075767 ,  0.4775248 ,  0.5126509 ,  0.5104457 , -0.42632273,\n",
              "        -0.47391438,  0.502889  ,  0.467367  ,  0.45919603,  0.4235875 ,\n",
              "        -0.4700715 ,  0.46538937, -0.4756612 , -0.496609  , -0.4311051 ,\n",
              "        -0.48162916,  0.44334355, -0.50410914,  0.4793113 , -0.52414894,\n",
              "         0.5063476 , -0.47362205, -0.46028346, -0.48267248,  0.5110198 ,\n",
              "         0.48628157,  0.47986594,  0.51945716, -0.46160975,  0.48768044],\n",
              "       dtype=float32),\n",
              " 'highly': array([ 0.49387005,  0.5001761 , -0.4890606 , -0.47282803, -0.5064907 ,\n",
              "         0.49672076,  0.48914403, -0.49473917, -0.4608743 , -0.44285354,\n",
              "        -0.48102286,  0.4462561 ,  0.4395104 ,  0.44969523, -0.51567465,\n",
              "        -0.47564462,  0.5232462 ,  0.4607332 , -0.4995252 , -0.51694894,\n",
              "        -0.4840097 ,  0.46121937,  0.49469724,  0.4485172 , -0.49982172,\n",
              "        -0.49667534,  0.51165134,  0.45469418,  0.44060588,  0.5040917 ,\n",
              "        -0.47963583,  0.48227897, -0.5098245 , -0.45878822, -0.46225393,\n",
              "        -0.45481005,  0.46545786, -0.5317484 ,  0.5035653 , -0.4981345 ,\n",
              "         0.48627657, -0.5126628 , -0.5232179 , -0.4893425 ,  0.45282578,\n",
              "         0.4836875 ,  0.4765874 ,  0.49975476, -0.5160934 ,  0.4778849 ],\n",
              "       dtype=float32),\n",
              " 'unusual': array([ 0.48212913,  0.4483213 , -0.4878086 , -0.49327523, -0.46586835,\n",
              "         0.52784073,  0.46800637, -0.49911445, -0.4497062 , -0.47602582,\n",
              "        -0.51722807,  0.5145105 ,  0.5188735 ,  0.51800615, -0.55267376,\n",
              "        -0.46168917,  0.52302814,  0.47519717, -0.48682457, -0.45361164,\n",
              "        -0.5033046 ,  0.46432316,  0.44584334,  0.48856735, -0.42970562,\n",
              "        -0.5080865 ,  0.4389721 ,  0.5001756 ,  0.45693746,  0.4392516 ,\n",
              "        -0.4717897 ,  0.5169977 , -0.5175435 , -0.5285167 , -0.4516094 ,\n",
              "        -0.48562714,  0.5071035 , -0.4575213 ,  0.51034015, -0.52093315,\n",
              "         0.47043544, -0.48713604, -0.4412001 , -0.4885868 ,  0.49689332,\n",
              "         0.45433566,  0.46599942,  0.445339  , -0.48609787,  0.44255626],\n",
              "       dtype=float32),\n",
              " 'portfolios': array([ 0.5525599 ,  0.44628963, -0.48343873, -0.4688828 , -0.5015858 ,\n",
              "         0.49560004,  0.43283895, -0.47278082, -0.44734845, -0.42768443,\n",
              "        -0.5133746 ,  0.44705373,  0.48736668,  0.49046147, -0.5218524 ,\n",
              "        -0.49906847,  0.45965108,  0.4755602 , -0.4555594 , -0.47747225,\n",
              "        -0.4611576 ,  0.4593952 ,  0.4813941 ,  0.45478603, -0.4357156 ,\n",
              "        -0.44222263,  0.49997568,  0.5136147 ,  0.43560782,  0.4515557 ,\n",
              "        -0.4719387 ,  0.48304772, -0.501125  , -0.4627163 , -0.41971081,\n",
              "        -0.46485475,  0.49799117, -0.48386624,  0.5098816 , -0.45737457,\n",
              "         0.52744484, -0.51924014, -0.47862732, -0.4312867 ,  0.45045793,\n",
              "         0.43120867,  0.4776297 ,  0.49863684, -0.45053235,  0.47818398],\n",
              "       dtype=float32),\n",
              " 'supply': array([ 0.48424688,  0.41003713, -0.43656942, -0.5068716 , -0.5033738 ,\n",
              "         0.47284457,  0.4503538 , -0.5007141 , -0.47394586, -0.45312053,\n",
              "        -0.4261907 ,  0.50592947,  0.4856581 ,  0.4431043 , -0.50702214,\n",
              "        -0.46346796,  0.45264253,  0.47525573, -0.424197  , -0.48229676,\n",
              "        -0.50472206,  0.45165998,  0.4982452 ,  0.45117834, -0.4156226 ,\n",
              "        -0.48612532,  0.45553538,  0.50830036,  0.46700412,  0.4479338 ,\n",
              "        -0.47278413,  0.4345572 , -0.494348  , -0.4507459 , -0.4209455 ,\n",
              "        -0.506479  ,  0.49116883, -0.451638  ,  0.47287297, -0.48980522,\n",
              "         0.42523354, -0.47826308, -0.49040312, -0.4737147 ,  0.49865094,\n",
              "         0.46375653,  0.43548226,  0.47992238, -0.43639216,  0.45415106],\n",
              "       dtype=float32),\n",
              " 'ownership': array([ 0.50949687,  0.4493661 , -0.47598085, -0.41954148, -0.46220374,\n",
              "         0.49384624,  0.4933585 , -0.50617224, -0.47998074, -0.4324092 ,\n",
              "        -0.43839133,  0.45861706,  0.4586922 ,  0.48803848, -0.50342476,\n",
              "        -0.45909607,  0.5169756 ,  0.4690226 , -0.44427943, -0.48204696,\n",
              "        -0.44923776,  0.48950487,  0.4433994 ,  0.4924404 , -0.42657992,\n",
              "        -0.42538524,  0.44993684,  0.48751953,  0.42797574,  0.43690658,\n",
              "        -0.46324605,  0.5076698 , -0.53041416, -0.49881035, -0.50358266,\n",
              "        -0.43800342,  0.45271802, -0.4923989 ,  0.49615824, -0.47761336,\n",
              "         0.43287838, -0.47850797, -0.45560318, -0.4653874 ,  0.47265613,\n",
              "         0.48686218,  0.47740155,  0.4437427 , -0.44470134,  0.4520303 ],\n",
              "       dtype=float32),\n",
              " 'year-to-date': array([ 0.46199894,  0.43043897, -0.5136694 , -0.49118334, -0.44343382,\n",
              "         0.46992806,  0.4755339 , -0.5055272 , -0.43619177, -0.4188021 ,\n",
              "        -0.50414777,  0.4980923 ,  0.46646154,  0.46859404, -0.52576286,\n",
              "        -0.48790756,  0.43154463,  0.47162792, -0.4229608 , -0.43869692,\n",
              "        -0.43364882,  0.49114734,  0.46318075,  0.47534966, -0.45495743,\n",
              "        -0.48758277,  0.45509967,  0.44580665,  0.4277426 ,  0.45566568,\n",
              "        -0.48031336,  0.506911  , -0.5029461 , -0.43623668, -0.4630822 ,\n",
              "        -0.44575688,  0.43569902, -0.4555725 ,  0.49340102, -0.49736914,\n",
              "         0.49059677, -0.50868464, -0.4785168 , -0.4716778 ,  0.42989215,\n",
              "         0.46734896,  0.4852547 ,  0.48940423, -0.51114476,  0.47505373],\n",
              "       dtype=float32),\n",
              " 'franklin': array([ 0.43733883,  0.4632607 , -0.5057154 , -0.46748036, -0.47266623,\n",
              "         0.44081149,  0.43465292, -0.45530623, -0.43950677, -0.4103531 ,\n",
              "        -0.49864507,  0.4703396 ,  0.49615598,  0.43787253, -0.49002367,\n",
              "        -0.4729775 ,  0.50866866,  0.4773073 , -0.44141483, -0.4409887 ,\n",
              "        -0.47494584,  0.43719324,  0.49345672,  0.46582705, -0.49575895,\n",
              "        -0.4993401 ,  0.4884516 ,  0.48172534,  0.43677032,  0.49071577,\n",
              "        -0.49814397,  0.41866204, -0.46663895, -0.43355343, -0.47283646,\n",
              "        -0.4225062 ,  0.43374676, -0.42497665,  0.48604408, -0.4272714 ,\n",
              "         0.48655197, -0.5017535 , -0.41511628, -0.43823248,  0.47477803,\n",
              "         0.43257666,  0.5010777 ,  0.4489111 , -0.47016484,  0.4853933 ],\n",
              "       dtype=float32),\n",
              " 'activity': array([ 0.45778146,  0.41376972, -0.45648348, -0.47836676, -0.47729588,\n",
              "         0.4771852 ,  0.46599054, -0.43201554, -0.48489952, -0.41451022,\n",
              "        -0.4885481 ,  0.48759466,  0.49334365,  0.44108877, -0.48963192,\n",
              "        -0.42971158,  0.46183443,  0.48650315, -0.4571061 , -0.43405092,\n",
              "        -0.46125934,  0.46851957,  0.4329117 ,  0.43466026, -0.5002826 ,\n",
              "        -0.49040174,  0.4848258 ,  0.5038994 ,  0.47998935,  0.42093548,\n",
              "        -0.43365112,  0.45849752, -0.50605816, -0.47680625, -0.43085077,\n",
              "        -0.5148872 ,  0.5014858 , -0.4856111 ,  0.41521186, -0.49635413,\n",
              "         0.48507753, -0.43203256, -0.4515523 , -0.41866982,  0.43959776,\n",
              "         0.49760985,  0.48575565,  0.46769813, -0.43217146,  0.42970592],\n",
              "       dtype=float32),\n",
              " 'access': array([ 0.4855517 ,  0.44761145, -0.4279607 , -0.48832053, -0.52410024,\n",
              "         0.5003594 ,  0.4284735 , -0.4992662 , -0.519868  , -0.47036594,\n",
              "        -0.45813316,  0.4695886 ,  0.50469506,  0.46475616, -0.50333256,\n",
              "        -0.48319665,  0.48574728,  0.5198658 , -0.4384765 , -0.44575396,\n",
              "        -0.4286214 ,  0.50840217,  0.51319885,  0.455494  , -0.49498403,\n",
              "        -0.47140616,  0.5117531 ,  0.47136605,  0.43093017,  0.44018656,\n",
              "        -0.4666046 ,  0.4933359 , -0.46419585, -0.467382  , -0.4827035 ,\n",
              "        -0.4937259 ,  0.47090724, -0.4713692 ,  0.4341586 , -0.46760696,\n",
              "         0.49890688, -0.52207315, -0.5110921 , -0.5133014 ,  0.46806002,\n",
              "         0.51681846,  0.43895996,  0.5096202 , -0.46617538,  0.46268743],\n",
              "       dtype=float32),\n",
              " 'days': array([ 0.44283947,  0.4339252 , -0.44368723, -0.51322883, -0.5252175 ,\n",
              "         0.5201019 ,  0.48764592, -0.487442  , -0.5150276 , -0.49396068,\n",
              "        -0.5019851 ,  0.4470881 ,  0.4839004 ,  0.4710077 , -0.54853517,\n",
              "        -0.49093843,  0.48691133,  0.49408177, -0.5121549 , -0.42389455,\n",
              "        -0.5135124 ,  0.51577616,  0.5248985 ,  0.45152393, -0.5005893 ,\n",
              "        -0.49391782,  0.44698632,  0.48122576,  0.43170553,  0.51307946,\n",
              "        -0.50626695,  0.43926695, -0.4825153 , -0.44943526, -0.46045834,\n",
              "        -0.4497734 ,  0.52632976, -0.47932106,  0.42633718, -0.48689425,\n",
              "         0.49958986, -0.46832606, -0.50261915, -0.52346545,  0.46547434,\n",
              "         0.4566232 ,  0.5135737 ,  0.4349234 , -0.46202034,  0.46815705],\n",
              "       dtype=float32),\n",
              " 'assumed': array([ 0.46535382,  0.46593833, -0.46092764, -0.48160675, -0.50151104,\n",
              "         0.45168203,  0.4593495 , -0.4866077 , -0.50080895, -0.44223192,\n",
              "        -0.44335586,  0.44569987,  0.46419546,  0.478819  , -0.49477124,\n",
              "        -0.43610716,  0.47104394,  0.43422532, -0.47523016, -0.45018336,\n",
              "        -0.48729357,  0.46109125,  0.5177141 ,  0.49168733, -0.47314695,\n",
              "        -0.47677186,  0.42995483,  0.48938456,  0.4639601 ,  0.5104215 ,\n",
              "        -0.4528839 ,  0.44117767, -0.46750945, -0.5065509 , -0.47644907,\n",
              "        -0.46348545,  0.48136103, -0.5188206 ,  0.43092936, -0.50208807,\n",
              "         0.49998304, -0.50283647, -0.45173836, -0.42604777,  0.46625203,\n",
              "         0.44814265,  0.4493311 ,  0.4312588 , -0.5167066 ,  0.45884204],\n",
              "       dtype=float32),\n",
              " 'declining': array([ 0.45914146,  0.43576303, -0.49170452, -0.49733114, -0.4916331 ,\n",
              "         0.44087562,  0.45440137, -0.49266118, -0.46898377, -0.47840777,\n",
              "        -0.47107035,  0.5059679 ,  0.4617063 ,  0.49606514, -0.5147621 ,\n",
              "        -0.46419504,  0.5066558 ,  0.50449395, -0.46920842, -0.41101483,\n",
              "        -0.45810324,  0.4936726 ,  0.49990457,  0.41364723, -0.41675478,\n",
              "        -0.47973692,  0.42853954,  0.5116313 ,  0.46544433,  0.4345194 ,\n",
              "        -0.48847815,  0.4624309 , -0.44988006, -0.5137227 , -0.49383342,\n",
              "        -0.4653665 ,  0.5160177 , -0.48512334,  0.479857  , -0.49840072,\n",
              "         0.46146217, -0.5072042 , -0.42604506, -0.43666884,  0.41874006,\n",
              "         0.4845405 ,  0.45572045,  0.4895863 , -0.48790762,  0.4556877 ],\n",
              "       dtype=float32),\n",
              " 'extension': array([ 0.48999363,  0.44536215, -0.44836676, -0.48612362, -0.4588363 ,\n",
              "         0.43646577,  0.4972906 , -0.436814  , -0.48330703, -0.47507775,\n",
              "        -0.4646843 ,  0.44248113,  0.48364925,  0.50364995, -0.5074087 ,\n",
              "        -0.47645822,  0.43492645,  0.4787186 , -0.46958393, -0.43602353,\n",
              "        -0.5052728 ,  0.41918057,  0.51303846,  0.46231917, -0.48125666,\n",
              "        -0.4570708 ,  0.4396872 ,  0.53020763,  0.5031701 ,  0.4992168 ,\n",
              "        -0.4827261 ,  0.4663832 , -0.4953852 , -0.44407675, -0.481524  ,\n",
              "        -0.43294784,  0.51407516, -0.48105705,  0.4398976 , -0.434027  ,\n",
              "         0.4395235 , -0.43045726, -0.43863055, -0.48336393,  0.49521434,\n",
              "         0.49649024,  0.47271833,  0.468102  , -0.4601998 ,  0.41988736],\n",
              "       dtype=float32),\n",
              " 'europe': array([ 0.49165457,  0.49049795, -0.49140275, -0.45289344, -0.49167874,\n",
              "         0.44086912,  0.4892627 , -0.48412526, -0.46142453, -0.51171726,\n",
              "        -0.4635715 ,  0.4691046 ,  0.45622337,  0.5229565 , -0.5159527 ,\n",
              "        -0.44975707,  0.5232021 ,  0.46709678, -0.44713864, -0.44128823,\n",
              "        -0.47729927,  0.43059164,  0.4867863 ,  0.5155076 , -0.46210545,\n",
              "        -0.4939515 ,  0.48618823,  0.52880806,  0.43538272,  0.43154433,\n",
              "        -0.5174052 ,  0.48603764, -0.46721897, -0.44590563, -0.48861277,\n",
              "        -0.496516  ,  0.47067198, -0.4922218 ,  0.5027444 , -0.45943868,\n",
              "         0.45120746, -0.49172047, -0.4960365 , -0.46521106,  0.49896246,\n",
              "         0.49999812,  0.4379898 ,  0.49589893, -0.5121121 ,  0.4375622 ],\n",
              "       dtype=float32),\n",
              " 'imposition': array([ 0.4733239 ,  0.49851543, -0.51896983, -0.503644  , -0.52157474,\n",
              "         0.4691249 ,  0.45382494, -0.5000001 , -0.4847281 , -0.48859802,\n",
              "        -0.50213635,  0.43841428,  0.46767408,  0.4724775 , -0.47748956,\n",
              "        -0.45977312,  0.5165305 ,  0.4567459 , -0.4797124 , -0.4976677 ,\n",
              "        -0.5111434 ,  0.47573352,  0.47642028,  0.50723577, -0.43332535,\n",
              "        -0.47638142,  0.49066454,  0.5397215 ,  0.43027526,  0.48022002,\n",
              "        -0.50266004,  0.45578998, -0.4953905 , -0.4415626 , -0.5203595 ,\n",
              "        -0.4862857 ,  0.4794726 , -0.44418722,  0.44537845, -0.43726054,\n",
              "         0.48301855, -0.450414  , -0.43211773, -0.4673765 ,  0.44474924,\n",
              "         0.4750802 ,  0.48348755,  0.5134915 , -0.44432217,  0.5196079 ],\n",
              "       dtype=float32),\n",
              " 'calculation': array([ 0.51672304,  0.436324  , -0.49289072, -0.48398596, -0.49861258,\n",
              "         0.48218697,  0.44692224, -0.479404  , -0.46510014, -0.5004727 ,\n",
              "        -0.44164518,  0.52945685,  0.5074438 ,  0.49221706, -0.52577007,\n",
              "        -0.45994383,  0.48460513,  0.49698356, -0.5262618 , -0.48054108,\n",
              "        -0.50885147,  0.4895726 ,  0.53605986,  0.50272036, -0.5062519 ,\n",
              "        -0.48941493,  0.46553963,  0.5340358 ,  0.46274048,  0.46780553,\n",
              "        -0.5217209 ,  0.49405903, -0.48557514, -0.47188488, -0.4472656 ,\n",
              "        -0.49835762,  0.52725816, -0.50337946,  0.43946326, -0.48856843,\n",
              "         0.46214285, -0.5091838 , -0.44305316, -0.46132818,  0.50909853,\n",
              "         0.48540905,  0.45787853,  0.4463445 , -0.50220984,  0.44119948],\n",
              "       dtype=float32),\n",
              " 'hedged': array([ 0.4762254 ,  0.49505535, -0.54309154, -0.5411285 , -0.47371343,\n",
              "         0.54391176,  0.43449783, -0.5058475 , -0.48891658, -0.44088376,\n",
              "        -0.45445636,  0.4692749 ,  0.45262665,  0.5240328 , -0.54901373,\n",
              "        -0.43723583,  0.4975122 ,  0.56264067, -0.49136293, -0.47654146,\n",
              "        -0.47688693,  0.44630057,  0.5410683 ,  0.45487827, -0.5664357 ,\n",
              "        -0.48021597,  0.41807637,  0.5657708 ,  0.57780904,  0.50546247,\n",
              "        -0.571553  ,  0.50223553, -0.5465207 , -0.56089985, -0.53730965,\n",
              "        -0.5269403 ,  0.5499311 , -0.5207025 ,  0.43915343, -0.52262044,\n",
              "         0.49317366, -0.46026328, -0.45938683, -0.4452604 ,  0.45672655,\n",
              "         0.5563266 ,  0.50171006,  0.53903747, -0.4015168 ,  0.5175414 ],\n",
              "       dtype=float32),\n",
              " 'balanced': array([ 0.5102115 ,  0.4879843 , -0.44904593, -0.5223609 , -0.5293169 ,\n",
              "         0.45400283,  0.43212628, -0.44199204, -0.47591293, -0.45644462,\n",
              "        -0.46775472,  0.5243714 ,  0.5087011 ,  0.50078213, -0.55135435,\n",
              "        -0.44338325,  0.51904535,  0.46981588, -0.46084672, -0.45703045,\n",
              "        -0.42727318,  0.49746335,  0.48312533,  0.43767506, -0.48028696,\n",
              "        -0.49902683,  0.4670856 ,  0.5408204 ,  0.5146178 ,  0.44728574,\n",
              "        -0.5112204 ,  0.50748193, -0.47326022, -0.5122624 , -0.53436047,\n",
              "        -0.46013576,  0.4542546 , -0.51806957,  0.51417685, -0.46444252,\n",
              "         0.5359191 , -0.46668497, -0.47515717, -0.508982  ,  0.49307978,\n",
              "         0.4571791 ,  0.5129452 ,  0.42962697, -0.4390736 ,  0.50929725],\n",
              "       dtype=float32),\n",
              " 'quickly': array([ 0.4968016 ,  0.44377422, -0.42999706, -0.4954124 , -0.4796786 ,\n",
              "         0.4890697 ,  0.45704347, -0.48228627, -0.44870088, -0.47924125,\n",
              "        -0.48960105,  0.48939183,  0.4421075 ,  0.49247578, -0.53991675,\n",
              "        -0.48008072,  0.4829977 ,  0.50013644, -0.4667974 , -0.4187216 ,\n",
              "        -0.48419258,  0.465948  ,  0.49346092,  0.46116698, -0.41664654,\n",
              "        -0.49890035,  0.42970955,  0.45811617,  0.43035087,  0.4479188 ,\n",
              "        -0.49741215,  0.46631208, -0.5196503 , -0.4510836 , -0.44521463,\n",
              "        -0.43468806,  0.5032081 , -0.50039685,  0.49010202, -0.47838634,\n",
              "         0.49616638, -0.4954465 , -0.4093589 , -0.47736585,  0.48988983,\n",
              "         0.4583043 ,  0.44247803,  0.47293752, -0.4700548 ,  0.4412677 ],\n",
              "       dtype=float32),\n",
              " 'speculative': array([ 0.49977052,  0.4631732 , -0.5192291 , -0.43286526, -0.49612463,\n",
              "         0.49203998,  0.43274602, -0.49962565, -0.46373692, -0.48670503,\n",
              "        -0.46455586,  0.50768685,  0.4961387 ,  0.44441953, -0.5693229 ,\n",
              "        -0.43838057,  0.4519463 ,  0.4968251 , -0.48549587, -0.48053193,\n",
              "        -0.4955533 ,  0.422977  ,  0.46704394,  0.50904846, -0.45525745,\n",
              "        -0.513355  ,  0.51046246,  0.48646867,  0.50729716,  0.4330055 ,\n",
              "        -0.46123314,  0.4355746 , -0.46854004, -0.43826717, -0.46396363,\n",
              "        -0.5233652 ,  0.5208602 , -0.44264904,  0.47303227, -0.45687112,\n",
              "         0.46256924, -0.47588962, -0.4609581 , -0.44253546,  0.47899127,\n",
              "         0.4801016 ,  0.44384998,  0.47625166, -0.4371786 ,  0.49955252],\n",
              "       dtype=float32),\n",
              " 'goal': array([ 0.52600724,  0.43481442, -0.5116005 , -0.41966075, -0.50566113,\n",
              "         0.44112813,  0.4670993 , -0.47574177, -0.46383393, -0.48205194,\n",
              "        -0.4229938 ,  0.4663017 ,  0.42451292,  0.49236333, -0.47341543,\n",
              "        -0.4578004 ,  0.43434   ,  0.42776957, -0.4477704 , -0.46897656,\n",
              "        -0.487812  ,  0.45953694,  0.4596313 ,  0.42580596, -0.4928532 ,\n",
              "        -0.49997163,  0.42139885,  0.5068683 ,  0.49097252,  0.5030227 ,\n",
              "        -0.5129479 ,  0.46341318, -0.49718565, -0.5057313 , -0.4836435 ,\n",
              "        -0.484569  ,  0.5150423 , -0.48241556,  0.50534946, -0.4445973 ,\n",
              "         0.498923  , -0.4285728 , -0.4261308 , -0.45303863,  0.48171017,\n",
              "         0.51116985,  0.42777035,  0.50271195, -0.4328911 ,  0.43166468],\n",
              "       dtype=float32),\n",
              " 'reimbursements': array([ 0.46325773,  0.41104776, -0.49913758, -0.41349396, -0.45586652,\n",
              "         0.426212  ,  0.4098512 , -0.44010252, -0.48590767, -0.46199557,\n",
              "        -0.49305207,  0.4452259 ,  0.42255446,  0.4864933 , -0.5317655 ,\n",
              "        -0.4870056 ,  0.5006406 ,  0.42827854, -0.40487033, -0.4837211 ,\n",
              "        -0.41981226,  0.4439426 ,  0.48290142,  0.48850006, -0.46916965,\n",
              "        -0.48719394,  0.49344197,  0.4418765 ,  0.48430473,  0.43133128,\n",
              "        -0.46608913,  0.44998124, -0.5007844 , -0.42482552, -0.4880409 ,\n",
              "        -0.4183692 ,  0.42833322, -0.43510103,  0.49259254, -0.43418872,\n",
              "         0.43387285, -0.44393358, -0.4131672 , -0.4399369 ,  0.48142257,\n",
              "         0.45585036,  0.48366576,  0.4391557 , -0.486274  ,  0.44393852],\n",
              "       dtype=float32),\n",
              " 'complete': array([ 0.46122167,  0.4160429 , -0.45910847, -0.45837283, -0.5049804 ,\n",
              "         0.46359926,  0.4553608 , -0.42159232, -0.43868876, -0.46762913,\n",
              "        -0.47080466,  0.47661075,  0.40898234,  0.46888593, -0.48223808,\n",
              "        -0.4432397 ,  0.46349055,  0.45149887, -0.4935887 , -0.48875755,\n",
              "        -0.48019418,  0.46317035,  0.4778151 ,  0.49403316, -0.4947298 ,\n",
              "        -0.40961617,  0.43508434,  0.4757426 ,  0.48032078,  0.4148583 ,\n",
              "        -0.4988107 ,  0.43743804, -0.45925164, -0.4418722 , -0.45578316,\n",
              "        -0.4275738 ,  0.48754683, -0.47853902,  0.44900945, -0.4560322 ,\n",
              "         0.42805794, -0.45918664, -0.41440132, -0.47013953,  0.4376704 ,\n",
              "         0.50465006,  0.43989307,  0.47101268, -0.47880754,  0.46495298],\n",
              "       dtype=float32),\n",
              " 'much': array([ 0.5133977 ,  0.42381203, -0.48738313, -0.49707556, -0.45315713,\n",
              "         0.463628  ,  0.51610225, -0.5133045 , -0.4677806 , -0.49931005,\n",
              "        -0.4517703 ,  0.4503211 ,  0.43892497,  0.4436989 , -0.47530776,\n",
              "        -0.4727837 ,  0.454112  ,  0.4591277 , -0.43717018, -0.46037203,\n",
              "        -0.461588  ,  0.42738682,  0.4827641 ,  0.4419996 , -0.47812533,\n",
              "        -0.4728406 ,  0.4584596 ,  0.4724731 ,  0.45322597,  0.47436976,\n",
              "        -0.45512953,  0.5106203 , -0.4950408 , -0.52488256, -0.48229173,\n",
              "        -0.45255622,  0.5145184 , -0.44941685,  0.4969615 , -0.50611746,\n",
              "         0.50109696, -0.44476566, -0.42267793, -0.50681514,  0.4352698 ,\n",
              "         0.51929253,  0.4809137 ,  0.46931863, -0.46834055,  0.47165284],\n",
              "       dtype=float32),\n",
              " 'web': array([ 0.49954602,  0.42911613, -0.517243  , -0.42969802, -0.52964175,\n",
              "         0.50860703,  0.5053138 , -0.47622082, -0.46221843, -0.49585932,\n",
              "        -0.42829216,  0.49428496,  0.5088348 ,  0.5091233 , -0.51007926,\n",
              "        -0.46689802,  0.47428495,  0.5111145 , -0.4212045 , -0.45616356,\n",
              "        -0.4817727 ,  0.43818775,  0.4792557 ,  0.42175338, -0.49825737,\n",
              "        -0.5199988 ,  0.47369015,  0.5338477 ,  0.4463464 ,  0.4553412 ,\n",
              "        -0.5186068 ,  0.46019015, -0.5324311 , -0.4920621 , -0.48879996,\n",
              "        -0.5035682 ,  0.43601528, -0.49818763,  0.47721022, -0.4961391 ,\n",
              "         0.43257108, -0.45954865, -0.4925451 , -0.49927908,  0.50899196,\n",
              "         0.44037428,  0.43695116,  0.4734857 , -0.4295473 ,  0.49925923],\n",
              "       dtype=float32),\n",
              " 'site': array([ 0.5170473 ,  0.47229093, -0.44250894, -0.47368112, -0.5258504 ,\n",
              "         0.50681806,  0.4840067 , -0.47001857, -0.4429827 , -0.43947777,\n",
              "        -0.4998937 ,  0.5131464 ,  0.468429  ,  0.4383345 , -0.5199516 ,\n",
              "        -0.49976438,  0.49395755,  0.45443016, -0.4311008 , -0.48295218,\n",
              "        -0.4780364 ,  0.45808053,  0.45497447,  0.4329969 , -0.50482374,\n",
              "        -0.5111126 ,  0.45905796,  0.47671825,  0.48426533,  0.5152249 ,\n",
              "        -0.4615215 ,  0.50862014, -0.4720736 , -0.502956  , -0.4702059 ,\n",
              "        -0.4631169 ,  0.47897914, -0.5161132 ,  0.49987084, -0.48621938,\n",
              "         0.4568794 , -0.46536452, -0.43850952, -0.4396944 ,  0.50561225,\n",
              "         0.48735875,  0.43282932,  0.51774496, -0.5052026 ,  0.43169615],\n",
              "       dtype=float32),\n",
              " 'environment': array([ 0.52842003,  0.45338258, -0.50350916, -0.5085145 , -0.45273766,\n",
              "         0.47848988,  0.47544366, -0.44184524, -0.4850106 , -0.5080425 ,\n",
              "        -0.4828329 ,  0.4501891 ,  0.46662435,  0.50252104, -0.55238545,\n",
              "        -0.52561605,  0.44429618,  0.46486187, -0.4862482 , -0.45240247,\n",
              "        -0.46272752,  0.4687059 ,  0.4613189 ,  0.42516696, -0.47640556,\n",
              "        -0.50617534,  0.5111416 ,  0.52802587,  0.5096398 ,  0.43461904,\n",
              "        -0.49067134,  0.45570892, -0.54254   , -0.5225154 , -0.4682501 ,\n",
              "        -0.45962054,  0.4396768 , -0.4475267 ,  0.4671344 , -0.5167153 ,\n",
              "         0.44549483, -0.53258777, -0.49218717, -0.4493817 ,  0.5136669 ,\n",
              "         0.4864711 ,  0.47600338,  0.5106603 , -0.4465907 ,  0.45692232],\n",
              "       dtype=float32),\n",
              " 'obtain': array([ 0.4701579 ,  0.44591013, -0.44896364, -0.432789  , -0.4300511 ,\n",
              "         0.4961147 ,  0.4707808 , -0.50446284, -0.4344736 , -0.46766526,\n",
              "        -0.47225675,  0.51247245,  0.48325777,  0.48881054, -0.54207027,\n",
              "        -0.43771943,  0.51364946,  0.50027686, -0.43144888, -0.41610095,\n",
              "        -0.43480435,  0.43996385,  0.4880828 ,  0.47386065, -0.49613065,\n",
              "        -0.42661878,  0.4260544 ,  0.5302705 ,  0.46396354,  0.43846408,\n",
              "        -0.44666457,  0.44353   , -0.44107875, -0.49897453, -0.45879152,\n",
              "        -0.49649608,  0.44260147, -0.46127993,  0.45062494, -0.4897822 ,\n",
              "         0.504348  , -0.5188092 , -0.4243042 , -0.4443255 ,  0.46700424,\n",
              "         0.47017774,  0.42589036,  0.4976257 , -0.5042656 ,  0.46810555],\n",
              "       dtype=float32),\n",
              " 'reimburse': array([ 0.4750804 ,  0.42870137, -0.5081468 , -0.47398746, -0.5274161 ,\n",
              "         0.45163468,  0.4563225 , -0.514351  , -0.46412563, -0.48501876,\n",
              "        -0.5163974 ,  0.4471901 ,  0.49802363,  0.4768395 , -0.4638293 ,\n",
              "        -0.4412139 ,  0.48232532,  0.46872258, -0.4817956 , -0.5130325 ,\n",
              "        -0.44442216,  0.46833432,  0.51676005,  0.5063311 , -0.5206375 ,\n",
              "        -0.4767334 ,  0.5124486 ,  0.5007391 ,  0.45248315,  0.4410664 ,\n",
              "        -0.4912425 ,  0.5146298 , -0.4728986 , -0.44478828, -0.4427489 ,\n",
              "        -0.48365748,  0.50054294, -0.43914872,  0.4922497 , -0.4343596 ,\n",
              "         0.44593316, -0.527742  , -0.4535554 , -0.506085  ,  0.47860062,\n",
              "         0.46175265,  0.4788097 ,  0.49054837, -0.5204134 ,  0.46201494],\n",
              "       dtype=float32),\n",
              " 'impose': array([ 0.5110003 ,  0.4421124 , -0.49194288, -0.4712223 , -0.5164493 ,\n",
              "         0.4799706 ,  0.46195713, -0.47435102, -0.513169  , -0.50373566,\n",
              "        -0.4586659 ,  0.51852393,  0.5126648 ,  0.49326286, -0.51788175,\n",
              "        -0.47226006,  0.45169538,  0.5031364 , -0.4458675 , -0.4711565 ,\n",
              "        -0.4818028 ,  0.5279249 ,  0.47248116,  0.4773675 , -0.45802516,\n",
              "        -0.4473927 ,  0.50581604,  0.47198656,  0.49171507,  0.488769  ,\n",
              "        -0.47244933,  0.4533443 , -0.4735797 , -0.524997  , -0.43864492,\n",
              "        -0.5089266 ,  0.4936918 , -0.5202863 ,  0.4607129 , -0.45718047,\n",
              "         0.47137788, -0.5213635 , -0.5156351 , -0.5181979 ,  0.4520853 ,\n",
              "         0.45181778,  0.52051127,  0.48439625, -0.4585127 ,  0.5016031 ],\n",
              "       dtype=float32),\n",
              " 'caused': array([ 0.4630072 ,  0.47614443, -0.4424636 , -0.45529327, -0.46840474,\n",
              "         0.45214632,  0.4851516 , -0.50992775, -0.5218351 , -0.5085949 ,\n",
              "        -0.5028301 ,  0.5180208 ,  0.50590485,  0.46893108, -0.49060684,\n",
              "        -0.48405552,  0.51755476,  0.50599974, -0.50187916, -0.47328386,\n",
              "        -0.44789696,  0.4383484 ,  0.4984697 ,  0.44268823, -0.5008124 ,\n",
              "        -0.454185  ,  0.50285685,  0.5022615 ,  0.4629109 ,  0.46940744,\n",
              "        -0.49719185,  0.46231613, -0.53750366, -0.5326413 , -0.5145908 ,\n",
              "        -0.448743  ,  0.48287544, -0.45179123,  0.52358294, -0.53002006,\n",
              "         0.49213985, -0.46022522, -0.48763517, -0.49798578,  0.5330795 ,\n",
              "         0.5039344 ,  0.45908245,  0.50487345, -0.45350096,  0.45517293],\n",
              "       dtype=float32),\n",
              " 'mortgage-related': array([ 0.51508516,  0.50392574, -0.453456  , -0.45557377, -0.46021017,\n",
              "         0.45991212,  0.5247466 , -0.5388573 , -0.4992967 , -0.46258017,\n",
              "        -0.52378845,  0.47876647,  0.4840587 ,  0.460253  , -0.5562132 ,\n",
              "        -0.47500592,  0.51451313,  0.46771082, -0.4973291 , -0.45755312,\n",
              "        -0.52201277,  0.47898328,  0.5090692 ,  0.4569236 , -0.49565253,\n",
              "        -0.47557524,  0.4994439 ,  0.5666585 ,  0.5315511 ,  0.46731412,\n",
              "        -0.48578075,  0.4593554 , -0.4923702 , -0.47797903, -0.46948102,\n",
              "        -0.48603678,  0.45906988, -0.53843254,  0.5112508 , -0.55354226,\n",
              "         0.50249785, -0.5385077 , -0.51842487, -0.50268686,  0.49950394,\n",
              "         0.4876112 ,  0.482222  ,  0.50064456, -0.5232791 ,  0.52527153],\n",
              "       dtype=float32),\n",
              " 'borrowers': array([ 0.52053195,  0.5016687 , -0.464497  , -0.46517366, -0.47341925,\n",
              "         0.47239026,  0.47855458, -0.5327076 , -0.5323389 , -0.4465507 ,\n",
              "        -0.45658338,  0.47039095,  0.48777893,  0.46244693, -0.55279136,\n",
              "        -0.50616354,  0.5109732 ,  0.5385667 , -0.4958874 , -0.4737919 ,\n",
              "        -0.50827146,  0.50141734,  0.5328841 ,  0.49443972, -0.5078453 ,\n",
              "        -0.4811704 ,  0.47221407,  0.49636108,  0.46226934,  0.45706314,\n",
              "        -0.47563878,  0.49102017, -0.50248545, -0.45955604, -0.51433253,\n",
              "        -0.4958026 ,  0.5225358 , -0.45188493,  0.4585802 , -0.5024846 ,\n",
              "         0.50512433, -0.5268395 , -0.4706455 , -0.48261392,  0.52149475,\n",
              "         0.47979662,  0.47744033,  0.50059456, -0.4715651 ,  0.5250737 ],\n",
              "       dtype=float32),\n",
              " 'name': array([ 0.49995166,  0.3898854 , -0.48518172, -0.48355672, -0.44346622,\n",
              "         0.41898924,  0.42210686, -0.45722374, -0.48070163, -0.4627746 ,\n",
              "        -0.46929148,  0.41443402,  0.41242152,  0.40476006, -0.46530658,\n",
              "        -0.41922915,  0.40812325,  0.46268016, -0.43490568, -0.4298138 ,\n",
              "        -0.43019694,  0.48332375,  0.4224265 ,  0.48201206, -0.40211558,\n",
              "        -0.4460717 ,  0.45836002,  0.4295212 ,  0.43219444,  0.43117213,\n",
              "        -0.4227063 ,  0.42256743, -0.47015896, -0.4634021 , -0.43769714,\n",
              "        -0.4517102 ,  0.48480925, -0.47291678,  0.4031063 , -0.47238326,\n",
              "         0.46773165, -0.49376485, -0.41046655, -0.48285186,  0.4156803 ,\n",
              "         0.48915836,  0.43171796,  0.42401257, -0.44869184,  0.4617698 ],\n",
              "       dtype=float32),\n",
              " 'two': array([ 0.504812  ,  0.49149776, -0.47805816, -0.47104105, -0.5107227 ,\n",
              "         0.46701744,  0.45690367, -0.45290416, -0.4640812 , -0.47604066,\n",
              "        -0.43944052,  0.50021946,  0.46668532,  0.45022574, -0.4789678 ,\n",
              "        -0.4284857 ,  0.4429442 ,  0.4744671 , -0.46946797, -0.4241751 ,\n",
              "        -0.4980069 ,  0.4833019 ,  0.4545764 ,  0.50561184, -0.47579443,\n",
              "        -0.46749792,  0.42193654,  0.49987122,  0.45532003,  0.5079328 ,\n",
              "        -0.45950234,  0.42334676, -0.4942942 , -0.44775042, -0.46929798,\n",
              "        -0.5011502 ,  0.49399728, -0.49193996,  0.48304355, -0.48265424,\n",
              "         0.4358215 , -0.47285825, -0.4955923 , -0.42894852,  0.48890743,\n",
              "         0.44898254,  0.4624351 ,  0.4817441 , -0.4840106 ,  0.46336898],\n",
              "       dtype=float32),\n",
              " 'visiting': array([ 0.48018104,  0.49615592, -0.51719093, -0.52189726, -0.5073618 ,\n",
              "         0.5149468 ,  0.491866  , -0.48230562, -0.4630924 , -0.4422441 ,\n",
              "        -0.5072105 ,  0.48622635,  0.47556317,  0.49658266, -0.4803381 ,\n",
              "        -0.50599086,  0.49661267,  0.5102586 , -0.5162364 , -0.44689152,\n",
              "        -0.46432155,  0.49863058,  0.4559806 ,  0.5025534 , -0.45161963,\n",
              "        -0.48299354,  0.46873125,  0.47218487,  0.5015205 ,  0.5032948 ,\n",
              "        -0.4764108 ,  0.44377863, -0.45569235, -0.48532942, -0.5161663 ,\n",
              "        -0.4866637 ,  0.4915714 , -0.508937  ,  0.4501774 , -0.46462587,\n",
              "         0.47350407, -0.46036232, -0.47944817, -0.49737725,  0.46317542,\n",
              "         0.5046176 ,  0.42868885,  0.51245075, -0.4963524 ,  0.50756425],\n",
              "       dtype=float32),\n",
              " 'issues': array([ 0.51859576,  0.44150725, -0.45730323, -0.4663999 , -0.46980006,\n",
              "         0.44308516,  0.45218337, -0.4222353 , -0.44471848, -0.47575483,\n",
              "        -0.49769318,  0.42073974,  0.4915795 ,  0.4689488 , -0.5452524 ,\n",
              "        -0.41918293,  0.49536526,  0.43509755, -0.4339409 , -0.43792233,\n",
              "        -0.4172615 ,  0.4360978 ,  0.5133209 ,  0.41427866, -0.45694682,\n",
              "        -0.41782284,  0.42936212,  0.5018906 ,  0.44401714,  0.45196033,\n",
              "        -0.45188624,  0.45791557, -0.49483803, -0.45659867, -0.4670696 ,\n",
              "        -0.48577023,  0.462444  , -0.48274642,  0.44163847, -0.4251573 ,\n",
              "         0.511835  , -0.51238984, -0.46197763, -0.47989607,  0.44822082,\n",
              "         0.4564138 ,  0.44640675,  0.44815755, -0.43038285,  0.49581122],\n",
              "       dtype=float32),\n",
              " 'unfavorable': array([ 0.471482  ,  0.5038952 , -0.5158462 , -0.4513224 , -0.47795945,\n",
              "         0.51416457,  0.5039097 , -0.49957347, -0.47505155, -0.46319404,\n",
              "        -0.47951534,  0.44516334,  0.5216124 ,  0.49058896, -0.4740742 ,\n",
              "        -0.5038065 ,  0.49991083,  0.45616126, -0.49659124, -0.4437957 ,\n",
              "        -0.43127137,  0.5005311 ,  0.4491844 ,  0.5003509 , -0.48659298,\n",
              "        -0.5147315 ,  0.5246517 ,  0.5177905 ,  0.42958152,  0.45481628,\n",
              "        -0.4470866 ,  0.50170296, -0.4720105 , -0.45576108, -0.51743037,\n",
              "        -0.45593086,  0.5220563 , -0.47195268,  0.4686068 , -0.45979345,\n",
              "         0.48073027, -0.54153323, -0.50957274, -0.51667696,  0.524967  ,\n",
              "         0.49361253,  0.48099667,  0.5040773 , -0.513278  ,  0.44827643],\n",
              "       dtype=float32),\n",
              " 'ex': array([ 0.45913777,  0.4798427 , -0.51346964, -0.5229579 , -0.45490846,\n",
              "         0.46817377,  0.44185117, -0.47044522, -0.48125723, -0.44287297,\n",
              "        -0.51863235,  0.443341  ,  0.45843875,  0.46808416, -0.5187549 ,\n",
              "        -0.47816497,  0.46960938,  0.51882786, -0.5186005 , -0.50117356,\n",
              "        -0.52257943,  0.46250296,  0.47795928,  0.5050607 , -0.4871933 ,\n",
              "        -0.46262792,  0.47189808,  0.4849059 ,  0.4615274 ,  0.46165755,\n",
              "        -0.53703564,  0.479565  , -0.49685144, -0.45792833, -0.51085395,\n",
              "        -0.4513947 ,  0.48561716, -0.47523433,  0.4399933 , -0.48749793,\n",
              "         0.45086923, -0.46471834, -0.5124431 , -0.4482042 ,  0.45736805,\n",
              "         0.49837828,  0.4827163 ,  0.46878862, -0.46668223,  0.5088584 ],\n",
              "       dtype=float32),\n",
              " 'mainstay': array([ 0.4839924 ,  0.46565804, -0.5176181 , -0.48860162, -0.52491105,\n",
              "         0.46951443,  0.4379694 , -0.44742116, -0.45178217, -0.51028305,\n",
              "        -0.45666158,  0.47176298,  0.47100583,  0.4469207 , -0.5927538 ,\n",
              "        -0.48679695,  0.52960753,  0.46381924, -0.46637094, -0.43200105,\n",
              "        -0.48445502,  0.42876658,  0.49082714,  0.43988752, -0.51309603,\n",
              "        -0.44372717,  0.44776148,  0.5089537 ,  0.4401112 ,  0.49369523,\n",
              "        -0.5158461 ,  0.49024105, -0.54554737, -0.48722726, -0.4740667 ,\n",
              "        -0.4697266 ,  0.45470554, -0.5029079 ,  0.46425474, -0.47420985,\n",
              "         0.5178529 , -0.46662146, -0.5025609 , -0.44140816,  0.45826223,\n",
              "         0.50024617,  0.44244576,  0.4614505 , -0.5171588 ,  0.51632696],\n",
              "       dtype=float32),\n",
              " 'mid-cap': array([ 0.45920286,  0.45843652, -0.4572704 , -0.4467464 , -0.4631447 ,\n",
              "         0.5219251 ,  0.47974783, -0.5021812 , -0.48185274, -0.45325994,\n",
              "        -0.45968574,  0.47460276,  0.5029384 ,  0.51704466, -0.47950733,\n",
              "        -0.5214604 ,  0.48458436,  0.5239961 , -0.4804209 , -0.44848397,\n",
              "        -0.50601286,  0.48744076,  0.5235665 ,  0.45388234, -0.49004477,\n",
              "        -0.5194795 ,  0.42695168,  0.5160365 ,  0.5025264 ,  0.50091946,\n",
              "        -0.47371575,  0.50678265, -0.5115269 , -0.5176132 , -0.4825012 ,\n",
              "        -0.5081438 ,  0.45846328, -0.4447565 ,  0.4422525 , -0.5096699 ,\n",
              "         0.48853076, -0.44346637, -0.43460336, -0.48462328,  0.48722512,\n",
              "         0.45760307,  0.50664955,  0.45839983, -0.47350597,  0.45193914],\n",
              "       dtype=float32),\n",
              " 'consistent': array([ 0.5251641 ,  0.42947286, -0.47656322, -0.43918332, -0.457834  ,\n",
              "         0.48067978,  0.50197315, -0.47337013, -0.48203084, -0.4426667 ,\n",
              "        -0.46737581,  0.47676703,  0.44102857,  0.4880208 , -0.48594528,\n",
              "        -0.47009403,  0.51888406,  0.47345725, -0.44115362, -0.4779063 ,\n",
              "        -0.49757153,  0.4897501 ,  0.46188802,  0.47217533, -0.5110287 ,\n",
              "        -0.46057692,  0.47321463,  0.51763105,  0.49896932,  0.4449239 ,\n",
              "        -0.5002762 ,  0.4464499 , -0.45841146, -0.45967102, -0.439267  ,\n",
              "        -0.45835304,  0.5027736 , -0.4937015 ,  0.51526225, -0.450709  ,\n",
              "         0.442053  , -0.49968395, -0.4452433 , -0.46141884,  0.522063  ,\n",
              "         0.4858718 ,  0.50115204,  0.45709178, -0.51473397,  0.43384776],\n",
              "       dtype=float32),\n",
              " 'illiquidity': array([ 0.5371246 ,  0.41522196, -0.43318275, -0.4850218 , -0.45749587,\n",
              "         0.49857986,  0.47095057, -0.4484637 , -0.46105054, -0.47234488,\n",
              "        -0.46330574,  0.4583003 ,  0.4574935 ,  0.44343928, -0.50996846,\n",
              "        -0.4947163 ,  0.4792255 ,  0.4840939 , -0.46439153, -0.45356762,\n",
              "        -0.43837985,  0.506536  ,  0.45694187,  0.49255446, -0.4896812 ,\n",
              "        -0.47388056,  0.43369478,  0.450119  ,  0.42632005,  0.4403187 ,\n",
              "        -0.50591236,  0.4532664 , -0.5011264 , -0.4504967 , -0.4944926 ,\n",
              "        -0.45404762,  0.45473564, -0.47904396,  0.47958922, -0.5157442 ,\n",
              "         0.44292235, -0.48537868, -0.44483772, -0.50781024,  0.50719905,\n",
              "         0.435048  ,  0.4642522 ,  0.44066182, -0.43033576,  0.5122651 ],\n",
              "       dtype=float32),\n",
              " 'reit': array([ 0.48848072,  0.42974907, -0.5274789 , -0.48540157, -0.48306748,\n",
              "         0.44733027,  0.52527803, -0.5227488 , -0.4857937 , -0.46034816,\n",
              "        -0.49828818,  0.47259063,  0.4694934 ,  0.49160212, -0.50660497,\n",
              "        -0.43736   ,  0.45023918,  0.4877884 , -0.4818411 , -0.48366874,\n",
              "        -0.4999373 ,  0.51162374,  0.48883486,  0.4292174 , -0.5058592 ,\n",
              "        -0.49872616,  0.5000558 ,  0.5116262 ,  0.4381288 ,  0.44053617,\n",
              "        -0.46879107,  0.5115915 , -0.48477665, -0.49548402, -0.44836843,\n",
              "        -0.4538476 ,  0.47180843, -0.47974387,  0.48958012, -0.47707376,\n",
              "         0.46436435, -0.45144045, -0.48174125, -0.45697066,  0.435624  ,\n",
              "         0.4875344 ,  0.44103986,  0.5170672 , -0.51638377,  0.43515596],\n",
              "       dtype=float32),\n",
              " 'set': array([ 0.51987636,  0.48255095, -0.47917515, -0.4376402 , -0.49659178,\n",
              "         0.46287495,  0.4638316 , -0.44348583, -0.43068147, -0.4628042 ,\n",
              "        -0.42621008,  0.47895837,  0.43220633,  0.49677193, -0.5131686 ,\n",
              "        -0.42632774,  0.47049257,  0.5088354 , -0.43983516, -0.4628755 ,\n",
              "        -0.46914914,  0.41439378,  0.47229522,  0.44231865, -0.44116858,\n",
              "        -0.5021643 ,  0.46996155,  0.49637026,  0.4348923 ,  0.4329675 ,\n",
              "        -0.48107865,  0.45803878, -0.52289295, -0.499917  , -0.47123843,\n",
              "        -0.46470675,  0.4425472 , -0.4855742 ,  0.42362395, -0.4294465 ,\n",
              "         0.46507254, -0.50324845, -0.48344445, -0.45187622,  0.47353584,\n",
              "         0.43762586,  0.44870394,  0.4202517 , -0.46969447,  0.50570714],\n",
              "       dtype=float32),\n",
              " 'right': array([ 0.5252763 ,  0.48072743, -0.4718167 , -0.4897146 , -0.46725437,\n",
              "         0.47626498,  0.4323819 , -0.45264968, -0.49330613, -0.4603541 ,\n",
              "        -0.4379584 ,  0.47543013,  0.49289402,  0.4732312 , -0.4766399 ,\n",
              "        -0.49611712,  0.51213783,  0.4285861 , -0.4745445 , -0.45291433,\n",
              "        -0.45783696,  0.4608453 ,  0.4688221 ,  0.4757118 , -0.47750217,\n",
              "        -0.42907658,  0.47545317,  0.49297476,  0.50411654,  0.43438673,\n",
              "        -0.48155284,  0.42623118, -0.46929693, -0.47857583, -0.4224483 ,\n",
              "        -0.5114069 ,  0.48021558, -0.48267275,  0.44779152, -0.48960584,\n",
              "         0.51710355, -0.5094486 , -0.5042522 , -0.45797685,  0.46097815,\n",
              "         0.5152041 ,  0.45383367,  0.45847142, -0.49132815,  0.4581523 ],\n",
              "       dtype=float32),\n",
              " 'falling': array([ 0.50554997,  0.5020307 , -0.47415972, -0.507153  , -0.45993027,\n",
              "         0.46310842,  0.5226178 , -0.47786906, -0.47838703, -0.4693375 ,\n",
              "        -0.5197512 ,  0.48067215,  0.4665093 ,  0.5322505 , -0.51330364,\n",
              "        -0.49061766,  0.5382435 ,  0.4777522 , -0.43382886, -0.46551198,\n",
              "        -0.5007157 ,  0.44210175,  0.47202927,  0.50848246, -0.4660193 ,\n",
              "        -0.45815575,  0.51460564,  0.50026995,  0.4863677 ,  0.4396538 ,\n",
              "        -0.4933794 ,  0.48275214, -0.4998792 , -0.52743125, -0.50830096,\n",
              "        -0.53521794,  0.49794024, -0.53203976,  0.46006656, -0.4485479 ,\n",
              "         0.49214855, -0.4895862 , -0.45366958, -0.4713926 ,  0.47751653,\n",
              "         0.49286404,  0.4597476 ,  0.5013502 , -0.513767  ,  0.43843332],\n",
              "       dtype=float32),\n",
              " 'geographic': array([ 0.4816372 ,  0.49470147, -0.4517639 , -0.49642086, -0.46972468,\n",
              "         0.52285266,  0.4621738 , -0.47907066, -0.45210427, -0.44024646,\n",
              "        -0.47584015,  0.47916678,  0.48051018,  0.5001788 , -0.47949538,\n",
              "        -0.5031917 ,  0.4787231 ,  0.46164995, -0.48400706, -0.42377812,\n",
              "        -0.44659305,  0.5174508 ,  0.44918007,  0.5092665 , -0.5082664 ,\n",
              "        -0.48766175,  0.43039614,  0.49672088,  0.44302556,  0.4342659 ,\n",
              "        -0.4889111 ,  0.47681338, -0.51070744, -0.49667123, -0.50442886,\n",
              "        -0.469543  ,  0.5224729 , -0.44087225,  0.49132228, -0.5172623 ,\n",
              "         0.5249213 , -0.51167744, -0.44667187, -0.5000149 ,  0.45585608,\n",
              "         0.45157003,  0.47951895,  0.5049653 , -0.500283  ,  0.44313735],\n",
              "       dtype=float32),\n",
              " 'non-diversified': array([ 0.53317165,  0.49777997, -0.4659396 , -0.47374722, -0.4534532 ,\n",
              "         0.4510895 ,  0.50307006, -0.53128994, -0.52784455, -0.5141668 ,\n",
              "        -0.4663867 ,  0.5096315 ,  0.46347088,  0.5427246 , -0.49087304,\n",
              "        -0.4982498 ,  0.5032958 ,  0.5040927 , -0.48025024, -0.47772428,\n",
              "        -0.47290897,  0.4914782 ,  0.52708024,  0.52695924, -0.444666  ,\n",
              "        -0.52170974,  0.48917502,  0.5104277 ,  0.47960126,  0.45631346,\n",
              "        -0.5207027 ,  0.44058514, -0.5230585 , -0.4918373 , -0.45761916,\n",
              "        -0.53170276,  0.44855154, -0.453068  ,  0.5381782 , -0.52908134,\n",
              "         0.49876767, -0.49885863, -0.48611912, -0.4843863 ,  0.48895043,\n",
              "         0.5440501 ,  0.50404185,  0.4871631 , -0.45781583,  0.48054147],\n",
              "       dtype=float32),\n",
              " 'accordingly': array([ 0.52173173,  0.467134  , -0.53836805, -0.46240458, -0.48850086,\n",
              "         0.46915707,  0.46240994, -0.5432847 , -0.52324504, -0.447395  ,\n",
              "        -0.4957253 ,  0.45077622,  0.4595937 ,  0.5219813 , -0.57290053,\n",
              "        -0.49031633,  0.4528437 ,  0.5263226 , -0.51976955, -0.49337935,\n",
              "        -0.50607616,  0.5209212 ,  0.5375673 ,  0.5131555 , -0.50462556,\n",
              "        -0.48644006,  0.46050575,  0.4794683 ,  0.45940107,  0.52572954,\n",
              "        -0.4951869 ,  0.46472615, -0.46185055, -0.5450855 , -0.50135314,\n",
              "        -0.49988455,  0.46854103, -0.49842697,  0.5069345 , -0.47509396,\n",
              "         0.45468968, -0.5166719 , -0.4628901 , -0.47247177,  0.46522903,\n",
              "         0.5003631 ,  0.46091986,  0.5026257 , -0.46404412,  0.4760963 ],\n",
              "       dtype=float32),\n",
              " 'cases': array([ 0.474644  ,  0.47280547, -0.5277777 , -0.52759945, -0.49754187,\n",
              "         0.4868895 ,  0.43329784, -0.4678661 , -0.4614291 , -0.5122691 ,\n",
              "        -0.45146567,  0.49037004,  0.49162805,  0.48053116, -0.48123735,\n",
              "        -0.45627683,  0.48384798,  0.5157649 , -0.45227635, -0.47434065,\n",
              "        -0.46955726,  0.451084  ,  0.4678203 ,  0.43674597, -0.46043658,\n",
              "        -0.49202207,  0.44892827,  0.5187893 ,  0.4845853 ,  0.5204587 ,\n",
              "        -0.48794785,  0.451552  , -0.5203037 , -0.48975903, -0.4311759 ,\n",
              "        -0.51088464,  0.5110366 , -0.52814686,  0.5197175 , -0.49737903,\n",
              "         0.5071709 , -0.48615363, -0.5023831 , -0.51900756,  0.46639752,\n",
              "         0.50891596,  0.49388534,  0.51843894, -0.5131806 ,  0.4917084 ],\n",
              "       dtype=float32),\n",
              " 'spreads': array([ 0.5039682 ,  0.4757843 , -0.444214  , -0.4497501 , -0.5086223 ,\n",
              "         0.47416595,  0.48277208, -0.4857482 , -0.4951243 , -0.47176445,\n",
              "        -0.48533338,  0.50820225,  0.44551647,  0.51763535, -0.5326245 ,\n",
              "        -0.4591435 ,  0.5204422 ,  0.48329806, -0.51084733, -0.43593076,\n",
              "        -0.49406356,  0.5104084 ,  0.51216775,  0.45218867, -0.47846043,\n",
              "        -0.5002614 ,  0.4692024 ,  0.5466145 ,  0.43144792,  0.5071882 ,\n",
              "        -0.4692847 ,  0.45119935, -0.53843147, -0.47888052, -0.44343388,\n",
              "        -0.52719975,  0.50388753, -0.51577365,  0.4815092 , -0.44156206,\n",
              "         0.45863315, -0.44958392, -0.4579335 , -0.50216204,  0.4568434 ,\n",
              "         0.45244557,  0.45175308,  0.49896023, -0.44771653,  0.4925964 ],\n",
              "       dtype=float32),\n",
              " 'successful': array([ 0.4811084 ,  0.43267116, -0.45023552, -0.4363554 , -0.4755871 ,\n",
              "         0.50663185,  0.46255356, -0.47526968, -0.4742862 , -0.4351096 ,\n",
              "        -0.50593436,  0.47843844,  0.49253246,  0.4740953 , -0.48906136,\n",
              "        -0.46837014,  0.5108984 ,  0.44758072, -0.4235864 , -0.4732445 ,\n",
              "        -0.49969554,  0.45556864,  0.44706488,  0.4985606 , -0.48563147,\n",
              "        -0.47119948,  0.49511844,  0.50694656,  0.44064745,  0.42333817,\n",
              "        -0.43465322,  0.46737155, -0.4561901 , -0.49656233, -0.48909283,\n",
              "        -0.48148137,  0.5075525 , -0.5009428 ,  0.508299  , -0.44705948,\n",
              "         0.50575066, -0.47768775, -0.45598733, -0.45981774,  0.47157493,\n",
              "         0.47809052,  0.4955156 ,  0.4500769 , -0.4971198 ,  0.47738755],\n",
              "       dtype=float32),\n",
              " 'employed': array([ 0.5257612 ,  0.41400453, -0.5115075 , -0.4369952 , -0.44832665,\n",
              "         0.45750406,  0.5152029 , -0.440404  , -0.43351334, -0.4857673 ,\n",
              "        -0.49026924,  0.49663338,  0.46490735,  0.5190191 , -0.5568692 ,\n",
              "        -0.43655697,  0.46484908,  0.47947034, -0.48892373, -0.4334163 ,\n",
              "        -0.502128  ,  0.448857  ,  0.46282357,  0.46139744, -0.48260668,\n",
              "        -0.48738503,  0.49035957,  0.48396325,  0.43344772,  0.44674045,\n",
              "        -0.45142195,  0.47657815, -0.52185065, -0.48559436, -0.42202738,\n",
              "        -0.4961995 ,  0.5230941 , -0.44292244,  0.48486775, -0.4661212 ,\n",
              "         0.45944422, -0.507375  , -0.46693143, -0.4385354 ,  0.4577343 ,\n",
              "         0.49477994,  0.46337697,  0.43215588, -0.5092385 ,  0.4393625 ],\n",
              "       dtype=float32),\n",
              " 'reporting': array([ 0.45585686,  0.48813862, -0.4276618 , -0.4565109 , -0.48092848,\n",
              "         0.46701458,  0.44188818, -0.47489592, -0.48718473, -0.49059615,\n",
              "        -0.50174415,  0.4702897 ,  0.4723612 ,  0.4784569 , -0.4651387 ,\n",
              "        -0.4916469 ,  0.42694238,  0.4762868 , -0.49816462, -0.43312064,\n",
              "        -0.4559248 ,  0.47801223,  0.50920486,  0.4208687 , -0.4260175 ,\n",
              "        -0.41646823,  0.44024807,  0.46817088,  0.46569553,  0.4208456 ,\n",
              "        -0.46434295,  0.46256676, -0.43390334, -0.45183107, -0.47225916,\n",
              "        -0.429455  ,  0.43513244, -0.4930156 ,  0.49500465, -0.50269306,\n",
              "         0.44844723, -0.44976372, -0.443201  , -0.482813  ,  0.42527696,\n",
              "         0.44595972,  0.4722306 ,  0.4997993 , -0.48022935,  0.41405275],\n",
              "       dtype=float32),\n",
              " 'trends': array([ 0.52054524,  0.4423347 , -0.46425906, -0.47971827, -0.49495512,\n",
              "         0.49809754,  0.4361501 , -0.48760372, -0.473853  , -0.44700137,\n",
              "        -0.4426559 ,  0.48182085,  0.48106736,  0.42777684, -0.44998053,\n",
              "        -0.4842332 ,  0.45415497,  0.5034108 , -0.4615973 , -0.4576553 ,\n",
              "        -0.48586026,  0.44889238,  0.45625985,  0.48214078, -0.48292124,\n",
              "        -0.43568638,  0.49346676,  0.49431232,  0.46220034,  0.4933502 ,\n",
              "        -0.4582525 ,  0.4115525 , -0.46145883, -0.4848617 , -0.4397191 ,\n",
              "        -0.4773959 ,  0.48519644, -0.4413988 ,  0.45407888, -0.50141996,\n",
              "         0.45362946, -0.4209467 , -0.4633644 , -0.4259235 ,  0.4310798 ,\n",
              "         0.4804338 ,  0.44144055,  0.4728454 , -0.4427737 ,  0.41375354],\n",
              "       dtype=float32),\n",
              " 'november': array([ 0.52277833,  0.4335433 , -0.46469387, -0.50644237, -0.51499736,\n",
              "         0.44018796,  0.47629184, -0.45371965, -0.5024869 , -0.42692715,\n",
              "        -0.47970042,  0.4897343 ,  0.48894528,  0.45031896, -0.49864754,\n",
              "        -0.47564238,  0.51225513,  0.5178206 , -0.42300642, -0.4815513 ,\n",
              "        -0.4641866 ,  0.4223972 ,  0.45938912,  0.4368453 , -0.501354  ,\n",
              "        -0.4468059 ,  0.48795786,  0.49162263,  0.45654178,  0.51192296,\n",
              "        -0.44522512,  0.48329377, -0.47567764, -0.48958987, -0.41824672,\n",
              "        -0.48796386,  0.47944394, -0.49426675,  0.48239738, -0.51224315,\n",
              "         0.4691558 , -0.47568762, -0.46883425, -0.43686405,  0.45997214,\n",
              "         0.4285722 ,  0.47533083,  0.42412686, -0.49037254,  0.42833272],\n",
              "       dtype=float32),\n",
              " 'commodity-linked': array([ 0.48649848,  0.47553933, -0.44274947, -0.43948603, -0.49104038,\n",
              "         0.42471385,  0.4473624 , -0.4421648 , -0.48758745, -0.46385914,\n",
              "        -0.43510756,  0.40443987,  0.43795583,  0.41632304, -0.4680998 ,\n",
              "        -0.48767757,  0.43988046,  0.47586137, -0.47696146, -0.45758176,\n",
              "        -0.48442098,  0.43958908,  0.4439541 ,  0.42870283, -0.48175794,\n",
              "        -0.432078  ,  0.44871598,  0.44227847,  0.44669643,  0.47708794,\n",
              "        -0.49890584,  0.45435277, -0.45366892, -0.4219181 , -0.4379676 ,\n",
              "        -0.45785695,  0.4505236 , -0.49484268,  0.45722818, -0.4309898 ,\n",
              "         0.49099898, -0.43144348, -0.4479841 , -0.41075525,  0.4652323 ,\n",
              "         0.4935153 ,  0.45960993,  0.48289487, -0.47187993,  0.47277212],\n",
              "       dtype=float32),\n",
              " 'beginning': array([ 0.45314845,  0.48751122, -0.4809612 , -0.4985974 , -0.51535773,\n",
              "         0.51454103,  0.44579548, -0.43993914, -0.4856969 , -0.48800167,\n",
              "        -0.5032467 ,  0.5044117 ,  0.4780382 ,  0.44067132, -0.5031583 ,\n",
              "        -0.46185592,  0.52322394,  0.46549535, -0.4850617 , -0.48533115,\n",
              "        -0.45691565,  0.42356312,  0.4298832 ,  0.50003904, -0.44693813,\n",
              "        -0.49664956,  0.4341665 ,  0.4582918 ,  0.4651409 ,  0.42875153,\n",
              "        -0.5232734 ,  0.47494018, -0.53097844, -0.51268536, -0.47151533,\n",
              "        -0.44859675,  0.5226479 , -0.43685544,  0.4838651 , -0.43815562,\n",
              "         0.522573  , -0.46434093, -0.4584386 , -0.4356632 ,  0.47594738,\n",
              "         0.46704638,  0.48064703,  0.48277918, -0.42372614,  0.45747203],\n",
              "       dtype=float32),\n",
              " 'obtained': array([ 0.54202205,  0.48555475, -0.49917907, -0.49255636, -0.47637713,\n",
              "         0.49917185,  0.46807492, -0.46111   , -0.47211453, -0.44026276,\n",
              "        -0.4580528 ,  0.5170684 ,  0.44689253,  0.46081132, -0.53890103,\n",
              "        -0.51746136,  0.5217944 ,  0.48661107, -0.46398053, -0.46667078,\n",
              "        -0.4839869 ,  0.48917687,  0.47953138,  0.5071359 , -0.43198016,\n",
              "        -0.44175825,  0.4804127 ,  0.5310733 ,  0.4579385 ,  0.5245173 ,\n",
              "        -0.47363377,  0.49141055, -0.45424113, -0.47939453, -0.42659244,\n",
              "        -0.46670058,  0.46189225, -0.47529447,  0.44852203, -0.49287322,\n",
              "         0.5227645 , -0.48179165, -0.49840122, -0.52034616,  0.46547517,\n",
              "         0.49505958,  0.43653125,  0.50479645, -0.47264448,  0.4372104 ],\n",
              "       dtype=float32),\n",
              " 'lesser': array([ 0.5063668 ,  0.4411704 , -0.53406745, -0.51308304, -0.47778293,\n",
              "         0.5156285 ,  0.4585684 , -0.5274713 , -0.44983697, -0.4543763 ,\n",
              "        -0.5073368 ,  0.52809   ,  0.4545455 ,  0.52780646, -0.48734683,\n",
              "        -0.44231185,  0.54143083,  0.4632812 , -0.4735016 , -0.5141635 ,\n",
              "        -0.5345669 ,  0.5040434 ,  0.4937337 ,  0.47893965, -0.4663926 ,\n",
              "        -0.5292771 ,  0.45762858,  0.46784788,  0.45920035,  0.45944223,\n",
              "        -0.47421375,  0.47178116, -0.463866  , -0.494187  , -0.515166  ,\n",
              "        -0.535397  ,  0.45593327, -0.50401074,  0.4467013 , -0.48185623,\n",
              "         0.50073785, -0.44532526, -0.52406895, -0.50077236,  0.4663283 ,\n",
              "         0.48304856,  0.5269176 ,  0.4631169 , -0.47710168,  0.49684864],\n",
              "       dtype=float32),\n",
              " 'regional': array([ 0.4771283 ,  0.4570369 , -0.49021244, -0.5154209 , -0.5222159 ,\n",
              "         0.4841126 ,  0.47971016, -0.48859423, -0.45988846, -0.42957988,\n",
              "        -0.47578788,  0.48233894,  0.4666584 ,  0.43540826, -0.47025892,\n",
              "        -0.4490475 ,  0.50498354,  0.52558744, -0.44323057, -0.4331707 ,\n",
              "        -0.4939596 ,  0.45448154,  0.4855516 ,  0.4822657 , -0.4248947 ,\n",
              "        -0.46780428,  0.45523602,  0.5310852 ,  0.46560702,  0.47761208,\n",
              "        -0.48084235,  0.5053439 , -0.4596314 , -0.5265341 , -0.4482352 ,\n",
              "        -0.5063294 ,  0.47730744, -0.49816772,  0.4390723 , -0.45183393,\n",
              "         0.45383158, -0.51717407, -0.49220458, -0.5114554 ,  0.46860433,\n",
              "         0.45819548,  0.48088422,  0.5127715 , -0.44285992,  0.43541116],\n",
              "       dtype=float32),\n",
              " 'sources': array([ 0.45437893,  0.46715388, -0.48049343, -0.51594645, -0.498693  ,\n",
              "         0.51673573,  0.42992094, -0.46825707, -0.49460554, -0.49368665,\n",
              "        -0.5066295 ,  0.4541966 ,  0.44985804,  0.46470934, -0.51202273,\n",
              "        -0.51245236,  0.4982169 ,  0.43792447, -0.4537801 , -0.45957306,\n",
              "        -0.4489222 ,  0.48707616,  0.45135903,  0.5091956 , -0.47029787,\n",
              "        -0.4959338 ,  0.51475525,  0.5403908 ,  0.50587547,  0.47153568,\n",
              "        -0.47958457,  0.5057303 , -0.4786015 , -0.4621052 , -0.48352665,\n",
              "        -0.46962252,  0.49078387, -0.47054914,  0.47368962, -0.49786785,\n",
              "         0.47513315, -0.5137532 , -0.46098942, -0.45875838,  0.43421057,\n",
              "         0.46239   ,  0.503215  ,  0.50511426, -0.43335354,  0.5105651 ],\n",
              "       dtype=float32),\n",
              " 'tax-exempt': array([ 0.49331453,  0.4428844 , -0.45862618, -0.48081258, -0.5129756 ,\n",
              "         0.50555396,  0.5033475 , -0.42040816, -0.43318048, -0.45122126,\n",
              "        -0.41602114,  0.47445422,  0.44463018,  0.44742578, -0.5042561 ,\n",
              "        -0.47327754,  0.4583781 ,  0.44790396, -0.4631197 , -0.42691728,\n",
              "        -0.4658241 ,  0.43517855,  0.42971087,  0.45475146, -0.4447668 ,\n",
              "        -0.48776644,  0.48682925,  0.48684227,  0.4679557 ,  0.425058  ,\n",
              "        -0.4856622 ,  0.4282047 , -0.5186194 , -0.5090566 , -0.4668451 ,\n",
              "        -0.46812934,  0.4662864 , -0.46202165,  0.4523226 , -0.47096175,\n",
              "         0.46331978, -0.49684376, -0.4843135 , -0.45363194,  0.499215  ,\n",
              "         0.43385318,  0.4464794 ,  0.48293716, -0.464424  ,  0.42950302],\n",
              "       dtype=float32),\n",
              " 'strength': array([ 0.48929098,  0.45915502, -0.50814694, -0.4451784 , -0.45999765,\n",
              "         0.4563971 ,  0.43376043, -0.4614311 , -0.46677008, -0.45321393,\n",
              "        -0.42852587,  0.44204426,  0.425309  ,  0.48959878, -0.536895  ,\n",
              "        -0.43221575,  0.4946769 ,  0.4987506 , -0.44693667, -0.4227211 ,\n",
              "        -0.50354576,  0.47752059,  0.4710175 ,  0.44160524, -0.4412433 ,\n",
              "        -0.4600091 ,  0.48132384,  0.45327306,  0.45857877,  0.47230983,\n",
              "        -0.51005495,  0.42698482, -0.48129386, -0.46913365, -0.4225559 ,\n",
              "        -0.43880883,  0.44521827, -0.46558544,  0.47285616, -0.5121071 ,\n",
              "         0.47993094, -0.46378827, -0.45305115, -0.49470413,  0.49590105,\n",
              "         0.42383188,  0.50883377,  0.47836235, -0.4767818 ,  0.4934104 ],\n",
              "       dtype=float32),\n",
              " 'internet': array([ 0.46471754,  0.49759316, -0.48964053, -0.50507885, -0.43206808,\n",
              "         0.51022685,  0.5033889 , -0.5020104 , -0.49696177, -0.4721647 ,\n",
              "        -0.47154033,  0.46812916,  0.41620767,  0.42523935, -0.4816008 ,\n",
              "        -0.46169907,  0.46805173,  0.50354934, -0.44129223, -0.42970577,\n",
              "        -0.4901879 ,  0.42921942,  0.5142403 ,  0.47170493, -0.4170029 ,\n",
              "        -0.48161766,  0.47394976,  0.52678597,  0.48487   ,  0.48972154,\n",
              "        -0.45667368,  0.43631366, -0.45134658, -0.4466565 , -0.4561683 ,\n",
              "        -0.44990894,  0.47348106, -0.4854646 ,  0.47917524, -0.43828663,\n",
              "         0.47369248, -0.4426768 , -0.44177994, -0.4374252 ,  0.47172627,\n",
              "         0.5020203 ,  0.45947397,  0.4692183 , -0.49766082,  0.47035083],\n",
              "       dtype=float32),\n",
              " 'nyse': array([ 0.5330862 ,  0.447517  , -0.45297757, -0.49973074, -0.5010981 ,\n",
              "         0.4733554 ,  0.46976298, -0.5049498 , -0.46780634, -0.46211624,\n",
              "        -0.4800437 ,  0.45746595,  0.4895932 ,  0.4374496 , -0.5007414 ,\n",
              "        -0.5093694 ,  0.4935058 ,  0.4817791 , -0.4854913 , -0.41758335,\n",
              "        -0.44143078,  0.4499639 ,  0.45495933,  0.42405474, -0.45122197,\n",
              "        -0.4606649 ,  0.48188677,  0.49244362,  0.49784642,  0.42048833,\n",
              "        -0.48564255,  0.48503885, -0.51854426, -0.47318646, -0.4758087 ,\n",
              "        -0.4718231 ,  0.5003236 , -0.5147646 ,  0.45681724, -0.43174633,\n",
              "         0.44091803, -0.5141341 , -0.4599212 , -0.47804672,  0.44812626,\n",
              "         0.46607125,  0.48926142,  0.46139485, -0.44793528,  0.507003  ],\n",
              "       dtype=float32),\n",
              " 'might': array([ 0.50725156,  0.48911184, -0.50699043, -0.44293398, -0.46528485,\n",
              "         0.48449793,  0.4932598 , -0.47471252, -0.4901071 , -0.45490494,\n",
              "        -0.51205283,  0.49320102,  0.46064022,  0.46869716, -0.54958946,\n",
              "        -0.46285084,  0.4765387 ,  0.52952516, -0.4893049 , -0.47405845,\n",
              "        -0.46760347,  0.43820304,  0.45985067,  0.4649026 , -0.49072462,\n",
              "        -0.44814882,  0.5024205 ,  0.49135488,  0.4831239 ,  0.50681406,\n",
              "        -0.5379156 ,  0.47164062, -0.47355312, -0.51595634, -0.4783136 ,\n",
              "        -0.52708036,  0.49126476, -0.49109185,  0.5257502 , -0.5198117 ,\n",
              "         0.45058557, -0.4749853 , -0.51130307, -0.48385188,  0.45554584,\n",
              "         0.46008456,  0.4600948 ,  0.5030236 , -0.4482236 ,  0.4873591 ],\n",
              "       dtype=float32),\n",
              " 'indexsm': array([ 0.49999785,  0.46844918, -0.48566502, -0.53352505, -0.53314424,\n",
              "         0.48109192,  0.49452356, -0.53525424, -0.45862225, -0.46186224,\n",
              "        -0.50427306,  0.5346767 ,  0.47477466,  0.4660372 , -0.49342507,\n",
              "        -0.5367173 ,  0.53460926,  0.5506235 , -0.52475804, -0.49398905,\n",
              "        -0.46866143,  0.5350644 ,  0.48589784,  0.46147478, -0.5286304 ,\n",
              "        -0.48508832,  0.48710302,  0.48109198,  0.47138768,  0.47159305,\n",
              "        -0.4707404 ,  0.4975071 , -0.5429585 , -0.5351764 , -0.48368058,\n",
              "        -0.53454447,  0.47406262, -0.47650635,  0.51145554, -0.50051713,\n",
              "         0.49591258, -0.49162054, -0.47013885, -0.48295584,  0.48543003,\n",
              "         0.54354763,  0.46532294,  0.4847703 , -0.5469757 ,  0.4845879 ],\n",
              "       dtype=float32),\n",
              " 'concentration': array([ 0.5523821 ,  0.5266213 , -0.48669603, -0.47072783, -0.47973835,\n",
              "         0.50540197,  0.52366805, -0.47982642, -0.4745621 , -0.526818  ,\n",
              "        -0.4653005 ,  0.50501066,  0.44624802,  0.5178061 , -0.5503578 ,\n",
              "        -0.48883408,  0.46965045,  0.45011923, -0.5298587 , -0.4860607 ,\n",
              "        -0.4512754 ,  0.4691109 ,  0.4844163 ,  0.5126581 , -0.46225375,\n",
              "        -0.47035298,  0.44998318,  0.5271492 ,  0.44779542,  0.4777695 ,\n",
              "        -0.5551677 ,  0.48368385, -0.528407  , -0.5021455 , -0.5325752 ,\n",
              "        -0.51741153,  0.50268924, -0.53662103,  0.50609964, -0.47367477,\n",
              "         0.47139964, -0.46231914, -0.5197114 , -0.4743959 ,  0.50616395,\n",
              "         0.5206038 ,  0.448905  ,  0.4559497 , -0.48849934,  0.45653754],\n",
              "       dtype=float32),\n",
              " 'realpath®': array([ 0.49237108,  0.4374716 , -0.49750674, -0.5311238 , -0.5030596 ,\n",
              "         0.50100213,  0.5197106 , -0.49841315, -0.5028813 , -0.50458163,\n",
              "        -0.52811235,  0.4902345 ,  0.4452994 ,  0.52238184, -0.55086875,\n",
              "        -0.45272845,  0.50640166,  0.5312758 , -0.4831432 , -0.44542226,\n",
              "        -0.46323508,  0.47221527,  0.51007414,  0.52513266, -0.5302198 ,\n",
              "        -0.48142433,  0.49801466,  0.5001504 ,  0.5072275 ,  0.5199931 ,\n",
              "        -0.52478325,  0.46936214, -0.5340158 , -0.5124544 , -0.4676108 ,\n",
              "        -0.4998786 ,  0.51885986, -0.49289867,  0.48809338, -0.47709545,\n",
              "         0.51066434, -0.5376337 , -0.4610142 , -0.49878022,  0.49377647,\n",
              "         0.5052784 ,  0.50759476,  0.46682063, -0.52177685,  0.49156702],\n",
              "       dtype=float32),\n",
              " 'opportunity': array([ 0.48020405,  0.48519573, -0.5239841 , -0.49587655, -0.45249045,\n",
              "         0.5098774 ,  0.5297067 , -0.48676476, -0.4712274 , -0.4843678 ,\n",
              "        -0.48234075,  0.45864573,  0.45846486,  0.48825997, -0.49501893,\n",
              "        -0.50784564,  0.45194548,  0.5237605 , -0.46450165, -0.45549875,\n",
              "        -0.521952  ,  0.43343672,  0.532504  ,  0.46128467, -0.499738  ,\n",
              "        -0.5016122 ,  0.5072439 ,  0.535755  ,  0.51387626,  0.50188744,\n",
              "        -0.526228  ,  0.4661468 , -0.5073805 , -0.527965  , -0.4979742 ,\n",
              "        -0.45628092,  0.52055645, -0.50779927,  0.5068725 , -0.51541066,\n",
              "         0.4967589 , -0.447674  , -0.44845712, -0.45887232,  0.46189436,\n",
              "         0.492393  ,  0.47153518,  0.47996113, -0.4797442 ,  0.46432307],\n",
              "       dtype=float32),\n",
              " 'lines': array([ 0.5486023 ,  0.46626374, -0.47395357, -0.42063797, -0.50189495,\n",
              "         0.5132118 ,  0.47969243, -0.44827977, -0.45070958, -0.44480813,\n",
              "        -0.49400437,  0.44262558,  0.5116522 ,  0.4952206 , -0.53767097,\n",
              "        -0.5083996 ,  0.4777625 ,  0.5174311 , -0.44388002, -0.43587306,\n",
              "        -0.47530967,  0.418042  ,  0.51587266,  0.44835323, -0.4736634 ,\n",
              "        -0.43644732,  0.4880702 ,  0.51885545,  0.49226844,  0.5062307 ,\n",
              "        -0.52748966,  0.4937461 , -0.52417374, -0.5226258 , -0.47388703,\n",
              "        -0.45250672,  0.44897664, -0.49784738,  0.46740195, -0.4483635 ,\n",
              "         0.4947935 , -0.4882211 , -0.46607432, -0.50217867,  0.46767852,\n",
              "         0.4662449 ,  0.504781  ,  0.45721665, -0.44005013,  0.46975705],\n",
              "       dtype=float32),\n",
              " 'vulnerable': array([ 0.527691  ,  0.44540042, -0.4580175 , -0.48795715, -0.514417  ,\n",
              "         0.45660502,  0.46115437, -0.49200618, -0.51379657, -0.44369587,\n",
              "        -0.49472052,  0.4525498 ,  0.51613516,  0.4871394 , -0.5420051 ,\n",
              "        -0.4648862 ,  0.5017283 ,  0.45621637, -0.5113459 , -0.4928761 ,\n",
              "        -0.4792942 ,  0.4899354 ,  0.5074437 ,  0.45945442, -0.45891157,\n",
              "        -0.50864863,  0.4686327 ,  0.5168058 ,  0.4743659 ,  0.49853945,\n",
              "        -0.5303227 ,  0.48689127, -0.549756  , -0.5063861 , -0.43086267,\n",
              "        -0.45464784,  0.5195266 , -0.52535456,  0.43593174, -0.512351  ,\n",
              "         0.5084107 , -0.45527732, -0.45847088, -0.42568955,  0.47201407,\n",
              "         0.48846415,  0.4338657 ,  0.49340692, -0.46485966,  0.5129564 ],\n",
              "       dtype=float32),\n",
              " 'admiral': array([ 0.51892483,  0.46395054, -0.52144367, -0.48410556, -0.45761338,\n",
              "         0.45792848,  0.4438548 , -0.51406413, -0.52663505, -0.47641474,\n",
              "        -0.47510448,  0.50111663,  0.48960087,  0.4551295 , -0.54612505,\n",
              "        -0.5423244 ,  0.47101858,  0.47543553, -0.5149183 , -0.5269401 ,\n",
              "        -0.49096763,  0.48448566,  0.5300491 ,  0.4835214 , -0.5029314 ,\n",
              "        -0.47499064,  0.46903428,  0.52445084,  0.45663252,  0.5288499 ,\n",
              "        -0.49217343,  0.5274342 , -0.46935523, -0.5052598 , -0.5057757 ,\n",
              "        -0.52656215,  0.45516533, -0.46194047,  0.46702135, -0.484168  ,\n",
              "         0.5401263 , -0.4946937 , -0.45210516, -0.4950857 ,  0.45835024,\n",
              "         0.5054596 ,  0.47935322,  0.5061885 , -0.5181285 ,  0.500565  ],\n",
              "       dtype=float32),\n",
              " 'war': array([ 0.5272147 ,  0.41305944, -0.43494686, -0.43022025, -0.45928523,\n",
              "         0.49442005,  0.46790755, -0.50082016, -0.43607938, -0.48331067,\n",
              "        -0.46029115,  0.47719985,  0.47197866,  0.42418927, -0.54008317,\n",
              "        -0.43247113,  0.43831238,  0.4801859 , -0.43230376, -0.45963877,\n",
              "        -0.45250636,  0.49215376,  0.4916559 ,  0.48616418, -0.4464931 ,\n",
              "        -0.4724157 ,  0.44263116,  0.46152467,  0.49602205,  0.45054087,\n",
              "        -0.47445592,  0.46065557, -0.5078976 , -0.49491942, -0.5003021 ,\n",
              "        -0.50379026,  0.50802845, -0.4654425 ,  0.46023414, -0.48825574,\n",
              "         0.51376134, -0.4785127 , -0.4253621 , -0.4578216 ,  0.4585666 ,\n",
              "         0.49097714,  0.46739233,  0.48051316, -0.51339847,  0.50306046],\n",
              "       dtype=float32),\n",
              " 'close': array([ 0.50303715,  0.47851235, -0.52652353, -0.5181178 , -0.4873041 ,\n",
              "         0.49854958,  0.51427984, -0.47359076, -0.44077042, -0.4357483 ,\n",
              "        -0.4470078 ,  0.50863075,  0.4919874 ,  0.5268409 , -0.4769248 ,\n",
              "        -0.43124703,  0.44670638,  0.5202149 , -0.437071  , -0.42673698,\n",
              "        -0.44451872,  0.48898587,  0.4729763 ,  0.43152788, -0.47454423,\n",
              "        -0.45097396,  0.44838494,  0.4537903 ,  0.48553473,  0.48244154,\n",
              "        -0.48478106,  0.4459621 , -0.51343924, -0.47459638, -0.4708321 ,\n",
              "        -0.52052456,  0.5133217 , -0.4710087 ,  0.506476  , -0.49452657,\n",
              "         0.47644734, -0.5116407 , -0.44897467, -0.44190717,  0.43263054,\n",
              "         0.47060472,  0.5103365 ,  0.49702775, -0.43275768,  0.49250853],\n",
              "       dtype=float32),\n",
              " 'weightings': array([ 0.53947854,  0.46078676, -0.4572738 , -0.49332404, -0.46777916,\n",
              "         0.5177492 ,  0.52257216, -0.49732506, -0.4407102 , -0.49157664,\n",
              "        -0.50863886,  0.5164679 ,  0.49768105,  0.44494808, -0.5379976 ,\n",
              "        -0.4622626 ,  0.47292203,  0.46318886, -0.4396432 , -0.4458457 ,\n",
              "        -0.50272053,  0.46824452,  0.4824626 ,  0.4380033 , -0.43643454,\n",
              "        -0.5214051 ,  0.48305005,  0.53866804,  0.49988285,  0.45139527,\n",
              "        -0.5206411 ,  0.46062362, -0.45911288, -0.44686466, -0.5126445 ,\n",
              "        -0.47306395,  0.5107343 , -0.4797538 ,  0.43828666, -0.47068253,\n",
              "         0.4864151 , -0.48114443, -0.49015367, -0.47906208,  0.45841122,\n",
              "         0.47745022,  0.5164182 ,  0.47644874, -0.48988858,  0.43775204],\n",
              "       dtype=float32),\n",
              " 'reed': array([ 0.49824676,  0.4332961 , -0.44177872, -0.46817532, -0.46308956,\n",
              "         0.4952675 ,  0.43878865, -0.4346477 , -0.44255838, -0.47268626,\n",
              "        -0.45755553,  0.5162729 ,  0.4406195 ,  0.4498271 , -0.5287072 ,\n",
              "        -0.4582766 ,  0.4587269 ,  0.47123998, -0.47427   , -0.5158193 ,\n",
              "        -0.48409516,  0.4537176 ,  0.45706764,  0.43780267, -0.43149263,\n",
              "        -0.49711347,  0.47700033,  0.53077686,  0.49932322,  0.48842227,\n",
              "        -0.5146292 ,  0.43205643, -0.5054077 , -0.4819993 , -0.44552863,\n",
              "        -0.50817555,  0.45531252, -0.5256405 ,  0.43721393, -0.46317336,\n",
              "         0.45716518, -0.45239705, -0.48165548, -0.47267002,  0.51337063,\n",
              "         0.43944836,  0.45141086,  0.49840632, -0.5181246 ,  0.4495627 ],\n",
              "       dtype=float32),\n",
              " 'subsidiary': array([ 0.54577196,  0.51141673, -0.46404457, -0.5056815 , -0.5094352 ,\n",
              "         0.47434577,  0.5034762 , -0.5095593 , -0.51749706, -0.46502545,\n",
              "        -0.51806444,  0.50427306,  0.51946414,  0.51048994, -0.5415615 ,\n",
              "        -0.5080664 ,  0.5196989 ,  0.46096146, -0.4771377 , -0.4737033 ,\n",
              "        -0.49178386,  0.4831825 ,  0.49400032,  0.44375223, -0.5039231 ,\n",
              "        -0.4572479 ,  0.50132704,  0.53417826,  0.5004841 ,  0.48539117,\n",
              "        -0.49227902,  0.4643579 , -0.5111369 , -0.5054759 , -0.4747237 ,\n",
              "        -0.47295922,  0.47710294, -0.5127419 ,  0.49786142, -0.4894449 ,\n",
              "         0.5187011 , -0.4647351 , -0.47720376, -0.47351333,  0.49097458,\n",
              "         0.4816426 ,  0.4802168 ,  0.49045187, -0.51825136,  0.47166955],\n",
              "       dtype=float32),\n",
              " 'universe': array([ 0.54596776,  0.4865047 , -0.515303  , -0.5172398 , -0.51111794,\n",
              "         0.5365233 ,  0.44170588, -0.50729114, -0.47639632, -0.4409065 ,\n",
              "        -0.5184914 ,  0.43724218,  0.5088201 ,  0.44139206, -0.5682796 ,\n",
              "        -0.49586114,  0.4682481 ,  0.4489465 , -0.48755157, -0.45952874,\n",
              "        -0.4246607 ,  0.46030217,  0.48117316,  0.4336387 , -0.43895826,\n",
              "        -0.44149783,  0.47929317,  0.49619716,  0.48421544,  0.45673317,\n",
              "        -0.49042425,  0.45664862, -0.5243993 , -0.51829743, -0.50975406,\n",
              "        -0.4955357 ,  0.44636813, -0.503044  ,  0.50893974, -0.46127152,\n",
              "         0.47355074, -0.50235224, -0.4759683 , -0.42054945,  0.47004828,\n",
              "         0.49235216,  0.47710666,  0.5093801 , -0.4315376 ,  0.46120608],\n",
              "       dtype=float32),\n",
              " 'analyses': array([ 0.44546005,  0.41398305, -0.4559854 , -0.4386542 , -0.5080319 ,\n",
              "         0.51305556,  0.42628285, -0.48178434, -0.43472648, -0.43047303,\n",
              "        -0.4652306 ,  0.43698087,  0.4407531 ,  0.4460121 , -0.4579664 ,\n",
              "        -0.44784367,  0.49899304,  0.4999668 , -0.45019397, -0.45331445,\n",
              "        -0.46305144,  0.48365244,  0.44399723,  0.44778618, -0.47096142,\n",
              "        -0.41716754,  0.41590506,  0.50538814,  0.4945535 ,  0.41509894,\n",
              "        -0.44347942,  0.45741355, -0.4418251 , -0.4559599 , -0.44106242,\n",
              "        -0.5037732 ,  0.5025323 , -0.48955718,  0.49356887, -0.50710785,\n",
              "         0.4824617 , -0.5091122 , -0.4308255 , -0.47763342,  0.4875536 ,\n",
              "         0.47996816,  0.47582376,  0.45333716, -0.4485625 ,  0.43512788],\n",
              "       dtype=float32),\n",
              " 'large-capitalization': array([ 0.48734102,  0.43188927, -0.43832445, -0.4893267 , -0.45741004,\n",
              "         0.5073619 ,  0.45623848, -0.45569676, -0.4549333 , -0.4710753 ,\n",
              "        -0.49787   ,  0.47503686,  0.495145  ,  0.4303105 , -0.4650373 ,\n",
              "        -0.5043379 ,  0.4625628 ,  0.48733526, -0.49483022, -0.46628213,\n",
              "        -0.46895665,  0.47799468,  0.4321503 ,  0.4742929 , -0.48759243,\n",
              "        -0.44624862,  0.4984864 ,  0.47313637,  0.4493136 ,  0.48144805,\n",
              "        -0.48284808,  0.48045498, -0.45798284, -0.51174563, -0.4937625 ,\n",
              "        -0.4275086 ,  0.45757928, -0.4933695 ,  0.44881707, -0.4928312 ,\n",
              "         0.45436138, -0.47364092, -0.44820803, -0.4781347 ,  0.48744363,\n",
              "         0.4193639 ,  0.42334598,  0.46868363, -0.45782796,  0.48204476],\n",
              "       dtype=float32),\n",
              " 'january': array([ 0.4820718 ,  0.4946077 , -0.46370634, -0.44332868, -0.4888217 ,\n",
              "         0.46709868,  0.46258   , -0.48517329, -0.4509619 , -0.50260556,\n",
              "        -0.4398627 ,  0.5087119 ,  0.48344386,  0.44199315, -0.5150007 ,\n",
              "        -0.5271163 ,  0.4851344 ,  0.46515465, -0.4404139 , -0.5009745 ,\n",
              "        -0.50312215,  0.45656008,  0.46691945,  0.43687716, -0.46049267,\n",
              "        -0.5167314 ,  0.45620227,  0.47244072,  0.4710151 ,  0.47244167,\n",
              "        -0.48836842,  0.4400191 , -0.5176416 , -0.46930572, -0.44151112,\n",
              "        -0.46014068,  0.5228166 , -0.49970552,  0.46639556, -0.48926824,\n",
              "         0.45324004, -0.5182012 , -0.4979155 , -0.45770037,  0.44575533,\n",
              "         0.5060708 ,  0.44009715,  0.4796622 , -0.45252478,  0.46829897],\n",
              "       dtype=float32),\n",
              " 'noted': array([ 0.48225448,  0.43893582, -0.5259233 , -0.47563246, -0.49605048,\n",
              "         0.50009966,  0.47939777, -0.45089486, -0.49452937, -0.43382436,\n",
              "        -0.4980688 ,  0.50819814,  0.5232385 ,  0.51425177, -0.53210783,\n",
              "        -0.45481113,  0.4715624 ,  0.52244675, -0.49872297, -0.45352724,\n",
              "        -0.5285007 ,  0.49326378,  0.5088343 ,  0.4402349 , -0.46428347,\n",
              "        -0.5242004 ,  0.48095715,  0.5361336 ,  0.4813689 ,  0.47765726,\n",
              "        -0.50706327,  0.47104922, -0.5211497 , -0.45704326, -0.4819357 ,\n",
              "        -0.43998742,  0.5340555 , -0.50439525,  0.5057629 , -0.48108372,\n",
              "         0.48493618, -0.47891003, -0.4398677 , -0.47102088,  0.5034243 ,\n",
              "         0.50436133,  0.4863371 ,  0.46967176, -0.4532066 ,  0.5089312 ],\n",
              "       dtype=float32),\n",
              " 'oversight': array([ 0.5267203 ,  0.46462747, -0.47239283, -0.47564146, -0.4373069 ,\n",
              "         0.4771262 ,  0.4739197 , -0.49948245, -0.44354776, -0.5016408 ,\n",
              "        -0.51718897,  0.47321507,  0.44562846,  0.48275384, -0.53177816,\n",
              "        -0.44537866,  0.43961757,  0.50007993, -0.46150517, -0.4361779 ,\n",
              "        -0.43290484,  0.4435524 ,  0.48507923,  0.43033805, -0.49407247,\n",
              "        -0.44301462,  0.46270722,  0.44823104,  0.4977167 ,  0.5025445 ,\n",
              "        -0.51625705,  0.43472877, -0.5130645 , -0.44915354, -0.50855654,\n",
              "        -0.5141158 ,  0.46414936, -0.5094487 ,  0.45625433, -0.465828  ,\n",
              "         0.47531354, -0.50967044, -0.43735594, -0.49172258,  0.53097385,\n",
              "         0.50074077,  0.47556075,  0.4374492 , -0.50556296,  0.49293166],\n",
              "       dtype=float32),\n",
              " 'investment-grade': array([ 0.51079166,  0.4594894 , -0.44384098, -0.4351072 , -0.49960896,\n",
              "         0.5082498 ,  0.43756947, -0.45834616, -0.44987875, -0.48110616,\n",
              "        -0.45628086,  0.4551371 ,  0.44300234,  0.51240873, -0.5662295 ,\n",
              "        -0.4877745 ,  0.48950195,  0.45985922, -0.4718515 , -0.4897266 ,\n",
              "        -0.48527718,  0.5131034 ,  0.50355947,  0.42532644, -0.48389623,\n",
              "        -0.51527077,  0.43227288,  0.52308726,  0.4948386 ,  0.4791341 ,\n",
              "        -0.46012527,  0.47502655, -0.4880678 , -0.50954866, -0.43359697,\n",
              "        -0.4643579 ,  0.52851546, -0.4760481 ,  0.43942904, -0.44615647,\n",
              "         0.49240154, -0.49074244, -0.42764258, -0.46336707,  0.4704937 ,\n",
              "         0.44293156,  0.459301  ,  0.49596086, -0.4323006 ,  0.43967843],\n",
              "       dtype=float32),\n",
              " 'property': array([ 0.53660625,  0.48854393, -0.43403113, -0.46912774, -0.5050961 ,\n",
              "         0.49480003,  0.4473318 , -0.45284855, -0.43705255, -0.4501968 ,\n",
              "        -0.43946555,  0.45426947,  0.469844  ,  0.4617548 , -0.54039174,\n",
              "        -0.4551193 ,  0.4818844 ,  0.50344914, -0.45423502, -0.47732264,\n",
              "        -0.46982187,  0.46711892,  0.5227693 ,  0.45056793, -0.48254633,\n",
              "        -0.49465656,  0.45126045,  0.46912476,  0.44069543,  0.42466027,\n",
              "        -0.4692096 ,  0.5129326 , -0.5321047 , -0.49603903, -0.4649116 ,\n",
              "        -0.4630466 ,  0.4931564 , -0.49959266,  0.4881065 , -0.504153  ,\n",
              "         0.47359878, -0.5134106 , -0.48838606, -0.43781897,  0.46201214,\n",
              "         0.48813957,  0.5117449 ,  0.4362767 , -0.48103416,  0.47599882],\n",
              "       dtype=float32),\n",
              " 'national': array([ 0.46828485,  0.46681798, -0.5090435 , -0.45442098, -0.48096037,\n",
              "         0.47222587,  0.45835555, -0.5214891 , -0.458255  , -0.47850013,\n",
              "        -0.45682314,  0.5107951 ,  0.476325  ,  0.5124073 , -0.4960077 ,\n",
              "        -0.51115924,  0.4429522 ,  0.44580978, -0.44687593, -0.49290606,\n",
              "        -0.42504853,  0.46368134,  0.48114228,  0.5128867 , -0.4440007 ,\n",
              "        -0.47220635,  0.4225314 ,  0.51677537,  0.48298928,  0.45628667,\n",
              "        -0.51808435,  0.4723808 , -0.5171727 , -0.501968  , -0.491535  ,\n",
              "        -0.49538788,  0.5178246 , -0.48630753,  0.44596568, -0.49630296,\n",
              "         0.49814457, -0.46872556, -0.45534098, -0.50917274,  0.46125072,\n",
              "         0.4915058 ,  0.4351491 ,  0.46490753, -0.5146493 ,  0.50757647],\n",
              "       dtype=float32),\n",
              " 'china': array([ 0.5138913 ,  0.45106477, -0.44439274, -0.44135028, -0.5064181 ,\n",
              "         0.43399245,  0.50083613, -0.5086906 , -0.4456797 , -0.4993545 ,\n",
              "        -0.45348158,  0.50811857,  0.47548613,  0.5163343 , -0.5306723 ,\n",
              "        -0.46115968,  0.45197603,  0.45546034, -0.4264007 , -0.4564615 ,\n",
              "        -0.45816988,  0.47427925,  0.45298916,  0.4287634 , -0.47450024,\n",
              "        -0.4522671 ,  0.4789754 ,  0.46581367,  0.4710644 ,  0.46360257,\n",
              "        -0.492331  ,  0.43049973, -0.52427286, -0.5171758 , -0.46494555,\n",
              "        -0.48407334,  0.5119997 , -0.45693183,  0.49495274, -0.4720856 ,\n",
              "         0.4301261 , -0.45283008, -0.4951227 , -0.45238987,  0.5006666 ,\n",
              "         0.43547657,  0.4915378 ,  0.49326783, -0.42922807,  0.44948673],\n",
              "       dtype=float32),\n",
              " 'believe': array([ 0.46346316,  0.45964867, -0.51927644, -0.4612662 , -0.50463855,\n",
              "         0.5344096 ,  0.4985305 , -0.53206336, -0.5254282 , -0.44941074,\n",
              "        -0.49607244,  0.49092293,  0.4681785 ,  0.48645607, -0.51354915,\n",
              "        -0.46229574,  0.49451745,  0.53398645, -0.47726896, -0.50449   ,\n",
              "        -0.4480504 ,  0.49086142,  0.4714788 ,  0.480845  , -0.5156285 ,\n",
              "        -0.5029992 ,  0.46191233,  0.50767934,  0.52071404,  0.46225506,\n",
              "        -0.5058906 ,  0.5045035 , -0.49639544, -0.4726112 , -0.49162507,\n",
              "        -0.51958895,  0.4656112 , -0.5111388 ,  0.44548598, -0.53380585,\n",
              "         0.49186972, -0.53522325, -0.511335  , -0.51047534,  0.47758946,\n",
              "         0.51788354,  0.47727528,  0.5024959 , -0.49674198,  0.5155777 ],\n",
              "       dtype=float32),\n",
              " 'estimated': array([ 0.538719  ,  0.5047369 , -0.48057607, -0.46462637, -0.4540545 ,\n",
              "         0.46312743,  0.45286554, -0.44974527, -0.50764024, -0.45586213,\n",
              "        -0.50828785,  0.46291485,  0.5262551 ,  0.50268894, -0.5158652 ,\n",
              "        -0.52090496,  0.5113162 ,  0.5106623 , -0.4757918 , -0.4737033 ,\n",
              "        -0.45980382,  0.45520335,  0.5101124 ,  0.4705876 , -0.43736058,\n",
              "        -0.47926214,  0.5192716 ,  0.4851351 ,  0.4955728 ,  0.5125339 ,\n",
              "        -0.45444763,  0.46244383, -0.5121241 , -0.5028598 , -0.51757395,\n",
              "        -0.4530608 ,  0.453287  , -0.48025805,  0.5214326 , -0.46013984,\n",
              "         0.47638303, -0.47161454, -0.5265816 , -0.47885928,  0.5114099 ,\n",
              "         0.44144112,  0.44669628,  0.51961946, -0.53089714,  0.5132425 ],\n",
              "       dtype=float32),\n",
              " 'fair': array([ 0.52667356,  0.51065326, -0.4649622 , -0.48105556, -0.4573127 ,\n",
              "         0.5016139 ,  0.47417885, -0.50789213, -0.50446016, -0.46008083,\n",
              "        -0.48832315,  0.4393168 ,  0.5047997 ,  0.4669541 , -0.53127605,\n",
              "        -0.44378042,  0.47742674,  0.469679  , -0.5142749 , -0.44609338,\n",
              "        -0.4861804 ,  0.5203775 ,  0.5241237 ,  0.43951812, -0.49444646,\n",
              "        -0.48457518,  0.44925106,  0.5046029 ,  0.48498827,  0.44768843,\n",
              "        -0.4464042 ,  0.46175066, -0.5325753 , -0.47599253, -0.45845154,\n",
              "        -0.49555683,  0.4926778 , -0.47534558,  0.45072252, -0.48095673,\n",
              "         0.48212576, -0.5002861 , -0.49475124, -0.4937447 ,  0.5167589 ,\n",
              "         0.45361674,  0.44798186,  0.43972927, -0.4809684 ,  0.47715327],\n",
              "       dtype=float32),\n",
              " 'appropriate': array([ 0.5312297 ,  0.4194361 , -0.4854421 , -0.44071466, -0.48254424,\n",
              "         0.4769477 ,  0.49512613, -0.46959528, -0.45347935, -0.4312501 ,\n",
              "        -0.4329471 ,  0.4647528 ,  0.494151  ,  0.51456976, -0.48096514,\n",
              "        -0.4644786 ,  0.4689183 ,  0.44712448, -0.49803665, -0.49977925,\n",
              "        -0.4302426 ,  0.50188226,  0.48782906,  0.4483578 , -0.42859715,\n",
              "        -0.4633228 ,  0.46192077,  0.47091565,  0.42300564,  0.4368829 ,\n",
              "        -0.50560904,  0.4374948 , -0.49347958, -0.49391165, -0.41956472,\n",
              "        -0.50437635,  0.51040643, -0.4779435 ,  0.4319604 , -0.46396604,\n",
              "         0.48769096, -0.45814896, -0.41672847, -0.45077103,  0.46982065,\n",
              "         0.43409115,  0.47846878,  0.45454368, -0.46547374,  0.48845428],\n",
              "       dtype=float32),\n",
              " 'offer': array([ 0.45869333,  0.43628308, -0.44833058, -0.4322295 , -0.48219827,\n",
              "         0.4963834 ,  0.46055925, -0.5183661 , -0.4518885 , -0.48144734,\n",
              "        -0.4433622 ,  0.4766841 ,  0.45239294,  0.4422179 , -0.51446134,\n",
              "        -0.4833474 ,  0.4427076 ,  0.5197306 , -0.48711324, -0.46960837,\n",
              "        -0.42750043,  0.4343602 ,  0.48114017,  0.47770366, -0.45978338,\n",
              "        -0.49604535,  0.48932427,  0.4581012 ,  0.4308407 ,  0.46652007,\n",
              "        -0.45974597,  0.49857515, -0.5002953 , -0.4428012 , -0.4273234 ,\n",
              "        -0.4474128 ,  0.46125263, -0.46590868,  0.44499883, -0.4882275 ,\n",
              "         0.43589434, -0.5224756 , -0.49165112, -0.43632844,  0.466716  ,\n",
              "         0.5192276 ,  0.4423147 ,  0.42954203, -0.45342737,  0.48773095],\n",
              "       dtype=float32),\n",
              " 'disclosure': array([ 0.46459976,  0.4513509 , -0.49509647, -0.4351933 , -0.48840338,\n",
              "         0.4352677 ,  0.43579698, -0.44205406, -0.46858364, -0.4283958 ,\n",
              "        -0.46300635,  0.50983095,  0.44383383,  0.42098472, -0.54525185,\n",
              "        -0.44583535,  0.4409224 ,  0.48668286, -0.41558862, -0.46778175,\n",
              "        -0.41440776,  0.4796394 ,  0.46448997,  0.4123902 , -0.4275268 ,\n",
              "        -0.49128443,  0.4860956 ,  0.5208402 ,  0.49072406,  0.4526377 ,\n",
              "        -0.4342334 ,  0.46639475, -0.4623216 , -0.486467  , -0.47446883,\n",
              "        -0.45107603,  0.4468413 , -0.4516731 ,  0.48604006, -0.48301888,\n",
              "         0.47044078, -0.48532328, -0.45998755, -0.42039284,  0.43295616,\n",
              "         0.49642336,  0.4303116 ,  0.46933344, -0.49266958,  0.49184215],\n",
              "       dtype=float32),\n",
              " 'category': array([ 0.46779746,  0.4933519 , -0.49695724, -0.47313404, -0.5264542 ,\n",
              "         0.48997977,  0.5156792 , -0.49532077, -0.49105   , -0.51040286,\n",
              "        -0.44872677,  0.5182796 ,  0.446565  ,  0.52233744, -0.5221309 ,\n",
              "        -0.51498026,  0.5276371 ,  0.45777917, -0.45677048, -0.4470694 ,\n",
              "        -0.4350667 ,  0.5037559 ,  0.49795145,  0.45059687, -0.47131842,\n",
              "        -0.45443046,  0.454076  ,  0.5412847 ,  0.45541632,  0.43368208,\n",
              "        -0.4985559 ,  0.4995801 , -0.4516402 , -0.5166651 , -0.50507355,\n",
              "        -0.44182783,  0.48425037, -0.47997656,  0.49826282, -0.46827567,\n",
              "         0.43906587, -0.5121597 , -0.509428  , -0.47070408,  0.43611833,\n",
              "         0.519667  ,  0.43429247,  0.4854311 , -0.50569046,  0.45928946],\n",
              "       dtype=float32),\n",
              " 'programs': array([ 0.51794225,  0.44460395, -0.5155204 , -0.49679056, -0.49599272,\n",
              "         0.5107552 ,  0.49096692, -0.4900362 , -0.4888655 , -0.51770073,\n",
              "        -0.5279024 ,  0.47094414,  0.51799875,  0.5187218 , -0.5290102 ,\n",
              "        -0.50683814,  0.47398478,  0.4544775 , -0.463539  , -0.44601727,\n",
              "        -0.44584644,  0.4425599 ,  0.5109792 ,  0.44624352, -0.46205527,\n",
              "        -0.44116017,  0.4512563 ,  0.5241989 ,  0.4593681 ,  0.48302457,\n",
              "        -0.5204594 ,  0.4657    , -0.51668406, -0.47969687, -0.5164321 ,\n",
              "        -0.5035368 ,  0.5028186 , -0.46984342,  0.47486603, -0.47554392,\n",
              "         0.4708035 , -0.452224  , -0.47842264, -0.4675411 ,  0.52937675,\n",
              "         0.4907479 ,  0.47162074,  0.51222944, -0.52064013,  0.5169024 ],\n",
              "       dtype=float32),\n",
              " 'development': array([ 0.51885605,  0.46959388, -0.46708107, -0.4407757 , -0.5285659 ,\n",
              "         0.5015186 ,  0.5066328 , -0.48313087, -0.48325136, -0.48729712,\n",
              "        -0.4233712 ,  0.51578206,  0.48762825,  0.4405372 , -0.5438635 ,\n",
              "        -0.51879245,  0.43551707,  0.49064648, -0.46091005, -0.50436854,\n",
              "        -0.515748  ,  0.4937072 ,  0.43711677,  0.45424196, -0.42547256,\n",
              "        -0.48427868,  0.4586394 ,  0.4982641 ,  0.49943167,  0.45062348,\n",
              "        -0.46608603,  0.4951923 , -0.5049711 , -0.4842271 , -0.45359161,\n",
              "        -0.49148166,  0.47007218, -0.44258377,  0.47433853, -0.5043308 ,\n",
              "         0.44723943, -0.5117818 , -0.48421443, -0.51795197,  0.44878575,\n",
              "         0.4876455 ,  0.44128937,  0.51285326, -0.47981364,  0.45440838],\n",
              "       dtype=float32),\n",
              " 'equal': array([ 0.50138825,  0.46545583, -0.4452794 , -0.51031595, -0.5359151 ,\n",
              "         0.45511955,  0.45188633, -0.49955395, -0.4903506 , -0.46154588,\n",
              "        -0.4882627 ,  0.5020625 ,  0.5146735 ,  0.44299325, -0.48540077,\n",
              "        -0.5036143 ,  0.50456387,  0.4890955 , -0.43692046, -0.4299024 ,\n",
              "        -0.43913597,  0.4776384 ,  0.47385788,  0.52187395, -0.50733626,\n",
              "        -0.45591295,  0.49093008,  0.49771973,  0.5037587 ,  0.46944472,\n",
              "        -0.50071967,  0.43954784, -0.5452785 , -0.4857636 , -0.46469808,\n",
              "        -0.47078475,  0.4756385 , -0.47681695,  0.49067807, -0.4479977 ,\n",
              "         0.4556362 , -0.52539325, -0.5141297 , -0.4813747 ,  0.44352835,\n",
              "         0.49311405,  0.47186524,  0.50520575, -0.5218571 ,  0.45704   ],\n",
              "       dtype=float32),\n",
              " 'waddell': array([ 0.47963357,  0.44256288, -0.46298152, -0.45624655, -0.51628363,\n",
              "         0.4601736 ,  0.4637237 , -0.5252761 , -0.5133913 , -0.5069939 ,\n",
              "        -0.47877675,  0.50230855,  0.49050567,  0.52911776, -0.4784148 ,\n",
              "        -0.4786386 ,  0.51341736,  0.4843036 , -0.48549864, -0.4451076 ,\n",
              "        -0.48179507,  0.42798996,  0.4787337 ,  0.48898724, -0.5145806 ,\n",
              "        -0.4407012 ,  0.47341895,  0.52158225,  0.4569083 ,  0.50490654,\n",
              "        -0.4568312 ,  0.4666368 , -0.45856032, -0.5281862 , -0.43901443,\n",
              "        -0.4704193 ,  0.44935152, -0.45162052,  0.4828366 , -0.50535935,\n",
              "         0.48177755, -0.4423793 , -0.48364764, -0.4306679 ,  0.48155248,\n",
              "         0.44866344,  0.43559277,  0.47113854, -0.4437439 ,  0.44828102],\n",
              "       dtype=float32),\n",
              " 'plus': array([ 0.4746707 ,  0.45224476, -0.4628835 , -0.46554786, -0.53194284,\n",
              "         0.5313127 ,  0.46857548, -0.50822264, -0.4724603 , -0.49831372,\n",
              "        -0.48729554,  0.49731213,  0.51461476,  0.46103117, -0.49314523,\n",
              "        -0.46530664,  0.5004453 ,  0.53171515, -0.43788886, -0.46644217,\n",
              "        -0.48029602,  0.44268835,  0.47380966,  0.47903895, -0.48335934,\n",
              "        -0.4460232 ,  0.44017524,  0.55207956,  0.52996343,  0.50960577,\n",
              "        -0.47246897,  0.5251164 , -0.533278  , -0.4627829 , -0.4788081 ,\n",
              "        -0.47843337,  0.48266053, -0.46305415,  0.47653016, -0.5316976 ,\n",
              "         0.52784353, -0.48554656, -0.454734  , -0.49031585,  0.52055717,\n",
              "         0.46325687,  0.46392155,  0.53450006, -0.44720367,  0.50737095],\n",
              "       dtype=float32),\n",
              " 'serves': array([ 0.5185332 ,  0.40354347, -0.5103317 , -0.45248955, -0.5204816 ,\n",
              "         0.49957743,  0.4743048 , -0.48355606, -0.43120164, -0.49887997,\n",
              "        -0.49465615,  0.47614914,  0.47827786,  0.46360558, -0.59142274,\n",
              "        -0.45202646,  0.48744136,  0.4482881 , -0.43567604, -0.4104817 ,\n",
              "        -0.45222846,  0.47065234,  0.47916597,  0.47976092, -0.474051  ,\n",
              "        -0.42537218,  0.4541722 ,  0.47946262,  0.44613075,  0.42851344,\n",
              "        -0.51419914,  0.506221  , -0.47107193, -0.5196593 , -0.48100185,\n",
              "        -0.51984173,  0.52562827, -0.4700266 ,  0.47704247, -0.43295425,\n",
              "         0.43753278, -0.506646  , -0.4191972 , -0.4418044 ,  0.4524532 ,\n",
              "         0.4736314 ,  0.44295433,  0.5048139 , -0.43495256,  0.4674667 ],\n",
              "       dtype=float32),\n",
              " 'correlate': array([ 0.5316061 ,  0.42691085, -0.51153994, -0.46346408, -0.45233667,\n",
              "         0.46073577,  0.5039588 , -0.46549448, -0.5138061 , -0.47397017,\n",
              "        -0.491221  ,  0.4894272 ,  0.4836294 ,  0.45055673, -0.50729203,\n",
              "        -0.4966048 ,  0.5186361 ,  0.5081848 , -0.43017754, -0.5051264 ,\n",
              "        -0.50751853,  0.49380642,  0.5032387 ,  0.4498995 , -0.47511175,\n",
              "        -0.43450493,  0.4802466 ,  0.5169772 ,  0.49341637,  0.4989189 ,\n",
              "        -0.48601443,  0.4398854 , -0.51979697, -0.48140207, -0.4505241 ,\n",
              "        -0.4752136 ,  0.4630251 , -0.46603847,  0.4779479 , -0.47401953,\n",
              "         0.5006779 , -0.53010094, -0.49179748, -0.48068696,  0.43420175,\n",
              "         0.5027526 ,  0.47010106,  0.5114526 , -0.48767757,  0.4347788 ],\n",
              "       dtype=float32),\n",
              " 'regardless': array([ 0.46242696,  0.41127226, -0.4593929 , -0.46885827, -0.44106543,\n",
              "         0.5112945 ,  0.43663356, -0.42974275, -0.48346484, -0.46843266,\n",
              "        -0.4620033 ,  0.43969244,  0.48496243,  0.44704917, -0.54690677,\n",
              "        -0.47137368,  0.44789362,  0.43349954, -0.43774587, -0.47595936,\n",
              "        -0.42655197,  0.4157701 ,  0.4400232 ,  0.4594816 , -0.47539398,\n",
              "        -0.46400997,  0.45808077,  0.49758965,  0.48694882,  0.46774307,\n",
              "        -0.45787367,  0.5006355 , -0.49150717, -0.45560855, -0.4875507 ,\n",
              "        -0.49160314,  0.45898628, -0.5012887 ,  0.44074398, -0.50072145,\n",
              "         0.50516665, -0.4502463 , -0.41621834, -0.48025617,  0.50764394,\n",
              "         0.45296997,  0.49942195,  0.4561542 , -0.4899469 ,  0.43136597],\n",
              "       dtype=float32),\n",
              " 'extraordinary': array([ 0.5311644 ,  0.45706356, -0.4642362 , -0.4817537 , -0.5102744 ,\n",
              "         0.4784644 ,  0.48731136, -0.50752103, -0.47022223, -0.4177204 ,\n",
              "        -0.49713415,  0.44672728,  0.4700534 ,  0.4769466 , -0.49780023,\n",
              "        -0.5050286 ,  0.49445897,  0.4636941 , -0.5043108 , -0.48902744,\n",
              "        -0.5033782 ,  0.4428258 ,  0.50714666,  0.4499108 , -0.4645933 ,\n",
              "        -0.4944238 ,  0.492788  ,  0.5186914 ,  0.44875672,  0.47833318,\n",
              "        -0.44244164,  0.5014293 , -0.45592648, -0.51414317, -0.43564576,\n",
              "        -0.44068485,  0.4399481 , -0.46435797,  0.49359939, -0.4670099 ,\n",
              "         0.47269937, -0.49212426, -0.43502074, -0.46302015,  0.4289508 ,\n",
              "         0.4336966 ,  0.4490907 ,  0.48138368, -0.5056474 ,  0.43413273],\n",
              "       dtype=float32),\n",
              " 'year-by-year': array([ 0.5111501 ,  0.49548018, -0.50211304, -0.5152272 , -0.52268916,\n",
              "         0.47868156,  0.43408424, -0.44808108, -0.4705956 , -0.49172288,\n",
              "        -0.46571925,  0.4763591 ,  0.4861688 ,  0.43987185, -0.48609003,\n",
              "        -0.4602535 ,  0.52037126,  0.4574361 , -0.44450408, -0.4986494 ,\n",
              "        -0.48930383,  0.50956196,  0.48676527,  0.5052363 , -0.48156276,\n",
              "        -0.46583435,  0.43691596,  0.52108353,  0.49609   ,  0.4369717 ,\n",
              "        -0.48393875,  0.4667121 , -0.46218058, -0.4675779 , -0.45241252,\n",
              "        -0.4809939 ,  0.48303428, -0.46163327,  0.43716744, -0.47754732,\n",
              "         0.5233313 , -0.4916126 , -0.44470283, -0.45980012,  0.51498944,\n",
              "         0.5034222 ,  0.4350398 ,  0.44121128, -0.4587136 ,  0.5154535 ],\n",
              "       dtype=float32),\n",
              " 'front-end': array([ 0.45291528,  0.5020879 , -0.46032673, -0.47972962, -0.45904204,\n",
              "         0.45163926,  0.45496473, -0.48677865, -0.50616735, -0.45101726,\n",
              "        -0.47835752,  0.51764375,  0.49179897,  0.5151106 , -0.5103066 ,\n",
              "        -0.46571425,  0.48410884,  0.43340394, -0.43752566, -0.4878692 ,\n",
              "        -0.48370853,  0.43090904,  0.4377101 ,  0.50175345, -0.4385889 ,\n",
              "        -0.5042717 ,  0.465757  ,  0.48479593,  0.45234078,  0.45133716,\n",
              "        -0.5209319 ,  0.48033267, -0.47307122, -0.4740607 , -0.50112784,\n",
              "        -0.4606606 ,  0.47369495, -0.5180334 ,  0.46021846, -0.5107865 ,\n",
              "         0.49747044, -0.51972073, -0.49033648, -0.480178  ,  0.5034688 ,\n",
              "         0.5025491 ,  0.43060356,  0.43640697, -0.46979985,  0.4430815 ],\n",
              "       dtype=float32),\n",
              " 'waived': array([ 0.47127873,  0.4381197 , -0.5402093 , -0.4472456 , -0.47851846,\n",
              "         0.51509774,  0.5133572 , -0.5009449 , -0.5201451 , -0.4360259 ,\n",
              "        -0.46224955,  0.49788618,  0.48342413,  0.4939865 , -0.5634258 ,\n",
              "        -0.45723066,  0.53080916,  0.49348003, -0.49209708, -0.46561056,\n",
              "        -0.46455282,  0.4617017 ,  0.5333869 ,  0.48805642, -0.49119073,\n",
              "        -0.49486917,  0.5013646 ,  0.54844636,  0.445105  ,  0.5213655 ,\n",
              "        -0.5182858 ,  0.4699818 , -0.48736677, -0.50707686, -0.44145155,\n",
              "        -0.5164984 ,  0.54130304, -0.4971359 ,  0.48028097, -0.46611434,\n",
              "         0.47043633, -0.5265653 , -0.45400697, -0.49045277,  0.47526702,\n",
              "         0.45132092,  0.47640553,  0.5113308 , -0.50178915,  0.51270914],\n",
              "       dtype=float32),\n",
              " 'relies': array([ 0.5328162 ,  0.5194984 , -0.5259422 , -0.45134363, -0.50872517,\n",
              "         0.5312145 ,  0.516016  , -0.5303052 , -0.5128117 , -0.485034  ,\n",
              "        -0.46498123,  0.48305008,  0.49815074,  0.52412003, -0.4820385 ,\n",
              "        -0.5212622 ,  0.53265494,  0.47235012, -0.5155536 , -0.46075535,\n",
              "        -0.49594286,  0.46753702,  0.45922235,  0.5046596 , -0.443465  ,\n",
              "        -0.45028052,  0.5172479 ,  0.5138493 ,  0.43706545,  0.53032535,\n",
              "        -0.509409  ,  0.46466514, -0.45644754, -0.5167101 , -0.466188  ,\n",
              "        -0.4899145 ,  0.467829  , -0.47628123,  0.51005983, -0.53812665,\n",
              "         0.5037822 , -0.49505192, -0.49941862, -0.47958833,  0.4895451 ,\n",
              "         0.51281905,  0.48692685,  0.45414588, -0.43960455,  0.5143074 ],\n",
              "       dtype=float32),\n",
              " 'instrumentalities': array([ 0.4709927 ,  0.493397  , -0.5073165 , -0.4805934 , -0.4564354 ,\n",
              "         0.45075676,  0.4939111 , -0.46381775, -0.49349138, -0.49915278,\n",
              "        -0.44962472,  0.4845989 ,  0.45262137,  0.5080931 , -0.5325811 ,\n",
              "        -0.45173928,  0.48404646,  0.51285976, -0.507797  , -0.43434203,\n",
              "        -0.4648496 ,  0.48025638,  0.4789865 ,  0.5215334 , -0.49556866,\n",
              "        -0.44076902,  0.44411254,  0.55342555,  0.5132938 ,  0.5107089 ,\n",
              "        -0.4843067 ,  0.49435997, -0.4601914 , -0.45633176, -0.48943782,\n",
              "        -0.48349172,  0.45533317, -0.4836221 ,  0.5049938 , -0.52853274,\n",
              "         0.5212021 , -0.45778713, -0.45119357, -0.5252056 ,  0.45218754,\n",
              "         0.49094465,  0.48611492,  0.5384444 , -0.43949106,  0.504001  ],\n",
              "       dtype=float32),\n",
              " 'small-cap': array([ 0.53233886,  0.4921558 , -0.46635032, -0.5079737 , -0.50547403,\n",
              "         0.4732106 ,  0.44743204, -0.52153254, -0.45939496, -0.4885875 ,\n",
              "        -0.45309064,  0.51801556,  0.5034808 ,  0.53264457, -0.51259494,\n",
              "        -0.50969315,  0.50189203,  0.46297702, -0.47880602, -0.45881823,\n",
              "        -0.47619945,  0.46621883,  0.46512645,  0.4535332 , -0.48227823,\n",
              "        -0.4638908 ,  0.50822896,  0.5146256 ,  0.4330839 ,  0.43512976,\n",
              "        -0.536874  ,  0.47747764, -0.5550848 , -0.486381  , -0.46322536,\n",
              "        -0.5440725 ,  0.5179299 , -0.45719233,  0.48341936, -0.47177982,\n",
              "         0.4860089 , -0.5051018 , -0.4918271 , -0.4927397 ,  0.5260536 ,\n",
              "         0.4821519 ,  0.44520643,  0.48571593, -0.5000547 ,  0.51495093],\n",
              "       dtype=float32),\n",
              " 'chance': array([ 0.46568185,  0.4412864 , -0.52054626, -0.52590114, -0.5115359 ,\n",
              "         0.5270305 ,  0.495246  , -0.50521743, -0.48422304, -0.5091715 ,\n",
              "        -0.47758082,  0.48343757,  0.5119641 ,  0.4877044 , -0.5066505 ,\n",
              "        -0.44813737,  0.52324855,  0.5009555 , -0.46602753, -0.5139817 ,\n",
              "        -0.52121323,  0.43985584,  0.47808182,  0.48824605, -0.45892456,\n",
              "        -0.4764953 ,  0.49862805,  0.5136124 ,  0.5053857 ,  0.43168357,\n",
              "        -0.444931  ,  0.5231631 , -0.47306317, -0.50569654, -0.4299595 ,\n",
              "        -0.45486858,  0.45487   , -0.4370726 ,  0.48928186, -0.47146386,\n",
              "         0.44561535, -0.493924  , -0.5085657 , -0.50151473,  0.4622119 ,\n",
              "         0.4995807 ,  0.48522928,  0.45843062, -0.5102872 ,  0.47632372],\n",
              "       dtype=float32),\n",
              " 'writing': array([ 0.4602552 ,  0.45986608, -0.46944875, -0.4587649 , -0.45822218,\n",
              "         0.4794005 ,  0.48091298, -0.5239379 , -0.49730605, -0.46439946,\n",
              "        -0.4601263 ,  0.45838365,  0.48345575,  0.45742372, -0.55489546,\n",
              "        -0.46402946,  0.49164522,  0.4518132 , -0.5059675 , -0.4583992 ,\n",
              "        -0.44595551,  0.47932157,  0.52573943,  0.51022905, -0.5066395 ,\n",
              "        -0.50471413,  0.4621311 ,  0.51581866,  0.44530204,  0.45639026,\n",
              "        -0.5230203 ,  0.44199955, -0.53864145, -0.4821864 , -0.42956284,\n",
              "        -0.4464255 ,  0.5403202 , -0.4834901 ,  0.4575305 , -0.51245594,\n",
              "         0.486448  , -0.50294214, -0.4869947 , -0.50536484,  0.5152279 ,\n",
              "         0.51777494,  0.52236235,  0.47181317, -0.5245287 ,  0.47245532],\n",
              "       dtype=float32),\n",
              " 'commenced': array([ 0.5505904 ,  0.46208012, -0.4712377 , -0.44666967, -0.5070996 ,\n",
              "         0.46869335,  0.44197127, -0.4715325 , -0.4484409 , -0.52531564,\n",
              "        -0.50681007,  0.4696849 ,  0.5047576 ,  0.4643156 , -0.51798123,\n",
              "        -0.52682453,  0.48007447,  0.50361395, -0.46914023, -0.4586884 ,\n",
              "        -0.52917504,  0.50184906,  0.500926  ,  0.49576724, -0.49540967,\n",
              "        -0.5264978 ,  0.48144656,  0.5152572 ,  0.47049394,  0.51591444,\n",
              "        -0.46458927,  0.47395214, -0.50782233, -0.48585144, -0.47818708,\n",
              "        -0.5051281 ,  0.48706296, -0.46152836,  0.4635224 , -0.477342  ,\n",
              "         0.51574653, -0.5578927 , -0.53812265, -0.4633313 ,  0.51393926,\n",
              "         0.4887044 ,  0.48346612,  0.50900966, -0.47404248,  0.53218806],\n",
              "       dtype=float32),\n",
              " 'respective': array([ 0.54185957,  0.49655783, -0.4563276 , -0.46049374, -0.45813844,\n",
              "         0.5451668 ,  0.47587416, -0.47634453, -0.49318874, -0.51118416,\n",
              "        -0.4821211 ,  0.4882334 ,  0.46647382,  0.54266334, -0.5199901 ,\n",
              "        -0.5122147 ,  0.52316266,  0.45858914, -0.4462182 , -0.513511  ,\n",
              "        -0.46790558,  0.4809803 ,  0.5325651 ,  0.5152146 , -0.47810784,\n",
              "        -0.4864926 ,  0.48408332,  0.53936976,  0.53810817,  0.5133891 ,\n",
              "        -0.53571594,  0.49413663, -0.54098064, -0.4831644 , -0.44645995,\n",
              "        -0.52285403,  0.50615287, -0.5079442 ,  0.5146707 , -0.515877  ,\n",
              "         0.54135096, -0.505656  , -0.49112868, -0.5004552 ,  0.54299796,\n",
              "         0.53938353,  0.46101418,  0.47425264, -0.45345533,  0.4673344 ],\n",
              "       dtype=float32),\n",
              " 'large-cap': array([ 0.4395555 ,  0.43799755, -0.48199987, -0.48184386, -0.43623596,\n",
              "         0.4601263 ,  0.46045208, -0.46682724, -0.4171893 , -0.49716318,\n",
              "        -0.44237977,  0.50550354,  0.46000594,  0.48714396, -0.5232842 ,\n",
              "        -0.49911666,  0.47574425,  0.47296044, -0.50394   , -0.4839563 ,\n",
              "        -0.5027881 ,  0.4458796 ,  0.47963685,  0.43215913, -0.42962602,\n",
              "        -0.43685237,  0.4615962 ,  0.52103114,  0.452112  ,  0.46314543,\n",
              "        -0.45010957,  0.48066345, -0.47828606, -0.48680982, -0.43512112,\n",
              "        -0.45499918,  0.4486678 , -0.4573943 ,  0.48688748, -0.44282764,\n",
              "         0.46701595, -0.48815665, -0.4403279 , -0.48885053,  0.42574018,\n",
              "         0.4763296 ,  0.47181502,  0.48191825, -0.46349964,  0.44022712],\n",
              "       dtype=float32),\n",
              " 'decreased': array([ 0.51296717,  0.45555222, -0.50410223, -0.42132732, -0.47611135,\n",
              "         0.49580762,  0.43380743, -0.4903391 , -0.4215445 , -0.44085783,\n",
              "        -0.42690143,  0.4797911 ,  0.4368637 ,  0.49062905, -0.5315798 ,\n",
              "        -0.48238844,  0.45036972,  0.44563457, -0.50072384, -0.4438204 ,\n",
              "        -0.4327664 ,  0.4979388 ,  0.48233545,  0.49564654, -0.49853104,\n",
              "        -0.45239252,  0.48072097,  0.4477713 ,  0.4798349 ,  0.42700845,\n",
              "        -0.44612426,  0.47466478, -0.4422391 , -0.45197496, -0.49356365,\n",
              "        -0.42649937,  0.47445387, -0.46871924,  0.49257755, -0.4710931 ,\n",
              "         0.50060475, -0.48137784, -0.47248432, -0.46768326,  0.4572755 ,\n",
              "         0.46368477,  0.4543163 ,  0.4185607 , -0.4667742 ,  0.44251218],\n",
              "       dtype=float32),\n",
              " 'unwilling': array([ 0.45970353,  0.4655013 , -0.49437994, -0.48950642, -0.4449068 ,\n",
              "         0.46434337,  0.48140082, -0.46709242, -0.42371178, -0.44762525,\n",
              "        -0.44729364,  0.44789603,  0.48495775,  0.47133407, -0.5122479 ,\n",
              "        -0.43858883,  0.48442587,  0.4566752 , -0.48605216, -0.45154816,\n",
              "        -0.5087347 ,  0.43285263,  0.46360806,  0.4934172 , -0.44949117,\n",
              "        -0.45422378,  0.47699404,  0.45461437,  0.4211845 ,  0.44530067,\n",
              "        -0.4462043 ,  0.47320828, -0.48737264, -0.43842223, -0.4618269 ,\n",
              "        -0.4566543 ,  0.47698638, -0.4321146 ,  0.47825888, -0.4415559 ,\n",
              "         0.42855835, -0.4463631 , -0.41321796, -0.46608728,  0.44705465,\n",
              "         0.4350515 ,  0.43918705,  0.5067273 , -0.46226296,  0.48297882],\n",
              "       dtype=float32),\n",
              " 'laws': array([ 0.47035465,  0.40455866, -0.44353575, -0.4833903 , -0.47017366,\n",
              "         0.44853902,  0.43587536, -0.46533173, -0.49336392, -0.45155725,\n",
              "        -0.4676983 ,  0.44619715,  0.46077877,  0.45791686, -0.5228522 ,\n",
              "        -0.47394344,  0.47291002,  0.47057414, -0.4753946 , -0.46490142,\n",
              "        -0.4264858 ,  0.44130993,  0.44453335,  0.47609898, -0.47280002,\n",
              "        -0.44504294,  0.40526715,  0.4699388 ,  0.49028248,  0.46655193,\n",
              "        -0.5011091 ,  0.403638  , -0.48813212, -0.42263103, -0.44416228,\n",
              "        -0.4905275 ,  0.47988355, -0.4952279 ,  0.42026156, -0.44986102,\n",
              "         0.42241278, -0.50124997, -0.4062199 , -0.41343117,  0.48097318,\n",
              "         0.47123063,  0.42128783,  0.42909065, -0.40790215,  0.4747051 ],\n",
              "       dtype=float32),\n",
              " 'adjustments': array([ 0.51577246,  0.44942048, -0.47718325, -0.47198677, -0.447561  ,\n",
              "         0.43760517,  0.48153844, -0.50614244, -0.4162486 , -0.47319403,\n",
              "        -0.44081166,  0.45046538,  0.471583  ,  0.4687311 , -0.4852956 ,\n",
              "        -0.4194025 ,  0.45007727,  0.4271347 , -0.46710685, -0.42948806,\n",
              "        -0.46142069,  0.49145842,  0.48887596,  0.4327777 , -0.48287612,\n",
              "        -0.48304722,  0.4918828 ,  0.45475367,  0.47635955,  0.4336451 ,\n",
              "        -0.43006188,  0.4654462 , -0.449366  , -0.49210098, -0.47450396,\n",
              "        -0.43533844,  0.47428057, -0.4800688 ,  0.4338729 , -0.4673786 ,\n",
              "         0.42893898, -0.45008174, -0.4487102 , -0.4573586 ,  0.42463046,\n",
              "         0.4295226 ,  0.4649118 ,  0.43583316, -0.4439465 ,  0.42121342],\n",
              "       dtype=float32),\n",
              " 'applies': array([ 0.5221158 ,  0.46130967, -0.44996497, -0.47664365, -0.4707607 ,\n",
              "         0.4332839 ,  0.45023358, -0.46363768, -0.4600917 , -0.41072956,\n",
              "        -0.48445326,  0.4346077 ,  0.486617  ,  0.4408641 , -0.5071527 ,\n",
              "        -0.45495862,  0.48420352,  0.4929488 , -0.4214193 , -0.4300051 ,\n",
              "        -0.4832506 ,  0.4717535 ,  0.4979944 ,  0.42583075, -0.49452436,\n",
              "        -0.45490003,  0.46977016,  0.52613914,  0.4972237 ,  0.4535056 ,\n",
              "        -0.4626229 ,  0.48598528, -0.45133728, -0.49727488, -0.43448722,\n",
              "        -0.4624698 ,  0.42617226, -0.50097066,  0.49470207, -0.4268334 ,\n",
              "         0.5045878 , -0.47538012, -0.48096335, -0.44416052,  0.44146442,\n",
              "         0.44345534,  0.47066873,  0.50579554, -0.49082735,  0.4399436 ],\n",
              "       dtype=float32),\n",
              " 'hypothetical': array([ 0.525766  ,  0.44322905, -0.43508396, -0.4766199 , -0.49483272,\n",
              "         0.50105464,  0.47713462, -0.46108145, -0.49139282, -0.43345568,\n",
              "        -0.50774723,  0.46513823,  0.4615417 ,  0.44448403, -0.48076764,\n",
              "        -0.45285872,  0.44407636,  0.4787424 , -0.44006222, -0.48467293,\n",
              "        -0.4504464 ,  0.47017482,  0.47464547,  0.44465378, -0.51213205,\n",
              "        -0.43889335,  0.4750328 ,  0.5077441 ,  0.46453357,  0.5109252 ,\n",
              "        -0.464507  ,  0.46737438, -0.52094495, -0.44322747, -0.49638826,\n",
              "        -0.45071286,  0.5184924 , -0.473987  ,  0.45325765, -0.4638399 ,\n",
              "         0.45017713, -0.5117489 , -0.48748463, -0.44951895,  0.5155508 ,\n",
              "         0.4472832 ,  0.5165073 ,  0.47349364, -0.43235466,  0.48377553],\n",
              "       dtype=float32),\n",
              " 'employee': array([ 0.4587873 ,  0.42467064, -0.498399  , -0.44434416, -0.45429078,\n",
              "         0.51695377,  0.48236194, -0.4577729 , -0.45126382, -0.46506327,\n",
              "        -0.48659846,  0.44852152,  0.45024216,  0.43407282, -0.5081209 ,\n",
              "        -0.4761592 ,  0.5026834 ,  0.4375793 , -0.48959634, -0.45118085,\n",
              "        -0.48903695,  0.48235613,  0.42695513,  0.4761213 , -0.42643437,\n",
              "        -0.49004856,  0.49015972,  0.51245904,  0.4480166 ,  0.48780167,\n",
              "        -0.4632866 ,  0.48806524, -0.49709362, -0.46869808, -0.43721828,\n",
              "        -0.4256997 ,  0.5129543 , -0.42808056,  0.4731249 , -0.4642707 ,\n",
              "         0.47472334, -0.44480807, -0.45888165, -0.4319432 ,  0.47053692,\n",
              "         0.44722456,  0.4631173 ,  0.45748386, -0.49594915,  0.44512254],\n",
              "       dtype=float32),\n",
              " 'consumer': array([ 0.4872698 ,  0.47236985, -0.46710312, -0.47869486, -0.502979  ,\n",
              "         0.46329755,  0.49608707, -0.5222637 , -0.5004042 , -0.470142  ,\n",
              "        -0.45822617,  0.4638533 ,  0.5101847 ,  0.4613049 , -0.51890075,\n",
              "        -0.50800246,  0.44921666,  0.43550992, -0.45295435, -0.5026946 ,\n",
              "        -0.4944315 ,  0.4513576 ,  0.48906392,  0.4888416 , -0.4640315 ,\n",
              "        -0.5024004 ,  0.46167704,  0.45976755,  0.43162367,  0.47706658,\n",
              "        -0.49393702,  0.4292956 , -0.5369894 , -0.5285058 , -0.4832481 ,\n",
              "        -0.4468935 ,  0.50870514, -0.4599722 ,  0.51002425, -0.49304476,\n",
              "         0.5000044 , -0.4695746 , -0.5024305 , -0.4811172 ,  0.5236278 ,\n",
              "         0.4943182 ,  0.5052625 ,  0.46397752, -0.49483526,  0.4603761 ],\n",
              "       dtype=float32),\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# 从文件中加载word2vec\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/word2vec.pkl', 'rb') as f:\n",
        "    loaded_word2vec = pickle.load(f)\n",
        "\n",
        "# 现在可以使用加载后的word2vec进行相关操作\n",
        "print(loaded_word2vec['applies'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_VDFMeBruOJ",
        "outputId": "6d8cf0f9-8994-4271-ebe7-54033ca53245"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.48905805 -0.43040475  0.4603558   0.461403   -0.42947713 -0.45979092\n",
            "  0.44053638 -0.42230797 -0.47899082 -0.46278995 -0.48420826 -0.4864065\n",
            "  0.44117197 -0.42199233 -0.45487538  0.47733703  0.41916835  0.50453067\n",
            " -0.4984742  -0.4262048   0.42631742  0.47542867 -0.49554628  0.48354647\n",
            "  0.4932434  -0.4994184  -0.454211    0.53255147  0.48170587 -0.45658368\n",
            "  0.44266874  0.4947176  -0.42010725  0.45141485 -0.4331438   0.48644522\n",
            "  0.4968722   0.45813453  0.43166316 -0.44834274 -0.44689882 -0.49011874\n",
            "  0.44589198 -0.47001058  0.4996367   0.48835784 -0.48706433 -0.53097725\n",
            "  0.46866944  0.4927562 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemma_tokenizer(text):\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text.replace(\"'\",\" \"))]\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\")+list(string.punctuation)+['``',\"''\",\"’\"]+[\"]\",\"[\",\"*\"]+['doe', 'ha', 'wa'] +['--']+ [''])"
      ],
      "metadata": {
        "id": "B8HTJeyTbDUv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 18\n",
        "tfidf = TfidfVectorizer(input='content', tokenizer=lemma_tokenizer, stop_words=list(stop_words), max_features=max_features)\n",
        "tfidf_train = tfidf.fit_transform(X_train)\n",
        "key_words = tfidf.get_feature_names_out() # 常用关键词"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDWBISmobDWt",
        "outputId": "9b0a492d-187c-4a7d-a0db-dcf639ac8bd0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6OnyWBYoqiq",
        "outputId": "a216a9fd-0b87-421b-a9d4-547b4c10816c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['class', 'company', 'expense', 'fee', 'fund', 'index',\n",
              "       'investment', 'market', 'may', 'performance', 'portfolio',\n",
              "       'return', 'risk', 'security', 'share', 'tax', 'value', 'year'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_n_closer(word, n, word2vec):\n",
        "    vect = word2vec[word]\n",
        "    dist_dict = {k: cosine(v, vect) for k, v in word2vec.items()}\n",
        "    closer_words = []\n",
        "    for _ in range(n):\n",
        "        min_key = min(dist_dict.keys(), key=lambda k: dist_dict[k])\n",
        "        closer_words.append(min_key)\n",
        "        del dist_dict[min_key]\n",
        "    return closer_words\n",
        "\n",
        "##knowledge base\n",
        "def create_knowledge_base(num_neighbors, word2vec, key_words):\n",
        "    knowledge_base = set()\n",
        "    out = display(progress(0, len(key_words)-1), display_id=True)\n",
        "    for ii, key_word in enumerate(key_words) :\n",
        "        knowledge_base.add(key_word)\n",
        "        neighbors = []\n",
        "        try :\n",
        "            neighbors = get_n_closer(key_word, num_neighbors, word2vec)\n",
        "        except :\n",
        "            print(key_word + ' not in word2vec')\n",
        "\n",
        "        knowledge_base.update(neighbors)\n",
        "\n",
        "        out.update(progress(ii, len(key_words)-1))\n",
        "    return knowledge_base\n",
        "\n",
        "knowledge_base = create_knowledge_base(5, loaded_word2vec, key_words)\n",
        "print(knowledge_base)\n",
        "\n",
        "# Takes a summary, the knowledge base and some hyper parameters and returns the \"num_sent\" sentences\n",
        "# of the summary that are closer to the the knowledge base in term of spacial distances.\n",
        "def extract_sentence_distance(summary, knowledge, n_closer, n_reject, num_sent):\n",
        "    # Split the summary into sentences.\n",
        "    sentences = sent_tokenize(summary)\n",
        "    sentence_scores = []\n",
        "    # Loop over the sentences.\n",
        "    for j, sentence in enumerate(sentences):\n",
        "        # we tokenize and clean the sentence\n",
        "        tokens = tokenizer(sentence)\n",
        "\n",
        "        sentence_barycentre = np.zeros(embedding_size)\n",
        "        effective_len = 0\n",
        "        # Compute the barycentre of the sentence\n",
        "        for token in tokens :\n",
        "            try :\n",
        "                sentence_barycentre += np.array(loaded_word2vec[token])\n",
        "                effective_len += 1\n",
        "            except KeyError :\n",
        "                pass\n",
        "            except :\n",
        "                raise\n",
        "\n",
        "        # Reject sentences with less than n_reject words in our word2vec map\n",
        "        if effective_len <= n_reject :\n",
        "            sentence_scores.append(1)\n",
        "\n",
        "        else :\n",
        "            sentence_barycentre = sentence_barycentre/effective_len\n",
        "            # Compute the distance sentece_barycentre -> words in our knowledge base\n",
        "            barycentre_distance = [cosine(sentence_barycentre, loaded_word2vec[key_word]) for key_word in knowledge]\n",
        "            barycentre_distance.sort()\n",
        "            # Create the score of the sentence by averaging the \"n_closer\" smallest distances\n",
        "            score = np.mean(barycentre_distance[:n_closer])\n",
        "            sentence_scores.append(score)\n",
        "    # Select the \"num_sent\" sentences that have the smallest score (smallest distance score with the knowledge base)\n",
        "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
        "    top_sentences = sentences[:num_sent]\n",
        "    return ' '.join(top_sentences)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "bPHfDiS5bDYp",
        "outputId": "d29747f7-9211-458e-ad98-3926cf9dbcdd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='17'\n",
              "            max='17',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            17\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'vogt', 'force', 'class', 'fourth', 'company', 'expense', 'combination', 'bank', 'james', 'c', 'intended', 'manager', 'market', 'willing', 'allocating', 'best', 'active', 'inaccurate', 'replicate', 'statement', 'portfolio', 'reimbursement', 'floating', 'index', 'advised', 'rents', 'selection', 'investment', 'calculate', 'shares', 'differ', 'year', 'quarter', 'comparing', 'performance', 'employ', 'limitation', 'l.', 'r', 'prepayments', 'maximum', 'improvement', 'downgraded', 'americanfunds.com', 'security', '529-a', 'non-diversified', 'relatively', 'resulted', 'increases', 'tax', 'r5', 'return', 'proper', 'regarded', 'assess', 'hypothetical', 'north', 'multi-factor', 'unsponsored', 'required', 'pursuant', 'established', 'ric', 'value', 'fee', 'approve', 'n', '12b-1', 'commodity', 'lengthen', 'share', 'advisors', 'fair', 'fund', 'risk', 'sovereign', 'gain', 'project', 'reasons', 'assuming', 'andrew', 'satisfy', 'management', 'may', 'r1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Measure Distance"
      ],
      "metadata": {
        "id": "Tr1sAobfb5wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the train,validation and test dataframe\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "X_valid_df = pd.DataFrame(X_valid)\n",
        "X_test_df = pd.DataFrame(X_test)\n",
        "\n",
        "X_train_df['sentences_distance'] = X_train_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
        "X_valid_df['sentences_distance'] = X_valid_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
        "X_test_df['sentences_distance'] = X_test_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsItC879fq8S",
        "outputId": "5139dd00-363b-46a9-849f-f0713cd00b69"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentence_match(summary, knowledge, num_sent):\n",
        "    sentences = sent_tokenize(summary)\n",
        "    sentence_scores = []\n",
        "    for j, sentence in enumerate(sentences):\n",
        "        set_tokens = set(tokenizer(sentence))\n",
        "\n",
        "        # Find the number of common words between the knowledge base and the sentence\n",
        "        inter_knwoledge = set_tokens.intersection(knowledge)\n",
        "\n",
        "        sentence_scores.append(len(inter_knwoledge))\n",
        "\n",
        "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
        "    top_sentences = sentences[len(sentences)-num_sent-1:]\n",
        "    return ' '.join(top_sentences)\n",
        "\n",
        "X_train_df['sentences_match'] = X_train_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "X_valid_df['sentences_match'] = X_valid_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "X_test_df['sentences_match'] = X_test_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "\n",
        "# produce train_X and test_X\n",
        "train_X = X_train_df['sentences_match'].values\n",
        "train_X = [' '.join(tokenizer(txt)) for txt in train_X]\n",
        "\n",
        "valid_X = X_valid_df['sentences_match'].values\n",
        "valid_X = [' '.join(tokenizer(txt)) for txt in valid_X]\n",
        "\n",
        "test_X = X_test_df['sentences_match'].values\n",
        "test_X = [' '.join(tokenizer(txt)) for txt in test_X]\n",
        "\n",
        "# produce train_y and valid_y\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "encoded_train_y = encoder.fit_transform(y_train)\n",
        "label_train_y = to_categorical(encoded_train_y, num_classes=3)\n",
        "\n",
        "encoded_valid_y = encoder.transform(y_valid)\n",
        "label_valid_y = to_categorical(encoded_valid_y, num_classes=3)\n",
        "\n",
        "encoded_test_y = encoder.fit_transform(y_test)\n",
        "label_test_y = to_categorical(encoded_test_y, num_classes=3)\n",
        "\n",
        "num_words = 2500 # Size of the vocabulary used. we only consider the 2500 most common words. The other words are removed from the texts.\n",
        "maxlen = 150 # Number of word considered for each document. we cut or lengthen the texts to have texts of 150 words.\n",
        "word_dimension = 50 # dimension of our word vectors.\n",
        "\n",
        "keras_tokenizer = Tokenizer(num_words=num_words)\n",
        "\n",
        "keras_tokenizer.fit_on_texts(train_X)\n",
        "\n",
        "word_index = keras_tokenizer.word_index\n",
        "\n",
        "sequences_train = keras_tokenizer.texts_to_sequences(train_X)\n",
        "sequences_valid = keras_tokenizer.texts_to_sequences(valid_X)\n",
        "sequences_test = keras_tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "# truncate or lenthen each text so they have the same length.\n",
        "feature_train = pad_sequences(sequences_train, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "feature_valid = pad_sequences(sequences_valid, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "feature_test = pad_sequences(sequences_test, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "\n",
        "# create our embedding matrix\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, word_dimension))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = loaded_word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "9myjGWaxb8i7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classifier"
      ],
      "metadata": {
        "id": "iQJBBnlOcVrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "9a8wIuYsv0If",
        "outputId": "0a9cc1a6-4c1e-4a49-d256-9ba8bf0ac3ca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               summary  \\\n",
              "149  MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...   \n",
              "436  INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...   \n",
              "394  Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...   \n",
              "440  INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...   \n",
              "330  Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...   \n",
              "..                                                 ...   \n",
              "152  MainStay VP MacKay International Equity Portfo...   \n",
              "158  MainStay VP MacKay Small Cap Core Portfolio\\n\\...   \n",
              "374  Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...   \n",
              "308  PIMCO Gurtin California Municipal Intermediate...   \n",
              "287  Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...   \n",
              "\n",
              "                                    sentences_distance  \\\n",
              "149  19\\n\\nMainStay VP Epoch U.S. Small Cap Portfol...   \n",
              "436  1 Year\\t3 Years\\t5 Years\\t10 Years\\n$93\\t$290\\...   \n",
              "394  2. 3. A higher portfolio turnover rate may ind...   \n",
              "440  1 Year\\t3 Years\\t5 Years\\t10 Years\\nFund Share...   \n",
              "330  \"Growth\" Investing. \"Growth\" stocks can perfor...   \n",
              "..                                                 ...   \n",
              "152  35\\n\\nMainStay VP MacKay International Equity ...   \n",
              "158  47\\n\\nMainStay VP MacKay Small Cap Core Portfo...   \n",
              "374  A figure of 1.00 would indicate perfect correl...   \n",
              "308  A higher portfolio turnover rate may indicate ...   \n",
              "287  2 \\tWith limited exceptions, for Class A share...   \n",
              "\n",
              "                                       sentences_match  \n",
              "149  The security selection process focuses on free...  \n",
              "436  There are no minimum initial or subsequent pur...  \n",
              "394  Total annual Fund operating expenses differ fr...  \n",
              "440  These payments may create a conflict of intere...  \n",
              "330  This example helps compare the cost of investi...  \n",
              "..                                                 ...  \n",
              "152  The table does not include any separate accoun...  \n",
              "158  The table does not include any separate accoun...  \n",
              "374  Tracking error may occur because of difference...  \n",
              "308  This Fee Waiver Agreement renews annually unle...  \n",
              "287  ∎\\t \\tInitial Public Offering (IPO) Risk. ∎\\t ...  \n",
              "\n",
              "[294 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-589292cf-0d54-47f3-8241-ae6c4c01168d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>sentences_distance</th>\n",
              "      <th>sentences_match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...</td>\n",
              "      <td>19\\n\\nMainStay VP Epoch U.S. Small Cap Portfol...</td>\n",
              "      <td>The security selection process focuses on free...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...</td>\n",
              "      <td>1 Year\\t3 Years\\t5 Years\\t10 Years\\n$93\\t$290\\...</td>\n",
              "      <td>There are no minimum initial or subsequent pur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...</td>\n",
              "      <td>2. 3. A higher portfolio turnover rate may ind...</td>\n",
              "      <td>Total annual Fund operating expenses differ fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...</td>\n",
              "      <td>1 Year\\t3 Years\\t5 Years\\t10 Years\\nFund Share...</td>\n",
              "      <td>These payments may create a conflict of intere...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...</td>\n",
              "      <td>\"Growth\" Investing. \"Growth\" stocks can perfor...</td>\n",
              "      <td>This example helps compare the cost of investi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>MainStay VP MacKay International Equity Portfo...</td>\n",
              "      <td>35\\n\\nMainStay VP MacKay International Equity ...</td>\n",
              "      <td>The table does not include any separate accoun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>MainStay VP MacKay Small Cap Core Portfolio\\n\\...</td>\n",
              "      <td>47\\n\\nMainStay VP MacKay Small Cap Core Portfo...</td>\n",
              "      <td>The table does not include any separate accoun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...</td>\n",
              "      <td>A figure of 1.00 would indicate perfect correl...</td>\n",
              "      <td>Tracking error may occur because of difference...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>PIMCO Gurtin California Municipal Intermediate...</td>\n",
              "      <td>A higher portfolio turnover rate may indicate ...</td>\n",
              "      <td>This Fee Waiver Agreement renews annually unle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...</td>\n",
              "      <td>2 \\tWith limited exceptions, for Class A share...</td>\n",
              "      <td>∎\\t \\tInitial Public Offering (IPO) Risk. ∎\\t ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-589292cf-0d54-47f3-8241-ae6c4c01168d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-589292cf-0d54-47f3-8241-ae6c4c01168d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-589292cf-0d54-47f3-8241-ae6c4c01168d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a3fe5988-4d8b-4451-9397-2d06c64dbae4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a3fe5988-4d8b-4451-9397-2d06c64dbae4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a3fe5988-4d8b-4451-9397-2d06c64dbae4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df",
              "summary": "{\n  \"name\": \"X_train_df\",\n  \"rows\": 294,\n  \"fields\": [\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 292,\n        \"samples\": [\n          \"Vanguard Short-Term Treasury Index Fund\\n\\nInvestment Objective\\n\\nThe Fund seeks to track the performance of a market-weighted Treasury index with a short-term dollar-weighted average maturity.\\n\\nFees and Expenses\\n\\nThe following table describes the fees and expenses you may pay if you buy and hold Admiral Shares of the Fund.\\n\\nShareholder Fees\\t \\n(Fees paid directly from your investment)\\t \\n \\nSales Charge (Load) Imposed on Purchases\\tNone\\nPurchase Fee\\tNone\\nSales Charge (Load) Imposed on Reinvested Dividends\\tNone\\nRedemption Fee\\tNone\\nAccount Service Fee (for certain fund account balances below $10,000)\\t$20/year\\n \\nAnnual Fund Operating Expenses\\t \\n(Expenses that you pay each year as a percentage of the value of your investment)\\t \\n \\nManagement Fees\\t0.06%\\n12b-1 Distribution Fee\\tNone\\nOther Expenses\\t0.01%\\nTotal Annual Fund Operating Expenses\\t0.07%\\n \\n\\nExample\\n\\nThe following example is intended to help you compare the cost of investing in the Fund\\u2019s Admiral Shares with the cost of investing in other mutual funds. It illustrates the hypothetical expenses that you would incur over various periods if you were to invest $10,000 in the Fund\\u2019s shares. This example assumes that the shares provide a return of 5% each year and that total annual fund operating expenses remain as stated in the preceding table. You would incur these hypothetical expenses whether or not you redeem your investment at the end of the given period. Although your actual costs may be higher or lower, based on these assumptions your costs would be:\\n\\n1 Year\\t3 Years\\t5 Years\\t10 Years\\n$7\\t$23\\t$40\\t$90\\n \\n\\n1\\n\\n \\n\\nPortfolio Turnover\\n\\nThe Fund pays transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d its portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in more taxes when Fund shares are held in a taxable account. These costs, which are not reflected in annual fund operating expenses or in the previous expense example, reduce the Fund\\u2019s performance. During the most recent fiscal year, the Fund\\u2019s portfolio turnover rate was 67% of the average value of its portfolio.\\n\\nPrincipal Investment Strategies\\n\\nThe Fund employs an indexing investment approach designed to track the performance of the Bloomberg Barclays US Treasury 1\\u20133 Year Bond Index. This Index includes fixed income securities issued by the U.S. Treasury (not including inflation-protected securities), all with maturities between 1 and 3 years.\\n\\nThe Fund invests by sampling the Index, meaning that it holds a range of securities that, in the aggregate, approximates the full Index in terms of key risk factors and other characteristics. All of the Fund\\u2019s investments will be selected through the sampling process, and under normal circumstances, at least 80% of the Fund\\u2019s assets will be invested in bonds included in the Index. The Fund maintains a dollar-weighted average maturity consistent with that of the Index. As of August 31, 2018, the dollar-weighted average maturity of the Index was 2.0 years.\\n\\nPrincipal Risks\\n\\nThe Fund is designed for investors with a low tolerance for risk, but you could still lose money by investing in it. The Fund is subject to the following risks, which could affect the Fund\\u2019s performance:\\n\\n\\u2022 Interest rate risk, which is the chance that bond prices will decline because of rising interest rates. Interest rate risk should be low for the Fund because it invests primarily in short-term bonds, whose prices are less sensitive to interest rate changes than are the prices of longer-term bonds.\\n\\n\\u2022 Income risk, which is the chance that the Fund\\u2019s income will decline because of falling interest rates. Income risk is generally high for short-term bond funds, so investors should expect the Fund\\u2019s monthly income to fluctuate.\\n\\n\\u2022 Index sampling risk, which is the chance that the securities selected for the Fund, in the aggregate, will not provide investment performance matching that of the Fund\\u2018s target index. Index sampling risk for the Fund is expected to be low.\\n\\nAn investment in the Fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency.\\n\\n2\\n\\n \\n\\nAnnual Total Returns\\n\\nThe following bar chart and table are intended to help you understand the risks of investing in the Fund. The bar chart shows how the performance of the Fund\\u2019s Admiral Shares has varied from one calendar year to another over the periods shown. The table shows how the average annual total returns of the Admiral Shares compare with those of the Fund\\u2019s target index, which has investment characteristics similar to those of the Fund. The Fund\\u2019s Signal\\u00ae Shares were renamed Admiral Shares on October 16, 2013. Keep in mind that the Fund\\u2019s past performance (before and after taxes) does not indicate how the Fund will perform in the future. Updated performance information is available on our website at vanguard.com/performance or by calling Vanguard toll-free at 800-662-7447.\\n\\nAnnual Total Returns \\u2014 Vanguard Short-Term Treasury Index Fund Admiral Shares1\\n\\n\\n1 The year-to-date return as of the most recent calendar quarter, which ended on September 30, 2018, was 0.17%.\\n\\nDuring the periods shown in the bar chart, the highest return for a calendar quarter was 1.16% (quarter ended June 30, 2010), and the lowest return for a quarter was \\u20130.45% (quarter ended December 31, 2016).\\n\\n3\\n\\n \\n\\nAverage Annual Total Returns for Periods Ended December 31, 2017\\t \\t \\n \\t \\t \\tSince\\n \\t \\t \\tInception\\n \\t \\t \\t(Dec. 28,\\n \\t1 Year\\t5 Years\\t2009)\\nVanguard Short-Term Treasury Index Fund Admiral Shares\\t \\t \\t \\nReturn Before Taxes\\t0.40%\\t0.50%\\t0.81%\\nReturn After Taxes on Distributions\\t\\u20130.08\\t0.20\\t0.54\\nReturn After Taxes on Distributions and Sale of Fund Shares\\t0.22\\t0.25\\t0.51\\nBloomberg Barclays US Treasury 1-3 Year Bond Index\\t \\t \\t \\n(reflects no deduction for fees, expenses, or taxes)\\t0.42%\\t0.57%\\t0.89%\\n \\n\\nActual after-tax returns depend on your tax situation and may differ from those shown in the preceding table. When after-tax returns are calculated, it is assumed that the shareholder was in the highest individual federal marginal income tax bracket at the time of each distribution of income or capital gains or upon redemption. State and local income taxes are not reflected in the calculations. Please note that after-tax returns are not relevant for a shareholder who holds fund shares in a tax-deferred account, such as an individual retirement account or a 401(k) plan. Also, figures captioned Return After Taxes on Distributions and Sale of Fund Shares may be higher than other figures for the same period if a capital loss occurs upon redemption and results in an assumed tax deduction for the shareholder.\\n\\nInvestment Advisor\\nThe Vanguard Group, Inc. (Vanguard)\\n\\nPortfolio Manager\\n\\nJoshua C. Barrickman, CFA, Principal of Vanguard and head of Vanguard\\u2019s Fixed Income Indexing Americas. He has managed the Fund since 2013.\\n\\n4\\n\\n \\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase or redeem shares online through our website (vanguard.com), by mail (The Vanguard Group, P.O. Box 1110, Valley Forge, PA 19482-1110), or by telephone (800-662-2739). The minimum investment amount required to open and maintain a Fund account for Admiral Shares is $3,000. The minimum investment amount required to add to an existing Fund account is generally $1. Financial intermediaries, institutional, and Vanguard-advised clients should contact Vanguard for information on special eligibility rules that may apply to them regarding Admiral Shares. If you are investing through an intermediary, please contact that firm directly for more information regarding your eligibility. If you are investing through an employer-sponsored retirement or savings plan, your plan administrator or your benefits office can provide you with detailed information on how you can invest through your plan.\\n\\nTax Information\\n\\nThe Fund\\u2019s distributions may be taxable as ordinary income or capital gain. If you are investing through a tax-advantaged account, such as an IRA or an employer-sponsored retirement or savings plan, special tax rules apply.\\n\\nPayments to Financial Intermediaries\\n\\nThe Fund and its investment advisor do not pay financial intermediaries for sales of Fund shares.\",\n          \"Eaton Vance TABS Intermediate-Term Municipal Bond Fund\\n\\nInvestment Objective\\n\\nThe Fund\\u2019s investment objective is to seek after-tax total return.\\n\\nFees and Expenses of the Fund\\n\\nThis table describes the fees and expenses that you may pay if you buy and hold shares of the Fund. Investors may also pay commissions or other fees to their financial intermediary when they buy and hold shares of the Fund, which are not reflected below. You may qualify for a reduced sales charge on purchases of Class A shares if you invest, or agree to invest over a 13-month period, at least $100,000 in Eaton Vance funds. Certain financial intermediaries also may offer variations in Fund sales charges to their customers as described in Appendix A \\u2013 Financial Intermediary Sales Charge Variations in this Prospectus. More information about these and other discounts is available from your financial intermediary and in Sales Charges beginning on page 20 of this Prospectus and page 21 of the Fund\\u2019s Statement of Additional Information.\\n\\nShareholder Fees (fees paid directly from your investment)\\tClass A\\tClass C\\tClass I\\nMaximum Sales Charge (Load) Imposed on Purchases (as a percentage of offering price)\\t2.25%\\tNone\\tNone\\nMaximum Deferred Sales Charge (Load) (as a percentage of the lower of net asset value at purchase or redemption)\\tNone\\t1.00%\\tNone\\n \\n\\nAnnual Fund Operating Expenses (expenses you pay each year as a percentage of the value of your investment)\\tClass A\\tClass C\\tClass I\\nManagement Fees\\t0.60%\\t0.60%\\t0.60%\\nDistribution and Service (12b-1) Fees\\t0.25%\\t1.00%\\tNone\\nOther Expenses\\t0.12%\\t0.12%\\t0.12%\\nTotal Annual Fund Operating Expenses\\t0.97%\\t1.72%\\t0.72%\\nExpense Reimbursement (1)\\t(0.07)%\\t(0.07)%\\t(0.07)%\\nTotal Annual Fund Operating Expenses After Expense Reimbursement\\t0.90%\\t1.65%\\t0.65%\\n(1)\\tThe investment adviser and administrator has agreed to reimburse the Fund\\u2019s expenses to the extent that Total Annual Fund Operating Expenses exceed 0.90% for Class A shares, 1.65% for Class C shares and 0.65% for Class I shares. This expense reimbursement will continue through May 31, 2019. Any amendment to or termination of this reimbursement would require approval of the Board of Trustees. The expense reimbursement relates to ordinary operating expenses only and does not include expenses such as: brokerage commissions, acquired fund fees and expenses of unaffiliated funds, interest expense, taxes or litigation expenses. Amounts reimbursed may be recouped by the investment adviser and administrator during the same fiscal year to the extent actual expenses are less than the contractual expense cap during such year.\\nExample. This Example is intended to help you compare the cost of investing in the Fund with the cost of investing in other mutual funds. The Example assumes that you invest $10,000 in the Fund for the time periods indicated and then redeem all of your shares at the end of those periods. The Example also assumes that your investment has a 5% return each year, that the operating expenses remain the same and that any expense reimbursement arrangement remains in place for the contractual period. Although your actual costs may be higher or lower, based on these assumptions your costs would be:\\n\\n \\tExpenses with Redemption\\tExpenses without Redemption\\n \\t1 Year\\t3 Years\\t5 Years\\t10 Years\\t1 Year\\t3 Years\\t5 Years\\t10 Years\\nClass A shares\\t$315\\t$520\\t$743\\t$1,382\\t$315\\t$520\\t$743\\t$1,382\\nClass C shares\\t$268\\t$535\\t$927\\t$2,024\\t$168\\t$535\\t$927\\t$2,024\\nClass I shares\\t$66\\t$223\\t$394\\t$888\\t$66\\t$223\\t$394\\t$888\\nPortfolio Turnover\\n\\nThe Fund pays transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d the portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when Fund shares are held in a taxable account. These costs, which are not reflected in Annual Fund Operating Expenses or in the Example, affect the Fund\\u2019s performance. During the most recent fiscal year, the Fund's portfolio turnover rate was 62% of the average value of its portfolio.\\n\\nEaton Vance TABS Municipal Bond Funds\\t7\\tProspectus dated June 1, 2018\\n \\n \\n\\nPrincipal Investment Strategies\\n\\nUnder normal market conditions, the Fund invests at least 80% of its net assets (plus any borrowings for investment purposes) in a diversified portfolio of municipal obligations the interest on which is exempt from regular federal income tax (the \\u201c80% Policy\\u201d). In seeking the Fund\\u2019s investment objective, the portfolio managers emphasize tax-exempt income. The Fund normally invests in municipal obligations rated in the three highest rating categories (those rated A or higher by S&P Global Ratings (\\u201cS&P\\u201d), Fitch Ratings (\\u201cFitch\\u201d) or Moody\\u2019s Investors Service, Inc. (\\u201cMoody\\u2019s\\u201d)) or, if unrated, determined by the investment adviser to be of comparable quality at the time of purchase. The Fund will not invest more than 50% of its net assets in municipal obligations rated A at the time of purchase by S&P, Fitch or Moody\\u2019s or, if unrated determined by the investment adviser to be of comparable quality. For purposes of rating restrictions, if securities are rated differently by two or more rating agencies, the highest rating is used. The Fund may continue to hold securities that are downgraded (including bonds downgraded to below investment grade credit quality (\\u201cjunk bonds\\u201d)) if the investment adviser believes it would be advantageous to do so. The Fund will not invest in a municipal obligation the interest on which the Fund\\u2019s investment adviser believes is subject to the federal alternative minimum tax.\\n\\nFor its investment in municipal obligations, the Fund invests primarily in general obligation or revenue bonds. The Fund currently targets an average portfolio duration of approximately 5 - 7 years and an average weighted portfolio maturity of approximately 5 - 13 years, but may invest in securities of any maturity or duration, and may in the future alter its maturity or duration target range. The Fund may use various techniques to shorten or lengthen its dollar weighted average portfolio duration, including the acquisition of municipal obligations at a premium or discount. The portfolio managers generally will seek to enhance after-tax total return by actively engaging in relative value trading within the portfolio to take advantage of price opportunities in the markets for municipal obligations. With respect to 20% of its net assets, the Fund may invest in municipal obligations that are not exempt from regular federal income tax, direct obligations of the U.S. Treasury and/or obligations of U.S. Government agencies, instrumentalities and government-sponsored enterprises. The Fund may also invest in cash and money market instruments.\\n\\nThe investment adviser\\u2019s process for selecting municipal obligations for purchase and sale generally includes consideration of the creditworthiness of the issuer or person obligated to repay the obligation. In evaluating creditworthiness, the investment adviser considers ratings assigned by rating agencies and generally performs additional credit and investment analysis.\\n\\nPrincipal Risks\\n\\nMarket Risk. The value of investments held by the Fund may increase or decrease in response to economic, political and financial events (whether real, expected or perceived) in the U.S. and global markets. The frequency and magnitude of such changes in value cannot be predicted. Certain securities and other investments held by the Fund may experience increased volatility, illiquidity, or other potentially adverse effects in reaction to changing market conditions. Actions taken by the U.S. Federal Reserve or foreign central banks to stimulate or stabilize economic growth, such as decreases or increases in short-term interest rates, could cause high volatility in markets. No active trading market may exist for certain investments, which may impair the ability of the Fund to sell or to realize the current valuation of such investments in the event of the need to liquidate such assets. Fixed-income markets may experience periods of relatively high volatility in an environment where U.S. treasury yields are rising.\\n\\nMunicipal Obligation Risk. The amount of public information available about municipal obligations is generally less than for corporate equities or bonds, meaning that the investment performance of municipal obligations may be more dependent on the analytical abilities of the investment adviser than stock or corporate bond investments. The secondary market for municipal obligations also tends to be less well-developed and less liquid than many other securities markets, which may limit the Fund\\u2019s ability to sell its municipal obligations at attractive prices. The differences between the price at which an obligation can be purchased and the price at which it can be sold may widen during periods of market distress. Less liquid obligations can become more difficult to value and be subject to erratic price movements. The increased presence of nontraditional participants (such as proprietary trading desks of investment banks and hedge funds) or the absence of traditional participants (such as individuals, insurance companies, banks and life insurance companies) in the municipal markets may lead to greater volatility in the markets because non-traditional participants may trade more frequently or in greater volume. \\n\\nInterest Rate Risk. In general, the value of income securities will fluctuate based on changes in interest rates. The value of these securities is likely to increase when interest rates fall and decline when interest rates rise. Generally, securities with longer durations are more sensitive to changes in interest rates than shorter duration securities, causing them to be more volatile. Conversely, fixed income securities with shorter durations will be less volatile but may provide lower returns than fixed income securities with longer durations. In a rising interest rate environment, the durations of income securities that have the ability to be prepaid or called by the issuer may be extended. In a declining interest rate environment, the proceeds from prepaid or maturing instruments may have to be reinvested at a lower interest rate.\\n\\nEaton Vance TABS Municipal Bond Funds\\t8\\tProspectus dated June 1, 2018\\n \\nCredit Risk. Investments in municipal obligations and other debt obligations (referred to below as \\u201cdebt instruments\\u201d) are subject to the risk of non-payment of scheduled principal and interest. Changes in economic conditions or other circumstances may reduce the capacity of the party obligated to make principal and interest payments on such instruments and may lead to defaults. Such non-payments and defaults may reduce the value of Fund shares and income distributions. The value of debt instruments also may decline because of concerns about the issuer\\u2019s ability to make principal and interest payments. In addition, the credit ratings of debt instruments may be lowered if the financial condition of the party obligated to make payments with respect to such instruments deteriorates. In order to enforce its rights in the event of a default, bankruptcy or similar situation, the Fund may be required to retain legal or similar counsel, which may increase the Fund\\u2019s operating expenses and adversely affect net asset value. Municipal obligations may be insured as to principal and interest payments. If the claims-paying ability or other rating of the insurer is downgraded by a rating agency, the value of such obligations may be negatively affected.\\n\\nRisks of Principal Only Investments. Principal only investments entitle the Fund to receive the stated value of such investment when held to maturity. The values of principal only investments are subject to greater fluctuation in response to changes in market interest rates than obligations that pay interest currently. The Fund will accrue income on these investments and distribute that income each year. The Fund may be required to sell other investments to obtain cash needed for such income distributions.\\n\\nU.S. Government Securities Risk. Although certain U.S. Government-sponsored agencies (such as the Federal Home Loan Mortgage Corporation and the Federal National Mortgage Association) may be chartered or sponsored by acts of Congress, their securities are neither issued nor guaranteed by the U.S. Treasury. U.S. Treasury securities generally have a lower return than other obligations because of their higher credit quality and market liquidity.\\n\\nTax Risk. Income from tax-exempt municipal obligations could be declared taxable because of changes in tax laws, adverse interpretations by the relevant taxing authority or the non-compliant conduct of the issuer of an obligation.\\n\\nMoney Market Instrument Risk. Money market instruments may be adversely affected by market and economic events, such as a sharp rise in prevailing short-term interest rates; adverse developments in the banking industry, which issues or guarantees many money market instruments; adverse economic, political or other developments affecting issuers of money market instruments; changes in the credit quality of issuers; and default by a counterparty.\\n\\nGeneral Fund Investing Risks. The Fund is not a complete investment program and there is no guarantee that the Fund will achieve its investment objective. It is possible to lose money by investing in the Fund. The Fund is designed to be a long-term investment vehicle and is not suited for short-term trading. Investors in the Fund should have a long-term investment perspective and be able to tolerate potentially sharp declines in value. Purchase and redemption activities by Fund shareholders may impact the management of the Fund and its ability to achieve its investment objective(s). In addition, the redemption by one or more large shareholders or groups of shareholders of their holdings in the Fund could have an adverse impact on the remaining shareholders in the Fund. An investment in the Fund is not a deposit in a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency.\\n\\nPerformance\\n\\nThe following bar chart and table provide some indication of the risks of investing in the Fund by showing changes in the Fund\\u2019s performance from year to year and how the Fund\\u2019s average annual returns over time compare with those of two broad-based securities market indices. The returns in the bar chart are for Class A shares and do not reflect a sales charge. If the sales charge was reflected, the returns would be lower. Past performance (both before and after taxes) is not necessarily an indication of how the Fund will perform in the future. The Fund\\u2019s performance reflects the effects of expense reductions. Absent these reductions, performance would have been lower. Updated Fund performance information can be obtained by visiting www.eatonvance.com.\\n\\n\\n\\nDuring the period from December 31, 2010 to December 31, 2017, the highest quarterly total return for Class A was 3.75% for the quarter ended June 30, 2011, and the lowest quarterly return was \\u20134.07% for the quarter ended December 31, 2016. The year-to-date total return through the end of the most recent calendar quarter (December 31, 2017 to March 31, 2018) was -1.71%.\\n\\nEaton Vance TABS Municipal Bond Funds\\t9\\tProspectus dated June 1, 2018\\n \\n \\n\\nAverage Annual Total Return as of December 31, 2017\\tOne Year\\tFive Years\\tLife of Fund\\nClass A Return Before Taxes\\t2.08%\\t2.12%\\t4.21%\\nClass A Return After Taxes on Distributions\\t2.05%\\t2.08%\\t4.09%\\nClass A Return After Taxes on Distributions and the Sale of Class A Shares\\t1.94%\\t1.97%\\t3.59%\\nClass C Return Before Taxes\\t2.67%\\t1.82%\\t3.74%\\nClass I Return Before Taxes\\t4.71%\\t2.82%\\t4.78%\\nBloomberg Barclays Municipal Managed Money Intermediate (1-17 Year) Bond Index (reflects no deduction for fees, expenses or taxes)\\t4.88%\\t2.55%\\t4.26%\\nBloomberg Barclays 7 Year Municipal Bond Index (reflects no deduction for fees, expenses or taxes)\\t4.49%\\t2.44%\\t4.30%\\nThese returns reflect the maximum sales charge for Class A (2.25%) and any applicable contingent deferred sales charge (\\u201cCDSC\\u201d) for Class C. Class A, Class C and Class I commenced operations on February 1, 2010. Effective February 17, 2015, the Fund changed its name, objective and investment strategy to invest at least 80% of its net assets in a diversified portfolio of municipal obligations, the interest on which is exempt from regular federal income tax. Investors cannot invest directly in an Index.\\n\\nAfter-tax returns are calculated using the highest historical individual federal income tax rates and does not reflect the impact of state and local taxes. Actual after-tax returns depend on a shareholder\\u2019s tax situation and the actual characterization of distributions, and may differ from those shown. After-tax returns are not relevant to shareholders who hold shares in tax-deferred accounts or to shares held by non-taxable entities. After-tax returns for other Classes of shares will vary from the after-tax returns presented for Class A shares. Return After Taxes on Distributions for a period may be the same as Return Before Taxes for that period because no taxable distributions were made during that period. Also, Return After Taxes on distributions and Sale of Fund Shares for a period may be greater than or equal to Return After Taxes on Distributions for the same period because of losses realized on the sale of Fund shares.\\n\\nManagement\\n\\nInvestment Adviser. Eaton Vance Management (\\u201cEaton Vance\\u201d).\\n\\nPortfolio Managers\\n\\nThe portfolio managers of the Fund are part of Eaton Vance\\u2019s Tax-Advantaged Bond Strategies (\\u201cTABS\\u201d) division.\\n\\nJames H. Evans, (lead portfolio manager), Vice President of Eaton Vance, has managed the Fund since it commenced operations in February 2010.\\n\\nBrian C. Barney, Vice President of Eaton Vance, has managed the Fund since June 2010.\\n\\nChristopher J. Harshman, Vice President of Eaton Vance, has managed the Fund since June 2010.\\n\\nFor important information about purchase and sale of shares, taxes and financial intermediary compensation, please turn to \\u201cImportant Information Regarding Fund Shares\\u201d on page 11 of this Prospectus.\\n\\nEaton Vance TABS Municipal Bond Funds\\t10\\tProspectus dated June 1, 2018\\n \\n \\n\\nImportant Information Regarding Fund Shares\\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase, redeem or exchange Fund shares on any business day, which is any day the New York Stock Exchange is open for business. You may purchase, redeem or exchange Fund shares either through your financial intermediary or (except for purchases of Class C shares by accounts with no specified financial intermediary) directly from a Fund either by writing to the Fund, P.O. Box 9653, Providence, RI 02940-9653, or by calling 1-800-262-1122. The minimum initial purchase or exchange into a Fund is $1,000 for each Class (with the exception of Class I) and $250,000 for Class I (waived in certain circumstances). There is no minimum for subsequent investments.\\n\\nTax Information\\n\\nEach Fund\\u2019s distributions are expected to primarily be exempt from regular federal income tax. However, the Fund may also distribute taxable income to the extent that it invests in taxable municipal obligations or other obligations which generate taxable income. Distributions of any net realized gains are expected to be taxable.\\n\\nPayments to Broker-Dealers and Other Financial Intermediaries\\n\\nIf you purchase a Fund\\u2019s shares through a broker-dealer or other financial intermediary (such as a bank) (collectively, \\u201cfinancial intermediaries\\u201d), the Fund, its principal underwriter and its affiliates may pay the financial intermediary for the sale of Fund shares and related services. These payments may create a conflict of interest by influencing the financial intermediary and your salesperson to recommend a Fund over another investment. Ask your salesperson or visit your financial intermediary\\u2019s website for more information.\",\n          \"Fund Summary\\n\\nFund/Class:\\nFidelity Freedom\\u00ae 2050 Fund/Fidelity Freedom\\u00ae 2050 Fund\\n\\nInvestment Objective\\n\\nThe fund seeks high total return until its target retirement date. Thereafter the fund's objective will be to seek high current income and, as a secondary objective, capital appreciation.\\n\\nFee Table\\n\\nThe following table describes the fees and expenses that may be incurred when you buy and hold shares of the fund.\\n\\nShareholder fees\\n\\n(fees paid directly from your investment) \\tNone \\nAnnual Operating Expenses\\n\\n(expenses that you pay each year as a % of the value of your investment)\\n\\nManagement fee(a) \\t \\t0.75% \\nDistribution and/or Service (12b-1) fees \\t \\tNone \\nOther expenses(a) \\t \\t0.00% \\nTotal annual operating expenses(a) \\t \\t0.75% \\n(a)   Adjusted to reflect current fees.\\n\\nThis example helps compare the cost of investing in the fund with the cost of investing in other funds.\\n\\nLet's say, hypothetically, that the annual return for shares of the fund is 5% and that your shareholder fees and the annual operating expenses for shares of the fund are exactly as described in the fee table. This example illustrates the effect of fees and expenses, but is not meant to suggest actual or expected fees and expenses or returns, all of which may vary. For every $10,000 you invested, here's how much you would pay in total expenses if you sell all of your shares at the end of each time period indicated:\\n\\n1 year \\t$76 \\n3 years \\t$238 \\n5 years \\t$415 \\n10 years \\t$926 \\nPortfolio Turnover\\n\\nThe fund will not incur transaction costs, such as commissions, when it buys and sells shares of underlying Fidelity\\u00ae funds (or \\\"turns over\\\" its portfolio), but it could incur transaction costs if it were to buy and sell other types of securities directly. If the fund were to buy and sell other types of securities directly, a higher portfolio turnover rate could indicate higher transaction costs and could result in higher taxes when fund shares are held in a taxable account. Such costs, if incurred, would not be reflected in annual operating expenses or in the example and would affect the fund's performance. During the most recent fiscal year, the fund's portfolio turnover rate was 16% of the average value of its portfolio.\\n\\nPrincipal Investment Strategies\\n\\nInvesting in a combination of Fidelity\\u00ae domestic equity funds, international equity funds (developed and emerging markets), bond funds, and short-term funds (underlying Fidelity\\u00ae funds).\\nAllocating assets according to a neutral asset allocation strategy shown in the glide path below that becomes increasingly conservative until it reaches an allocation similar to that of the Fidelity Freedom\\u00ae Income Fund, approximately 10 to 19 years after the year 2050 (approximately 17% in domestic equity funds, 7% in international equity funds, 46% in bond funds, and 30% in short-term funds).\\nBuying and selling futures contracts (both long and short positions) in an effort to manage cash flows efficiently, remain fully invested, or facilitate asset allocation.\\nFMR Co., Inc. (FMRC) may continue to seek high total return for several years beyond the fund's target retirement date in an effort to achieve the fund's overall investment objective.\\n\\nAs of March 31, 2018, the fund's neutral asset allocation to underlying Fidelity\\u00ae funds and futures was approximately:\\n   \\tDomestic Equity Funds* \\t63% \\n   \\tInternational Equity Funds* \\t27% \\n   \\tBond Funds* \\t10% \\n \\tShort-Term Funds* \\t0% \\n\\n* FMRC may change these percentages over time. As a result of the active asset allocation strategy (discussed below), actual allocations may differ from the neutral allocations above. The allocation percentages may not add to 100% due to rounding.\\n\\nFMRC may use an active asset allocation strategy to increase or decrease neutral asset class exposures reflected above by up to 10% for equity funds (includes domestic equity and international equity funds), bond funds and short-term funds to reflect FMRC's market outlook, which is primarily focused on the intermediate term. The asset allocations in the glide path and pie chart above are referred to as neutral because they do not reflect any decisions made by FMRC to overweight or underweight an asset class.\\nFMRC may also make active asset allocations within other asset classes (including commodities, high yield debt, floating rate debt, real estate debt, inflation-protected debt, and emerging markets debt) from 0% to 10% individually but no more than 25% in aggregate within those other asset classes. Such asset classes are not reflected in the neutral asset allocations reflected in the glide path and pie chart above.\\nDesigned for investors who anticipate retiring in or within a few years of 2050 (target retirement date) at or around age 65 and plan to gradually withdraw the value of their account in the fund over time.\\nPrincipal Investment Risks\\n\\nShareholders should consider that no target date fund is intended as a complete retirement program and there is no guarantee that any single fund will provide sufficient retirement income at or through your retirement. The fund's share price fluctuates, which means you could lose money by investing in the fund, including losses near, at or after the target retirement date.\\n\\nAsset Allocation Risk.  The fund is subject to risks resulting from the Adviser's asset allocation decisions. The selection of underlying funds and the allocation of the fund's assets among various asset classes could cause the fund to lose value or its results to lag relevant benchmarks or other funds with similar objectives. In addition, the fund's active asset allocation strategy may cause the fund to have a risk profile different than that portrayed above from time to time and may increase losses.\\nInvesting in Other Funds.  The fund bears all risks of investment strategies employed by the underlying funds, including the risk that the underlying funds will not meet their investment objectives.\\nStock Market Volatility.  Stock markets are volatile and can decline significantly in response to adverse issuer, political, regulatory, market, or economic developments. Different parts of the market, including different market sectors, and different types of securities can react differently to these developments.\\nInterest Rate Changes.  Interest rate increases can cause the price of a debt or money market security to decrease.\\nForeign Exposure.  Foreign markets, particularly emerging markets, can be more volatile than the U.S. market due to increased risks of adverse issuer, political, regulatory, market, or economic developments and can perform differently from the U.S. market. Emerging markets can be subject to greater social, economic, regulatory, and political uncertainties and can be extremely volatile. Foreign exchange rates also can be extremely volatile.\\nIndustry Exposure.  Market conditions, interest rates, and economic, regulatory, or financial developments could significantly affect a single industry or group of related industries.\\nIssuer-Specific Changes.  The value of an individual security or particular type of security can be more volatile than, and can perform differently from, the market as a whole. Lower-quality debt securities (those of less than investment-grade quality, also referred to as high yield debt securities or junk bonds) and certain types of other securities involve greater risk of default or price changes due to changes in the credit quality of the issuer. The value of lower-quality debt securities and certain types of other securities can be more volatile due to increased sensitivity to adverse issuer, political, regulatory, market, or economic developments.\\nLeverage Risk.  Leverage can increase market exposure, magnify investment risks, and cause losses to be realized more quickly.\\n\\\"Growth\\\" Investing.  \\\"Growth\\\" stocks can perform differently from the market as a whole and other types of stocks and can be more volatile than other types of stocks.\\n\\\"Value\\\" Investing.  \\\"Value\\\" stocks can perform differently from the market as a whole and other types of stocks and can continue to be undervalued by the market for long periods of time.\\nCommodity-Linked Investing.  The value of commodities and commodity-linked investments may be affected by the performance of the overall commodities markets as well as weather, political, tax, and other regulatory and market developments. Commodity-linked investments may be more volatile and less liquid than the underlying commodity, instruments, or measures.\\nAn investment in the fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency. You could lose money by investing in the fund.\\n\\nPerformance\\n\\nThe following information is intended to help you understand the risks of investing in the fund. The information illustrates the changes in the performance of the fund's shares from year to year and compares the performance of the fund's shares to the performance of a securities market index and a hypothetical composite of market indexes over various periods of time. The indexes have characteristics relevant to the fund's investment strategies. Index descriptions appear in the \\\"Additional Index Information\\\" section of the prospectus. Prior to June 1, 2017, the fund operated under a different pricing structure. The fund\\u2019s historical performance prior to June 1, 2017 does not reflect the fund\\u2019s current pricing structure. Past performance (before and after taxes) is not an indication of future performance.\\n\\nVisit www.fidelity.com for more recent performance information.\\n\\nYear-by-Year Returns\\n\\n\\n\\nDuring the periods shown in the chart: \\tReturns \\tQuarter ended \\nHighest Quarter Return \\t19.11% \\tJune 30, 2009 \\nLowest Quarter Return \\t(23.40)% \\tDecember 31, 2008 \\nYear-to-Date Return \\t(0.25)% \\tMarch 31, 2018 \\nAverage Annual Returns\\n\\nAfter-tax returns are calculated using the historical highest individual federal marginal income tax rates, but do not reflect the impact of state or local taxes. Actual after-tax returns may differ depending on your individual circumstances. The after-tax returns shown are not relevant if you hold your shares in a retirement account or in another tax-deferred arrangement, such as an employee benefit plan (profit sharing, 401(k), or 403(b) plan).\\n\\nFor the periods ended December 31, 2017 \\tPast 1 year \\tPast 5 years \\tPast 10 years \\nFidelity Freedom\\u00ae 2050 Fund \\nReturn Before Taxes \\t22.28% \\t11.30% \\t5.31% \\nReturn After Taxes on Distributions \\t21.05% \\t9.54% \\t4.09% \\nReturn After Taxes on Distributions and Sale of Fund Shares \\t13.44% \\t8.52% \\t3.85% \\nS&P 500\\u00ae Index\\n(reflects no deduction for fees, expenses, or taxes) \\t21.83% \\t15.79% \\t8.50% \\nFidelity Freedom 2050 Composite Index\\u2120\\n(reflects no deduction for fees or expenses) \\t20.95% \\t12.05% \\t6.60% \\nInvestment Adviser\\n\\nFMRC (the Adviser), an affiliate of Fidelity Management & Research Company (FMR), is the fund's manager.\\n\\nPortfolio Manager(s)\\n\\nAndrew Dierdorf (co-manager) has managed the fund since June 2011.\\n\\nBrett Sumsion (co-manager) has managed the fund since January 2014.\\n\\nPurchase and Sale of Shares\\n\\nYou may buy or sell shares through a Fidelity\\u00ae brokerage or mutual fund account, through a retirement account, or through an investment professional. You may buy or sell shares in various ways:\\n\\nInternet\\n\\nwww.fidelity.com\\n\\nPhone\\n\\nFidelity Automated Service Telephone (FAST\\u00ae) 1-800-544-5555\\n\\nTo reach a Fidelity representative 1-800-544-6666\\n\\nMail\\n\\nAdditional purchases:\\n\\nFidelity Investments\\nP.O. Box 770001\\nCincinnati, OH 45277-0003\\nRedemptions:\\n\\nFidelity Investments\\nP.O. Box 770001\\nCincinnati, OH 45277-0035\\nTDD- Service for the Deaf and Hearing Impaired\\n\\n1-800-544-0118\\n\\nThe price to buy one share is its net asset value per share (NAV). Shares will be bought at the NAV next calculated after an order is received in proper form.\\n\\nThe price to sell one share is its NAV. Shares will be sold at the NAV next calculated after an order is received in proper form.\\n\\nThe fund is open for business each day the New York Stock Exchange (NYSE) is open.\\n\\nInitial Purchase Minimum \\t$2,500 \\nFor Fidelity\\u00ae Simplified Employee Pension-IRA, Keogh, and Investment Only Retirement accounts \\t$500 \\nThrough regular investment plans in Fidelity\\u00ae Traditional IRAs, Roth IRAs, and Rollover IRAs (requires monthly purchases of $200 until fund balance is $2,500) \\t$200 \\nThe fund may waive or lower purchase minimums in other circumstances.\\n\\nTax Information\\n\\nDistributions you receive from the fund are subject to federal income tax and generally will be taxed as ordinary income or capital gains, and may also be subject to state or local taxes, unless you are investing through a tax-advantaged retirement account (in which case you may be taxed later, upon withdrawal of your investment from such account).\\n\\nPayments to Broker-Dealers and Other Financial Intermediaries\\n\\nThe fund, the Adviser, Fidelity Distributors Corporation (FDC), and/or their affiliates may pay intermediaries, which may include banks, broker-dealers, retirement plan sponsors, administrators, or service-providers (who may be affiliated with the Adviser or FDC), for the sale of fund shares and related services. These payments may create a conflict of interest by influencing your intermediary and your investment professional to recommend the fund over another investment. Ask your investment professional or visit your intermediary's web site for more information.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_distance\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 253,\n        \"samples\": [\n          \"2 \\tWith limited exceptions, for Class A shares, if your Fund account balance is below $650 at the start of business on the Friday prior to the last full week of September of each year, the account will be assessed an account fee of $20. 3 \\tThrough July 31, 2019, Ivy Investment Management Company (IICO), the Fund\\u2019s investment manager, Ivy Distributors, Inc. (IDI), the Fund\\u2019s distributor, and/or Waddell & Reed Services Company, doing business as WI Services Company (WISC), the Fund\\u2019s transfer agent, have contractually agreed to reimburse sufficient management fees, 12b-1 fees and/or shareholder servicing fees to cap the total annual ordinary fund operating expenses (which would exclude interest, taxes, brokerage commissions, acquired fund fees and expenses and extraordinary expenses, if any) as follows: Class E shares at 1.10%. 4\\t \\tProspectus\\t \\tDomestic Equity Funds\\nTable of Contents\\nExample\\n\\nThis example is intended to help you compare the cost of investing in the shares of the Fund with the cost of investing in other mutual funds. 4 \\tThrough July 31, 2020, IICO, IDI and/or WISC have contractually agreed to reimburse sufficient management fees, 12b-1 fees and/or shareholder servicing fees to cap the total annual ordinary fund operating expenses (which would exclude interest, taxes, brokerage commissions, acquired fund fees and expenses and extraordinary expenses, if any) as follows: Class A shares at 1.04%; Class B shares at 2.13%; Class E shares at 1.13%; and Class I shares and Class Y shares at 0.84%. 5 \\tThrough July 31, 2020, IDI and/or WISC have contractually agreed to reimburse sufficient 12b-1 and/or shareholder servicing fees to ensure that the total annual ordinary fund operating expenses of the Class Y shares do not exceed the total annual ordinary fund operating expenses of the Class A shares, as calculated at the end of each month.\",\n          \"A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when fund shares are held in a taxable account. An investment in the fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency. Ask your investment professional or visit your intermediary's web site for more information. Box 770001\\nCincinnati, OH 45277-0035\\nOvernight Express:\\nFidelity Investments\\n100 Crosby Parkway\\nCovington, KY 41015\\nTDD- Service for the Deaf and Hearing Impaired\\n\\n1-800-544-0118\\n\\nThe price to buy one share is its net asset value per share (NAV). Currently, the Board of Trustees of the fund has not authorized such payments for shares of the fund.\",\n          \"2\\n\\n \\n\\nAnnual Total Returns\\n\\nThe following bar chart and table are intended to help you understand the risks of investing in the Fund. 3\\n\\n \\n\\nAverage Annual Total Returns for Periods Ended December 31, 2017\\t \\t \\n \\t \\t \\tSince\\n \\t \\t \\tInception\\n \\t \\t \\t(Dec. 28,\\n \\t1 Year\\t5 Years\\t2009)\\nVanguard Short-Term Treasury Index Fund Admiral Shares\\t \\t \\t \\nReturn Before Taxes\\t0.40%\\t0.50%\\t0.81%\\nReturn After Taxes on Distributions\\t\\u20130.08\\t0.20\\t0.54\\nReturn After Taxes on Distributions and Sale of Fund Shares\\t0.22\\t0.25\\t0.51\\nBloomberg Barclays US Treasury 1-3 Year Bond Index\\t \\t \\t \\n(reflects no deduction for fees, expenses, or taxes)\\t0.42%\\t0.57%\\t0.89%\\n \\n\\nActual after-tax returns depend on your tax situation and may differ from those shown in the preceding table. 4\\n\\n \\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase or redeem shares online through our website (vanguard.com), by mail (The Vanguard Group, P.O. A higher portfolio turnover rate may indicate higher transaction costs and may result in more taxes when Fund shares are held in a taxable account. All of the Fund\\u2019s investments will be selected through the sampling process, and under normal circumstances, at least 80% of the Fund\\u2019s assets will be invested in bonds included in the Index.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_match\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 255,\n        \"samples\": [\n          \"Tracking error may occur because of differences between the securities held in the Fund\\u2019s portfolio and those included in the Emerging Markets Underlying Index, pricing differences (including differences between a security\\u2019s price at the local market close and the Fund\\u2019s valuation of a security at the time of calculation of the Fund\\u2019s NAV), transaction costs, the Fund\\u2019s holding of cash, differences in timing of the accrual of dividends or interest, tax gains or losses, changes to the Emerging Markets Underlying Index or the need to meet various new or existing regulatory requirements. Under the representative sampling technique, the investment manager will select securities that collectively have an investment profile similar to that of the Emerging Markets Underlying Index, including securities that resemble those included in the Emerging Markets Underlying Index in terms of risk factors, performance attributes and other characteristics, such as market capitalization and industry weightings. When there are more sellers than buyers, prices tend to fall. While MSCI provides descriptions of what the Emerging Markets Underlying Index is designed to achieve, MSCI does not guarantee the quality, accuracy or completeness of data in respect of its indices, and does not guarantee that the Emerging Markets Underlying Index will be in line with the described index methodology. You can obtain updated performance information at libertyshares.com or by calling (800) DIAL BEN/342-5236. You may also incur usual and customary brokerage commissions when buying or selling shares of the Fund, which are not reflected in the Example that follows.\",\n          \"This example does not include any fees paid at the fee-based account or plan level. This example helps compare the cost of investing in the fund with the cost of investing in other funds. This example illustrates the effect of fees and expenses, but is not meant to suggest actual or expected fees and expenses or returns, all of which may vary. Using fundamental analysis of factors such as each issuer's financial condition and industry position, as well as market and economic conditions, to select investments. You could lose money by investing in the fund. You may buy or sell shares in various ways:\\n\\nInternet\\n\\nPlan Accounts:\\n\\nwww.401k.com\\t\\nAll Other Accounts:\\n\\nwww.fidelity.com\\nPhone\\n\\nPlan Accounts:\\n\\nFor Individual Accounts (investing through a retirement plan sponsor or other institution), refer to your plan materials or contact that institution directly.\",\n          \"\\u220e\\t \\tLow-Rated Securities Risk. \\u220e\\t \\tManagement Risk. \\u220e\\t \\tMarket Risk. \\u220e\\t \\tMortgage-Backed and Asset-Backed Securities Risk. \\u220e\\t \\tU.S. Government Securities Risk. \\u220e\\t \\tValue Stock Risk.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOAXUuCpv6-h",
        "outputId": "cef05c4a-6e6d-4015-a7f5-323a4cec3140"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask',\n",
              " 'input_ids token_type_ids attention_mask']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3YP2QEev9DA",
        "outputId": "5758b2bc-27af-4cf5-91e2-1ad585a2d79d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pa2n9DywMEF",
        "outputId": "6b7d2f69-acd8-42b2-851b-a137291ef372"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149    MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...\n",
              "436    INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...\n",
              "394    Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...\n",
              "440    INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...\n",
              "330    Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...\n",
              "                             ...                        \n",
              "152    MainStay VP MacKay International Equity Portfo...\n",
              "158    MainStay VP MacKay Small Cap Core Portfolio\\n\\...\n",
              "374    Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...\n",
              "308    PIMCO Gurtin California Municipal Intermediate...\n",
              "287    Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...\n",
              "Name: summary, Length: 294, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1F10N1SwRC1",
        "outputId": "eea73862-48dd-41d0-c1d9-2eb80c1590a5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149          Equity Long Only (Low Risk)\n",
              "436             Balanced Fund (Low Risk)\n",
              "394    Fixed Income Long Only (Low Risk)\n",
              "440          Equity Long Only (Low Risk)\n",
              "330             Balanced Fund (Low Risk)\n",
              "                     ...                \n",
              "152          Equity Long Only (Low Risk)\n",
              "158          Equity Long Only (Low Risk)\n",
              "374          Equity Long Only (Low Risk)\n",
              "308    Fixed Income Long Only (Low Risk)\n",
              "287          Equity Long Only (Low Risk)\n",
              "Name: Ivestment Strategy, Length: 294, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_CNN_model():\n",
        "    CNN = Sequential()\n",
        "    # The Embedding layer takes the embedding matrix as an argument and transform the inputed the sequences of index to sequences of vectors.\n",
        "    CNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False))\n",
        "\n",
        "    CNN.add(Convolution1D(64, 5, activation = 'relu'))\n",
        "    CNN.add(MaxPooling1D(pool_size = 5))\n",
        "\n",
        "    CNN.add(Convolution1D(32, 5, activation = 'relu'))\n",
        "    CNN.add(MaxPooling1D(pool_size = 5))\n",
        "\n",
        "    CNN.add(Flatten())\n",
        "    CNN.add(Dense(units = 128 , activation = 'relu'))\n",
        "    CNN.add(Dropout(0.5))\n",
        "    CNN.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "    CNN.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    return CNN\n",
        "\n",
        "CNN_model = create_CNN_model()\n",
        "CNN_history = CNN_model.fit(feature_train, label_train_y, epochs=800, batch_size=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgkctFJIcCRh",
        "outputId": "a1de4991-b274-4744-84f2-2dec8bbb447b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "3/3 [==============================] - 3s 45ms/step - loss: 1.0726 - accuracy: 0.4422\n",
            "Epoch 2/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9968 - accuracy: 0.5612\n",
            "Epoch 3/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9982 - accuracy: 0.5680\n",
            "Epoch 4/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9756 - accuracy: 0.5612\n",
            "Epoch 5/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9894 - accuracy: 0.5714\n",
            "Epoch 6/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9825 - accuracy: 0.5714\n",
            "Epoch 7/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9923 - accuracy: 0.5748\n",
            "Epoch 8/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9836 - accuracy: 0.5748\n",
            "Epoch 9/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9861 - accuracy: 0.5714\n",
            "Epoch 10/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.9737 - accuracy: 0.5714\n",
            "Epoch 11/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9638 - accuracy: 0.5748\n",
            "Epoch 12/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9613 - accuracy: 0.5714\n",
            "Epoch 13/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9568 - accuracy: 0.5714\n",
            "Epoch 14/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9513 - accuracy: 0.5748\n",
            "Epoch 15/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9375 - accuracy: 0.5748\n",
            "Epoch 16/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.9374 - accuracy: 0.5748\n",
            "Epoch 17/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.9274 - accuracy: 0.5884\n",
            "Epoch 18/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.9327 - accuracy: 0.6088\n",
            "Epoch 19/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.9165 - accuracy: 0.5952\n",
            "Epoch 20/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.9266 - accuracy: 0.6054\n",
            "Epoch 21/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9026 - accuracy: 0.6156\n",
            "Epoch 22/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.8866 - accuracy: 0.6122\n",
            "Epoch 23/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8689 - accuracy: 0.6224\n",
            "Epoch 24/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8591 - accuracy: 0.6395\n",
            "Epoch 25/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8686 - accuracy: 0.6224\n",
            "Epoch 26/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8462 - accuracy: 0.6429\n",
            "Epoch 27/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8493 - accuracy: 0.6395\n",
            "Epoch 28/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8252 - accuracy: 0.6429\n",
            "Epoch 29/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8164 - accuracy: 0.6463\n",
            "Epoch 30/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8203 - accuracy: 0.6565\n",
            "Epoch 31/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7930 - accuracy: 0.6701\n",
            "Epoch 32/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7733 - accuracy: 0.6803\n",
            "Epoch 33/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7759 - accuracy: 0.6701\n",
            "Epoch 34/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7565 - accuracy: 0.7041\n",
            "Epoch 35/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7526 - accuracy: 0.6803\n",
            "Epoch 36/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7394 - accuracy: 0.6871\n",
            "Epoch 37/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7288 - accuracy: 0.6803\n",
            "Epoch 38/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7046 - accuracy: 0.7415\n",
            "Epoch 39/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6665 - accuracy: 0.7313\n",
            "Epoch 40/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6845 - accuracy: 0.7279\n",
            "Epoch 41/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6494 - accuracy: 0.7313\n",
            "Epoch 42/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6493 - accuracy: 0.7279\n",
            "Epoch 43/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6139 - accuracy: 0.7585\n",
            "Epoch 44/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6111 - accuracy: 0.7653\n",
            "Epoch 45/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5750 - accuracy: 0.7687\n",
            "Epoch 46/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5869 - accuracy: 0.7823\n",
            "Epoch 47/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5680 - accuracy: 0.7755\n",
            "Epoch 48/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5456 - accuracy: 0.7925\n",
            "Epoch 49/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5360 - accuracy: 0.7891\n",
            "Epoch 50/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4928 - accuracy: 0.8435\n",
            "Epoch 51/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5199 - accuracy: 0.8231\n",
            "Epoch 52/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4681 - accuracy: 0.8265\n",
            "Epoch 53/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4710 - accuracy: 0.8401\n",
            "Epoch 54/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.4375 - accuracy: 0.8469\n",
            "Epoch 55/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.4344 - accuracy: 0.8401\n",
            "Epoch 56/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.4308 - accuracy: 0.8605\n",
            "Epoch 57/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4147 - accuracy: 0.8469\n",
            "Epoch 58/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3871 - accuracy: 0.8707\n",
            "Epoch 59/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3849 - accuracy: 0.8605\n",
            "Epoch 60/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3893 - accuracy: 0.8571\n",
            "Epoch 61/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3426 - accuracy: 0.8878\n",
            "Epoch 62/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3538 - accuracy: 0.8810\n",
            "Epoch 63/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3424 - accuracy: 0.8741\n",
            "Epoch 64/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3517 - accuracy: 0.8844\n",
            "Epoch 65/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3345 - accuracy: 0.8912\n",
            "Epoch 66/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3402 - accuracy: 0.8741\n",
            "Epoch 67/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2985 - accuracy: 0.8844\n",
            "Epoch 68/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2936 - accuracy: 0.8912\n",
            "Epoch 69/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3049 - accuracy: 0.8946\n",
            "Epoch 70/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2982 - accuracy: 0.8844\n",
            "Epoch 71/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2754 - accuracy: 0.9014\n",
            "Epoch 72/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2711 - accuracy: 0.9184\n",
            "Epoch 73/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2730 - accuracy: 0.9048\n",
            "Epoch 74/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2672 - accuracy: 0.9082\n",
            "Epoch 75/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2730 - accuracy: 0.9014\n",
            "Epoch 76/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2569 - accuracy: 0.8946\n",
            "Epoch 77/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2369 - accuracy: 0.9218\n",
            "Epoch 78/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2689 - accuracy: 0.8980\n",
            "Epoch 79/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2348 - accuracy: 0.9184\n",
            "Epoch 80/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2262 - accuracy: 0.9286\n",
            "Epoch 81/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2228 - accuracy: 0.9150\n",
            "Epoch 82/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2203 - accuracy: 0.9150\n",
            "Epoch 83/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2232 - accuracy: 0.9286\n",
            "Epoch 84/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2302 - accuracy: 0.9014\n",
            "Epoch 85/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2160 - accuracy: 0.9286\n",
            "Epoch 86/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2401 - accuracy: 0.9082\n",
            "Epoch 87/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1947 - accuracy: 0.9456\n",
            "Epoch 88/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2289 - accuracy: 0.9116\n",
            "Epoch 89/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1910 - accuracy: 0.9388\n",
            "Epoch 90/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2107 - accuracy: 0.9286\n",
            "Epoch 91/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2036 - accuracy: 0.9252\n",
            "Epoch 92/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2048 - accuracy: 0.9286\n",
            "Epoch 93/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1855 - accuracy: 0.9456\n",
            "Epoch 94/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1793 - accuracy: 0.9490\n",
            "Epoch 95/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1812 - accuracy: 0.9354\n",
            "Epoch 96/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1818 - accuracy: 0.9422\n",
            "Epoch 97/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1895 - accuracy: 0.9286\n",
            "Epoch 98/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1703 - accuracy: 0.9422\n",
            "Epoch 99/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1629 - accuracy: 0.9456\n",
            "Epoch 100/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1554 - accuracy: 0.9456\n",
            "Epoch 101/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1558 - accuracy: 0.9456\n",
            "Epoch 102/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1583 - accuracy: 0.9524\n",
            "Epoch 103/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1655 - accuracy: 0.9320\n",
            "Epoch 104/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1569 - accuracy: 0.9524\n",
            "Epoch 105/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1401 - accuracy: 0.9524\n",
            "Epoch 106/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1470 - accuracy: 0.9524\n",
            "Epoch 107/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1627 - accuracy: 0.9320\n",
            "Epoch 108/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1462 - accuracy: 0.9490\n",
            "Epoch 109/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1384 - accuracy: 0.9490\n",
            "Epoch 110/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1473 - accuracy: 0.9388\n",
            "Epoch 111/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1526 - accuracy: 0.9388\n",
            "Epoch 112/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1542 - accuracy: 0.9456\n",
            "Epoch 113/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1404 - accuracy: 0.9524\n",
            "Epoch 114/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1376 - accuracy: 0.9524\n",
            "Epoch 115/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1466 - accuracy: 0.9456\n",
            "Epoch 116/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1447 - accuracy: 0.9422\n",
            "Epoch 117/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1610 - accuracy: 0.9320\n",
            "Epoch 118/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1339 - accuracy: 0.9524\n",
            "Epoch 119/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1549 - accuracy: 0.9388\n",
            "Epoch 120/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1378 - accuracy: 0.9524\n",
            "Epoch 121/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1314 - accuracy: 0.9524\n",
            "Epoch 122/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1292 - accuracy: 0.9490\n",
            "Epoch 123/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1377 - accuracy: 0.9490\n",
            "Epoch 124/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1323 - accuracy: 0.9558\n",
            "Epoch 125/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1198 - accuracy: 0.9558\n",
            "Epoch 126/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1267 - accuracy: 0.9524\n",
            "Epoch 127/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9388\n",
            "Epoch 128/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1078 - accuracy: 0.9592\n",
            "Epoch 129/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1178 - accuracy: 0.9592\n",
            "Epoch 130/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1194 - accuracy: 0.9524\n",
            "Epoch 131/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1176 - accuracy: 0.9592\n",
            "Epoch 132/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1294 - accuracy: 0.9524\n",
            "Epoch 133/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1192 - accuracy: 0.9558\n",
            "Epoch 134/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1195 - accuracy: 0.9558\n",
            "Epoch 135/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9592\n",
            "Epoch 136/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1331 - accuracy: 0.9456\n",
            "Epoch 137/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1246 - accuracy: 0.9558\n",
            "Epoch 138/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1022 - accuracy: 0.9592\n",
            "Epoch 139/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1121 - accuracy: 0.9558\n",
            "Epoch 140/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1042 - accuracy: 0.9660\n",
            "Epoch 141/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1241 - accuracy: 0.9592\n",
            "Epoch 142/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1148 - accuracy: 0.9524\n",
            "Epoch 143/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1249 - accuracy: 0.9456\n",
            "Epoch 144/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1020 - accuracy: 0.9592\n",
            "Epoch 145/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1147 - accuracy: 0.9524\n",
            "Epoch 146/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1099 - accuracy: 0.9626\n",
            "Epoch 147/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0996 - accuracy: 0.9592\n",
            "Epoch 148/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9558\n",
            "Epoch 149/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0977 - accuracy: 0.9660\n",
            "Epoch 150/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0999 - accuracy: 0.9558\n",
            "Epoch 151/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1052 - accuracy: 0.9558\n",
            "Epoch 152/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1210 - accuracy: 0.9456\n",
            "Epoch 153/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9558\n",
            "Epoch 154/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1018 - accuracy: 0.9626\n",
            "Epoch 155/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9592\n",
            "Epoch 156/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1143 - accuracy: 0.9558\n",
            "Epoch 157/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1146 - accuracy: 0.9558\n",
            "Epoch 158/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0989 - accuracy: 0.9626\n",
            "Epoch 159/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1003 - accuracy: 0.9558\n",
            "Epoch 160/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1073 - accuracy: 0.9660\n",
            "Epoch 161/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1059 - accuracy: 0.9524\n",
            "Epoch 162/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0975 - accuracy: 0.9592\n",
            "Epoch 163/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0958 - accuracy: 0.9558\n",
            "Epoch 164/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1050 - accuracy: 0.9558\n",
            "Epoch 165/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0959 - accuracy: 0.9592\n",
            "Epoch 166/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.9592\n",
            "Epoch 167/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0993 - accuracy: 0.9592\n",
            "Epoch 168/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0939 - accuracy: 0.9558\n",
            "Epoch 169/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0891 - accuracy: 0.9592\n",
            "Epoch 170/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0946 - accuracy: 0.9558\n",
            "Epoch 171/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0938 - accuracy: 0.9762\n",
            "Epoch 172/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0990 - accuracy: 0.9660\n",
            "Epoch 173/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0903 - accuracy: 0.9694\n",
            "Epoch 174/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0945 - accuracy: 0.9660\n",
            "Epoch 175/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0904 - accuracy: 0.9626\n",
            "Epoch 176/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0959 - accuracy: 0.9592\n",
            "Epoch 177/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0879 - accuracy: 0.9660\n",
            "Epoch 178/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0912 - accuracy: 0.9694\n",
            "Epoch 179/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0847 - accuracy: 0.9728\n",
            "Epoch 180/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0947 - accuracy: 0.9558\n",
            "Epoch 181/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0795 - accuracy: 0.9728\n",
            "Epoch 182/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0915 - accuracy: 0.9626\n",
            "Epoch 183/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.9524\n",
            "Epoch 184/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0978 - accuracy: 0.9626\n",
            "Epoch 185/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0902 - accuracy: 0.9592\n",
            "Epoch 186/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0745 - accuracy: 0.9728\n",
            "Epoch 187/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0898 - accuracy: 0.9694\n",
            "Epoch 188/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0895 - accuracy: 0.9694\n",
            "Epoch 189/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0850 - accuracy: 0.9592\n",
            "Epoch 190/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0775 - accuracy: 0.9728\n",
            "Epoch 191/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0937 - accuracy: 0.9694\n",
            "Epoch 192/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0762 - accuracy: 0.9694\n",
            "Epoch 193/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0873 - accuracy: 0.9626\n",
            "Epoch 194/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0856 - accuracy: 0.9728\n",
            "Epoch 195/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0769 - accuracy: 0.9762\n",
            "Epoch 196/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0821 - accuracy: 0.9728\n",
            "Epoch 197/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0872 - accuracy: 0.9660\n",
            "Epoch 198/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0752 - accuracy: 0.9762\n",
            "Epoch 199/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0767 - accuracy: 0.9762\n",
            "Epoch 200/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0871 - accuracy: 0.9694\n",
            "Epoch 201/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0728 - accuracy: 0.9796\n",
            "Epoch 202/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0742 - accuracy: 0.9762\n",
            "Epoch 203/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0740 - accuracy: 0.9796\n",
            "Epoch 204/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0787 - accuracy: 0.9728\n",
            "Epoch 205/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0847 - accuracy: 0.9762\n",
            "Epoch 206/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0767 - accuracy: 0.9694\n",
            "Epoch 207/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0672 - accuracy: 0.9762\n",
            "Epoch 208/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0783 - accuracy: 0.9728\n",
            "Epoch 209/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0647 - accuracy: 0.9830\n",
            "Epoch 210/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0688 - accuracy: 0.9660\n",
            "Epoch 211/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0807 - accuracy: 0.9728\n",
            "Epoch 212/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.9830\n",
            "Epoch 213/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0704 - accuracy: 0.9660\n",
            "Epoch 214/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0706 - accuracy: 0.9660\n",
            "Epoch 215/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0700 - accuracy: 0.9796\n",
            "Epoch 216/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0690 - accuracy: 0.9830\n",
            "Epoch 217/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0631 - accuracy: 0.9728\n",
            "Epoch 218/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9796\n",
            "Epoch 219/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0649 - accuracy: 0.9830\n",
            "Epoch 220/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0706 - accuracy: 0.9694\n",
            "Epoch 221/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0783 - accuracy: 0.9660\n",
            "Epoch 222/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0575 - accuracy: 0.9864\n",
            "Epoch 223/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.9864\n",
            "Epoch 224/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0620 - accuracy: 0.9728\n",
            "Epoch 225/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0587 - accuracy: 0.9796\n",
            "Epoch 226/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0707 - accuracy: 0.9762\n",
            "Epoch 227/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0587 - accuracy: 0.9796\n",
            "Epoch 228/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0717 - accuracy: 0.9660\n",
            "Epoch 229/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0584 - accuracy: 0.9796\n",
            "Epoch 230/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0649 - accuracy: 0.9864\n",
            "Epoch 231/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0593 - accuracy: 0.9796\n",
            "Epoch 232/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0547 - accuracy: 0.9864\n",
            "Epoch 233/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0543 - accuracy: 0.9762\n",
            "Epoch 234/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0647 - accuracy: 0.9694\n",
            "Epoch 235/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0645 - accuracy: 0.9796\n",
            "Epoch 236/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0535 - accuracy: 0.9864\n",
            "Epoch 237/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0433 - accuracy: 0.9830\n",
            "Epoch 238/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0569 - accuracy: 0.9796\n",
            "Epoch 239/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0453 - accuracy: 0.9864\n",
            "Epoch 240/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0551 - accuracy: 0.9830\n",
            "Epoch 241/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0552 - accuracy: 0.9864\n",
            "Epoch 242/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0555 - accuracy: 0.9830\n",
            "Epoch 243/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0495 - accuracy: 0.9830\n",
            "Epoch 244/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0563 - accuracy: 0.9762\n",
            "Epoch 245/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0524 - accuracy: 0.9796\n",
            "Epoch 246/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0655 - accuracy: 0.9864\n",
            "Epoch 247/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0521 - accuracy: 0.9864\n",
            "Epoch 248/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0516 - accuracy: 0.9796\n",
            "Epoch 249/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0498 - accuracy: 0.9796\n",
            "Epoch 250/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0499 - accuracy: 0.9830\n",
            "Epoch 251/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 0.9830\n",
            "Epoch 252/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0431 - accuracy: 0.9932\n",
            "Epoch 253/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0479 - accuracy: 0.9864\n",
            "Epoch 254/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0424 - accuracy: 0.9932\n",
            "Epoch 255/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0423 - accuracy: 0.9796\n",
            "Epoch 256/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0488 - accuracy: 0.9762\n",
            "Epoch 257/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0502 - accuracy: 0.9796\n",
            "Epoch 258/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0508 - accuracy: 0.9830\n",
            "Epoch 259/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 0.9830\n",
            "Epoch 260/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0492 - accuracy: 0.9864\n",
            "Epoch 261/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0484 - accuracy: 0.9830\n",
            "Epoch 262/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0476 - accuracy: 0.9830\n",
            "Epoch 263/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0625 - accuracy: 0.9796\n",
            "Epoch 264/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0556 - accuracy: 0.9762\n",
            "Epoch 265/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0385 - accuracy: 0.9932\n",
            "Epoch 266/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0558 - accuracy: 0.9762\n",
            "Epoch 267/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0454 - accuracy: 0.9898\n",
            "Epoch 268/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0422 - accuracy: 0.9898\n",
            "Epoch 269/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0477 - accuracy: 0.9830\n",
            "Epoch 270/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0603 - accuracy: 0.9796\n",
            "Epoch 271/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0396 - accuracy: 0.9864\n",
            "Epoch 272/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0487 - accuracy: 0.9796\n",
            "Epoch 273/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0433 - accuracy: 0.9762\n",
            "Epoch 274/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0528 - accuracy: 0.9830\n",
            "Epoch 275/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0551 - accuracy: 0.9762\n",
            "Epoch 276/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0432 - accuracy: 0.9898\n",
            "Epoch 277/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0413 - accuracy: 0.9898\n",
            "Epoch 278/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0469 - accuracy: 0.9864\n",
            "Epoch 279/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0442 - accuracy: 0.9898\n",
            "Epoch 280/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0400 - accuracy: 0.9898\n",
            "Epoch 281/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0537 - accuracy: 0.9796\n",
            "Epoch 282/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0344 - accuracy: 0.9864\n",
            "Epoch 283/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0343 - accuracy: 0.9864\n",
            "Epoch 284/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0424 - accuracy: 0.9830\n",
            "Epoch 285/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0369 - accuracy: 0.9898\n",
            "Epoch 286/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0465 - accuracy: 0.9762\n",
            "Epoch 287/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.9898\n",
            "Epoch 288/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0327 - accuracy: 0.9864\n",
            "Epoch 289/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0400 - accuracy: 0.9796\n",
            "Epoch 290/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0308 - accuracy: 0.9932\n",
            "Epoch 291/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0394 - accuracy: 0.9864\n",
            "Epoch 292/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0305 - accuracy: 0.9898\n",
            "Epoch 293/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0346 - accuracy: 0.9898\n",
            "Epoch 294/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0397 - accuracy: 0.9830\n",
            "Epoch 295/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0293 - accuracy: 0.9898\n",
            "Epoch 296/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0363 - accuracy: 0.9830\n",
            "Epoch 297/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0410 - accuracy: 0.9762\n",
            "Epoch 298/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0330 - accuracy: 0.9864\n",
            "Epoch 299/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0334 - accuracy: 0.9864\n",
            "Epoch 300/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0382 - accuracy: 0.9932\n",
            "Epoch 301/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0379 - accuracy: 0.9864\n",
            "Epoch 302/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0349 - accuracy: 0.9898\n",
            "Epoch 303/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0310 - accuracy: 0.9864\n",
            "Epoch 304/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 0.9830\n",
            "Epoch 305/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0458 - accuracy: 0.9796\n",
            "Epoch 306/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0329 - accuracy: 0.9864\n",
            "Epoch 307/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0341 - accuracy: 0.9932\n",
            "Epoch 308/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0316 - accuracy: 0.9898\n",
            "Epoch 309/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0422 - accuracy: 0.9864\n",
            "Epoch 310/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.9966\n",
            "Epoch 311/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0305 - accuracy: 0.9932\n",
            "Epoch 312/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0388 - accuracy: 0.9796\n",
            "Epoch 313/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0306 - accuracy: 0.9864\n",
            "Epoch 314/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0304 - accuracy: 0.9898\n",
            "Epoch 315/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0359 - accuracy: 0.9898\n",
            "Epoch 316/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0271 - accuracy: 0.9932\n",
            "Epoch 317/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0353 - accuracy: 0.9864\n",
            "Epoch 318/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0299 - accuracy: 0.9898\n",
            "Epoch 319/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0299 - accuracy: 0.9898\n",
            "Epoch 320/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0273 - accuracy: 0.9966\n",
            "Epoch 321/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0387 - accuracy: 0.9864\n",
            "Epoch 322/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0293 - accuracy: 0.9932\n",
            "Epoch 323/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0277 - accuracy: 0.9932\n",
            "Epoch 324/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0261 - accuracy: 0.9966\n",
            "Epoch 325/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0314 - accuracy: 0.9932\n",
            "Epoch 326/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0229 - accuracy: 0.9932\n",
            "Epoch 327/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0311 - accuracy: 0.9830\n",
            "Epoch 328/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0329 - accuracy: 0.9830\n",
            "Epoch 329/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0259 - accuracy: 0.9898\n",
            "Epoch 330/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0200 - accuracy: 0.9966\n",
            "Epoch 331/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0278 - accuracy: 0.9864\n",
            "Epoch 332/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0241 - accuracy: 0.9932\n",
            "Epoch 333/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0282 - accuracy: 0.9932\n",
            "Epoch 334/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0259 - accuracy: 0.9932\n",
            "Epoch 335/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0216 - accuracy: 0.9932\n",
            "Epoch 336/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0262 - accuracy: 0.9932\n",
            "Epoch 337/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0283 - accuracy: 0.9864\n",
            "Epoch 338/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0269 - accuracy: 0.9932\n",
            "Epoch 339/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0272 - accuracy: 0.9966\n",
            "Epoch 340/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0239 - accuracy: 0.9898\n",
            "Epoch 341/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0334 - accuracy: 0.9864\n",
            "Epoch 342/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0219 - accuracy: 0.9966\n",
            "Epoch 343/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0286 - accuracy: 0.9898\n",
            "Epoch 344/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0247 - accuracy: 0.9898\n",
            "Epoch 345/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0219 - accuracy: 0.9898\n",
            "Epoch 346/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0231 - accuracy: 0.9932\n",
            "Epoch 347/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0257 - accuracy: 0.9932\n",
            "Epoch 348/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0297 - accuracy: 0.9864\n",
            "Epoch 349/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0241 - accuracy: 0.9966\n",
            "Epoch 350/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0242 - accuracy: 0.9932\n",
            "Epoch 351/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0184 - accuracy: 0.9932\n",
            "Epoch 352/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0291 - accuracy: 0.9898\n",
            "Epoch 353/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 0.9932\n",
            "Epoch 354/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 0.9898\n",
            "Epoch 355/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0211 - accuracy: 0.9966\n",
            "Epoch 356/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0270 - accuracy: 0.9898\n",
            "Epoch 357/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0202 - accuracy: 0.9932\n",
            "Epoch 358/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0214 - accuracy: 0.9966\n",
            "Epoch 359/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0220 - accuracy: 0.9898\n",
            "Epoch 360/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0227 - accuracy: 0.9966\n",
            "Epoch 361/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0224 - accuracy: 0.9898\n",
            "Epoch 362/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0224 - accuracy: 0.9932\n",
            "Epoch 363/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0259 - accuracy: 0.9898\n",
            "Epoch 364/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0230 - accuracy: 0.9932\n",
            "Epoch 365/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0249 - accuracy: 0.9932\n",
            "Epoch 366/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0219 - accuracy: 0.9966\n",
            "Epoch 367/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0206 - accuracy: 0.9932\n",
            "Epoch 368/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0203 - accuracy: 0.9932\n",
            "Epoch 369/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0274 - accuracy: 0.9864\n",
            "Epoch 370/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0285 - accuracy: 0.9932\n",
            "Epoch 371/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0232 - accuracy: 0.9966\n",
            "Epoch 372/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0277 - accuracy: 0.9932\n",
            "Epoch 373/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0217 - accuracy: 0.9932\n",
            "Epoch 374/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 0.9898\n",
            "Epoch 375/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0231 - accuracy: 0.9898\n",
            "Epoch 376/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0206 - accuracy: 0.9966\n",
            "Epoch 377/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0181 - accuracy: 0.9966\n",
            "Epoch 378/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0196 - accuracy: 0.9932\n",
            "Epoch 379/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 0.9966\n",
            "Epoch 380/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0221 - accuracy: 0.9932\n",
            "Epoch 381/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0254 - accuracy: 0.9898\n",
            "Epoch 382/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0193 - accuracy: 0.9932\n",
            "Epoch 383/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0220 - accuracy: 0.9932\n",
            "Epoch 384/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0219 - accuracy: 0.9898\n",
            "Epoch 385/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0333 - accuracy: 0.9864\n",
            "Epoch 386/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0353 - accuracy: 0.9898\n",
            "Epoch 387/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 0.9966\n",
            "Epoch 388/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0185 - accuracy: 0.9966\n",
            "Epoch 389/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0236 - accuracy: 0.9898\n",
            "Epoch 390/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0165 - accuracy: 0.9932\n",
            "Epoch 391/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0222 - accuracy: 0.9966\n",
            "Epoch 392/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0200 - accuracy: 0.9966\n",
            "Epoch 393/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0176 - accuracy: 0.9932\n",
            "Epoch 394/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0157 - accuracy: 0.9932\n",
            "Epoch 395/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0205 - accuracy: 0.9932\n",
            "Epoch 396/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 0.9966\n",
            "Epoch 397/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0240 - accuracy: 0.9932\n",
            "Epoch 398/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.9966\n",
            "Epoch 399/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0173 - accuracy: 1.0000\n",
            "Epoch 400/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0173 - accuracy: 0.9932\n",
            "Epoch 401/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0201 - accuracy: 0.9898\n",
            "Epoch 402/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 0.9932\n",
            "Epoch 403/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0174 - accuracy: 0.9966\n",
            "Epoch 404/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0265 - accuracy: 0.9898\n",
            "Epoch 405/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0163 - accuracy: 0.9932\n",
            "Epoch 406/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0201 - accuracy: 0.9966\n",
            "Epoch 407/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0198 - accuracy: 0.9898\n",
            "Epoch 408/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 0.9966\n",
            "Epoch 409/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0220 - accuracy: 0.9864\n",
            "Epoch 410/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0239 - accuracy: 0.9898\n",
            "Epoch 411/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0238 - accuracy: 0.9932\n",
            "Epoch 412/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0223 - accuracy: 0.9932\n",
            "Epoch 413/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0224 - accuracy: 0.9864\n",
            "Epoch 414/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0211 - accuracy: 0.9932\n",
            "Epoch 415/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0215 - accuracy: 0.9898\n",
            "Epoch 416/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0160 - accuracy: 0.9932\n",
            "Epoch 417/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0220 - accuracy: 0.9898\n",
            "Epoch 418/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0229 - accuracy: 0.9932\n",
            "Epoch 419/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0155 - accuracy: 0.9932\n",
            "Epoch 420/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0213 - accuracy: 0.9932\n",
            "Epoch 421/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0185 - accuracy: 0.9898\n",
            "Epoch 422/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0160 - accuracy: 0.9932\n",
            "Epoch 423/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0178 - accuracy: 0.9932\n",
            "Epoch 424/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.9966\n",
            "Epoch 425/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 0.9932\n",
            "Epoch 426/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 427/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 0.9898\n",
            "Epoch 428/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 0.9966\n",
            "Epoch 429/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0202 - accuracy: 0.9898\n",
            "Epoch 430/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0150 - accuracy: 0.9932\n",
            "Epoch 431/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0180 - accuracy: 0.9932\n",
            "Epoch 432/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 0.9966\n",
            "Epoch 433/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9966\n",
            "Epoch 434/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 0.9898\n",
            "Epoch 435/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0180 - accuracy: 0.9898\n",
            "Epoch 436/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0215 - accuracy: 0.9932\n",
            "Epoch 437/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 0.9966\n",
            "Epoch 438/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 1.0000\n",
            "Epoch 439/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0179 - accuracy: 0.9932\n",
            "Epoch 440/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0227 - accuracy: 0.9898\n",
            "Epoch 441/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 442/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.9966\n",
            "Epoch 443/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0166 - accuracy: 0.9932\n",
            "Epoch 444/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 0.9932\n",
            "Epoch 445/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 0.9932\n",
            "Epoch 446/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 0.9932\n",
            "Epoch 447/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0160 - accuracy: 0.9932\n",
            "Epoch 448/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 0.9932\n",
            "Epoch 449/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0162 - accuracy: 0.9898\n",
            "Epoch 450/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 0.9966\n",
            "Epoch 451/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0194 - accuracy: 0.9898\n",
            "Epoch 452/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9932\n",
            "Epoch 453/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0178 - accuracy: 0.9932\n",
            "Epoch 454/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.9966\n",
            "Epoch 455/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0148 - accuracy: 0.9932\n",
            "Epoch 456/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0180 - accuracy: 0.9898\n",
            "Epoch 457/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 0.9932\n",
            "Epoch 458/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0148 - accuracy: 0.9966\n",
            "Epoch 459/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 0.9898\n",
            "Epoch 460/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0143 - accuracy: 0.9966\n",
            "Epoch 461/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0159 - accuracy: 0.9898\n",
            "Epoch 462/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0156 - accuracy: 0.9966\n",
            "Epoch 463/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0147 - accuracy: 0.9966\n",
            "Epoch 464/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0163 - accuracy: 0.9932\n",
            "Epoch 465/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0139 - accuracy: 0.9932\n",
            "Epoch 466/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0140 - accuracy: 0.9898\n",
            "Epoch 467/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.9966\n",
            "Epoch 468/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 0.9898\n",
            "Epoch 469/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0142 - accuracy: 0.9966\n",
            "Epoch 470/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0167 - accuracy: 0.9932\n",
            "Epoch 471/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 0.9898\n",
            "Epoch 472/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0193 - accuracy: 0.9898\n",
            "Epoch 473/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 0.9932\n",
            "Epoch 474/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 0.9932\n",
            "Epoch 475/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 0.9966\n",
            "Epoch 476/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.9966\n",
            "Epoch 477/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0153 - accuracy: 0.9898\n",
            "Epoch 478/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 0.9966\n",
            "Epoch 479/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0176 - accuracy: 0.9932\n",
            "Epoch 480/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.9932\n",
            "Epoch 481/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0141 - accuracy: 0.9932\n",
            "Epoch 482/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 0.9966\n",
            "Epoch 483/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0142 - accuracy: 0.9898\n",
            "Epoch 484/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 0.9932\n",
            "Epoch 485/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 0.9966\n",
            "Epoch 486/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0164 - accuracy: 0.9898\n",
            "Epoch 487/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0205 - accuracy: 0.9898\n",
            "Epoch 488/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 0.9898\n",
            "Epoch 489/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 0.9966\n",
            "Epoch 490/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 0.9966\n",
            "Epoch 491/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 0.9898\n",
            "Epoch 492/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 0.9932\n",
            "Epoch 493/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0125 - accuracy: 0.9966\n",
            "Epoch 494/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 0.9932\n",
            "Epoch 495/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0131 - accuracy: 0.9966\n",
            "Epoch 496/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 0.9966\n",
            "Epoch 497/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0125 - accuracy: 0.9932\n",
            "Epoch 498/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.9966\n",
            "Epoch 499/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0129 - accuracy: 0.9966\n",
            "Epoch 500/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 0.9966\n",
            "Epoch 501/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0130 - accuracy: 0.9932\n",
            "Epoch 502/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0146 - accuracy: 0.9898\n",
            "Epoch 503/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9966\n",
            "Epoch 504/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 0.9966\n",
            "Epoch 505/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0156 - accuracy: 0.9932\n",
            "Epoch 506/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.9966\n",
            "Epoch 507/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0143 - accuracy: 0.9898\n",
            "Epoch 508/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 0.9966\n",
            "Epoch 509/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0181 - accuracy: 0.9898\n",
            "Epoch 510/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 0.9966\n",
            "Epoch 511/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 0.9932\n",
            "Epoch 512/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.9966\n",
            "Epoch 513/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 0.9898\n",
            "Epoch 514/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 0.9932\n",
            "Epoch 515/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 0.9932\n",
            "Epoch 516/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 0.9966\n",
            "Epoch 517/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9898\n",
            "Epoch 518/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.9966\n",
            "Epoch 519/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9966\n",
            "Epoch 520/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 0.9932\n",
            "Epoch 521/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0167 - accuracy: 0.9932\n",
            "Epoch 522/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0156 - accuracy: 0.9932\n",
            "Epoch 523/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0223 - accuracy: 0.9898\n",
            "Epoch 524/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 0.9932\n",
            "Epoch 525/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0151 - accuracy: 0.9966\n",
            "Epoch 526/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0138 - accuracy: 0.9966\n",
            "Epoch 527/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 0.9966\n",
            "Epoch 528/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 0.9932\n",
            "Epoch 529/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 0.9966\n",
            "Epoch 530/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 0.9966\n",
            "Epoch 531/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.9966\n",
            "Epoch 532/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 0.9966\n",
            "Epoch 533/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0137 - accuracy: 0.9966\n",
            "Epoch 534/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 0.9966\n",
            "Epoch 535/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0164 - accuracy: 0.9932\n",
            "Epoch 536/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 0.9932\n",
            "Epoch 537/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0159 - accuracy: 0.9932\n",
            "Epoch 538/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9932\n",
            "Epoch 539/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0142 - accuracy: 0.9932\n",
            "Epoch 540/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 0.9966\n",
            "Epoch 541/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.9966\n",
            "Epoch 542/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 0.9966\n",
            "Epoch 543/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 544/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 0.9966\n",
            "Epoch 545/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 546/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 0.9932\n",
            "Epoch 547/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 0.9932\n",
            "Epoch 548/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0102 - accuracy: 0.9966\n",
            "Epoch 549/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 0.9932\n",
            "Epoch 550/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0106 - accuracy: 0.9932\n",
            "Epoch 551/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0198 - accuracy: 0.9932\n",
            "Epoch 552/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 553/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.9966\n",
            "Epoch 554/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 0.9966\n",
            "Epoch 555/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0086 - accuracy: 0.9966\n",
            "Epoch 556/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 0.9966\n",
            "Epoch 557/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.9966\n",
            "Epoch 558/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.9966\n",
            "Epoch 559/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.9966\n",
            "Epoch 560/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0100 - accuracy: 0.9966\n",
            "Epoch 561/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 0.9966\n",
            "Epoch 562/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0131 - accuracy: 0.9966\n",
            "Epoch 563/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.9932\n",
            "Epoch 564/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 0.9932\n",
            "Epoch 565/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 0.9966\n",
            "Epoch 566/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0085 - accuracy: 0.9966\n",
            "Epoch 567/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 0.9932\n",
            "Epoch 568/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 0.9966\n",
            "Epoch 569/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0149 - accuracy: 0.9932\n",
            "Epoch 570/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9932\n",
            "Epoch 571/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0193 - accuracy: 0.9932\n",
            "Epoch 572/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 0.9966\n",
            "Epoch 573/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0261 - accuracy: 0.9898\n",
            "Epoch 574/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0252 - accuracy: 0.9898\n",
            "Epoch 575/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0245 - accuracy: 0.9864\n",
            "Epoch 576/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0390 - accuracy: 0.9830\n",
            "Epoch 577/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0203 - accuracy: 0.9932\n",
            "Epoch 578/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0697 - accuracy: 0.9728\n",
            "Epoch 579/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0248 - accuracy: 0.9932\n",
            "Epoch 580/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0544 - accuracy: 0.9830\n",
            "Epoch 581/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0402 - accuracy: 0.9898\n",
            "Epoch 582/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0371 - accuracy: 0.9830\n",
            "Epoch 583/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0282 - accuracy: 0.9898\n",
            "Epoch 584/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0373 - accuracy: 0.9898\n",
            "Epoch 585/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0371 - accuracy: 0.9864\n",
            "Epoch 586/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0223 - accuracy: 0.9898\n",
            "Epoch 587/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0361 - accuracy: 0.9864\n",
            "Epoch 588/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0191 - accuracy: 0.9932\n",
            "Epoch 589/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0366 - accuracy: 0.9830\n",
            "Epoch 590/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0203 - accuracy: 0.9932\n",
            "Epoch 591/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0177 - accuracy: 0.9932\n",
            "Epoch 592/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0292 - accuracy: 0.9830\n",
            "Epoch 593/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0144 - accuracy: 0.9966\n",
            "Epoch 594/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0351 - accuracy: 0.9830\n",
            "Epoch 595/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0187 - accuracy: 1.0000\n",
            "Epoch 596/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 0.9932\n",
            "Epoch 597/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0149 - accuracy: 0.9966\n",
            "Epoch 598/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 0.9932\n",
            "Epoch 599/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0139 - accuracy: 0.9966\n",
            "Epoch 600/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0293 - accuracy: 0.9898\n",
            "Epoch 601/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0210 - accuracy: 0.9932\n",
            "Epoch 602/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0153 - accuracy: 0.9932\n",
            "Epoch 603/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0139 - accuracy: 0.9932\n",
            "Epoch 604/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 0.9932\n",
            "Epoch 605/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 0.9966\n",
            "Epoch 606/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 0.9932\n",
            "Epoch 607/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0205 - accuracy: 0.9864\n",
            "Epoch 608/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0138 - accuracy: 0.9966\n",
            "Epoch 609/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 610/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0173 - accuracy: 0.9966\n",
            "Epoch 611/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0150 - accuracy: 0.9966\n",
            "Epoch 612/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0134 - accuracy: 0.9966\n",
            "Epoch 613/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 0.9864\n",
            "Epoch 614/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0164 - accuracy: 0.9864\n",
            "Epoch 615/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0163 - accuracy: 0.9932\n",
            "Epoch 616/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0106 - accuracy: 1.0000\n",
            "Epoch 617/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 0.9966\n",
            "Epoch 618/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0127 - accuracy: 0.9966\n",
            "Epoch 619/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0130 - accuracy: 0.9966\n",
            "Epoch 620/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 0.9966\n",
            "Epoch 621/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 0.9966\n",
            "Epoch 622/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0181 - accuracy: 0.9932\n",
            "Epoch 623/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0137 - accuracy: 0.9932\n",
            "Epoch 624/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 0.9932\n",
            "Epoch 625/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 0.9966\n",
            "Epoch 626/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.9966\n",
            "Epoch 627/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0091 - accuracy: 0.9966\n",
            "Epoch 628/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 1.0000\n",
            "Epoch 629/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 630/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 0.9966\n",
            "Epoch 631/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.9932\n",
            "Epoch 632/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.9966\n",
            "Epoch 633/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0094 - accuracy: 0.9966\n",
            "Epoch 634/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 0.9966\n",
            "Epoch 635/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.9966\n",
            "Epoch 636/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 0.9966\n",
            "Epoch 637/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 638/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0155 - accuracy: 0.9898\n",
            "Epoch 639/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0168 - accuracy: 0.9932\n",
            "Epoch 640/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0182 - accuracy: 0.9898\n",
            "Epoch 641/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 0.9932\n",
            "Epoch 642/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0132 - accuracy: 0.9932\n",
            "Epoch 643/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.9966\n",
            "Epoch 644/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0292 - accuracy: 0.9864\n",
            "Epoch 645/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0163 - accuracy: 0.9966\n",
            "Epoch 646/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0179 - accuracy: 0.9932\n",
            "Epoch 647/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 0.9966\n",
            "Epoch 648/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0132 - accuracy: 0.9966\n",
            "Epoch 649/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0192 - accuracy: 0.9932\n",
            "Epoch 650/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0151 - accuracy: 0.9932\n",
            "Epoch 651/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 0.9966\n",
            "Epoch 652/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0155 - accuracy: 0.9932\n",
            "Epoch 653/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 0.9966\n",
            "Epoch 654/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0141 - accuracy: 1.0000\n",
            "Epoch 655/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.9966\n",
            "Epoch 656/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 0.9966\n",
            "Epoch 657/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0174 - accuracy: 0.9932\n",
            "Epoch 658/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 0.9932\n",
            "Epoch 659/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0125 - accuracy: 0.9966\n",
            "Epoch 660/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0175 - accuracy: 0.9932\n",
            "Epoch 661/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0149 - accuracy: 0.9898\n",
            "Epoch 662/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 663/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.9932\n",
            "Epoch 664/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9932\n",
            "Epoch 665/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0144 - accuracy: 0.9898\n",
            "Epoch 666/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.9932\n",
            "Epoch 667/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0127 - accuracy: 0.9932\n",
            "Epoch 668/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0107 - accuracy: 0.9966\n",
            "Epoch 669/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0091 - accuracy: 0.9966\n",
            "Epoch 670/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 671/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.9966\n",
            "Epoch 672/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 0.9966\n",
            "Epoch 673/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0100 - accuracy: 0.9966\n",
            "Epoch 674/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0100 - accuracy: 0.9966\n",
            "Epoch 675/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0083 - accuracy: 0.9966\n",
            "Epoch 676/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 0.9966\n",
            "Epoch 677/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.9966\n",
            "Epoch 678/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 0.9932\n",
            "Epoch 679/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 0.9966\n",
            "Epoch 680/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.9966\n",
            "Epoch 681/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0084 - accuracy: 0.9966\n",
            "Epoch 682/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.9966\n",
            "Epoch 683/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 0.9966\n",
            "Epoch 684/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0105 - accuracy: 0.9966\n",
            "Epoch 685/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0077 - accuracy: 0.9966\n",
            "Epoch 686/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 0.9932\n",
            "Epoch 687/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.9932\n",
            "Epoch 688/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.9966\n",
            "Epoch 689/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0099 - accuracy: 0.9932\n",
            "Epoch 690/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 0.9966\n",
            "Epoch 691/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 0.9966\n",
            "Epoch 692/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.9966\n",
            "Epoch 693/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.9966\n",
            "Epoch 694/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0144 - accuracy: 0.9898\n",
            "Epoch 695/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9966\n",
            "Epoch 696/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 0.9966\n",
            "Epoch 697/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 0.9966\n",
            "Epoch 698/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0078 - accuracy: 0.9966\n",
            "Epoch 699/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 700/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 0.9966\n",
            "Epoch 701/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 0.9966\n",
            "Epoch 702/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0086 - accuracy: 0.9966\n",
            "Epoch 703/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0107 - accuracy: 0.9932\n",
            "Epoch 704/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 0.9966\n",
            "Epoch 705/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0143 - accuracy: 0.9932\n",
            "Epoch 706/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 0.9966\n",
            "Epoch 707/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0084 - accuracy: 0.9966\n",
            "Epoch 708/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9966\n",
            "Epoch 709/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0105 - accuracy: 0.9966\n",
            "Epoch 710/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 0.9966\n",
            "Epoch 711/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 0.9966\n",
            "Epoch 712/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 713/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 0.9966\n",
            "Epoch 714/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 715/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 0.9966\n",
            "Epoch 716/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0083 - accuracy: 0.9966\n",
            "Epoch 717/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.9966\n",
            "Epoch 718/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0064 - accuracy: 0.9966\n",
            "Epoch 719/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0085 - accuracy: 0.9966\n",
            "Epoch 720/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0075 - accuracy: 0.9966\n",
            "Epoch 721/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.9966\n",
            "Epoch 722/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0063 - accuracy: 0.9966\n",
            "Epoch 723/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 0.9966\n",
            "Epoch 724/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0063 - accuracy: 0.9966\n",
            "Epoch 725/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0063 - accuracy: 0.9966\n",
            "Epoch 726/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0142 - accuracy: 0.9932\n",
            "Epoch 727/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 728/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 729/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0061 - accuracy: 0.9966\n",
            "Epoch 730/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.9932\n",
            "Epoch 731/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 732/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0073 - accuracy: 0.9966\n",
            "Epoch 733/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0078 - accuracy: 0.9966\n",
            "Epoch 734/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0141 - accuracy: 0.9966\n",
            "Epoch 735/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 0.9966\n",
            "Epoch 736/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0073 - accuracy: 0.9932\n",
            "Epoch 737/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9932\n",
            "Epoch 738/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0081 - accuracy: 0.9966\n",
            "Epoch 739/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 740/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 741/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 742/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 0.9966\n",
            "Epoch 743/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 0.9966\n",
            "Epoch 744/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9966\n",
            "Epoch 745/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 746/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 747/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 0.9966\n",
            "Epoch 748/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0150 - accuracy: 0.9932\n",
            "Epoch 749/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 0.9932\n",
            "Epoch 750/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0091 - accuracy: 0.9966\n",
            "Epoch 751/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0091 - accuracy: 0.9932\n",
            "Epoch 752/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9932\n",
            "Epoch 753/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 0.9966\n",
            "Epoch 754/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 755/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.9966\n",
            "Epoch 756/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 0.9966\n",
            "Epoch 757/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.9932\n",
            "Epoch 758/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 0.9932\n",
            "Epoch 759/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0066 - accuracy: 0.9966\n",
            "Epoch 760/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9966\n",
            "Epoch 761/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 0.9966\n",
            "Epoch 762/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0081 - accuracy: 0.9966\n",
            "Epoch 763/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9966\n",
            "Epoch 764/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 765/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 0.9966\n",
            "Epoch 766/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9966\n",
            "Epoch 767/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 0.9966\n",
            "Epoch 768/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 0.9932\n",
            "Epoch 769/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 770/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 771/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 0.9966\n",
            "Epoch 772/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 773/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0068 - accuracy: 0.9966\n",
            "Epoch 774/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0064 - accuracy: 0.9966\n",
            "Epoch 775/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 776/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 0.9966\n",
            "Epoch 777/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 0.9966\n",
            "Epoch 778/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0066 - accuracy: 0.9966\n",
            "Epoch 779/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0074 - accuracy: 0.9966\n",
            "Epoch 780/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.9932\n",
            "Epoch 781/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.9966\n",
            "Epoch 782/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 0.9966\n",
            "Epoch 783/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 0.9932\n",
            "Epoch 784/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 0.9932\n",
            "Epoch 785/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 786/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.9966\n",
            "Epoch 787/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.9966\n",
            "Epoch 788/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 0.9932\n",
            "Epoch 789/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0087 - accuracy: 0.9966\n",
            "Epoch 790/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0071 - accuracy: 0.9966\n",
            "Epoch 791/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 0.9966\n",
            "Epoch 792/800\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.0083 - accuracy: 0.9966\n",
            "Epoch 793/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 0.9966\n",
            "Epoch 794/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0080 - accuracy: 0.9966\n",
            "Epoch 795/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0081 - accuracy: 0.9966\n",
            "Epoch 796/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0084 - accuracy: 0.9966\n",
            "Epoch 797/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 0.9966\n",
            "Epoch 798/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0086 - accuracy: 0.9966\n",
            "Epoch 799/800\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0072 - accuracy: 0.9966\n",
            "Epoch 800/800\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0071 - accuracy: 0.9966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(CNN_history.history['accuracy'])\n",
        "plt.title('CNN Model accuracy with class=3')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "IpeWC-qfcfYJ",
        "outputId": "99711aad-c845-4ea0-d318-8de577054202"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss values\n",
        "plt.plot(CNN_history.history['loss'])\n",
        "plt.title('CNN Model loss with class=3')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "oICNYaJecisq",
        "outputId": "f5eba2f7-d9ad-4a78-9129-bb40a7eb28c4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "v7AcicTnclp2"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_CNN = CNN_model.predict(feature_valid)\n",
        "\n",
        "# convert the validation vector\n",
        "valid_y_CNN = y_valid_CNN.copy()\n",
        "for i in range(len(y_valid_CNN)):\n",
        "    j = np.where(y_valid_CNN[i] == np.amax(y_valid_CNN[i]))\n",
        "    valid_y_CNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aD6kQPFcn4X",
        "outputId": "47278488-c4bc-48e4-b76a-b8b55a1b4da4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 20ms/step\n",
            "0.6756756756756757\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.53      0.59        15\n",
            "           1       0.65      0.89      0.75        36\n",
            "           2       0.77      0.43      0.56        23\n",
            "\n",
            "   micro avg       0.68      0.68      0.68        74\n",
            "   macro avg       0.70      0.62      0.63        74\n",
            "weighted avg       0.69      0.68      0.66        74\n",
            " samples avg       0.68      0.68      0.68        74\n",
            "\n",
            "auc score:  0.713836045047331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_CNN = CNN_model.predict(feature_test)\n",
        "# convert the test vector\n",
        "test_y_CNN = y_test_CNN.copy()\n",
        "for i in range(len(y_test_CNN)):\n",
        "    j = np.where(y_test_CNN[i] == np.amax(y_test_CNN[i]))\n",
        "    test_y_CNN[i] = [0, 0, 0]\n",
        "    test_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_test_y,test_y_CNN))\n",
        "print(classification_report(label_test_y,test_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_test_y,test_y_CNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ_cj8dTcc6e",
        "outputId": "91920f4b-6c9f-4bbc-b7b2-ed5d3c70a9a9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 19ms/step\n",
            "0.5698924731182796\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.53      0.67        17\n",
            "           1       0.56      0.79      0.65        43\n",
            "           2       0.45      0.30      0.36        33\n",
            "\n",
            "   micro avg       0.57      0.57      0.57        93\n",
            "   macro avg       0.64      0.54      0.56        93\n",
            "weighted avg       0.58      0.57      0.55        93\n",
            " samples avg       0.57      0.57      0.57        93\n",
            "\n",
            "auc score:  0.644996974569658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WkHcgfdb7IX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yzLyaISB7IaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9E4HqsZA7Ica"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def create_CNN_model():\n",
        "    CNN = Sequential()\n",
        "    CNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=True))\n",
        "\n",
        "    CNN.add(Convolution1D(256, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Convolution1D(128, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Convolution1D(64, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Flatten())\n",
        "    CNN.add(Dense(units = 512 , activation = 'relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(Dropout(0.3))\n",
        "    CNN.add(Dense(units = 256 , activation = 'relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(Dropout(0.3))\n",
        "    CNN.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "    opt = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "    CNN.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    return CNN\n",
        "\n",
        "CNN_model = create_CNN_model()\n",
        "\n",
        "# 创建ModelCheckpoint回调函数\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5',\n",
        "                             monitor='val_accuracy',\n",
        "                             save_best_only=True,\n",
        "                             mode='max',\n",
        "                             verbose=1)\n",
        "\n",
        "class_weights = {0: 1.0, 1: 0.5, 2: 3.0}\n",
        "CNN_history = CNN_model.fit(feature_train, label_train_y,\n",
        "                            epochs=700, batch_size=128,\n",
        "                            validation_data=(feature_valid, label_valid_y),\n",
        "                            class_weight=class_weights,\n",
        "                            callbacks=[checkpoint])  # 将回调函数传递给fit函数"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mva0MOGzxmY",
        "outputId": "d7e976c2-e2e2-4e8e-d966-c127c5056a2b"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 9.0830 - accuracy: 0.3707\n",
            "Epoch 1: val_accuracy improved from -inf to 0.31081, saving model to best_model.h5\n",
            "3/3 [==============================] - 3s 252ms/step - loss: 9.0830 - accuracy: 0.3707 - val_loss: 7.9690 - val_accuracy: 0.3108\n",
            "Epoch 2/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 8.1236 - accuracy: 0.2656"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 8.0880 - accuracy: 0.3027 - val_loss: 7.8911 - val_accuracy: 0.3108\n",
            "Epoch 3/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.6254 - accuracy: 0.5034\n",
            "Epoch 3: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 7.6254 - accuracy: 0.5034 - val_loss: 7.8668 - val_accuracy: 0.3108\n",
            "Epoch 4/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.5257 - accuracy: 0.4626\n",
            "Epoch 4: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 7.5257 - accuracy: 0.4626 - val_loss: 7.8083 - val_accuracy: 0.3108\n",
            "Epoch 5/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.4340 - accuracy: 0.5306\n",
            "Epoch 5: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 7.4340 - accuracy: 0.5306 - val_loss: 7.7605 - val_accuracy: 0.3108\n",
            "Epoch 6/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.2647 - accuracy: 0.5238\n",
            "Epoch 6: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 7.2647 - accuracy: 0.5238 - val_loss: 7.6657 - val_accuracy: 0.3108\n",
            "Epoch 7/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 7.0760 - accuracy: 0.6837\n",
            "Epoch 7: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 7.0760 - accuracy: 0.6837 - val_loss: 7.5702 - val_accuracy: 0.3108\n",
            "Epoch 8/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.9598 - accuracy: 0.7041\n",
            "Epoch 8: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 6.9598 - accuracy: 0.7041 - val_loss: 7.4943 - val_accuracy: 0.3108\n",
            "Epoch 9/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.8002 - accuracy: 0.7381\n",
            "Epoch 9: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 6.8002 - accuracy: 0.7381 - val_loss: 7.4109 - val_accuracy: 0.3108\n",
            "Epoch 10/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.7152 - accuracy: 0.7687\n",
            "Epoch 10: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 6.7152 - accuracy: 0.7687 - val_loss: 7.3394 - val_accuracy: 0.3108\n",
            "Epoch 11/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.5313 - accuracy: 0.8231\n",
            "Epoch 11: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 6.5313 - accuracy: 0.8231 - val_loss: 7.2658 - val_accuracy: 0.3108\n",
            "Epoch 12/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.4286 - accuracy: 0.8810\n",
            "Epoch 12: val_accuracy did not improve from 0.31081\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 6.4286 - accuracy: 0.8810 - val_loss: 7.1958 - val_accuracy: 0.3108\n",
            "Epoch 13/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.3219 - accuracy: 0.8912\n",
            "Epoch 13: val_accuracy improved from 0.31081 to 0.33784, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 6.3219 - accuracy: 0.8912 - val_loss: 7.1162 - val_accuracy: 0.3378\n",
            "Epoch 14/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.2208 - accuracy: 0.9184\n",
            "Epoch 14: val_accuracy improved from 0.33784 to 0.37838, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 6.2208 - accuracy: 0.9184 - val_loss: 7.0436 - val_accuracy: 0.3784\n",
            "Epoch 15/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.1634 - accuracy: 0.8946\n",
            "Epoch 15: val_accuracy improved from 0.37838 to 0.47297, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 6.1634 - accuracy: 0.8946 - val_loss: 6.9603 - val_accuracy: 0.4730\n",
            "Epoch 16/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 6.0614 - accuracy: 0.8980\n",
            "Epoch 16: val_accuracy improved from 0.47297 to 0.48649, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 6.0614 - accuracy: 0.8980 - val_loss: 6.8847 - val_accuracy: 0.4865\n",
            "Epoch 17/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.9576 - accuracy: 0.9490\n",
            "Epoch 17: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 5.9576 - accuracy: 0.9490 - val_loss: 6.8205 - val_accuracy: 0.4865\n",
            "Epoch 18/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.8762 - accuracy: 0.9490\n",
            "Epoch 18: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 5.8762 - accuracy: 0.9490 - val_loss: 6.7558 - val_accuracy: 0.4865\n",
            "Epoch 19/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.7810 - accuracy: 0.9660\n",
            "Epoch 19: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 5.7810 - accuracy: 0.9660 - val_loss: 6.6906 - val_accuracy: 0.4865\n",
            "Epoch 20/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.6937 - accuracy: 0.9728\n",
            "Epoch 20: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 5.6937 - accuracy: 0.9728 - val_loss: 6.6258 - val_accuracy: 0.4865\n",
            "Epoch 21/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.6217 - accuracy: 0.9844\n",
            "Epoch 21: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 5.6338 - accuracy: 0.9830 - val_loss: 6.5594 - val_accuracy: 0.4865\n",
            "Epoch 22/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.6030 - accuracy: 0.9762\n",
            "Epoch 22: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 5.6030 - accuracy: 0.9762 - val_loss: 6.4915 - val_accuracy: 0.4865\n",
            "Epoch 23/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.4750 - accuracy: 0.9796\n",
            "Epoch 23: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 5.4750 - accuracy: 0.9796 - val_loss: 6.4278 - val_accuracy: 0.4865\n",
            "Epoch 24/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.4102 - accuracy: 0.9864\n",
            "Epoch 24: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 5.4102 - accuracy: 0.9864 - val_loss: 6.3643 - val_accuracy: 0.4865\n",
            "Epoch 25/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.3560 - accuracy: 0.9796\n",
            "Epoch 25: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 5.3560 - accuracy: 0.9796 - val_loss: 6.2946 - val_accuracy: 0.4865\n",
            "Epoch 26/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.2984 - accuracy: 0.9796\n",
            "Epoch 26: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 5.2984 - accuracy: 0.9796 - val_loss: 6.2286 - val_accuracy: 0.4865\n",
            "Epoch 27/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.2395 - accuracy: 0.9796\n",
            "Epoch 27: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 5.2395 - accuracy: 0.9796 - val_loss: 6.1591 - val_accuracy: 0.4865\n",
            "Epoch 28/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.1700 - accuracy: 0.9626\n",
            "Epoch 28: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 5.1700 - accuracy: 0.9626 - val_loss: 6.0892 - val_accuracy: 0.4865\n",
            "Epoch 29/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.1142 - accuracy: 0.9796\n",
            "Epoch 29: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 5.1142 - accuracy: 0.9796 - val_loss: 6.0225 - val_accuracy: 0.4865\n",
            "Epoch 30/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 5.0596 - accuracy: 0.9762\n",
            "Epoch 30: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 5.0596 - accuracy: 0.9762 - val_loss: 5.9585 - val_accuracy: 0.4865\n",
            "Epoch 31/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.9854 - accuracy: 0.9728\n",
            "Epoch 31: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 4.9854 - accuracy: 0.9728 - val_loss: 5.8964 - val_accuracy: 0.4865\n",
            "Epoch 32/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.9492 - accuracy: 0.9626\n",
            "Epoch 32: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 4.9492 - accuracy: 0.9626 - val_loss: 5.8383 - val_accuracy: 0.4865\n",
            "Epoch 33/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.8865 - accuracy: 0.9728\n",
            "Epoch 33: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 4.8865 - accuracy: 0.9728 - val_loss: 5.7811 - val_accuracy: 0.4865\n",
            "Epoch 34/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.8294 - accuracy: 0.9609\n",
            "Epoch 34: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 4.7996 - accuracy: 0.9762 - val_loss: 5.7206 - val_accuracy: 0.4865\n",
            "Epoch 35/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.7306 - accuracy: 0.9728\n",
            "Epoch 35: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 4.7306 - accuracy: 0.9728 - val_loss: 5.6651 - val_accuracy: 0.4865\n",
            "Epoch 36/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.6826 - accuracy: 0.9830\n",
            "Epoch 36: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 4.6826 - accuracy: 0.9830 - val_loss: 5.6145 - val_accuracy: 0.4865\n",
            "Epoch 37/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.6215 - accuracy: 0.9728\n",
            "Epoch 37: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 4.6215 - accuracy: 0.9728 - val_loss: 5.5620 - val_accuracy: 0.4865\n",
            "Epoch 38/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.5644 - accuracy: 0.9830\n",
            "Epoch 38: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 4.5644 - accuracy: 0.9830 - val_loss: 5.5117 - val_accuracy: 0.4865\n",
            "Epoch 39/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.5244 - accuracy: 0.9830\n",
            "Epoch 39: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 4.5244 - accuracy: 0.9830 - val_loss: 5.4593 - val_accuracy: 0.4865\n",
            "Epoch 40/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.4660 - accuracy: 0.9796\n",
            "Epoch 40: val_accuracy improved from 0.48649 to 0.52703, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 4.4660 - accuracy: 0.9796 - val_loss: 5.4084 - val_accuracy: 0.5270\n",
            "Epoch 41/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.4140 - accuracy: 0.9694\n",
            "Epoch 41: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 4.4140 - accuracy: 0.9694 - val_loss: 5.3554 - val_accuracy: 0.5270\n",
            "Epoch 42/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.3702 - accuracy: 0.9762\n",
            "Epoch 42: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 4.3702 - accuracy: 0.9762 - val_loss: 5.3017 - val_accuracy: 0.5270\n",
            "Epoch 43/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.3045 - accuracy: 0.9830\n",
            "Epoch 43: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 4.3045 - accuracy: 0.9830 - val_loss: 5.2428 - val_accuracy: 0.5270\n",
            "Epoch 44/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.2607 - accuracy: 0.9796\n",
            "Epoch 44: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 4.2607 - accuracy: 0.9796 - val_loss: 5.1814 - val_accuracy: 0.4865\n",
            "Epoch 45/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.1902 - accuracy: 0.9864\n",
            "Epoch 45: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 4.1902 - accuracy: 0.9864 - val_loss: 5.1230 - val_accuracy: 0.4865\n",
            "Epoch 46/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.1451 - accuracy: 0.9830\n",
            "Epoch 46: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 4.1451 - accuracy: 0.9830 - val_loss: 5.0692 - val_accuracy: 0.4865\n",
            "Epoch 47/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.0844 - accuracy: 0.9898\n",
            "Epoch 47: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 4.0844 - accuracy: 0.9898 - val_loss: 5.0132 - val_accuracy: 0.4865\n",
            "Epoch 48/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 4.0591 - accuracy: 0.9898\n",
            "Epoch 48: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 4.0591 - accuracy: 0.9898 - val_loss: 4.9577 - val_accuracy: 0.4865\n",
            "Epoch 49/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.9956 - accuracy: 0.9898\n",
            "Epoch 49: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 3.9956 - accuracy: 0.9898 - val_loss: 4.9100 - val_accuracy: 0.4865\n",
            "Epoch 50/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.9591 - accuracy: 0.9805\n",
            "Epoch 50: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.9550 - accuracy: 0.9796 - val_loss: 4.8671 - val_accuracy: 0.4865\n",
            "Epoch 51/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.9009 - accuracy: 0.9830\n",
            "Epoch 51: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 3.9009 - accuracy: 0.9830 - val_loss: 4.8211 - val_accuracy: 0.4865\n",
            "Epoch 52/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.8534 - accuracy: 0.9898\n",
            "Epoch 52: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 3.8534 - accuracy: 0.9898 - val_loss: 4.7728 - val_accuracy: 0.4865\n",
            "Epoch 53/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.7977 - accuracy: 0.9830\n",
            "Epoch 53: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 3.7977 - accuracy: 0.9830 - val_loss: 4.7264 - val_accuracy: 0.4865\n",
            "Epoch 54/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.7600 - accuracy: 0.9830\n",
            "Epoch 54: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 3.7600 - accuracy: 0.9830 - val_loss: 4.6804 - val_accuracy: 0.4865\n",
            "Epoch 55/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.7300 - accuracy: 0.9805\n",
            "Epoch 55: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.7241 - accuracy: 0.9830 - val_loss: 4.6455 - val_accuracy: 0.4865\n",
            "Epoch 56/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.6686 - accuracy: 0.9864\n",
            "Epoch 56: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.6686 - accuracy: 0.9864 - val_loss: 4.6158 - val_accuracy: 0.4865\n",
            "Epoch 57/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.6246 - accuracy: 0.9830\n",
            "Epoch 57: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 3.6246 - accuracy: 0.9830 - val_loss: 4.5877 - val_accuracy: 0.4865\n",
            "Epoch 58/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.5853 - accuracy: 0.9766\n",
            "Epoch 58: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 3.5761 - accuracy: 0.9830 - val_loss: 4.5450 - val_accuracy: 0.4865\n",
            "Epoch 59/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.5358 - accuracy: 0.9864\n",
            "Epoch 59: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.5358 - accuracy: 0.9864 - val_loss: 4.4959 - val_accuracy: 0.4865\n",
            "Epoch 60/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.5060 - accuracy: 0.9864\n",
            "Epoch 60: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 3.5060 - accuracy: 0.9864 - val_loss: 4.4565 - val_accuracy: 0.4865\n",
            "Epoch 61/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4572 - accuracy: 1.0000\n",
            "Epoch 61: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 3.4522 - accuracy: 0.9864 - val_loss: 4.4010 - val_accuracy: 0.5000\n",
            "Epoch 62/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4335 - accuracy: 0.9766\n",
            "Epoch 62: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 3.4449 - accuracy: 0.9864 - val_loss: 4.3879 - val_accuracy: 0.5000\n",
            "Epoch 63/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.3740 - accuracy: 0.9805\n",
            "Epoch 63: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.3689 - accuracy: 0.9830 - val_loss: 4.3905 - val_accuracy: 0.5135\n",
            "Epoch 64/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.3363 - accuracy: 0.9922\n",
            "Epoch 64: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 3.3432 - accuracy: 0.9864 - val_loss: 4.3472 - val_accuracy: 0.5135\n",
            "Epoch 65/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.3022 - accuracy: 0.9762\n",
            "Epoch 65: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 3.3022 - accuracy: 0.9762 - val_loss: 4.3043 - val_accuracy: 0.5000\n",
            "Epoch 66/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.2547 - accuracy: 0.9864\n",
            "Epoch 66: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.2547 - accuracy: 0.9864 - val_loss: 4.2845 - val_accuracy: 0.5000\n",
            "Epoch 67/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.2246 - accuracy: 0.9796\n",
            "Epoch 67: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 3.2246 - accuracy: 0.9796 - val_loss: 4.2580 - val_accuracy: 0.5000\n",
            "Epoch 68/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1874 - accuracy: 0.9844\n",
            "Epoch 68: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 3.1902 - accuracy: 0.9830 - val_loss: 4.2290 - val_accuracy: 0.5000\n",
            "Epoch 69/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.1425 - accuracy: 0.9830\n",
            "Epoch 69: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 3.1425 - accuracy: 0.9830 - val_loss: 4.1644 - val_accuracy: 0.5000\n",
            "Epoch 70/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.0950 - accuracy: 0.9922\n",
            "Epoch 70: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.1073 - accuracy: 0.9864 - val_loss: 4.0976 - val_accuracy: 0.5000\n",
            "Epoch 71/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0655 - accuracy: 0.9796\n",
            "Epoch 71: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 3.0655 - accuracy: 0.9796 - val_loss: 4.0288 - val_accuracy: 0.5000\n",
            "Epoch 72/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.0325 - accuracy: 0.9844\n",
            "Epoch 72: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 3.0334 - accuracy: 0.9796 - val_loss: 3.9656 - val_accuracy: 0.5000\n",
            "Epoch 73/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0107 - accuracy: 0.9762\n",
            "Epoch 73: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 3.0107 - accuracy: 0.9762 - val_loss: 3.9127 - val_accuracy: 0.5000\n",
            "Epoch 74/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.9564 - accuracy: 0.9883\n",
            "Epoch 74: val_accuracy did not improve from 0.52703\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 2.9597 - accuracy: 0.9864 - val_loss: 3.8661 - val_accuracy: 0.5135\n",
            "Epoch 75/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9258 - accuracy: 0.9844\n",
            "Epoch 75: val_accuracy improved from 0.52703 to 0.54054, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 2.9233 - accuracy: 0.9830 - val_loss: 3.8156 - val_accuracy: 0.5405\n",
            "Epoch 76/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.8994 - accuracy: 0.9762\n",
            "Epoch 76: val_accuracy did not improve from 0.54054\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 2.8994 - accuracy: 0.9762 - val_loss: 3.7681 - val_accuracy: 0.5405\n",
            "Epoch 77/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.8493 - accuracy: 0.9883\n",
            "Epoch 77: val_accuracy improved from 0.54054 to 0.58108, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 2.8568 - accuracy: 0.9796 - val_loss: 3.7160 - val_accuracy: 0.5811\n",
            "Epoch 78/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.8230 - accuracy: 0.9830\n",
            "Epoch 78: val_accuracy improved from 0.58108 to 0.60811, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 2.8230 - accuracy: 0.9830 - val_loss: 3.6726 - val_accuracy: 0.6081\n",
            "Epoch 79/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.7874 - accuracy: 0.9796\n",
            "Epoch 79: val_accuracy did not improve from 0.60811\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 2.7874 - accuracy: 0.9796 - val_loss: 3.6322 - val_accuracy: 0.6081\n",
            "Epoch 80/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7801 - accuracy: 0.9766\n",
            "Epoch 80: val_accuracy did not improve from 0.60811\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 2.7647 - accuracy: 0.9830 - val_loss: 3.5922 - val_accuracy: 0.6081\n",
            "Epoch 81/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7523 - accuracy: 0.9766\n",
            "Epoch 81: val_accuracy did not improve from 0.60811\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 2.7327 - accuracy: 0.9796 - val_loss: 3.5533 - val_accuracy: 0.6081\n",
            "Epoch 82/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6869 - accuracy: 0.9864\n",
            "Epoch 82: val_accuracy improved from 0.60811 to 0.66216, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 2.6869 - accuracy: 0.9864 - val_loss: 3.5155 - val_accuracy: 0.6622\n",
            "Epoch 83/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6606 - accuracy: 0.9830\n",
            "Epoch 83: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 2.6606 - accuracy: 0.9830 - val_loss: 3.4815 - val_accuracy: 0.6486\n",
            "Epoch 84/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6288 - accuracy: 0.9830\n",
            "Epoch 84: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 2.6288 - accuracy: 0.9830 - val_loss: 3.4538 - val_accuracy: 0.6486\n",
            "Epoch 85/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5912 - accuracy: 0.9898\n",
            "Epoch 85: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 2.5912 - accuracy: 0.9898 - val_loss: 3.4268 - val_accuracy: 0.6486\n",
            "Epoch 86/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5712 - accuracy: 0.9796\n",
            "Epoch 86: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 2.5712 - accuracy: 0.9796 - val_loss: 3.3964 - val_accuracy: 0.6486\n",
            "Epoch 87/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5283 - accuracy: 0.9830\n",
            "Epoch 87: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 2.5283 - accuracy: 0.9830 - val_loss: 3.3620 - val_accuracy: 0.6351\n",
            "Epoch 88/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5061 - accuracy: 0.9762\n",
            "Epoch 88: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 2.5061 - accuracy: 0.9762 - val_loss: 3.3332 - val_accuracy: 0.6351\n",
            "Epoch 89/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4748 - accuracy: 0.9796\n",
            "Epoch 89: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.4748 - accuracy: 0.9796 - val_loss: 3.3074 - val_accuracy: 0.5946\n",
            "Epoch 90/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4396 - accuracy: 0.9830\n",
            "Epoch 90: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.4396 - accuracy: 0.9830 - val_loss: 3.2804 - val_accuracy: 0.5946\n",
            "Epoch 91/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4163 - accuracy: 0.9796\n",
            "Epoch 91: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.4163 - accuracy: 0.9796 - val_loss: 3.2583 - val_accuracy: 0.5946\n",
            "Epoch 92/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.3854 - accuracy: 0.9796\n",
            "Epoch 92: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 2.3854 - accuracy: 0.9796 - val_loss: 3.2529 - val_accuracy: 0.5946\n",
            "Epoch 93/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3500 - accuracy: 0.9844\n",
            "Epoch 93: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.3526 - accuracy: 0.9796 - val_loss: 3.2364 - val_accuracy: 0.5946\n",
            "Epoch 94/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.3246 - accuracy: 0.9796\n",
            "Epoch 94: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 2.3246 - accuracy: 0.9796 - val_loss: 3.2129 - val_accuracy: 0.5946\n",
            "Epoch 95/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2967 - accuracy: 0.9844\n",
            "Epoch 95: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.3092 - accuracy: 0.9762 - val_loss: 3.2264 - val_accuracy: 0.5811\n",
            "Epoch 96/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2699 - accuracy: 0.9922\n",
            "Epoch 96: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.2678 - accuracy: 0.9796 - val_loss: 3.2554 - val_accuracy: 0.5946\n",
            "Epoch 97/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.2434 - accuracy: 0.9830\n",
            "Epoch 97: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 2.2434 - accuracy: 0.9830 - val_loss: 3.2895 - val_accuracy: 0.5676\n",
            "Epoch 98/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.2227 - accuracy: 0.9796\n",
            "Epoch 98: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.2227 - accuracy: 0.9796 - val_loss: 3.3436 - val_accuracy: 0.5541\n",
            "Epoch 99/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2127 - accuracy: 0.9766\n",
            "Epoch 99: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 2.1983 - accuracy: 0.9796 - val_loss: 3.3640 - val_accuracy: 0.5541\n",
            "Epoch 100/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1826 - accuracy: 0.9766\n",
            "Epoch 100: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 2.1670 - accuracy: 0.9796 - val_loss: 3.3792 - val_accuracy: 0.5541\n",
            "Epoch 101/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1685 - accuracy: 0.9688\n",
            "Epoch 101: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 2.1406 - accuracy: 0.9796 - val_loss: 3.3873 - val_accuracy: 0.5541\n",
            "Epoch 102/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.1153 - accuracy: 0.9844\n",
            "Epoch 102: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 2.1112 - accuracy: 0.9864 - val_loss: 3.3530 - val_accuracy: 0.5405\n",
            "Epoch 103/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0843 - accuracy: 0.9844\n",
            "Epoch 103: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 2.0839 - accuracy: 0.9864 - val_loss: 3.2974 - val_accuracy: 0.5405\n",
            "Epoch 104/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.0592 - accuracy: 0.9932\n",
            "Epoch 104: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 2.0592 - accuracy: 0.9932 - val_loss: 3.2130 - val_accuracy: 0.5676\n",
            "Epoch 105/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.0391 - accuracy: 0.9796\n",
            "Epoch 105: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 2.0391 - accuracy: 0.9796 - val_loss: 3.1275 - val_accuracy: 0.5541\n",
            "Epoch 106/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0077 - accuracy: 0.9844\n",
            "Epoch 106: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0115 - accuracy: 0.9830 - val_loss: 3.0562 - val_accuracy: 0.5541\n",
            "Epoch 107/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9850 - accuracy: 0.9830\n",
            "Epoch 107: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 1.9850 - accuracy: 0.9830 - val_loss: 3.0155 - val_accuracy: 0.5946\n",
            "Epoch 108/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9679 - accuracy: 0.9830\n",
            "Epoch 108: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 1.9679 - accuracy: 0.9830 - val_loss: 2.9945 - val_accuracy: 0.5946\n",
            "Epoch 109/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9425 - accuracy: 0.9830\n",
            "Epoch 109: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.9425 - accuracy: 0.9830 - val_loss: 2.9866 - val_accuracy: 0.5946\n",
            "Epoch 110/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9200 - accuracy: 0.9830\n",
            "Epoch 110: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 1.9200 - accuracy: 0.9830 - val_loss: 2.9479 - val_accuracy: 0.6216\n",
            "Epoch 111/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9014 - accuracy: 0.9922\n",
            "Epoch 111: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.8972 - accuracy: 0.9830 - val_loss: 2.8894 - val_accuracy: 0.6486\n",
            "Epoch 112/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8789 - accuracy: 0.9844\n",
            "Epoch 112: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8688 - accuracy: 0.9898 - val_loss: 2.8539 - val_accuracy: 0.6486\n",
            "Epoch 113/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.8616 - accuracy: 0.9898\n",
            "Epoch 113: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 1.8616 - accuracy: 0.9898 - val_loss: 2.8715 - val_accuracy: 0.6351\n",
            "Epoch 114/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.8299 - accuracy: 0.9898\n",
            "Epoch 114: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 1.8299 - accuracy: 0.9898 - val_loss: 2.8898 - val_accuracy: 0.6216\n",
            "Epoch 115/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.8076 - accuracy: 0.9864\n",
            "Epoch 115: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.8076 - accuracy: 0.9864 - val_loss: 2.9285 - val_accuracy: 0.5676\n",
            "Epoch 116/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7919 - accuracy: 0.9922\n",
            "Epoch 116: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.7834 - accuracy: 0.9864 - val_loss: 2.9258 - val_accuracy: 0.5811\n",
            "Epoch 117/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7695 - accuracy: 0.9922\n",
            "Epoch 117: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.7780 - accuracy: 0.9830 - val_loss: 2.9548 - val_accuracy: 0.5811\n",
            "Epoch 118/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7587 - accuracy: 0.9766\n",
            "Epoch 118: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.7456 - accuracy: 0.9830 - val_loss: 2.9765 - val_accuracy: 0.5811\n",
            "Epoch 119/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.7199 - accuracy: 0.9830\n",
            "Epoch 119: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 1.7199 - accuracy: 0.9830 - val_loss: 3.0315 - val_accuracy: 0.5676\n",
            "Epoch 120/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7118 - accuracy: 0.9922\n",
            "Epoch 120: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.7086 - accuracy: 0.9796 - val_loss: 3.0268 - val_accuracy: 0.5676\n",
            "Epoch 121/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.6841 - accuracy: 0.9805\n",
            "Epoch 121: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 1.6841 - accuracy: 0.9796 - val_loss: 2.9750 - val_accuracy: 0.5811\n",
            "Epoch 122/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.6672 - accuracy: 0.9830\n",
            "Epoch 122: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.6672 - accuracy: 0.9830 - val_loss: 3.0275 - val_accuracy: 0.5946\n",
            "Epoch 123/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.6440 - accuracy: 0.9864\n",
            "Epoch 123: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.6440 - accuracy: 0.9864 - val_loss: 3.1135 - val_accuracy: 0.5946\n",
            "Epoch 124/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6202 - accuracy: 1.0000\n",
            "Epoch 124: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.6255 - accuracy: 0.9898 - val_loss: 3.1640 - val_accuracy: 0.5946\n",
            "Epoch 125/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.6079 - accuracy: 0.9898\n",
            "Epoch 125: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.6079 - accuracy: 0.9898 - val_loss: 3.1534 - val_accuracy: 0.5946\n",
            "Epoch 126/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.5870 - accuracy: 0.9864\n",
            "Epoch 126: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.5870 - accuracy: 0.9864 - val_loss: 3.1140 - val_accuracy: 0.5946\n",
            "Epoch 127/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5698 - accuracy: 0.9688\n",
            "Epoch 127: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5688 - accuracy: 0.9864 - val_loss: 3.2108 - val_accuracy: 0.5676\n",
            "Epoch 128/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.5452 - accuracy: 0.9864\n",
            "Epoch 128: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 1.5452 - accuracy: 0.9864 - val_loss: 3.3448 - val_accuracy: 0.5270\n",
            "Epoch 129/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5521 - accuracy: 0.9922\n",
            "Epoch 129: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5451 - accuracy: 0.9762 - val_loss: 3.3333 - val_accuracy: 0.5270\n",
            "Epoch 130/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.5190 - accuracy: 0.9766\n",
            "Epoch 130: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 1.5148 - accuracy: 0.9796 - val_loss: 3.2886 - val_accuracy: 0.5270\n",
            "Epoch 131/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4949 - accuracy: 0.9844\n",
            "Epoch 131: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4960 - accuracy: 0.9830 - val_loss: 3.1780 - val_accuracy: 0.5676\n",
            "Epoch 132/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4710 - accuracy: 0.9864\n",
            "Epoch 132: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 1.4710 - accuracy: 0.9864 - val_loss: 2.9894 - val_accuracy: 0.5676\n",
            "Epoch 133/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4575 - accuracy: 0.9830\n",
            "Epoch 133: val_accuracy did not improve from 0.66216\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 1.4575 - accuracy: 0.9830 - val_loss: 2.8383 - val_accuracy: 0.6351\n",
            "Epoch 134/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4459 - accuracy: 0.9844\n",
            "Epoch 134: val_accuracy improved from 0.66216 to 0.67568, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 1.4412 - accuracy: 0.9830 - val_loss: 2.7469 - val_accuracy: 0.6757\n",
            "Epoch 135/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4252 - accuracy: 0.9898\n",
            "Epoch 135: val_accuracy did not improve from 0.67568\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 1.4252 - accuracy: 0.9898 - val_loss: 2.6764 - val_accuracy: 0.6757\n",
            "Epoch 136/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4139 - accuracy: 0.9844\n",
            "Epoch 136: val_accuracy improved from 0.67568 to 0.68919, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 1.4116 - accuracy: 0.9830 - val_loss: 2.6010 - val_accuracy: 0.6892\n",
            "Epoch 137/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3902 - accuracy: 0.9844\n",
            "Epoch 137: val_accuracy improved from 0.68919 to 0.70270, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 1.3919 - accuracy: 0.9830 - val_loss: 2.5148 - val_accuracy: 0.7027\n",
            "Epoch 138/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.3869 - accuracy: 0.9762\n",
            "Epoch 138: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 1.3869 - accuracy: 0.9762 - val_loss: 2.4215 - val_accuracy: 0.7027\n",
            "Epoch 139/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3526 - accuracy: 0.9844\n",
            "Epoch 139: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3594 - accuracy: 0.9830 - val_loss: 2.3814 - val_accuracy: 0.6757\n",
            "Epoch 140/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3480 - accuracy: 0.9688\n",
            "Epoch 140: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.3475 - accuracy: 0.9762 - val_loss: 2.3561 - val_accuracy: 0.6757\n",
            "Epoch 141/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3376 - accuracy: 0.9766\n",
            "Epoch 141: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.3367 - accuracy: 0.9864 - val_loss: 2.3442 - val_accuracy: 0.6757\n",
            "Epoch 142/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.3120 - accuracy: 0.9864\n",
            "Epoch 142: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.3120 - accuracy: 0.9864 - val_loss: 2.3441 - val_accuracy: 0.6757\n",
            "Epoch 143/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.2965 - accuracy: 0.9864\n",
            "Epoch 143: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.2965 - accuracy: 0.9864 - val_loss: 2.3399 - val_accuracy: 0.6757\n",
            "Epoch 144/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2886 - accuracy: 0.9844\n",
            "Epoch 144: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.2817 - accuracy: 0.9898 - val_loss: 2.3281 - val_accuracy: 0.6757\n",
            "Epoch 145/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.2613 - accuracy: 0.9966\n",
            "Epoch 145: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 1.2613 - accuracy: 0.9966 - val_loss: 2.3317 - val_accuracy: 0.6892\n",
            "Epoch 146/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.2520 - accuracy: 0.9883\n",
            "Epoch 146: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.2487 - accuracy: 0.9898 - val_loss: 2.3115 - val_accuracy: 0.6892\n",
            "Epoch 147/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.2436 - accuracy: 0.9830\n",
            "Epoch 147: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 1.2436 - accuracy: 0.9830 - val_loss: 2.2735 - val_accuracy: 0.6892\n",
            "Epoch 148/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.2327 - accuracy: 0.9805\n",
            "Epoch 148: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.2279 - accuracy: 0.9830 - val_loss: 2.2706 - val_accuracy: 0.6892\n",
            "Epoch 149/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.2080 - accuracy: 0.9844\n",
            "Epoch 149: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.2099 - accuracy: 0.9796 - val_loss: 2.2736 - val_accuracy: 0.6892\n",
            "Epoch 150/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.1949 - accuracy: 0.9830\n",
            "Epoch 150: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.1949 - accuracy: 0.9830 - val_loss: 2.2666 - val_accuracy: 0.6892\n",
            "Epoch 151/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1939 - accuracy: 0.9766\n",
            "Epoch 151: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1791 - accuracy: 0.9796 - val_loss: 2.2254 - val_accuracy: 0.6892\n",
            "Epoch 152/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.1648 - accuracy: 0.9898\n",
            "Epoch 152: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 1.1648 - accuracy: 0.9898 - val_loss: 2.1916 - val_accuracy: 0.6892\n",
            "Epoch 153/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1557 - accuracy: 0.9688\n",
            "Epoch 153: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1557 - accuracy: 0.9830 - val_loss: 2.1551 - val_accuracy: 0.6757\n",
            "Epoch 154/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1431 - accuracy: 0.9766\n",
            "Epoch 154: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1426 - accuracy: 0.9830 - val_loss: 2.1050 - val_accuracy: 0.6892\n",
            "Epoch 155/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1202 - accuracy: 0.9844\n",
            "Epoch 155: val_accuracy did not improve from 0.70270\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.1290 - accuracy: 0.9864 - val_loss: 2.0350 - val_accuracy: 0.7027\n",
            "Epoch 156/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1272 - accuracy: 0.9609\n",
            "Epoch 156: val_accuracy improved from 0.70270 to 0.71622, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 1.1219 - accuracy: 0.9796 - val_loss: 1.9751 - val_accuracy: 0.7162\n",
            "Epoch 157/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.9864\n",
            "Epoch 157: val_accuracy improved from 0.71622 to 0.72973, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 1.0988 - accuracy: 0.9864 - val_loss: 1.9478 - val_accuracy: 0.7297\n",
            "Epoch 158/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.0827 - accuracy: 0.9883\n",
            "Epoch 158: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 1.0861 - accuracy: 0.9830 - val_loss: 1.9383 - val_accuracy: 0.7162\n",
            "Epoch 159/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0758 - accuracy: 0.9922\n",
            "Epoch 159: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0733 - accuracy: 0.9864 - val_loss: 1.9386 - val_accuracy: 0.7162\n",
            "Epoch 160/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0689 - accuracy: 1.0000\n",
            "Epoch 160: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0659 - accuracy: 0.9830 - val_loss: 1.9468 - val_accuracy: 0.7027\n",
            "Epoch 161/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0819 - accuracy: 0.9609\n",
            "Epoch 161: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0527 - accuracy: 0.9796 - val_loss: 1.9532 - val_accuracy: 0.7027\n",
            "Epoch 162/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0519 - accuracy: 0.9688\n",
            "Epoch 162: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0352 - accuracy: 0.9830 - val_loss: 1.9536 - val_accuracy: 0.7297\n",
            "Epoch 163/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0241 - accuracy: 1.0000\n",
            "Epoch 163: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.0243 - accuracy: 0.9932 - val_loss: 1.9447 - val_accuracy: 0.7297\n",
            "Epoch 164/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0297 - accuracy: 0.9844\n",
            "Epoch 164: val_accuracy did not improve from 0.72973\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.0207 - accuracy: 0.9830 - val_loss: 1.9294 - val_accuracy: 0.7027\n",
            "Epoch 165/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9950 - accuracy: 0.9922\n",
            "Epoch 165: val_accuracy improved from 0.72973 to 0.74324, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 1.0012 - accuracy: 0.9830 - val_loss: 1.9349 - val_accuracy: 0.7432\n",
            "Epoch 166/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9750 - accuracy: 1.0000\n",
            "Epoch 166: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.9906 - accuracy: 0.9830 - val_loss: 1.9282 - val_accuracy: 0.7297\n",
            "Epoch 167/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9753 - accuracy: 0.9830\n",
            "Epoch 167: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.9753 - accuracy: 0.9830 - val_loss: 1.9132 - val_accuracy: 0.7297\n",
            "Epoch 168/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9617 - accuracy: 0.9922\n",
            "Epoch 168: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.9660 - accuracy: 0.9796 - val_loss: 1.9045 - val_accuracy: 0.7297\n",
            "Epoch 169/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9500 - accuracy: 0.9844\n",
            "Epoch 169: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.9588 - accuracy: 0.9830 - val_loss: 1.9143 - val_accuracy: 0.7297\n",
            "Epoch 170/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9407 - accuracy: 0.9844\n",
            "Epoch 170: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.9463 - accuracy: 0.9830 - val_loss: 1.9240 - val_accuracy: 0.7297\n",
            "Epoch 171/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9285 - accuracy: 0.9898\n",
            "Epoch 171: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.9285 - accuracy: 0.9898 - val_loss: 1.9194 - val_accuracy: 0.7297\n",
            "Epoch 172/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9242 - accuracy: 0.9922\n",
            "Epoch 172: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.9219 - accuracy: 0.9796 - val_loss: 1.9203 - val_accuracy: 0.7297\n",
            "Epoch 173/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9236 - accuracy: 0.9688\n",
            "Epoch 173: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.9128 - accuracy: 0.9796 - val_loss: 1.9248 - val_accuracy: 0.7432\n",
            "Epoch 174/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.9121 - accuracy: 0.9766\n",
            "Epoch 174: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.9078 - accuracy: 0.9796 - val_loss: 1.9256 - val_accuracy: 0.7162\n",
            "Epoch 175/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.9017 - accuracy: 0.9922\n",
            "Epoch 175: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.8975 - accuracy: 0.9796 - val_loss: 1.9238 - val_accuracy: 0.7162\n",
            "Epoch 176/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8853 - accuracy: 0.9830\n",
            "Epoch 176: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.8853 - accuracy: 0.9830 - val_loss: 1.9149 - val_accuracy: 0.7297\n",
            "Epoch 177/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8749 - accuracy: 0.9796\n",
            "Epoch 177: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.8749 - accuracy: 0.9796 - val_loss: 1.9027 - val_accuracy: 0.7297\n",
            "Epoch 178/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8781 - accuracy: 0.9844\n",
            "Epoch 178: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.8639 - accuracy: 0.9796 - val_loss: 1.8704 - val_accuracy: 0.7432\n",
            "Epoch 179/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8563 - accuracy: 0.9762\n",
            "Epoch 179: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.8563 - accuracy: 0.9762 - val_loss: 1.8284 - val_accuracy: 0.7297\n",
            "Epoch 180/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8451 - accuracy: 1.0000\n",
            "Epoch 180: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.8399 - accuracy: 0.9864 - val_loss: 1.8092 - val_accuracy: 0.7432\n",
            "Epoch 181/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8527 - accuracy: 0.9688\n",
            "Epoch 181: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.8322 - accuracy: 0.9830 - val_loss: 1.8103 - val_accuracy: 0.7162\n",
            "Epoch 182/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8226 - accuracy: 0.9864\n",
            "Epoch 182: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.8226 - accuracy: 0.9864 - val_loss: 1.8278 - val_accuracy: 0.7297\n",
            "Epoch 183/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8103 - accuracy: 0.9830\n",
            "Epoch 183: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.8103 - accuracy: 0.9830 - val_loss: 1.8372 - val_accuracy: 0.7297\n",
            "Epoch 184/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.8007 - accuracy: 0.9844\n",
            "Epoch 184: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.8044 - accuracy: 0.9796 - val_loss: 1.8271 - val_accuracy: 0.7297\n",
            "Epoch 185/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.7990 - accuracy: 0.9805\n",
            "Epoch 185: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.7981 - accuracy: 0.9796 - val_loss: 1.8101 - val_accuracy: 0.7297\n",
            "Epoch 186/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7904 - accuracy: 0.9766\n",
            "Epoch 186: val_accuracy did not improve from 0.74324\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.7851 - accuracy: 0.9796 - val_loss: 1.8074 - val_accuracy: 0.7432\n",
            "Epoch 187/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7956 - accuracy: 0.9609\n",
            "Epoch 187: val_accuracy improved from 0.74324 to 0.75676, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.7801 - accuracy: 0.9796 - val_loss: 1.7966 - val_accuracy: 0.7568\n",
            "Epoch 188/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7717 - accuracy: 0.9796\n",
            "Epoch 188: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.7717 - accuracy: 0.9796 - val_loss: 1.7700 - val_accuracy: 0.7568\n",
            "Epoch 189/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.7631 - accuracy: 0.9844\n",
            "Epoch 189: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.7598 - accuracy: 0.9864 - val_loss: 1.7485 - val_accuracy: 0.7568\n",
            "Epoch 190/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7660 - accuracy: 0.9766\n",
            "Epoch 190: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.7560 - accuracy: 0.9762 - val_loss: 1.7093 - val_accuracy: 0.7432\n",
            "Epoch 191/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7422 - accuracy: 0.9766\n",
            "Epoch 191: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.7426 - accuracy: 0.9830 - val_loss: 1.6652 - val_accuracy: 0.7297\n",
            "Epoch 192/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7636 - accuracy: 0.9766\n",
            "Epoch 192: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.7378 - accuracy: 0.9864 - val_loss: 1.6196 - val_accuracy: 0.7297\n",
            "Epoch 193/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7245 - accuracy: 0.9830\n",
            "Epoch 193: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.7245 - accuracy: 0.9830 - val_loss: 1.6003 - val_accuracy: 0.7162\n",
            "Epoch 194/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7295 - accuracy: 0.9844\n",
            "Epoch 194: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.7170 - accuracy: 0.9864 - val_loss: 1.5883 - val_accuracy: 0.6892\n",
            "Epoch 195/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.7023 - accuracy: 0.9844\n",
            "Epoch 195: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.7101 - accuracy: 0.9898 - val_loss: 1.5947 - val_accuracy: 0.7027\n",
            "Epoch 196/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6872 - accuracy: 0.9922\n",
            "Epoch 196: val_accuracy did not improve from 0.75676\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.6987 - accuracy: 0.9898 - val_loss: 1.5682 - val_accuracy: 0.7432\n",
            "Epoch 197/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6895 - accuracy: 0.9864\n",
            "Epoch 197: val_accuracy improved from 0.75676 to 0.77027, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.6895 - accuracy: 0.9864 - val_loss: 1.6424 - val_accuracy: 0.7703\n",
            "Epoch 198/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6784 - accuracy: 0.9844\n",
            "Epoch 198: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.6850 - accuracy: 0.9830 - val_loss: 1.7145 - val_accuracy: 0.7162\n",
            "Epoch 199/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6760 - accuracy: 0.9922\n",
            "Epoch 199: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.6777 - accuracy: 0.9796 - val_loss: 1.7648 - val_accuracy: 0.7027\n",
            "Epoch 200/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6627 - accuracy: 0.9844\n",
            "Epoch 200: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.6691 - accuracy: 0.9796 - val_loss: 1.7760 - val_accuracy: 0.7027\n",
            "Epoch 201/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6500 - accuracy: 0.9844\n",
            "Epoch 201: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.6589 - accuracy: 0.9796 - val_loss: 1.7972 - val_accuracy: 0.6892\n",
            "Epoch 202/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6457 - accuracy: 0.9844\n",
            "Epoch 202: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.6583 - accuracy: 0.9762 - val_loss: 1.8059 - val_accuracy: 0.7027\n",
            "Epoch 203/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6453 - accuracy: 0.9762\n",
            "Epoch 203: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.6453 - accuracy: 0.9762 - val_loss: 1.7938 - val_accuracy: 0.7027\n",
            "Epoch 204/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6372 - accuracy: 0.9830\n",
            "Epoch 204: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.6372 - accuracy: 0.9830 - val_loss: 1.7809 - val_accuracy: 0.7027\n",
            "Epoch 205/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6398 - accuracy: 0.9844\n",
            "Epoch 205: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6353 - accuracy: 0.9762 - val_loss: 1.7589 - val_accuracy: 0.7162\n",
            "Epoch 206/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6235 - accuracy: 0.9766\n",
            "Epoch 206: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.6249 - accuracy: 0.9796 - val_loss: 1.7503 - val_accuracy: 0.7297\n",
            "Epoch 207/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6174 - accuracy: 0.9864\n",
            "Epoch 207: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.6174 - accuracy: 0.9864 - val_loss: 1.8069 - val_accuracy: 0.7297\n",
            "Epoch 208/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6119 - accuracy: 0.9922\n",
            "Epoch 208: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.6081 - accuracy: 0.9898 - val_loss: 1.8358 - val_accuracy: 0.7297\n",
            "Epoch 209/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.6088 - accuracy: 0.9844\n",
            "Epoch 209: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.6029 - accuracy: 0.9796 - val_loss: 1.9086 - val_accuracy: 0.7027\n",
            "Epoch 210/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5874 - accuracy: 0.9766\n",
            "Epoch 210: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.5961 - accuracy: 0.9796 - val_loss: 1.9518 - val_accuracy: 0.7027\n",
            "Epoch 211/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.9830\n",
            "Epoch 211: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.5877 - accuracy: 0.9830 - val_loss: 1.9230 - val_accuracy: 0.7027\n",
            "Epoch 212/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5830 - accuracy: 0.9864\n",
            "Epoch 212: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.5830 - accuracy: 0.9864 - val_loss: 1.8625 - val_accuracy: 0.7027\n",
            "Epoch 213/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5760 - accuracy: 0.9830\n",
            "Epoch 213: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.5760 - accuracy: 0.9830 - val_loss: 1.7696 - val_accuracy: 0.7297\n",
            "Epoch 214/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5599 - accuracy: 0.9844\n",
            "Epoch 214: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.5663 - accuracy: 0.9830 - val_loss: 1.6813 - val_accuracy: 0.7568\n",
            "Epoch 215/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5538 - accuracy: 0.9922\n",
            "Epoch 215: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5647 - accuracy: 0.9830 - val_loss: 1.6288 - val_accuracy: 0.7703\n",
            "Epoch 216/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5708 - accuracy: 0.9766\n",
            "Epoch 216: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.5634 - accuracy: 0.9796 - val_loss: 1.6189 - val_accuracy: 0.7703\n",
            "Epoch 217/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5446 - accuracy: 0.9844\n",
            "Epoch 217: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5518 - accuracy: 0.9796 - val_loss: 1.5847 - val_accuracy: 0.7703\n",
            "Epoch 218/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5524 - accuracy: 0.9922\n",
            "Epoch 218: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5434 - accuracy: 0.9864 - val_loss: 1.5714 - val_accuracy: 0.7703\n",
            "Epoch 219/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5296 - accuracy: 0.9844\n",
            "Epoch 219: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5407 - accuracy: 0.9796 - val_loss: 1.5626 - val_accuracy: 0.7297\n",
            "Epoch 220/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5373 - accuracy: 0.9766\n",
            "Epoch 220: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5390 - accuracy: 0.9796 - val_loss: 1.5271 - val_accuracy: 0.7297\n",
            "Epoch 221/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5303 - accuracy: 0.9805\n",
            "Epoch 221: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.5269 - accuracy: 0.9830 - val_loss: 1.5127 - val_accuracy: 0.7162\n",
            "Epoch 222/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5135 - accuracy: 0.9844\n",
            "Epoch 222: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.5190 - accuracy: 0.9830 - val_loss: 1.4854 - val_accuracy: 0.7432\n",
            "Epoch 223/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5154 - accuracy: 0.9844\n",
            "Epoch 223: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.5149 - accuracy: 0.9830 - val_loss: 1.4530 - val_accuracy: 0.7432\n",
            "Epoch 224/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.9830\n",
            "Epoch 224: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.5112 - accuracy: 0.9830 - val_loss: 1.4313 - val_accuracy: 0.7297\n",
            "Epoch 225/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5172 - accuracy: 0.9844\n",
            "Epoch 225: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.5034 - accuracy: 0.9830 - val_loss: 1.4172 - val_accuracy: 0.7297\n",
            "Epoch 226/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.5012 - accuracy: 0.9766\n",
            "Epoch 226: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4979 - accuracy: 0.9830 - val_loss: 1.4098 - val_accuracy: 0.7432\n",
            "Epoch 227/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4821 - accuracy: 0.9844\n",
            "Epoch 227: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4934 - accuracy: 0.9796 - val_loss: 1.4066 - val_accuracy: 0.7432\n",
            "Epoch 228/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4781 - accuracy: 0.9844\n",
            "Epoch 228: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4845 - accuracy: 0.9796 - val_loss: 1.3968 - val_accuracy: 0.7432\n",
            "Epoch 229/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4692 - accuracy: 0.9844\n",
            "Epoch 229: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4791 - accuracy: 0.9796 - val_loss: 1.3956 - val_accuracy: 0.7432\n",
            "Epoch 230/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4771 - accuracy: 0.9796\n",
            "Epoch 230: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.4771 - accuracy: 0.9796 - val_loss: 1.3988 - val_accuracy: 0.7432\n",
            "Epoch 231/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4670 - accuracy: 0.9864\n",
            "Epoch 231: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.4670 - accuracy: 0.9864 - val_loss: 1.4059 - val_accuracy: 0.7432\n",
            "Epoch 232/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4670 - accuracy: 0.9844\n",
            "Epoch 232: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4664 - accuracy: 0.9830 - val_loss: 1.4074 - val_accuracy: 0.7432\n",
            "Epoch 233/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4595 - accuracy: 0.9766\n",
            "Epoch 233: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4581 - accuracy: 0.9830 - val_loss: 1.4167 - val_accuracy: 0.7297\n",
            "Epoch 234/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4721 - accuracy: 0.9844\n",
            "Epoch 234: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4574 - accuracy: 0.9796 - val_loss: 1.4279 - val_accuracy: 0.7162\n",
            "Epoch 235/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4646 - accuracy: 0.9766\n",
            "Epoch 235: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4533 - accuracy: 0.9796 - val_loss: 1.4647 - val_accuracy: 0.7027\n",
            "Epoch 236/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4458 - accuracy: 0.9844\n",
            "Epoch 236: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4468 - accuracy: 0.9796 - val_loss: 1.4669 - val_accuracy: 0.7432\n",
            "Epoch 237/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4295 - accuracy: 0.9844\n",
            "Epoch 237: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4407 - accuracy: 0.9796 - val_loss: 1.4545 - val_accuracy: 0.7432\n",
            "Epoch 238/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4226 - accuracy: 0.9922\n",
            "Epoch 238: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4417 - accuracy: 0.9796 - val_loss: 1.4742 - val_accuracy: 0.7297\n",
            "Epoch 239/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4327 - accuracy: 0.9830\n",
            "Epoch 239: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.4327 - accuracy: 0.9830 - val_loss: 1.5477 - val_accuracy: 0.7297\n",
            "Epoch 240/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4181 - accuracy: 0.9844\n",
            "Epoch 240: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4271 - accuracy: 0.9830 - val_loss: 1.6373 - val_accuracy: 0.7297\n",
            "Epoch 241/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4415 - accuracy: 0.9688\n",
            "Epoch 241: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4256 - accuracy: 0.9830 - val_loss: 1.7219 - val_accuracy: 0.7162\n",
            "Epoch 242/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4121 - accuracy: 0.9844\n",
            "Epoch 242: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4208 - accuracy: 0.9796 - val_loss: 1.7422 - val_accuracy: 0.7162\n",
            "Epoch 243/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4080 - accuracy: 0.9766\n",
            "Epoch 243: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4138 - accuracy: 0.9830 - val_loss: 1.7089 - val_accuracy: 0.7162\n",
            "Epoch 244/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.9762\n",
            "Epoch 244: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.4117 - accuracy: 0.9762 - val_loss: 1.6561 - val_accuracy: 0.7027\n",
            "Epoch 245/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4145 - accuracy: 0.9766\n",
            "Epoch 245: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4037 - accuracy: 0.9796 - val_loss: 1.5927 - val_accuracy: 0.7027\n",
            "Epoch 246/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4099 - accuracy: 0.9766\n",
            "Epoch 246: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3996 - accuracy: 0.9796 - val_loss: 1.5082 - val_accuracy: 0.7162\n",
            "Epoch 247/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3985 - accuracy: 0.9766\n",
            "Epoch 247: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.3953 - accuracy: 0.9796 - val_loss: 1.4634 - val_accuracy: 0.7432\n",
            "Epoch 248/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4051 - accuracy: 0.9844\n",
            "Epoch 248: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3921 - accuracy: 0.9830 - val_loss: 1.4516 - val_accuracy: 0.7297\n",
            "Epoch 249/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4006 - accuracy: 0.9609\n",
            "Epoch 249: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3861 - accuracy: 0.9796 - val_loss: 1.4542 - val_accuracy: 0.7297\n",
            "Epoch 250/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.4174 - accuracy: 0.9531\n",
            "Epoch 250: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3874 - accuracy: 0.9762 - val_loss: 1.4400 - val_accuracy: 0.7162\n",
            "Epoch 251/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3835 - accuracy: 0.9844\n",
            "Epoch 251: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3740 - accuracy: 0.9830 - val_loss: 1.4449 - val_accuracy: 0.7162\n",
            "Epoch 252/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3795 - accuracy: 0.9922\n",
            "Epoch 252: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.3705 - accuracy: 0.9864 - val_loss: 1.4482 - val_accuracy: 0.7297\n",
            "Epoch 253/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3617 - accuracy: 0.9766\n",
            "Epoch 253: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3684 - accuracy: 0.9796 - val_loss: 1.4580 - val_accuracy: 0.7297\n",
            "Epoch 254/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3784 - accuracy: 0.9609\n",
            "Epoch 254: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.3650 - accuracy: 0.9830 - val_loss: 1.4768 - val_accuracy: 0.7297\n",
            "Epoch 255/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3641 - accuracy: 0.9844\n",
            "Epoch 255: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3605 - accuracy: 0.9864 - val_loss: 1.5100 - val_accuracy: 0.7297\n",
            "Epoch 256/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3608 - accuracy: 1.0000\n",
            "Epoch 256: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3559 - accuracy: 0.9932 - val_loss: 1.5192 - val_accuracy: 0.7297\n",
            "Epoch 257/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3436 - accuracy: 1.0000\n",
            "Epoch 257: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3528 - accuracy: 0.9864 - val_loss: 1.5231 - val_accuracy: 0.7297\n",
            "Epoch 258/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3516 - accuracy: 0.9922\n",
            "Epoch 258: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3518 - accuracy: 0.9864 - val_loss: 1.5022 - val_accuracy: 0.7297\n",
            "Epoch 259/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3491 - accuracy: 0.9844\n",
            "Epoch 259: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3440 - accuracy: 0.9864 - val_loss: 1.4861 - val_accuracy: 0.7297\n",
            "Epoch 260/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3507 - accuracy: 0.9844\n",
            "Epoch 260: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3435 - accuracy: 0.9796 - val_loss: 1.4577 - val_accuracy: 0.7432\n",
            "Epoch 261/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3598 - accuracy: 0.9609\n",
            "Epoch 261: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3399 - accuracy: 0.9762 - val_loss: 1.4392 - val_accuracy: 0.7432\n",
            "Epoch 262/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3637 - accuracy: 0.9688\n",
            "Epoch 262: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3408 - accuracy: 0.9762 - val_loss: 1.4020 - val_accuracy: 0.7432\n",
            "Epoch 263/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.9864\n",
            "Epoch 263: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.3301 - accuracy: 0.9864 - val_loss: 1.3803 - val_accuracy: 0.7432\n",
            "Epoch 264/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3467 - accuracy: 0.9766\n",
            "Epoch 264: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3297 - accuracy: 0.9830 - val_loss: 1.3577 - val_accuracy: 0.7432\n",
            "Epoch 265/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3259 - accuracy: 0.9796\n",
            "Epoch 265: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.3259 - accuracy: 0.9796 - val_loss: 1.3328 - val_accuracy: 0.7568\n",
            "Epoch 266/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3267 - accuracy: 0.9922\n",
            "Epoch 266: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3191 - accuracy: 0.9830 - val_loss: 1.3131 - val_accuracy: 0.7568\n",
            "Epoch 267/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3337 - accuracy: 0.9609\n",
            "Epoch 267: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3157 - accuracy: 0.9796 - val_loss: 1.3057 - val_accuracy: 0.7568\n",
            "Epoch 268/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.9796\n",
            "Epoch 268: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.3138 - accuracy: 0.9796 - val_loss: 1.3117 - val_accuracy: 0.7432\n",
            "Epoch 269/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3258 - accuracy: 0.9844\n",
            "Epoch 269: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.3151 - accuracy: 0.9830 - val_loss: 1.3192 - val_accuracy: 0.7162\n",
            "Epoch 270/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3089 - accuracy: 0.9844\n",
            "Epoch 270: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3090 - accuracy: 0.9830 - val_loss: 1.3169 - val_accuracy: 0.7027\n",
            "Epoch 271/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3106 - accuracy: 0.9766\n",
            "Epoch 271: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3028 - accuracy: 0.9796 - val_loss: 1.3085 - val_accuracy: 0.7027\n",
            "Epoch 272/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.9762\n",
            "Epoch 272: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.3078 - accuracy: 0.9762 - val_loss: 1.2874 - val_accuracy: 0.7297\n",
            "Epoch 273/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3140 - accuracy: 0.9766\n",
            "Epoch 273: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3011 - accuracy: 0.9796 - val_loss: 1.2779 - val_accuracy: 0.7432\n",
            "Epoch 274/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9796\n",
            "Epoch 274: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.2942 - accuracy: 0.9796 - val_loss: 1.2793 - val_accuracy: 0.7432\n",
            "Epoch 275/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.3004 - accuracy: 0.9844\n",
            "Epoch 275: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2923 - accuracy: 0.9796 - val_loss: 1.2894 - val_accuracy: 0.7432\n",
            "Epoch 276/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.9830\n",
            "Epoch 276: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.2922 - accuracy: 0.9830 - val_loss: 1.2954 - val_accuracy: 0.7432\n",
            "Epoch 277/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2866 - accuracy: 0.9805\n",
            "Epoch 277: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.2873 - accuracy: 0.9796 - val_loss: 1.3011 - val_accuracy: 0.7432\n",
            "Epoch 278/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2817 - accuracy: 0.9766\n",
            "Epoch 278: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2833 - accuracy: 0.9796 - val_loss: 1.2986 - val_accuracy: 0.7568\n",
            "Epoch 279/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2808 - accuracy: 0.9766\n",
            "Epoch 279: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2819 - accuracy: 0.9830 - val_loss: 1.2992 - val_accuracy: 0.7568\n",
            "Epoch 280/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2652 - accuracy: 1.0000\n",
            "Epoch 280: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2757 - accuracy: 0.9830 - val_loss: 1.2863 - val_accuracy: 0.7568\n",
            "Epoch 281/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2863 - accuracy: 0.9688\n",
            "Epoch 281: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2703 - accuracy: 0.9830 - val_loss: 1.2717 - val_accuracy: 0.7297\n",
            "Epoch 282/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2540 - accuracy: 1.0000\n",
            "Epoch 282: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2764 - accuracy: 0.9898 - val_loss: 1.2550 - val_accuracy: 0.7297\n",
            "Epoch 283/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2570 - accuracy: 0.9922\n",
            "Epoch 283: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2700 - accuracy: 0.9898 - val_loss: 1.2280 - val_accuracy: 0.7568\n",
            "Epoch 284/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2622 - accuracy: 0.9922\n",
            "Epoch 284: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2653 - accuracy: 0.9898 - val_loss: 1.2295 - val_accuracy: 0.7703\n",
            "Epoch 285/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.9796\n",
            "Epoch 285: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.2628 - accuracy: 0.9796 - val_loss: 1.2622 - val_accuracy: 0.7568\n",
            "Epoch 286/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2685 - accuracy: 0.9766\n",
            "Epoch 286: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2657 - accuracy: 0.9796 - val_loss: 1.2934 - val_accuracy: 0.7162\n",
            "Epoch 287/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2572 - accuracy: 0.9844\n",
            "Epoch 287: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2564 - accuracy: 0.9796 - val_loss: 1.3114 - val_accuracy: 0.7027\n",
            "Epoch 288/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2690 - accuracy: 0.9609\n",
            "Epoch 288: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2570 - accuracy: 0.9796 - val_loss: 1.2947 - val_accuracy: 0.7162\n",
            "Epoch 289/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2513 - accuracy: 0.9766\n",
            "Epoch 289: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2493 - accuracy: 0.9796 - val_loss: 1.2790 - val_accuracy: 0.7162\n",
            "Epoch 290/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.9796\n",
            "Epoch 290: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.2492 - accuracy: 0.9796 - val_loss: 1.2711 - val_accuracy: 0.7162\n",
            "Epoch 291/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2523 - accuracy: 0.9766\n",
            "Epoch 291: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2487 - accuracy: 0.9830 - val_loss: 1.2683 - val_accuracy: 0.7162\n",
            "Epoch 292/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2606 - accuracy: 0.9766\n",
            "Epoch 292: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2472 - accuracy: 0.9796 - val_loss: 1.2689 - val_accuracy: 0.7162\n",
            "Epoch 293/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2524 - accuracy: 0.9766\n",
            "Epoch 293: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2392 - accuracy: 0.9830 - val_loss: 1.2620 - val_accuracy: 0.7162\n",
            "Epoch 294/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2311 - accuracy: 0.9844\n",
            "Epoch 294: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.2370 - accuracy: 0.9864 - val_loss: 1.2515 - val_accuracy: 0.7297\n",
            "Epoch 295/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.9796\n",
            "Epoch 295: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.2386 - accuracy: 0.9796 - val_loss: 1.2704 - val_accuracy: 0.7297\n",
            "Epoch 296/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2432 - accuracy: 0.9844\n",
            "Epoch 296: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2354 - accuracy: 0.9830 - val_loss: 1.2877 - val_accuracy: 0.7297\n",
            "Epoch 297/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2463 - accuracy: 0.9844\n",
            "Epoch 297: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2367 - accuracy: 0.9762 - val_loss: 1.2931 - val_accuracy: 0.7297\n",
            "Epoch 298/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2233 - accuracy: 0.9805\n",
            "Epoch 298: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.2292 - accuracy: 0.9830 - val_loss: 1.2979 - val_accuracy: 0.7297\n",
            "Epoch 299/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2347 - accuracy: 0.9688\n",
            "Epoch 299: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2307 - accuracy: 0.9762 - val_loss: 1.3031 - val_accuracy: 0.7432\n",
            "Epoch 300/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 0.9796\n",
            "Epoch 300: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.2274 - accuracy: 0.9796 - val_loss: 1.2780 - val_accuracy: 0.7432\n",
            "Epoch 301/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2228 - accuracy: 0.9805\n",
            "Epoch 301: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.2240 - accuracy: 0.9796 - val_loss: 1.2614 - val_accuracy: 0.7297\n",
            "Epoch 302/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.9796\n",
            "Epoch 302: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.2221 - accuracy: 0.9796 - val_loss: 1.2259 - val_accuracy: 0.7297\n",
            "Epoch 303/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2144 - accuracy: 0.9766\n",
            "Epoch 303: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2198 - accuracy: 0.9864 - val_loss: 1.1963 - val_accuracy: 0.7297\n",
            "Epoch 304/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2218 - accuracy: 0.9688\n",
            "Epoch 304: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2183 - accuracy: 0.9796 - val_loss: 1.1813 - val_accuracy: 0.7432\n",
            "Epoch 305/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2083 - accuracy: 0.9922\n",
            "Epoch 305: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2151 - accuracy: 0.9796 - val_loss: 1.1705 - val_accuracy: 0.7432\n",
            "Epoch 306/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9796\n",
            "Epoch 306: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.2132 - accuracy: 0.9796 - val_loss: 1.1630 - val_accuracy: 0.7432\n",
            "Epoch 307/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2158 - accuracy: 0.9766\n",
            "Epoch 307: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2085 - accuracy: 0.9796 - val_loss: 1.1501 - val_accuracy: 0.7432\n",
            "Epoch 308/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2082 - accuracy: 0.9830\n",
            "Epoch 308: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.2082 - accuracy: 0.9830 - val_loss: 1.1547 - val_accuracy: 0.7568\n",
            "Epoch 309/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2192 - accuracy: 0.9844\n",
            "Epoch 309: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2108 - accuracy: 0.9830 - val_loss: 1.1606 - val_accuracy: 0.7703\n",
            "Epoch 310/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9762\n",
            "Epoch 310: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.2065 - accuracy: 0.9762 - val_loss: 1.1698 - val_accuracy: 0.7568\n",
            "Epoch 311/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1985 - accuracy: 0.9766\n",
            "Epoch 311: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2042 - accuracy: 0.9830 - val_loss: 1.1963 - val_accuracy: 0.7703\n",
            "Epoch 312/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1951 - accuracy: 0.9766\n",
            "Epoch 312: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2037 - accuracy: 0.9796 - val_loss: 1.2395 - val_accuracy: 0.7703\n",
            "Epoch 313/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1901 - accuracy: 0.9922\n",
            "Epoch 313: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2033 - accuracy: 0.9796 - val_loss: 1.2597 - val_accuracy: 0.7703\n",
            "Epoch 314/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1823 - accuracy: 0.9922\n",
            "Epoch 314: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2029 - accuracy: 0.9796 - val_loss: 1.2646 - val_accuracy: 0.7703\n",
            "Epoch 315/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1969 - accuracy: 0.9844\n",
            "Epoch 315: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1972 - accuracy: 0.9796 - val_loss: 1.2431 - val_accuracy: 0.7703\n",
            "Epoch 316/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1988 - accuracy: 0.9766\n",
            "Epoch 316: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1907 - accuracy: 0.9796 - val_loss: 1.2116 - val_accuracy: 0.7568\n",
            "Epoch 317/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1927 - accuracy: 0.9688\n",
            "Epoch 317: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1937 - accuracy: 0.9762 - val_loss: 1.1742 - val_accuracy: 0.7568\n",
            "Epoch 318/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.9830\n",
            "Epoch 318: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1897 - accuracy: 0.9830 - val_loss: 1.1639 - val_accuracy: 0.7703\n",
            "Epoch 319/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1719 - accuracy: 0.9922\n",
            "Epoch 319: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1872 - accuracy: 0.9796 - val_loss: 1.1623 - val_accuracy: 0.7703\n",
            "Epoch 320/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.2040 - accuracy: 0.9766\n",
            "Epoch 320: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1864 - accuracy: 0.9830 - val_loss: 1.1618 - val_accuracy: 0.7703\n",
            "Epoch 321/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1908 - accuracy: 0.9922\n",
            "Epoch 321: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1841 - accuracy: 0.9796 - val_loss: 1.1785 - val_accuracy: 0.7568\n",
            "Epoch 322/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1689 - accuracy: 0.9844\n",
            "Epoch 322: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1914 - accuracy: 0.9762 - val_loss: 1.2709 - val_accuracy: 0.7297\n",
            "Epoch 323/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1765 - accuracy: 0.9688\n",
            "Epoch 323: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1774 - accuracy: 0.9796 - val_loss: 1.4228 - val_accuracy: 0.7027\n",
            "Epoch 324/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1849 - accuracy: 0.9766\n",
            "Epoch 324: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1815 - accuracy: 0.9796 - val_loss: 1.5520 - val_accuracy: 0.6892\n",
            "Epoch 325/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1666 - accuracy: 0.9844\n",
            "Epoch 325: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1783 - accuracy: 0.9796 - val_loss: 1.5854 - val_accuracy: 0.6892\n",
            "Epoch 326/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1677 - accuracy: 0.9922\n",
            "Epoch 326: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1793 - accuracy: 0.9830 - val_loss: 1.5659 - val_accuracy: 0.6757\n",
            "Epoch 327/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9796\n",
            "Epoch 327: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1746 - accuracy: 0.9796 - val_loss: 1.4963 - val_accuracy: 0.6757\n",
            "Epoch 328/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1798 - accuracy: 0.9609\n",
            "Epoch 328: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1736 - accuracy: 0.9830 - val_loss: 1.4026 - val_accuracy: 0.6892\n",
            "Epoch 329/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1931 - accuracy: 0.9688\n",
            "Epoch 329: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1727 - accuracy: 0.9830 - val_loss: 1.3404 - val_accuracy: 0.7027\n",
            "Epoch 330/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9864\n",
            "Epoch 330: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1724 - accuracy: 0.9864 - val_loss: 1.3092 - val_accuracy: 0.6892\n",
            "Epoch 331/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1731 - accuracy: 0.9922\n",
            "Epoch 331: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1682 - accuracy: 0.9864 - val_loss: 1.2799 - val_accuracy: 0.6892\n",
            "Epoch 332/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1667 - accuracy: 0.9688\n",
            "Epoch 332: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1649 - accuracy: 0.9830 - val_loss: 1.2792 - val_accuracy: 0.6892\n",
            "Epoch 333/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1636 - accuracy: 0.9844\n",
            "Epoch 333: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1658 - accuracy: 0.9796 - val_loss: 1.2979 - val_accuracy: 0.7027\n",
            "Epoch 334/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1542 - accuracy: 1.0000\n",
            "Epoch 334: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1631 - accuracy: 0.9796 - val_loss: 1.2994 - val_accuracy: 0.7027\n",
            "Epoch 335/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1509 - accuracy: 0.9844\n",
            "Epoch 335: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1612 - accuracy: 0.9796 - val_loss: 1.3080 - val_accuracy: 0.7162\n",
            "Epoch 336/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1704 - accuracy: 0.9844\n",
            "Epoch 336: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1617 - accuracy: 0.9796 - val_loss: 1.3139 - val_accuracy: 0.7297\n",
            "Epoch 337/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1561 - accuracy: 0.9766\n",
            "Epoch 337: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1573 - accuracy: 0.9796 - val_loss: 1.2913 - val_accuracy: 0.7432\n",
            "Epoch 338/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1516 - accuracy: 0.9922\n",
            "Epoch 338: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1541 - accuracy: 0.9796 - val_loss: 1.2676 - val_accuracy: 0.7703\n",
            "Epoch 339/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9796\n",
            "Epoch 339: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1572 - accuracy: 0.9796 - val_loss: 1.2080 - val_accuracy: 0.7703\n",
            "Epoch 340/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1502 - accuracy: 0.9766\n",
            "Epoch 340: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1552 - accuracy: 0.9796 - val_loss: 1.1601 - val_accuracy: 0.7432\n",
            "Epoch 341/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1552 - accuracy: 0.9796\n",
            "Epoch 341: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1552 - accuracy: 0.9796 - val_loss: 1.1307 - val_accuracy: 0.7432\n",
            "Epoch 342/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1592 - accuracy: 0.9922\n",
            "Epoch 342: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1524 - accuracy: 0.9830 - val_loss: 1.1168 - val_accuracy: 0.7432\n",
            "Epoch 343/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1400 - accuracy: 0.9844\n",
            "Epoch 343: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1526 - accuracy: 0.9864 - val_loss: 1.0964 - val_accuracy: 0.7703\n",
            "Epoch 344/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1558 - accuracy: 0.9766\n",
            "Epoch 344: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1489 - accuracy: 0.9796 - val_loss: 1.1490 - val_accuracy: 0.7027\n",
            "Epoch 345/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1428 - accuracy: 0.9766\n",
            "Epoch 345: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1474 - accuracy: 0.9830 - val_loss: 1.2241 - val_accuracy: 0.6622\n",
            "Epoch 346/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1542 - accuracy: 0.9766\n",
            "Epoch 346: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1461 - accuracy: 0.9796 - val_loss: 1.2628 - val_accuracy: 0.6757\n",
            "Epoch 347/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1488 - accuracy: 0.9922\n",
            "Epoch 347: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1465 - accuracy: 0.9864 - val_loss: 1.2271 - val_accuracy: 0.6757\n",
            "Epoch 348/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9796\n",
            "Epoch 348: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1454 - accuracy: 0.9796 - val_loss: 1.1802 - val_accuracy: 0.6757\n",
            "Epoch 349/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9830\n",
            "Epoch 349: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1437 - accuracy: 0.9830 - val_loss: 1.1498 - val_accuracy: 0.6892\n",
            "Epoch 350/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1248 - accuracy: 0.9922\n",
            "Epoch 350: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1403 - accuracy: 0.9830 - val_loss: 1.1448 - val_accuracy: 0.7162\n",
            "Epoch 351/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1556 - accuracy: 0.9688\n",
            "Epoch 351: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1410 - accuracy: 0.9796 - val_loss: 1.1493 - val_accuracy: 0.7162\n",
            "Epoch 352/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1358 - accuracy: 0.9844\n",
            "Epoch 352: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1381 - accuracy: 0.9796 - val_loss: 1.1516 - val_accuracy: 0.7162\n",
            "Epoch 353/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9796\n",
            "Epoch 353: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.1369 - accuracy: 0.9796 - val_loss: 1.1574 - val_accuracy: 0.7162\n",
            "Epoch 354/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1247 - accuracy: 0.9844\n",
            "Epoch 354: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1363 - accuracy: 0.9796 - val_loss: 1.1756 - val_accuracy: 0.7027\n",
            "Epoch 355/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1463 - accuracy: 0.9688\n",
            "Epoch 355: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1333 - accuracy: 0.9796 - val_loss: 1.2301 - val_accuracy: 0.6757\n",
            "Epoch 356/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9796\n",
            "Epoch 356: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1315 - accuracy: 0.9796 - val_loss: 1.2219 - val_accuracy: 0.6892\n",
            "Epoch 357/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1461 - accuracy: 0.9766\n",
            "Epoch 357: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1306 - accuracy: 0.9864 - val_loss: 1.1856 - val_accuracy: 0.6892\n",
            "Epoch 358/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9796\n",
            "Epoch 358: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.1335 - accuracy: 0.9796 - val_loss: 1.1498 - val_accuracy: 0.7027\n",
            "Epoch 359/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9830\n",
            "Epoch 359: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1306 - accuracy: 0.9830 - val_loss: 1.0976 - val_accuracy: 0.7162\n",
            "Epoch 360/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1121 - accuracy: 0.9922\n",
            "Epoch 360: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1300 - accuracy: 0.9864 - val_loss: 1.0764 - val_accuracy: 0.7432\n",
            "Epoch 361/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1329 - accuracy: 0.9766\n",
            "Epoch 361: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1263 - accuracy: 0.9796 - val_loss: 1.0687 - val_accuracy: 0.7703\n",
            "Epoch 362/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1287 - accuracy: 0.9805\n",
            "Epoch 362: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.1261 - accuracy: 0.9830 - val_loss: 1.0917 - val_accuracy: 0.7568\n",
            "Epoch 363/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9796\n",
            "Epoch 363: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1242 - accuracy: 0.9796 - val_loss: 1.1156 - val_accuracy: 0.7703\n",
            "Epoch 364/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1325 - accuracy: 0.9766\n",
            "Epoch 364: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1230 - accuracy: 0.9796 - val_loss: 1.1375 - val_accuracy: 0.7703\n",
            "Epoch 365/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1253 - accuracy: 0.9805\n",
            "Epoch 365: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1248 - accuracy: 0.9796 - val_loss: 1.1437 - val_accuracy: 0.7568\n",
            "Epoch 366/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1240 - accuracy: 0.9766\n",
            "Epoch 366: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1234 - accuracy: 0.9830 - val_loss: 1.1544 - val_accuracy: 0.7568\n",
            "Epoch 367/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9864\n",
            "Epoch 367: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1242 - accuracy: 0.9864 - val_loss: 1.1649 - val_accuracy: 0.7568\n",
            "Epoch 368/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1114 - accuracy: 1.0000\n",
            "Epoch 368: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1241 - accuracy: 0.9796 - val_loss: 1.1721 - val_accuracy: 0.7568\n",
            "Epoch 369/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1227 - accuracy: 0.9766\n",
            "Epoch 369: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1192 - accuracy: 0.9796 - val_loss: 1.1946 - val_accuracy: 0.7297\n",
            "Epoch 370/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1347 - accuracy: 0.9766\n",
            "Epoch 370: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1209 - accuracy: 0.9796 - val_loss: 1.2029 - val_accuracy: 0.7297\n",
            "Epoch 371/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1182 - accuracy: 0.9688\n",
            "Epoch 371: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1195 - accuracy: 0.9796 - val_loss: 1.2100 - val_accuracy: 0.7432\n",
            "Epoch 372/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1339 - accuracy: 0.9688\n",
            "Epoch 372: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1175 - accuracy: 0.9796 - val_loss: 1.2180 - val_accuracy: 0.7432\n",
            "Epoch 373/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.1139 - accuracy: 0.9805\n",
            "Epoch 373: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1130 - accuracy: 0.9796 - val_loss: 1.2103 - val_accuracy: 0.7432\n",
            "Epoch 374/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.9762\n",
            "Epoch 374: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1174 - accuracy: 0.9762 - val_loss: 1.1936 - val_accuracy: 0.7432\n",
            "Epoch 375/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1028 - accuracy: 0.9922\n",
            "Epoch 375: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1154 - accuracy: 0.9796 - val_loss: 1.1761 - val_accuracy: 0.7432\n",
            "Epoch 376/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1179 - accuracy: 0.9766\n",
            "Epoch 376: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1130 - accuracy: 0.9796 - val_loss: 1.1535 - val_accuracy: 0.7432\n",
            "Epoch 377/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1392 - accuracy: 0.9531\n",
            "Epoch 377: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1129 - accuracy: 0.9796 - val_loss: 1.1380 - val_accuracy: 0.7432\n",
            "Epoch 378/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1213 - accuracy: 0.9688\n",
            "Epoch 378: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1113 - accuracy: 0.9830 - val_loss: 1.1325 - val_accuracy: 0.7162\n",
            "Epoch 379/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1112 - accuracy: 0.9922\n",
            "Epoch 379: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1110 - accuracy: 0.9830 - val_loss: 1.1257 - val_accuracy: 0.7297\n",
            "Epoch 380/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9762\n",
            "Epoch 380: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1152 - accuracy: 0.9762 - val_loss: 1.1242 - val_accuracy: 0.7162\n",
            "Epoch 381/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1095 - accuracy: 0.9844\n",
            "Epoch 381: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1075 - accuracy: 0.9864 - val_loss: 1.1287 - val_accuracy: 0.7162\n",
            "Epoch 382/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1109 - accuracy: 0.9922\n",
            "Epoch 382: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1069 - accuracy: 0.9796 - val_loss: 1.1347 - val_accuracy: 0.7162\n",
            "Epoch 383/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0937 - accuracy: 0.9922\n",
            "Epoch 383: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1036 - accuracy: 0.9864 - val_loss: 1.1396 - val_accuracy: 0.7027\n",
            "Epoch 384/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0907 - accuracy: 1.0000\n",
            "Epoch 384: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1049 - accuracy: 0.9898 - val_loss: 1.1656 - val_accuracy: 0.7568\n",
            "Epoch 385/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0954 - accuracy: 0.9844\n",
            "Epoch 385: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1028 - accuracy: 0.9830 - val_loss: 1.2376 - val_accuracy: 0.7568\n",
            "Epoch 386/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1072 - accuracy: 0.9830\n",
            "Epoch 386: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1072 - accuracy: 0.9830 - val_loss: 1.3116 - val_accuracy: 0.7703\n",
            "Epoch 387/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0988 - accuracy: 0.9922\n",
            "Epoch 387: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1048 - accuracy: 0.9796 - val_loss: 1.3797 - val_accuracy: 0.7432\n",
            "Epoch 388/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9796\n",
            "Epoch 388: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1042 - accuracy: 0.9796 - val_loss: 1.4221 - val_accuracy: 0.7027\n",
            "Epoch 389/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0989 - accuracy: 0.9844\n",
            "Epoch 389: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1051 - accuracy: 0.9796 - val_loss: 1.4386 - val_accuracy: 0.7027\n",
            "Epoch 390/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0918 - accuracy: 0.9922\n",
            "Epoch 390: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1028 - accuracy: 0.9796 - val_loss: 1.4548 - val_accuracy: 0.6757\n",
            "Epoch 391/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0933 - accuracy: 0.9766\n",
            "Epoch 391: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1031 - accuracy: 0.9796 - val_loss: 1.5410 - val_accuracy: 0.6486\n",
            "Epoch 392/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0937 - accuracy: 0.9766\n",
            "Epoch 392: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1018 - accuracy: 0.9796 - val_loss: 1.6611 - val_accuracy: 0.6757\n",
            "Epoch 393/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9796\n",
            "Epoch 393: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.1014 - accuracy: 0.9796 - val_loss: 1.7391 - val_accuracy: 0.6757\n",
            "Epoch 394/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0870 - accuracy: 0.9844\n",
            "Epoch 394: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0998 - accuracy: 0.9796 - val_loss: 1.7490 - val_accuracy: 0.6892\n",
            "Epoch 395/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0891 - accuracy: 0.9922\n",
            "Epoch 395: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1025 - accuracy: 0.9796 - val_loss: 1.7174 - val_accuracy: 0.6892\n",
            "Epoch 396/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0892 - accuracy: 0.9844\n",
            "Epoch 396: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0976 - accuracy: 0.9796 - val_loss: 1.6245 - val_accuracy: 0.6892\n",
            "Epoch 397/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0874 - accuracy: 0.9844\n",
            "Epoch 397: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0971 - accuracy: 0.9796 - val_loss: 1.5516 - val_accuracy: 0.6757\n",
            "Epoch 398/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.1000 - accuracy: 0.9766\n",
            "Epoch 398: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0986 - accuracy: 0.9796 - val_loss: 1.4730 - val_accuracy: 0.6622\n",
            "Epoch 399/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0978 - accuracy: 0.9922\n",
            "Epoch 399: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0956 - accuracy: 0.9830 - val_loss: 1.3779 - val_accuracy: 0.6757\n",
            "Epoch 400/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0762 - accuracy: 1.0000\n",
            "Epoch 400: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0955 - accuracy: 0.9830 - val_loss: 1.3122 - val_accuracy: 0.6892\n",
            "Epoch 401/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0799 - accuracy: 0.9844\n",
            "Epoch 401: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0975 - accuracy: 0.9762 - val_loss: 1.2697 - val_accuracy: 0.6892\n",
            "Epoch 402/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0998 - accuracy: 0.9844\n",
            "Epoch 402: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0915 - accuracy: 0.9864 - val_loss: 1.2302 - val_accuracy: 0.7162\n",
            "Epoch 403/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0820 - accuracy: 0.9766\n",
            "Epoch 403: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0906 - accuracy: 0.9830 - val_loss: 1.2192 - val_accuracy: 0.7162\n",
            "Epoch 404/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0997 - accuracy: 0.9844\n",
            "Epoch 404: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0922 - accuracy: 0.9796 - val_loss: 1.1912 - val_accuracy: 0.7297\n",
            "Epoch 405/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0790 - accuracy: 0.9922\n",
            "Epoch 405: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0890 - accuracy: 0.9898 - val_loss: 1.1919 - val_accuracy: 0.7297\n",
            "Epoch 406/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0757 - accuracy: 0.9922\n",
            "Epoch 406: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0902 - accuracy: 0.9864 - val_loss: 1.1978 - val_accuracy: 0.7297\n",
            "Epoch 407/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0886 - accuracy: 0.9766\n",
            "Epoch 407: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.0911 - accuracy: 0.9796 - val_loss: 1.2004 - val_accuracy: 0.7297\n",
            "Epoch 408/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0859 - accuracy: 0.9922\n",
            "Epoch 408: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0886 - accuracy: 0.9796 - val_loss: 1.1962 - val_accuracy: 0.7297\n",
            "Epoch 409/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0919 - accuracy: 0.9688\n",
            "Epoch 409: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0879 - accuracy: 0.9796 - val_loss: 1.1703 - val_accuracy: 0.7297\n",
            "Epoch 410/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0766 - accuracy: 0.9844\n",
            "Epoch 410: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0877 - accuracy: 0.9796 - val_loss: 1.1437 - val_accuracy: 0.7432\n",
            "Epoch 411/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0967 - accuracy: 0.9922\n",
            "Epoch 411: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0895 - accuracy: 0.9796 - val_loss: 1.1201 - val_accuracy: 0.7297\n",
            "Epoch 412/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9796\n",
            "Epoch 412: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0871 - accuracy: 0.9796 - val_loss: 1.1064 - val_accuracy: 0.7162\n",
            "Epoch 413/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0900 - accuracy: 0.9766\n",
            "Epoch 413: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0873 - accuracy: 0.9796 - val_loss: 1.0876 - val_accuracy: 0.7297\n",
            "Epoch 414/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9830\n",
            "Epoch 414: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0870 - accuracy: 0.9830 - val_loss: 1.0773 - val_accuracy: 0.7297\n",
            "Epoch 415/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9796\n",
            "Epoch 415: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0905 - accuracy: 0.9796 - val_loss: 1.0658 - val_accuracy: 0.7432\n",
            "Epoch 416/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0758 - accuracy: 0.9766\n",
            "Epoch 416: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0878 - accuracy: 0.9762 - val_loss: 1.0513 - val_accuracy: 0.7568\n",
            "Epoch 417/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0913 - accuracy: 0.9766\n",
            "Epoch 417: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0868 - accuracy: 0.9830 - val_loss: 1.0252 - val_accuracy: 0.7568\n",
            "Epoch 418/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0927 - accuracy: 0.9766\n",
            "Epoch 418: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0900 - accuracy: 0.9762 - val_loss: 1.0129 - val_accuracy: 0.7703\n",
            "Epoch 419/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0868 - accuracy: 0.9766\n",
            "Epoch 419: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0827 - accuracy: 0.9796 - val_loss: 1.0214 - val_accuracy: 0.7568\n",
            "Epoch 420/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0800 - accuracy: 0.9844\n",
            "Epoch 420: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0801 - accuracy: 0.9796 - val_loss: 1.0450 - val_accuracy: 0.7568\n",
            "Epoch 421/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0801 - accuracy: 0.9844\n",
            "Epoch 421: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0824 - accuracy: 0.9796 - val_loss: 1.0303 - val_accuracy: 0.7432\n",
            "Epoch 422/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0853 - accuracy: 0.9844\n",
            "Epoch 422: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0840 - accuracy: 0.9796 - val_loss: 1.0202 - val_accuracy: 0.7297\n",
            "Epoch 423/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9796\n",
            "Epoch 423: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0786 - accuracy: 0.9796 - val_loss: 1.0055 - val_accuracy: 0.7297\n",
            "Epoch 424/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0763 - accuracy: 0.9844\n",
            "Epoch 424: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0849 - accuracy: 0.9796 - val_loss: 0.9944 - val_accuracy: 0.7432\n",
            "Epoch 425/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0725 - accuracy: 0.9922\n",
            "Epoch 425: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0813 - accuracy: 0.9796 - val_loss: 0.9954 - val_accuracy: 0.7432\n",
            "Epoch 426/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0802 - accuracy: 0.9609\n",
            "Epoch 426: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0784 - accuracy: 0.9796 - val_loss: 0.9978 - val_accuracy: 0.7432\n",
            "Epoch 427/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0919 - accuracy: 0.9688\n",
            "Epoch 427: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0848 - accuracy: 0.9762 - val_loss: 0.9767 - val_accuracy: 0.7703\n",
            "Epoch 428/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0667 - accuracy: 0.9844\n",
            "Epoch 428: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0771 - accuracy: 0.9830 - val_loss: 0.9828 - val_accuracy: 0.7432\n",
            "Epoch 429/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0581 - accuracy: 1.0000\n",
            "Epoch 429: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0794 - accuracy: 0.9762 - val_loss: 1.0001 - val_accuracy: 0.7297\n",
            "Epoch 430/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0924 - accuracy: 0.9766\n",
            "Epoch 430: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0777 - accuracy: 0.9796 - val_loss: 1.0140 - val_accuracy: 0.7432\n",
            "Epoch 431/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0660 - accuracy: 0.9844\n",
            "Epoch 431: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0759 - accuracy: 0.9830 - val_loss: 1.0247 - val_accuracy: 0.7432\n",
            "Epoch 432/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0693 - accuracy: 0.9766\n",
            "Epoch 432: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0744 - accuracy: 0.9796 - val_loss: 1.0506 - val_accuracy: 0.7568\n",
            "Epoch 433/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0677 - accuracy: 0.9844\n",
            "Epoch 433: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0754 - accuracy: 0.9796 - val_loss: 1.0674 - val_accuracy: 0.7568\n",
            "Epoch 434/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0705 - accuracy: 0.9844\n",
            "Epoch 434: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0750 - accuracy: 0.9796 - val_loss: 1.0692 - val_accuracy: 0.7432\n",
            "Epoch 435/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0651 - accuracy: 0.9844\n",
            "Epoch 435: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0780 - accuracy: 0.9796 - val_loss: 1.0679 - val_accuracy: 0.7297\n",
            "Epoch 436/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0863 - accuracy: 0.9766\n",
            "Epoch 436: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0747 - accuracy: 0.9796 - val_loss: 1.0726 - val_accuracy: 0.7162\n",
            "Epoch 437/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0621 - accuracy: 0.9844\n",
            "Epoch 437: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0724 - accuracy: 0.9796 - val_loss: 1.0780 - val_accuracy: 0.7027\n",
            "Epoch 438/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0934 - accuracy: 0.9766\n",
            "Epoch 438: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0775 - accuracy: 0.9796 - val_loss: 1.0790 - val_accuracy: 0.7027\n",
            "Epoch 439/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0775 - accuracy: 0.9688\n",
            "Epoch 439: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0704 - accuracy: 0.9796 - val_loss: 1.0782 - val_accuracy: 0.7162\n",
            "Epoch 440/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0826 - accuracy: 0.9766\n",
            "Epoch 440: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0751 - accuracy: 0.9796 - val_loss: 1.0529 - val_accuracy: 0.7027\n",
            "Epoch 441/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0671 - accuracy: 0.9844\n",
            "Epoch 441: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0725 - accuracy: 0.9796 - val_loss: 1.0397 - val_accuracy: 0.7162\n",
            "Epoch 442/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0707 - accuracy: 0.9766\n",
            "Epoch 442: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0714 - accuracy: 0.9796 - val_loss: 1.0452 - val_accuracy: 0.7297\n",
            "Epoch 443/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0624 - accuracy: 0.9922\n",
            "Epoch 443: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0720 - accuracy: 0.9796 - val_loss: 1.0453 - val_accuracy: 0.7297\n",
            "Epoch 444/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0781 - accuracy: 0.9766\n",
            "Epoch 444: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0718 - accuracy: 0.9796 - val_loss: 1.0456 - val_accuracy: 0.7297\n",
            "Epoch 445/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0598 - accuracy: 0.9766\n",
            "Epoch 445: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0685 - accuracy: 0.9796 - val_loss: 1.0469 - val_accuracy: 0.7432\n",
            "Epoch 446/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0780 - accuracy: 0.9844\n",
            "Epoch 446: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0730 - accuracy: 0.9830 - val_loss: 1.0475 - val_accuracy: 0.7432\n",
            "Epoch 447/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0705 - accuracy: 0.9766\n",
            "Epoch 447: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0704 - accuracy: 0.9796 - val_loss: 1.0458 - val_accuracy: 0.7432\n",
            "Epoch 448/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0589 - accuracy: 0.9766\n",
            "Epoch 448: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0714 - accuracy: 0.9796 - val_loss: 1.0326 - val_accuracy: 0.7432\n",
            "Epoch 449/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0665 - accuracy: 0.9766\n",
            "Epoch 449: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0689 - accuracy: 0.9830 - val_loss: 1.0409 - val_accuracy: 0.7432\n",
            "Epoch 450/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0642 - accuracy: 0.9688\n",
            "Epoch 450: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0669 - accuracy: 0.9796 - val_loss: 1.0662 - val_accuracy: 0.7568\n",
            "Epoch 451/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0839 - accuracy: 0.9766\n",
            "Epoch 451: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0701 - accuracy: 0.9796 - val_loss: 1.0834 - val_accuracy: 0.7432\n",
            "Epoch 452/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9796\n",
            "Epoch 452: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0679 - accuracy: 0.9796 - val_loss: 1.0899 - val_accuracy: 0.7568\n",
            "Epoch 453/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0753 - accuracy: 0.9766\n",
            "Epoch 453: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0692 - accuracy: 0.9796 - val_loss: 1.1042 - val_accuracy: 0.7432\n",
            "Epoch 454/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0709 - accuracy: 0.9844\n",
            "Epoch 454: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0674 - accuracy: 0.9796 - val_loss: 1.1104 - val_accuracy: 0.7162\n",
            "Epoch 455/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0469 - accuracy: 1.0000\n",
            "Epoch 455: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0670 - accuracy: 0.9796 - val_loss: 1.1056 - val_accuracy: 0.7297\n",
            "Epoch 456/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0602 - accuracy: 0.9766\n",
            "Epoch 456: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0642 - accuracy: 0.9796 - val_loss: 1.0996 - val_accuracy: 0.7297\n",
            "Epoch 457/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9796\n",
            "Epoch 457: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0664 - accuracy: 0.9796 - val_loss: 1.0981 - val_accuracy: 0.7162\n",
            "Epoch 458/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0638 - accuracy: 0.9766\n",
            "Epoch 458: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0632 - accuracy: 0.9796 - val_loss: 1.1093 - val_accuracy: 0.7162\n",
            "Epoch 459/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0587 - accuracy: 0.9844\n",
            "Epoch 459: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0628 - accuracy: 0.9796 - val_loss: 1.0874 - val_accuracy: 0.7162\n",
            "Epoch 460/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0712 - accuracy: 0.9766\n",
            "Epoch 460: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0658 - accuracy: 0.9796 - val_loss: 1.0518 - val_accuracy: 0.7297\n",
            "Epoch 461/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0770 - accuracy: 0.9766\n",
            "Epoch 461: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0639 - accuracy: 0.9796 - val_loss: 1.0459 - val_accuracy: 0.7297\n",
            "Epoch 462/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0652 - accuracy: 0.9844\n",
            "Epoch 462: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0686 - accuracy: 0.9796 - val_loss: 1.0490 - val_accuracy: 0.7297\n",
            "Epoch 463/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0635 - accuracy: 0.9844\n",
            "Epoch 463: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0641 - accuracy: 0.9796 - val_loss: 1.0488 - val_accuracy: 0.7162\n",
            "Epoch 464/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0693 - accuracy: 0.9844\n",
            "Epoch 464: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0669 - accuracy: 0.9830 - val_loss: 1.0527 - val_accuracy: 0.7297\n",
            "Epoch 465/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0849 - accuracy: 0.9688\n",
            "Epoch 465: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0668 - accuracy: 0.9762 - val_loss: 1.0551 - val_accuracy: 0.7297\n",
            "Epoch 466/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - accuracy: 0.9844\n",
            "Epoch 466: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0634 - accuracy: 0.9796 - val_loss: 1.0389 - val_accuracy: 0.7162\n",
            "Epoch 467/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0462 - accuracy: 0.9922\n",
            "Epoch 467: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0630 - accuracy: 0.9796 - val_loss: 1.0375 - val_accuracy: 0.7297\n",
            "Epoch 468/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0809 - accuracy: 0.9609\n",
            "Epoch 468: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0641 - accuracy: 0.9796 - val_loss: 1.0434 - val_accuracy: 0.7162\n",
            "Epoch 469/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0581 - accuracy: 0.9922\n",
            "Epoch 469: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0616 - accuracy: 0.9796 - val_loss: 1.0263 - val_accuracy: 0.7432\n",
            "Epoch 470/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0607 - accuracy: 0.9844\n",
            "Epoch 470: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0632 - accuracy: 0.9796 - val_loss: 1.0113 - val_accuracy: 0.7568\n",
            "Epoch 471/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0731 - accuracy: 0.9688\n",
            "Epoch 471: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0623 - accuracy: 0.9796 - val_loss: 1.0066 - val_accuracy: 0.7432\n",
            "Epoch 472/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0785 - accuracy: 0.9766\n",
            "Epoch 472: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0634 - accuracy: 0.9796 - val_loss: 0.9946 - val_accuracy: 0.7432\n",
            "Epoch 473/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0483 - accuracy: 0.9844\n",
            "Epoch 473: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0613 - accuracy: 0.9796 - val_loss: 0.9801 - val_accuracy: 0.7432\n",
            "Epoch 474/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0493 - accuracy: 0.9844\n",
            "Epoch 474: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0581 - accuracy: 0.9796 - val_loss: 0.9676 - val_accuracy: 0.7432\n",
            "Epoch 475/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0625 - accuracy: 0.9766\n",
            "Epoch 475: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0587 - accuracy: 0.9796 - val_loss: 0.9631 - val_accuracy: 0.7432\n",
            "Epoch 476/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0558 - accuracy: 0.9688\n",
            "Epoch 476: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0660 - accuracy: 0.9762 - val_loss: 1.0146 - val_accuracy: 0.7432\n",
            "Epoch 477/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9844\n",
            "Epoch 477: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0584 - accuracy: 0.9796 - val_loss: 1.0769 - val_accuracy: 0.7568\n",
            "Epoch 478/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0584 - accuracy: 0.9844\n",
            "Epoch 478: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0603 - accuracy: 0.9796 - val_loss: 1.1103 - val_accuracy: 0.7568\n",
            "Epoch 479/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0549 - accuracy: 0.9766\n",
            "Epoch 479: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0603 - accuracy: 0.9796 - val_loss: 1.1249 - val_accuracy: 0.7568\n",
            "Epoch 480/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0469 - accuracy: 0.9844\n",
            "Epoch 480: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0607 - accuracy: 0.9796 - val_loss: 1.1234 - val_accuracy: 0.7432\n",
            "Epoch 481/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0458 - accuracy: 0.9844\n",
            "Epoch 481: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0589 - accuracy: 0.9796 - val_loss: 1.1193 - val_accuracy: 0.7432\n",
            "Epoch 482/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0587 - accuracy: 0.9844\n",
            "Epoch 482: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0580 - accuracy: 0.9796 - val_loss: 1.0887 - val_accuracy: 0.7568\n",
            "Epoch 483/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0608 - accuracy: 0.9844\n",
            "Epoch 483: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0583 - accuracy: 0.9796 - val_loss: 1.0567 - val_accuracy: 0.7568\n",
            "Epoch 484/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0591 - accuracy: 0.9766\n",
            "Epoch 484: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0578 - accuracy: 0.9796 - val_loss: 1.0176 - val_accuracy: 0.7568\n",
            "Epoch 485/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0659 - accuracy: 0.9688\n",
            "Epoch 485: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0596 - accuracy: 0.9762 - val_loss: 0.9829 - val_accuracy: 0.7568\n",
            "Epoch 486/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0667 - accuracy: 0.9766\n",
            "Epoch 486: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0569 - accuracy: 0.9796 - val_loss: 0.9621 - val_accuracy: 0.7568\n",
            "Epoch 487/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0423 - accuracy: 0.9922\n",
            "Epoch 487: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0566 - accuracy: 0.9796 - val_loss: 0.9626 - val_accuracy: 0.7568\n",
            "Epoch 488/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0629 - accuracy: 0.9609\n",
            "Epoch 488: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0568 - accuracy: 0.9796 - val_loss: 0.9782 - val_accuracy: 0.7297\n",
            "Epoch 489/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9796\n",
            "Epoch 489: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0566 - accuracy: 0.9796 - val_loss: 0.9883 - val_accuracy: 0.7297\n",
            "Epoch 490/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0467 - accuracy: 0.9844\n",
            "Epoch 490: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0558 - accuracy: 0.9796 - val_loss: 0.9769 - val_accuracy: 0.7432\n",
            "Epoch 491/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0533 - accuracy: 0.9922\n",
            "Epoch 491: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0573 - accuracy: 0.9796 - val_loss: 0.9634 - val_accuracy: 0.7432\n",
            "Epoch 492/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0661 - accuracy: 0.9844\n",
            "Epoch 492: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0585 - accuracy: 0.9796 - val_loss: 0.9537 - val_accuracy: 0.7432\n",
            "Epoch 493/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - accuracy: 0.9844\n",
            "Epoch 493: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0577 - accuracy: 0.9796 - val_loss: 0.9469 - val_accuracy: 0.7432\n",
            "Epoch 494/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0530 - accuracy: 0.9844\n",
            "Epoch 494: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0562 - accuracy: 0.9796 - val_loss: 0.9506 - val_accuracy: 0.7568\n",
            "Epoch 495/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9796\n",
            "Epoch 495: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0581 - accuracy: 0.9796 - val_loss: 0.9347 - val_accuracy: 0.7432\n",
            "Epoch 496/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0451 - accuracy: 1.0000\n",
            "Epoch 496: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0555 - accuracy: 0.9830 - val_loss: 0.9193 - val_accuracy: 0.7568\n",
            "Epoch 497/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0753 - accuracy: 0.9688\n",
            "Epoch 497: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0575 - accuracy: 0.9762 - val_loss: 0.9093 - val_accuracy: 0.7703\n",
            "Epoch 498/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9796\n",
            "Epoch 498: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0560 - accuracy: 0.9796 - val_loss: 0.9167 - val_accuracy: 0.7568\n",
            "Epoch 499/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0657 - accuracy: 0.9531\n",
            "Epoch 499: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0528 - accuracy: 0.9796 - val_loss: 0.9265 - val_accuracy: 0.7568\n",
            "Epoch 500/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0505 - accuracy: 0.9844\n",
            "Epoch 500: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0546 - accuracy: 0.9796 - val_loss: 0.9273 - val_accuracy: 0.7568\n",
            "Epoch 501/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0522 - accuracy: 0.9844\n",
            "Epoch 501: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0541 - accuracy: 0.9796 - val_loss: 0.9286 - val_accuracy: 0.7568\n",
            "Epoch 502/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0532 - accuracy: 0.9766\n",
            "Epoch 502: val_accuracy did not improve from 0.77027\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0545 - accuracy: 0.9796 - val_loss: 0.9234 - val_accuracy: 0.7568\n",
            "Epoch 503/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0554 - accuracy: 0.9766\n",
            "Epoch 503: val_accuracy improved from 0.77027 to 0.78378, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0525 - accuracy: 0.9796 - val_loss: 0.9112 - val_accuracy: 0.7838\n",
            "Epoch 504/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0545 - accuracy: 0.9688\n",
            "Epoch 504: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0526 - accuracy: 0.9796 - val_loss: 0.9064 - val_accuracy: 0.7838\n",
            "Epoch 505/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0430 - accuracy: 0.9844\n",
            "Epoch 505: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0531 - accuracy: 0.9796 - val_loss: 0.9026 - val_accuracy: 0.7432\n",
            "Epoch 506/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0498 - accuracy: 0.9844\n",
            "Epoch 506: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0551 - accuracy: 0.9796 - val_loss: 0.9030 - val_accuracy: 0.7297\n",
            "Epoch 507/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0562 - accuracy: 0.9844\n",
            "Epoch 507: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0525 - accuracy: 0.9796 - val_loss: 0.9067 - val_accuracy: 0.7297\n",
            "Epoch 508/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0475 - accuracy: 0.9922\n",
            "Epoch 508: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0535 - accuracy: 0.9796 - val_loss: 0.9180 - val_accuracy: 0.7297\n",
            "Epoch 509/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0568 - accuracy: 0.9766\n",
            "Epoch 509: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0525 - accuracy: 0.9796 - val_loss: 0.9332 - val_accuracy: 0.7432\n",
            "Epoch 510/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000\n",
            "Epoch 510: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0540 - accuracy: 0.9796 - val_loss: 0.9497 - val_accuracy: 0.7297\n",
            "Epoch 511/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0553 - accuracy: 0.9844\n",
            "Epoch 511: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0493 - accuracy: 0.9796 - val_loss: 0.9701 - val_accuracy: 0.7162\n",
            "Epoch 512/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
            "Epoch 512: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0529 - accuracy: 0.9796 - val_loss: 0.9828 - val_accuracy: 0.7162\n",
            "Epoch 513/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0444 - accuracy: 0.9766\n",
            "Epoch 513: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0518 - accuracy: 0.9796 - val_loss: 1.0002 - val_accuracy: 0.7297\n",
            "Epoch 514/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0388 - accuracy: 0.9922\n",
            "Epoch 514: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0525 - accuracy: 0.9830 - val_loss: 1.0044 - val_accuracy: 0.7297\n",
            "Epoch 515/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0317 - accuracy: 1.0000\n",
            "Epoch 515: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0538 - accuracy: 0.9796 - val_loss: 0.9917 - val_accuracy: 0.7297\n",
            "Epoch 516/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0472 - accuracy: 0.9766\n",
            "Epoch 516: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0536 - accuracy: 0.9796 - val_loss: 0.9698 - val_accuracy: 0.7432\n",
            "Epoch 517/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0470 - accuracy: 1.0000\n",
            "Epoch 517: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0490 - accuracy: 0.9830 - val_loss: 0.9541 - val_accuracy: 0.7568\n",
            "Epoch 518/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0571 - accuracy: 0.9688\n",
            "Epoch 518: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0499 - accuracy: 0.9830 - val_loss: 0.9619 - val_accuracy: 0.7432\n",
            "Epoch 519/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000\n",
            "Epoch 519: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0528 - accuracy: 0.9830 - val_loss: 0.9589 - val_accuracy: 0.7568\n",
            "Epoch 520/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9844\n",
            "Epoch 520: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0492 - accuracy: 0.9830 - val_loss: 0.9598 - val_accuracy: 0.7432\n",
            "Epoch 521/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0517 - accuracy: 0.9766\n",
            "Epoch 521: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0504 - accuracy: 0.9830 - val_loss: 0.9719 - val_accuracy: 0.7162\n",
            "Epoch 522/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0679 - accuracy: 0.9766\n",
            "Epoch 522: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0513 - accuracy: 0.9796 - val_loss: 0.9855 - val_accuracy: 0.7297\n",
            "Epoch 523/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0581 - accuracy: 0.9688\n",
            "Epoch 523: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0514 - accuracy: 0.9796 - val_loss: 0.9745 - val_accuracy: 0.7432\n",
            "Epoch 524/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0329 - accuracy: 0.9922\n",
            "Epoch 524: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0493 - accuracy: 0.9898 - val_loss: 0.9604 - val_accuracy: 0.7703\n",
            "Epoch 525/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0514 - accuracy: 0.9766\n",
            "Epoch 525: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0507 - accuracy: 0.9864 - val_loss: 0.9735 - val_accuracy: 0.7703\n",
            "Epoch 526/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0757 - accuracy: 0.9688\n",
            "Epoch 526: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0529 - accuracy: 0.9864 - val_loss: 0.9919 - val_accuracy: 0.7568\n",
            "Epoch 527/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0774 - accuracy: 0.9609\n",
            "Epoch 527: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0528 - accuracy: 0.9796 - val_loss: 1.0073 - val_accuracy: 0.7568\n",
            "Epoch 528/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0596 - accuracy: 0.9766\n",
            "Epoch 528: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0520 - accuracy: 0.9762 - val_loss: 1.0228 - val_accuracy: 0.7568\n",
            "Epoch 529/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000\n",
            "Epoch 529: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0481 - accuracy: 0.9864 - val_loss: 1.0252 - val_accuracy: 0.7568\n",
            "Epoch 530/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0654 - accuracy: 0.9844\n",
            "Epoch 530: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0491 - accuracy: 0.9830 - val_loss: 1.0312 - val_accuracy: 0.7568\n",
            "Epoch 531/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0435 - accuracy: 0.9688\n",
            "Epoch 531: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0473 - accuracy: 0.9796 - val_loss: 1.0500 - val_accuracy: 0.7568\n",
            "Epoch 532/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0537 - accuracy: 0.9766\n",
            "Epoch 532: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0466 - accuracy: 0.9796 - val_loss: 1.0673 - val_accuracy: 0.7838\n",
            "Epoch 533/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0424 - accuracy: 0.9922\n",
            "Epoch 533: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0483 - accuracy: 0.9830 - val_loss: 1.0713 - val_accuracy: 0.7838\n",
            "Epoch 534/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0602 - accuracy: 0.9609\n",
            "Epoch 534: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0475 - accuracy: 0.9796 - val_loss: 1.0613 - val_accuracy: 0.7838\n",
            "Epoch 535/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0309 - accuracy: 0.9922\n",
            "Epoch 535: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0457 - accuracy: 0.9864 - val_loss: 1.0506 - val_accuracy: 0.7703\n",
            "Epoch 536/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9796\n",
            "Epoch 536: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0467 - accuracy: 0.9796 - val_loss: 1.0494 - val_accuracy: 0.7703\n",
            "Epoch 537/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0407 - accuracy: 0.9766\n",
            "Epoch 537: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0464 - accuracy: 0.9796 - val_loss: 1.0521 - val_accuracy: 0.7568\n",
            "Epoch 538/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0644 - accuracy: 0.9609\n",
            "Epoch 538: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0493 - accuracy: 0.9796 - val_loss: 1.0617 - val_accuracy: 0.7703\n",
            "Epoch 539/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0597 - accuracy: 0.9766\n",
            "Epoch 539: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0509 - accuracy: 0.9796 - val_loss: 1.0705 - val_accuracy: 0.7568\n",
            "Epoch 540/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0595 - accuracy: 0.9766\n",
            "Epoch 540: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0529 - accuracy: 0.9796 - val_loss: 1.0789 - val_accuracy: 0.7432\n",
            "Epoch 541/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0558 - accuracy: 0.9609\n",
            "Epoch 541: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0503 - accuracy: 0.9796 - val_loss: 1.0846 - val_accuracy: 0.7432\n",
            "Epoch 542/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9796\n",
            "Epoch 542: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0478 - accuracy: 0.9796 - val_loss: 1.0843 - val_accuracy: 0.7432\n",
            "Epoch 543/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0486 - accuracy: 0.9766\n",
            "Epoch 543: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0464 - accuracy: 0.9796 - val_loss: 1.0870 - val_accuracy: 0.7432\n",
            "Epoch 544/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0513 - accuracy: 0.9766\n",
            "Epoch 544: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0480 - accuracy: 0.9796 - val_loss: 1.0898 - val_accuracy: 0.7297\n",
            "Epoch 545/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0497 - accuracy: 0.9922\n",
            "Epoch 545: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0466 - accuracy: 0.9796 - val_loss: 1.1136 - val_accuracy: 0.7027\n",
            "Epoch 546/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0555 - accuracy: 0.9766\n",
            "Epoch 546: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0451 - accuracy: 0.9830 - val_loss: 1.1445 - val_accuracy: 0.6892\n",
            "Epoch 547/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9844\n",
            "Epoch 547: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0456 - accuracy: 0.9830 - val_loss: 1.1421 - val_accuracy: 0.6757\n",
            "Epoch 548/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0394 - accuracy: 0.9844\n",
            "Epoch 548: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0442 - accuracy: 0.9864 - val_loss: 1.1308 - val_accuracy: 0.6892\n",
            "Epoch 549/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0386 - accuracy: 1.0000\n",
            "Epoch 549: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0476 - accuracy: 0.9830 - val_loss: 1.1233 - val_accuracy: 0.7162\n",
            "Epoch 550/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0364 - accuracy: 0.9766\n",
            "Epoch 550: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0455 - accuracy: 0.9864 - val_loss: 1.1036 - val_accuracy: 0.7297\n",
            "Epoch 551/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0426 - accuracy: 0.9844\n",
            "Epoch 551: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0466 - accuracy: 0.9830 - val_loss: 1.0519 - val_accuracy: 0.7297\n",
            "Epoch 552/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0396 - accuracy: 1.0000\n",
            "Epoch 552: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0441 - accuracy: 0.9830 - val_loss: 0.9980 - val_accuracy: 0.7432\n",
            "Epoch 553/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0492 - accuracy: 0.9922\n",
            "Epoch 553: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0466 - accuracy: 0.9830 - val_loss: 0.9648 - val_accuracy: 0.7568\n",
            "Epoch 554/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0456 - accuracy: 0.9766\n",
            "Epoch 554: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0440 - accuracy: 0.9796 - val_loss: 0.9463 - val_accuracy: 0.7568\n",
            "Epoch 555/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0400 - accuracy: 0.9844\n",
            "Epoch 555: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 0.9428 - val_accuracy: 0.7703\n",
            "Epoch 556/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0475 - accuracy: 0.9844\n",
            "Epoch 556: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0482 - accuracy: 0.9830 - val_loss: 0.9730 - val_accuracy: 0.7703\n",
            "Epoch 557/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0559 - accuracy: 0.9688\n",
            "Epoch 557: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0457 - accuracy: 0.9830 - val_loss: 1.0410 - val_accuracy: 0.7027\n",
            "Epoch 558/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9762\n",
            "Epoch 558: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0480 - accuracy: 0.9762 - val_loss: 1.0937 - val_accuracy: 0.7027\n",
            "Epoch 559/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0435 - accuracy: 0.9844\n",
            "Epoch 559: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0447 - accuracy: 0.9796 - val_loss: 1.1349 - val_accuracy: 0.7027\n",
            "Epoch 560/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0480 - accuracy: 0.9688\n",
            "Epoch 560: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0497 - accuracy: 0.9796 - val_loss: 1.1625 - val_accuracy: 0.6892\n",
            "Epoch 561/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9922\n",
            "Epoch 561: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0479 - accuracy: 0.9762 - val_loss: 1.1714 - val_accuracy: 0.6757\n",
            "Epoch 562/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0483 - accuracy: 0.9766\n",
            "Epoch 562: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0463 - accuracy: 0.9796 - val_loss: 1.1671 - val_accuracy: 0.6757\n",
            "Epoch 563/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0320 - accuracy: 0.9922\n",
            "Epoch 563: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0452 - accuracy: 0.9796 - val_loss: 1.1588 - val_accuracy: 0.6757\n",
            "Epoch 564/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0387 - accuracy: 0.9766\n",
            "Epoch 564: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0455 - accuracy: 0.9796 - val_loss: 1.1257 - val_accuracy: 0.6892\n",
            "Epoch 565/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0453 - accuracy: 0.9844\n",
            "Epoch 565: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0461 - accuracy: 0.9796 - val_loss: 1.0863 - val_accuracy: 0.6892\n",
            "Epoch 566/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - accuracy: 0.9609\n",
            "Epoch 566: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0443 - accuracy: 0.9796 - val_loss: 1.0588 - val_accuracy: 0.7027\n",
            "Epoch 567/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0514 - accuracy: 0.9766\n",
            "Epoch 567: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0444 - accuracy: 0.9796 - val_loss: 1.0361 - val_accuracy: 0.7027\n",
            "Epoch 568/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0491 - accuracy: 0.9766\n",
            "Epoch 568: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0428 - accuracy: 0.9796 - val_loss: 1.0204 - val_accuracy: 0.7027\n",
            "Epoch 569/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0525 - accuracy: 0.9844\n",
            "Epoch 569: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0427 - accuracy: 0.9796 - val_loss: 1.0032 - val_accuracy: 0.7027\n",
            "Epoch 570/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0490 - accuracy: 0.9844\n",
            "Epoch 570: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0415 - accuracy: 0.9796 - val_loss: 0.9919 - val_accuracy: 0.7027\n",
            "Epoch 571/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0442 - accuracy: 0.9844\n",
            "Epoch 571: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0432 - accuracy: 0.9830 - val_loss: 0.9862 - val_accuracy: 0.7027\n",
            "Epoch 572/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0565 - accuracy: 0.9688\n",
            "Epoch 572: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0459 - accuracy: 0.9796 - val_loss: 0.9828 - val_accuracy: 0.7027\n",
            "Epoch 573/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0530 - accuracy: 0.9844\n",
            "Epoch 573: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0434 - accuracy: 0.9864 - val_loss: 0.9924 - val_accuracy: 0.7027\n",
            "Epoch 574/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 574: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0456 - accuracy: 0.9864 - val_loss: 1.0157 - val_accuracy: 0.7027\n",
            "Epoch 575/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9766\n",
            "Epoch 575: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0433 - accuracy: 0.9830 - val_loss: 1.0382 - val_accuracy: 0.7027\n",
            "Epoch 576/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0549 - accuracy: 0.9766\n",
            "Epoch 576: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0454 - accuracy: 0.9830 - val_loss: 1.0552 - val_accuracy: 0.7027\n",
            "Epoch 577/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0473 - accuracy: 0.9766\n",
            "Epoch 577: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0458 - accuracy: 0.9864 - val_loss: 1.0768 - val_accuracy: 0.7027\n",
            "Epoch 578/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0388 - accuracy: 0.9922\n",
            "Epoch 578: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0415 - accuracy: 0.9898 - val_loss: 1.0869 - val_accuracy: 0.7027\n",
            "Epoch 579/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0430 - accuracy: 0.9766\n",
            "Epoch 579: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0428 - accuracy: 0.9830 - val_loss: 1.0908 - val_accuracy: 0.7027\n",
            "Epoch 580/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0541 - accuracy: 0.9844\n",
            "Epoch 580: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0416 - accuracy: 0.9898 - val_loss: 1.0988 - val_accuracy: 0.7162\n",
            "Epoch 581/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0263 - accuracy: 0.9922\n",
            "Epoch 581: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0432 - accuracy: 0.9796 - val_loss: 1.0849 - val_accuracy: 0.7162\n",
            "Epoch 582/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0473 - accuracy: 0.9844\n",
            "Epoch 582: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0420 - accuracy: 0.9796 - val_loss: 1.0691 - val_accuracy: 0.7432\n",
            "Epoch 583/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0405 - accuracy: 0.9844\n",
            "Epoch 583: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0441 - accuracy: 0.9796 - val_loss: 1.0528 - val_accuracy: 0.7432\n",
            "Epoch 584/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0312 - accuracy: 0.9922\n",
            "Epoch 584: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0429 - accuracy: 0.9796 - val_loss: 1.0324 - val_accuracy: 0.7297\n",
            "Epoch 585/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0636 - accuracy: 0.9688\n",
            "Epoch 585: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0454 - accuracy: 0.9796 - val_loss: 1.0210 - val_accuracy: 0.7432\n",
            "Epoch 586/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0470 - accuracy: 0.9766\n",
            "Epoch 586: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0413 - accuracy: 0.9796 - val_loss: 1.0117 - val_accuracy: 0.7432\n",
            "Epoch 587/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0424 - accuracy: 0.9766\n",
            "Epoch 587: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0411 - accuracy: 0.9796 - val_loss: 0.9999 - val_accuracy: 0.7432\n",
            "Epoch 588/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0510 - accuracy: 0.9688\n",
            "Epoch 588: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0455 - accuracy: 0.9796 - val_loss: 1.0036 - val_accuracy: 0.7432\n",
            "Epoch 589/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0258 - accuracy: 0.9922\n",
            "Epoch 589: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0384 - accuracy: 0.9796 - val_loss: 1.0166 - val_accuracy: 0.7568\n",
            "Epoch 590/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0385 - accuracy: 0.9688\n",
            "Epoch 590: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0445 - accuracy: 0.9762 - val_loss: 1.0256 - val_accuracy: 0.7432\n",
            "Epoch 591/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0417 - accuracy: 0.9844\n",
            "Epoch 591: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0411 - accuracy: 0.9796 - val_loss: 1.0293 - val_accuracy: 0.7297\n",
            "Epoch 592/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0546 - accuracy: 0.9844\n",
            "Epoch 592: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0417 - accuracy: 0.9830 - val_loss: 1.0396 - val_accuracy: 0.7297\n",
            "Epoch 593/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0470 - accuracy: 0.9844\n",
            "Epoch 593: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0437 - accuracy: 0.9864 - val_loss: 1.0390 - val_accuracy: 0.7297\n",
            "Epoch 594/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0417 - accuracy: 0.9844\n",
            "Epoch 594: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0405 - accuracy: 0.9864 - val_loss: 1.0406 - val_accuracy: 0.7297\n",
            "Epoch 595/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0454 - accuracy: 0.9844\n",
            "Epoch 595: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0413 - accuracy: 0.9796 - val_loss: 1.0459 - val_accuracy: 0.7297\n",
            "Epoch 596/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0550 - accuracy: 0.9766\n",
            "Epoch 596: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0414 - accuracy: 0.9796 - val_loss: 1.0423 - val_accuracy: 0.7432\n",
            "Epoch 597/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0306 - accuracy: 1.0000\n",
            "Epoch 597: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0413 - accuracy: 0.9796 - val_loss: 1.0355 - val_accuracy: 0.7297\n",
            "Epoch 598/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0315 - accuracy: 0.9766\n",
            "Epoch 598: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0405 - accuracy: 0.9796 - val_loss: 1.0329 - val_accuracy: 0.7432\n",
            "Epoch 599/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0352 - accuracy: 0.9766\n",
            "Epoch 599: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0396 - accuracy: 0.9796 - val_loss: 1.0296 - val_accuracy: 0.7432\n",
            "Epoch 600/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0444 - accuracy: 0.9766\n",
            "Epoch 600: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0421 - accuracy: 0.9796 - val_loss: 1.0296 - val_accuracy: 0.7297\n",
            "Epoch 601/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0441 - accuracy: 0.9688\n",
            "Epoch 601: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0397 - accuracy: 0.9796 - val_loss: 1.0376 - val_accuracy: 0.7162\n",
            "Epoch 602/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0422 - accuracy: 0.9844\n",
            "Epoch 602: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0417 - accuracy: 0.9830 - val_loss: 1.0466 - val_accuracy: 0.7297\n",
            "Epoch 603/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0363 - accuracy: 0.9844\n",
            "Epoch 603: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0412 - accuracy: 0.9796 - val_loss: 1.0627 - val_accuracy: 0.7297\n",
            "Epoch 604/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0361 - accuracy: 0.9844\n",
            "Epoch 604: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0382 - accuracy: 0.9830 - val_loss: 1.0836 - val_accuracy: 0.7297\n",
            "Epoch 605/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0334 - accuracy: 0.9766\n",
            "Epoch 605: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0421 - accuracy: 0.9796 - val_loss: 1.1041 - val_accuracy: 0.7162\n",
            "Epoch 606/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0617 - accuracy: 0.9766\n",
            "Epoch 606: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0397 - accuracy: 0.9898 - val_loss: 1.1187 - val_accuracy: 0.7297\n",
            "Epoch 607/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0507 - accuracy: 0.9844\n",
            "Epoch 607: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0394 - accuracy: 0.9864 - val_loss: 1.1274 - val_accuracy: 0.7297\n",
            "Epoch 608/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0396 - accuracy: 0.9922\n",
            "Epoch 608: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0376 - accuracy: 0.9898 - val_loss: 1.1367 - val_accuracy: 0.7297\n",
            "Epoch 609/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0429 - accuracy: 0.9922\n",
            "Epoch 609: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0405 - accuracy: 0.9864 - val_loss: 1.1370 - val_accuracy: 0.7297\n",
            "Epoch 610/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0299 - accuracy: 0.9844\n",
            "Epoch 610: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0373 - accuracy: 0.9830 - val_loss: 1.1332 - val_accuracy: 0.7162\n",
            "Epoch 611/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0460 - accuracy: 0.9922\n",
            "Epoch 611: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0361 - accuracy: 0.9932 - val_loss: 1.1321 - val_accuracy: 0.7297\n",
            "Epoch 612/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9830\n",
            "Epoch 612: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0381 - accuracy: 0.9830 - val_loss: 1.1343 - val_accuracy: 0.7162\n",
            "Epoch 613/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0274 - accuracy: 1.0000\n",
            "Epoch 613: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0392 - accuracy: 0.9898 - val_loss: 1.1430 - val_accuracy: 0.7297\n",
            "Epoch 614/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0377 - accuracy: 0.9922\n",
            "Epoch 614: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0371 - accuracy: 0.9932 - val_loss: 1.1411 - val_accuracy: 0.7027\n",
            "Epoch 615/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000\n",
            "Epoch 615: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0385 - accuracy: 0.9898 - val_loss: 1.1302 - val_accuracy: 0.7162\n",
            "Epoch 616/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0625 - accuracy: 0.9766\n",
            "Epoch 616: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0453 - accuracy: 0.9762 - val_loss: 1.1053 - val_accuracy: 0.7297\n",
            "Epoch 617/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0397 - accuracy: 0.9688\n",
            "Epoch 617: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0394 - accuracy: 0.9796 - val_loss: 1.0682 - val_accuracy: 0.7432\n",
            "Epoch 618/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0288 - accuracy: 1.0000\n",
            "Epoch 618: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0406 - accuracy: 0.9830 - val_loss: 1.0628 - val_accuracy: 0.7432\n",
            "Epoch 619/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0386 - accuracy: 0.9766\n",
            "Epoch 619: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0408 - accuracy: 0.9796 - val_loss: 1.0550 - val_accuracy: 0.7568\n",
            "Epoch 620/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9796\n",
            "Epoch 620: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0403 - accuracy: 0.9796 - val_loss: 1.0381 - val_accuracy: 0.7568\n",
            "Epoch 621/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0339 - accuracy: 0.9766\n",
            "Epoch 621: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0393 - accuracy: 0.9796 - val_loss: 1.0409 - val_accuracy: 0.7568\n",
            "Epoch 622/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0352 - accuracy: 0.9844\n",
            "Epoch 622: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0431 - accuracy: 0.9796 - val_loss: 1.0490 - val_accuracy: 0.7432\n",
            "Epoch 623/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0584 - accuracy: 0.9688\n",
            "Epoch 623: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0434 - accuracy: 0.9796 - val_loss: 1.0400 - val_accuracy: 0.7432\n",
            "Epoch 624/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9796\n",
            "Epoch 624: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0396 - accuracy: 0.9796 - val_loss: 1.0143 - val_accuracy: 0.7568\n",
            "Epoch 625/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0362 - accuracy: 0.9766\n",
            "Epoch 625: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0398 - accuracy: 0.9796 - val_loss: 1.0048 - val_accuracy: 0.7703\n",
            "Epoch 626/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0399 - accuracy: 0.9688\n",
            "Epoch 626: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0423 - accuracy: 0.9796 - val_loss: 1.0021 - val_accuracy: 0.7568\n",
            "Epoch 627/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0313 - accuracy: 0.9922\n",
            "Epoch 627: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0383 - accuracy: 0.9796 - val_loss: 1.0024 - val_accuracy: 0.7703\n",
            "Epoch 628/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9762\n",
            "Epoch 628: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0428 - accuracy: 0.9762 - val_loss: 1.0053 - val_accuracy: 0.7432\n",
            "Epoch 629/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9844\n",
            "Epoch 629: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0389 - accuracy: 0.9830 - val_loss: 0.9921 - val_accuracy: 0.7568\n",
            "Epoch 630/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0504 - accuracy: 0.9609\n",
            "Epoch 630: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0398 - accuracy: 0.9796 - val_loss: 0.9768 - val_accuracy: 0.7703\n",
            "Epoch 631/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0561 - accuracy: 0.9688\n",
            "Epoch 631: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0412 - accuracy: 0.9830 - val_loss: 0.9805 - val_accuracy: 0.7838\n",
            "Epoch 632/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0537 - accuracy: 0.9688\n",
            "Epoch 632: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0377 - accuracy: 0.9830 - val_loss: 0.9831 - val_accuracy: 0.7838\n",
            "Epoch 633/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - accuracy: 0.9766\n",
            "Epoch 633: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0425 - accuracy: 0.9796 - val_loss: 0.9730 - val_accuracy: 0.7838\n",
            "Epoch 634/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0296 - accuracy: 0.9844\n",
            "Epoch 634: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0414 - accuracy: 0.9796 - val_loss: 0.9657 - val_accuracy: 0.7703\n",
            "Epoch 635/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0376 - accuracy: 0.9766\n",
            "Epoch 635: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0445 - accuracy: 0.9762 - val_loss: 0.9545 - val_accuracy: 0.7703\n",
            "Epoch 636/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0552 - accuracy: 0.9766\n",
            "Epoch 636: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0400 - accuracy: 0.9796 - val_loss: 0.9563 - val_accuracy: 0.7703\n",
            "Epoch 637/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0305 - accuracy: 0.9922\n",
            "Epoch 637: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0376 - accuracy: 0.9932 - val_loss: 0.9651 - val_accuracy: 0.7703\n",
            "Epoch 638/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0421 - accuracy: 0.9922\n",
            "Epoch 638: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.0401 - accuracy: 0.9898 - val_loss: 0.9780 - val_accuracy: 0.7703\n",
            "Epoch 639/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0506 - accuracy: 0.9844\n",
            "Epoch 639: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0374 - accuracy: 0.9830 - val_loss: 0.9912 - val_accuracy: 0.7432\n",
            "Epoch 640/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0463 - accuracy: 0.9844\n",
            "Epoch 640: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0382 - accuracy: 0.9830 - val_loss: 0.9965 - val_accuracy: 0.7297\n",
            "Epoch 641/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0334 - accuracy: 0.9844\n",
            "Epoch 641: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0348 - accuracy: 0.9796 - val_loss: 0.9923 - val_accuracy: 0.7432\n",
            "Epoch 642/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0418 - accuracy: 0.9922\n",
            "Epoch 642: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0401 - accuracy: 0.9830 - val_loss: 0.9956 - val_accuracy: 0.7297\n",
            "Epoch 643/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0525 - accuracy: 0.9688\n",
            "Epoch 643: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0374 - accuracy: 0.9796 - val_loss: 1.0032 - val_accuracy: 0.7297\n",
            "Epoch 644/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0379 - accuracy: 0.9844\n",
            "Epoch 644: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0389 - accuracy: 0.9796 - val_loss: 1.0081 - val_accuracy: 0.7162\n",
            "Epoch 645/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0189 - accuracy: 1.0000\n",
            "Epoch 645: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0426 - accuracy: 0.9796 - val_loss: 1.0244 - val_accuracy: 0.7162\n",
            "Epoch 646/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0362 - accuracy: 0.9844\n",
            "Epoch 646: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0362 - accuracy: 0.9864 - val_loss: 1.0454 - val_accuracy: 0.6757\n",
            "Epoch 647/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0347 - accuracy: 1.0000\n",
            "Epoch 647: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0371 - accuracy: 0.9830 - val_loss: 1.0413 - val_accuracy: 0.6892\n",
            "Epoch 648/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - accuracy: 0.9766\n",
            "Epoch 648: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0411 - accuracy: 0.9796 - val_loss: 1.0356 - val_accuracy: 0.6892\n",
            "Epoch 649/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0352 - accuracy: 0.9922\n",
            "Epoch 649: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0375 - accuracy: 0.9864 - val_loss: 1.0373 - val_accuracy: 0.7027\n",
            "Epoch 650/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0411 - accuracy: 0.9688\n",
            "Epoch 650: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0396 - accuracy: 0.9864 - val_loss: 1.0399 - val_accuracy: 0.7027\n",
            "Epoch 651/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0281 - accuracy: 0.9844\n",
            "Epoch 651: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0379 - accuracy: 0.9830 - val_loss: 1.0586 - val_accuracy: 0.7027\n",
            "Epoch 652/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0299 - accuracy: 1.0000\n",
            "Epoch 652: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 1.0686 - val_accuracy: 0.7027\n",
            "Epoch 653/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0384 - accuracy: 0.9922\n",
            "Epoch 653: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0397 - accuracy: 0.9830 - val_loss: 1.0708 - val_accuracy: 0.7162\n",
            "Epoch 654/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0518 - accuracy: 0.9688\n",
            "Epoch 654: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0396 - accuracy: 0.9796 - val_loss: 1.0983 - val_accuracy: 0.7162\n",
            "Epoch 655/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0256 - accuracy: 0.9922\n",
            "Epoch 655: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0354 - accuracy: 0.9796 - val_loss: 1.1258 - val_accuracy: 0.7162\n",
            "Epoch 656/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0488 - accuracy: 0.9688\n",
            "Epoch 656: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0376 - accuracy: 0.9796 - val_loss: 1.1461 - val_accuracy: 0.6892\n",
            "Epoch 657/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0329 - accuracy: 0.9844\n",
            "Epoch 657: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0393 - accuracy: 0.9830 - val_loss: 1.1488 - val_accuracy: 0.6892\n",
            "Epoch 658/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0376 - accuracy: 0.9844\n",
            "Epoch 658: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0410 - accuracy: 0.9796 - val_loss: 1.1527 - val_accuracy: 0.6892\n",
            "Epoch 659/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0617 - accuracy: 0.9688\n",
            "Epoch 659: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0417 - accuracy: 0.9830 - val_loss: 1.1360 - val_accuracy: 0.7162\n",
            "Epoch 660/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0459 - accuracy: 0.9688\n",
            "Epoch 660: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0391 - accuracy: 0.9830 - val_loss: 1.1231 - val_accuracy: 0.7162\n",
            "Epoch 661/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0386 - accuracy: 0.9844\n",
            "Epoch 661: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0413 - accuracy: 0.9830 - val_loss: 1.1298 - val_accuracy: 0.7297\n",
            "Epoch 662/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0620 - accuracy: 0.9609\n",
            "Epoch 662: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0427 - accuracy: 0.9762 - val_loss: 1.1081 - val_accuracy: 0.7432\n",
            "Epoch 663/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0586 - accuracy: 0.9688\n",
            "Epoch 663: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0409 - accuracy: 0.9796 - val_loss: 1.0835 - val_accuracy: 0.7432\n",
            "Epoch 664/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0424 - accuracy: 0.9844\n",
            "Epoch 664: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0411 - accuracy: 0.9796 - val_loss: 1.0536 - val_accuracy: 0.7432\n",
            "Epoch 665/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0284 - accuracy: 0.9844\n",
            "Epoch 665: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0392 - accuracy: 0.9796 - val_loss: 1.0158 - val_accuracy: 0.7568\n",
            "Epoch 666/700\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.0399 - accuracy: 0.9766\n",
            "Epoch 666: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0372 - accuracy: 0.9796 - val_loss: 0.9838 - val_accuracy: 0.7838\n",
            "Epoch 667/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0340 - accuracy: 0.9922\n",
            "Epoch 667: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0377 - accuracy: 0.9796 - val_loss: 0.9672 - val_accuracy: 0.7838\n",
            "Epoch 668/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0347 - accuracy: 0.9766\n",
            "Epoch 668: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0389 - accuracy: 0.9796 - val_loss: 0.9468 - val_accuracy: 0.7703\n",
            "Epoch 669/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0511 - accuracy: 0.9609\n",
            "Epoch 669: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0379 - accuracy: 0.9796 - val_loss: 0.9233 - val_accuracy: 0.7838\n",
            "Epoch 670/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0212 - accuracy: 0.9922\n",
            "Epoch 670: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0368 - accuracy: 0.9796 - val_loss: 0.9146 - val_accuracy: 0.7838\n",
            "Epoch 671/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000\n",
            "Epoch 671: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0390 - accuracy: 0.9864 - val_loss: 0.9114 - val_accuracy: 0.7838\n",
            "Epoch 672/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0296 - accuracy: 0.9922\n",
            "Epoch 672: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0376 - accuracy: 0.9898 - val_loss: 0.9264 - val_accuracy: 0.7703\n",
            "Epoch 673/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0491 - accuracy: 0.9766\n",
            "Epoch 673: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0373 - accuracy: 0.9796 - val_loss: 0.9443 - val_accuracy: 0.7703\n",
            "Epoch 674/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0329 - accuracy: 0.9688\n",
            "Epoch 674: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0400 - accuracy: 0.9830 - val_loss: 0.9630 - val_accuracy: 0.7568\n",
            "Epoch 675/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0438 - accuracy: 0.9609\n",
            "Epoch 675: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0378 - accuracy: 0.9796 - val_loss: 0.9734 - val_accuracy: 0.7568\n",
            "Epoch 676/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0377 - accuracy: 0.9844\n",
            "Epoch 676: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0401 - accuracy: 0.9796 - val_loss: 0.9832 - val_accuracy: 0.7568\n",
            "Epoch 677/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0426 - accuracy: 0.9766\n",
            "Epoch 677: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0392 - accuracy: 0.9796 - val_loss: 0.9943 - val_accuracy: 0.7703\n",
            "Epoch 678/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9922\n",
            "Epoch 678: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0403 - accuracy: 0.9796 - val_loss: 0.9951 - val_accuracy: 0.7703\n",
            "Epoch 679/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0306 - accuracy: 0.9844\n",
            "Epoch 679: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0349 - accuracy: 0.9796 - val_loss: 0.9842 - val_accuracy: 0.7568\n",
            "Epoch 680/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0425 - accuracy: 0.9766\n",
            "Epoch 680: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0374 - accuracy: 0.9796 - val_loss: 0.9686 - val_accuracy: 0.7568\n",
            "Epoch 681/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9830\n",
            "Epoch 681: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0356 - accuracy: 0.9830 - val_loss: 0.9561 - val_accuracy: 0.7568\n",
            "Epoch 682/700\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9762\n",
            "Epoch 682: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0411 - accuracy: 0.9762 - val_loss: 0.9359 - val_accuracy: 0.7703\n",
            "Epoch 683/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0279 - accuracy: 0.9844\n",
            "Epoch 683: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0374 - accuracy: 0.9864 - val_loss: 0.9391 - val_accuracy: 0.7703\n",
            "Epoch 684/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0270 - accuracy: 0.9766\n",
            "Epoch 684: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0388 - accuracy: 0.9830 - val_loss: 0.9486 - val_accuracy: 0.7703\n",
            "Epoch 685/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0426 - accuracy: 0.9766\n",
            "Epoch 685: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0395 - accuracy: 0.9796 - val_loss: 0.9575 - val_accuracy: 0.7703\n",
            "Epoch 686/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0331 - accuracy: 0.9688\n",
            "Epoch 686: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0381 - accuracy: 0.9830 - val_loss: 0.9813 - val_accuracy: 0.7568\n",
            "Epoch 687/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0432 - accuracy: 0.9844\n",
            "Epoch 687: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0374 - accuracy: 0.9796 - val_loss: 1.0162 - val_accuracy: 0.7703\n",
            "Epoch 688/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0446 - accuracy: 0.9609\n",
            "Epoch 688: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0376 - accuracy: 0.9796 - val_loss: 1.0079 - val_accuracy: 0.7703\n",
            "Epoch 689/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0388 - accuracy: 0.9844\n",
            "Epoch 689: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0418 - accuracy: 0.9796 - val_loss: 0.9921 - val_accuracy: 0.7703\n",
            "Epoch 690/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0412 - accuracy: 0.9688\n",
            "Epoch 690: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0394 - accuracy: 0.9796 - val_loss: 0.9736 - val_accuracy: 0.7703\n",
            "Epoch 691/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0380 - accuracy: 0.9688\n",
            "Epoch 691: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0345 - accuracy: 0.9796 - val_loss: 0.9714 - val_accuracy: 0.7568\n",
            "Epoch 692/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0289 - accuracy: 0.9844\n",
            "Epoch 692: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0395 - accuracy: 0.9796 - val_loss: 0.9503 - val_accuracy: 0.7568\n",
            "Epoch 693/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0445 - accuracy: 0.9766\n",
            "Epoch 693: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0404 - accuracy: 0.9796 - val_loss: 0.9382 - val_accuracy: 0.7568\n",
            "Epoch 694/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9922\n",
            "Epoch 694: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0383 - accuracy: 0.9796 - val_loss: 0.9387 - val_accuracy: 0.7568\n",
            "Epoch 695/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0365 - accuracy: 0.9688\n",
            "Epoch 695: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0389 - accuracy: 0.9796 - val_loss: 0.9447 - val_accuracy: 0.7432\n",
            "Epoch 696/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0393 - accuracy: 0.9766\n",
            "Epoch 696: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0351 - accuracy: 0.9830 - val_loss: 0.9384 - val_accuracy: 0.7568\n",
            "Epoch 697/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9766\n",
            "Epoch 697: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0383 - accuracy: 0.9796 - val_loss: 0.9390 - val_accuracy: 0.7568\n",
            "Epoch 698/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0260 - accuracy: 0.9844\n",
            "Epoch 698: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.0391 - accuracy: 0.9796 - val_loss: 0.9395 - val_accuracy: 0.7703\n",
            "Epoch 699/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0460 - accuracy: 0.9844\n",
            "Epoch 699: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0371 - accuracy: 0.9796 - val_loss: 0.9221 - val_accuracy: 0.7703\n",
            "Epoch 700/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 0.0318 - accuracy: 0.9766\n",
            "Epoch 700: val_accuracy did not improve from 0.78378\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.0387 - accuracy: 0.9796 - val_loss: 0.9026 - val_accuracy: 0.7703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_CNN = CNN_model.predict(feature_valid)\n",
        "\n",
        "# convert the validation vector\n",
        "valid_y_CNN = y_valid_CNN.copy()\n",
        "for i in range(len(y_valid_CNN)):\n",
        "    j = np.where(y_valid_CNN[i] == np.amax(y_valid_CNN[i]))\n",
        "    valid_y_CNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOvZu2r_z1M-",
        "outputId": "a8445bef-23c0-4864-aa93-9b2e0dd6c4b5"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 4ms/step\n",
            "0.7702702702702703\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.60      0.72        15\n",
            "           1       0.73      0.89      0.80        36\n",
            "           2       0.80      0.70      0.74        23\n",
            "\n",
            "   micro avg       0.77      0.77      0.77        74\n",
            "   macro avg       0.81      0.73      0.75        74\n",
            "weighted avg       0.78      0.77      0.77        74\n",
            " samples avg       0.77      0.77      0.77        74\n",
            "\n",
            "auc score:  0.7955618440043882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_CNN = CNN_model.predict(feature_test)\n",
        "# convert the test vector\n",
        "test_y_CNN = y_test_CNN.copy()\n",
        "for i in range(len(y_test_CNN)):\n",
        "    j = np.where(y_test_CNN[i] == np.amax(y_test_CNN[i]))\n",
        "    test_y_CNN[i] = [0, 0, 0]\n",
        "    test_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_test_y,test_y_CNN))\n",
        "print(classification_report(label_test_y,test_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_test_y,test_y_CNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw0FKVhZz3ik",
        "outputId": "7634652c-f343-4a5b-9ca1-c2431fac926c"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 4ms/step\n",
            "0.7096774193548387\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.53      0.69        17\n",
            "           1       0.70      0.81      0.75        43\n",
            "           2       0.65      0.67      0.66        33\n",
            "\n",
            "   micro avg       0.71      0.71      0.71        93\n",
            "   macro avg       0.78      0.67      0.70        93\n",
            "weighted avg       0.74      0.71      0.71        93\n",
            " samples avg       0.71      0.71      0.71        93\n",
            "\n",
            "auc score:  0.751671986624107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss values\n",
        "plt.plot(CNN_history.history['loss'])\n",
        "plt.title('CNN Model loss with class=3')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "HSMxYgA4kl5V",
        "outputId": "28ea7000-5524-42f9-91fe-1b891af39d94"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(CNN_history.history['accuracy'])\n",
        "plt.title('CNN Model accuracy with class=3')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "WkNvEgUf6YI3",
        "outputId": "e4503d47-67b6-4506-b4c8-293a20172506"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tI8S4alF6afA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VLQol4zX6aha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#不好使的"
      ],
      "metadata": {
        "id": "Ye3Uy3jh6akF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_RNN_model():\n",
        "    RNN = Sequential()\n",
        "    RNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False))\n",
        "\n",
        "    RNN.add(Bidirectional(LSTM(word_dimension)))\n",
        "    RNN.add(Dense(word_dimension, activation='relu'))\n",
        "    RNN.add(Dense(3, activation='softmax'))\n",
        "    RNN.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "    return RNN\n",
        "\n",
        "RNN_model = create_RNN_model()\n",
        "RNN_history = RNN_model.fit(feature_train, label_train_y, epochs=800, batch_size=128, validation_data=(feature_valid, label_valid_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OO0KerungT4",
        "outputId": "d5571db1-3c60-4cd2-ca13-e7d071daf03b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "3/3 [==============================] - 4s 405ms/step - loss: 1.0818 - accuracy: 0.4150 - val_loss: 1.0438 - val_accuracy: 0.4865\n",
            "Epoch 2/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9848 - accuracy: 0.5714 - val_loss: 1.0760 - val_accuracy: 0.4865\n",
            "Epoch 3/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9833 - accuracy: 0.5714 - val_loss: 1.0695 - val_accuracy: 0.4865\n",
            "Epoch 4/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9751 - accuracy: 0.5714 - val_loss: 1.0536 - val_accuracy: 0.4865\n",
            "Epoch 5/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9748 - accuracy: 0.5714 - val_loss: 1.0464 - val_accuracy: 0.4865\n",
            "Epoch 6/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9744 - accuracy: 0.5714 - val_loss: 1.0396 - val_accuracy: 0.4865\n",
            "Epoch 7/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9762 - accuracy: 0.5714 - val_loss: 1.0414 - val_accuracy: 0.4865\n",
            "Epoch 8/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.9736 - accuracy: 0.5714 - val_loss: 1.0448 - val_accuracy: 0.4865\n",
            "Epoch 9/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9701 - accuracy: 0.5714 - val_loss: 1.0545 - val_accuracy: 0.4865\n",
            "Epoch 10/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.9717 - accuracy: 0.5714 - val_loss: 1.0592 - val_accuracy: 0.4865\n",
            "Epoch 11/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9766 - accuracy: 0.5714 - val_loss: 1.0527 - val_accuracy: 0.4865\n",
            "Epoch 12/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9674 - accuracy: 0.5714 - val_loss: 1.0395 - val_accuracy: 0.4865\n",
            "Epoch 13/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9700 - accuracy: 0.5714 - val_loss: 1.0428 - val_accuracy: 0.4865\n",
            "Epoch 14/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9756 - accuracy: 0.5714 - val_loss: 1.0414 - val_accuracy: 0.4865\n",
            "Epoch 15/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9736 - accuracy: 0.5714 - val_loss: 1.0384 - val_accuracy: 0.4865\n",
            "Epoch 16/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9668 - accuracy: 0.5714 - val_loss: 1.0349 - val_accuracy: 0.4865\n",
            "Epoch 17/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9683 - accuracy: 0.5714 - val_loss: 1.0361 - val_accuracy: 0.4865\n",
            "Epoch 18/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9693 - accuracy: 0.5714 - val_loss: 1.0359 - val_accuracy: 0.4865\n",
            "Epoch 19/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9683 - accuracy: 0.5714 - val_loss: 1.0496 - val_accuracy: 0.4865\n",
            "Epoch 20/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9665 - accuracy: 0.5714 - val_loss: 1.0462 - val_accuracy: 0.4865\n",
            "Epoch 21/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9635 - accuracy: 0.5714 - val_loss: 1.0312 - val_accuracy: 0.4865\n",
            "Epoch 22/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9644 - accuracy: 0.5714 - val_loss: 1.0290 - val_accuracy: 0.4865\n",
            "Epoch 23/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9636 - accuracy: 0.5714 - val_loss: 1.0334 - val_accuracy: 0.4865\n",
            "Epoch 24/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9605 - accuracy: 0.5714 - val_loss: 1.0414 - val_accuracy: 0.4865\n",
            "Epoch 25/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9608 - accuracy: 0.5714 - val_loss: 1.0447 - val_accuracy: 0.4865\n",
            "Epoch 26/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9613 - accuracy: 0.5714 - val_loss: 1.0375 - val_accuracy: 0.4865\n",
            "Epoch 27/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9595 - accuracy: 0.5714 - val_loss: 1.0277 - val_accuracy: 0.4865\n",
            "Epoch 28/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9591 - accuracy: 0.5714 - val_loss: 1.0224 - val_accuracy: 0.4865\n",
            "Epoch 29/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9599 - accuracy: 0.5714 - val_loss: 1.0295 - val_accuracy: 0.4865\n",
            "Epoch 30/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9566 - accuracy: 0.5714 - val_loss: 1.0259 - val_accuracy: 0.4865\n",
            "Epoch 31/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9552 - accuracy: 0.5714 - val_loss: 1.0199 - val_accuracy: 0.4865\n",
            "Epoch 32/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9561 - accuracy: 0.5714 - val_loss: 1.0203 - val_accuracy: 0.4865\n",
            "Epoch 33/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9531 - accuracy: 0.5714 - val_loss: 1.0309 - val_accuracy: 0.4865\n",
            "Epoch 34/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9531 - accuracy: 0.5714 - val_loss: 1.0277 - val_accuracy: 0.4865\n",
            "Epoch 35/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9528 - accuracy: 0.5714 - val_loss: 1.0248 - val_accuracy: 0.4865\n",
            "Epoch 36/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9544 - accuracy: 0.5714 - val_loss: 1.0482 - val_accuracy: 0.4865\n",
            "Epoch 37/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9569 - accuracy: 0.5714 - val_loss: 1.0287 - val_accuracy: 0.4865\n",
            "Epoch 38/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9495 - accuracy: 0.5714 - val_loss: 1.0053 - val_accuracy: 0.4865\n",
            "Epoch 39/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9507 - accuracy: 0.5714 - val_loss: 1.0148 - val_accuracy: 0.4865\n",
            "Epoch 40/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9457 - accuracy: 0.5714 - val_loss: 1.0328 - val_accuracy: 0.4865\n",
            "Epoch 41/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9495 - accuracy: 0.5714 - val_loss: 1.0172 - val_accuracy: 0.4865\n",
            "Epoch 42/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9611 - accuracy: 0.5714 - val_loss: 1.0134 - val_accuracy: 0.4865\n",
            "Epoch 43/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9550 - accuracy: 0.5714 - val_loss: 1.0329 - val_accuracy: 0.4865\n",
            "Epoch 44/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9472 - accuracy: 0.5714 - val_loss: 1.0120 - val_accuracy: 0.4865\n",
            "Epoch 45/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9463 - accuracy: 0.5714 - val_loss: 0.9965 - val_accuracy: 0.4865\n",
            "Epoch 46/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9481 - accuracy: 0.5714 - val_loss: 1.0056 - val_accuracy: 0.4865\n",
            "Epoch 47/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9441 - accuracy: 0.5714 - val_loss: 1.0300 - val_accuracy: 0.4865\n",
            "Epoch 48/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9466 - accuracy: 0.5714 - val_loss: 1.0153 - val_accuracy: 0.4865\n",
            "Epoch 49/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9406 - accuracy: 0.5748 - val_loss: 1.0021 - val_accuracy: 0.5000\n",
            "Epoch 50/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9401 - accuracy: 0.5986 - val_loss: 1.0070 - val_accuracy: 0.5000\n",
            "Epoch 51/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9339 - accuracy: 0.5884 - val_loss: 1.0311 - val_accuracy: 0.4865\n",
            "Epoch 52/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9404 - accuracy: 0.5816 - val_loss: 0.9848 - val_accuracy: 0.5000\n",
            "Epoch 53/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9446 - accuracy: 0.5952 - val_loss: 0.9817 - val_accuracy: 0.5000\n",
            "Epoch 54/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9396 - accuracy: 0.5884 - val_loss: 1.0175 - val_accuracy: 0.4865\n",
            "Epoch 55/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9463 - accuracy: 0.5782 - val_loss: 1.0517 - val_accuracy: 0.4865\n",
            "Epoch 56/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9410 - accuracy: 0.5850 - val_loss: 0.9869 - val_accuracy: 0.5000\n",
            "Epoch 57/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9535 - accuracy: 0.5952 - val_loss: 0.9889 - val_accuracy: 0.5000\n",
            "Epoch 58/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9448 - accuracy: 0.5918 - val_loss: 1.0111 - val_accuracy: 0.4865\n",
            "Epoch 59/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9350 - accuracy: 0.6020 - val_loss: 1.0391 - val_accuracy: 0.4865\n",
            "Epoch 60/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9423 - accuracy: 0.5952 - val_loss: 1.0062 - val_accuracy: 0.5000\n",
            "Epoch 61/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9313 - accuracy: 0.5952 - val_loss: 0.9835 - val_accuracy: 0.5000\n",
            "Epoch 62/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9300 - accuracy: 0.5884 - val_loss: 0.9929 - val_accuracy: 0.5000\n",
            "Epoch 63/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9308 - accuracy: 0.5986 - val_loss: 1.0171 - val_accuracy: 0.4865\n",
            "Epoch 64/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9315 - accuracy: 0.5986 - val_loss: 0.9973 - val_accuracy: 0.5000\n",
            "Epoch 65/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9238 - accuracy: 0.6020 - val_loss: 0.9834 - val_accuracy: 0.5000\n",
            "Epoch 66/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9285 - accuracy: 0.5986 - val_loss: 0.9791 - val_accuracy: 0.5000\n",
            "Epoch 67/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9258 - accuracy: 0.5952 - val_loss: 0.9950 - val_accuracy: 0.5000\n",
            "Epoch 68/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9214 - accuracy: 0.6020 - val_loss: 1.0020 - val_accuracy: 0.4865\n",
            "Epoch 69/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9206 - accuracy: 0.5986 - val_loss: 0.9990 - val_accuracy: 0.5000\n",
            "Epoch 70/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9181 - accuracy: 0.6020 - val_loss: 0.9840 - val_accuracy: 0.5000\n",
            "Epoch 71/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9162 - accuracy: 0.6020 - val_loss: 0.9875 - val_accuracy: 0.5000\n",
            "Epoch 72/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9125 - accuracy: 0.6020 - val_loss: 1.0031 - val_accuracy: 0.4865\n",
            "Epoch 73/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9157 - accuracy: 0.5952 - val_loss: 0.9776 - val_accuracy: 0.5000\n",
            "Epoch 74/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9144 - accuracy: 0.6054 - val_loss: 0.9726 - val_accuracy: 0.5000\n",
            "Epoch 75/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9082 - accuracy: 0.5986 - val_loss: 1.0463 - val_accuracy: 0.5000\n",
            "Epoch 76/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9280 - accuracy: 0.6020 - val_loss: 0.9925 - val_accuracy: 0.5000\n",
            "Epoch 77/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9236 - accuracy: 0.5986 - val_loss: 0.9806 - val_accuracy: 0.5000\n",
            "Epoch 78/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9109 - accuracy: 0.6054 - val_loss: 1.0334 - val_accuracy: 0.4865\n",
            "Epoch 79/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9277 - accuracy: 0.5952 - val_loss: 1.0385 - val_accuracy: 0.4865\n",
            "Epoch 80/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9189 - accuracy: 0.5952 - val_loss: 0.9826 - val_accuracy: 0.5000\n",
            "Epoch 81/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9065 - accuracy: 0.6054 - val_loss: 0.9640 - val_accuracy: 0.5676\n",
            "Epoch 82/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9058 - accuracy: 0.5986 - val_loss: 1.0012 - val_accuracy: 0.5000\n",
            "Epoch 83/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9048 - accuracy: 0.5986 - val_loss: 1.0307 - val_accuracy: 0.4865\n",
            "Epoch 84/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9090 - accuracy: 0.5918 - val_loss: 0.9928 - val_accuracy: 0.5000\n",
            "Epoch 85/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9003 - accuracy: 0.6054 - val_loss: 0.9772 - val_accuracy: 0.5000\n",
            "Epoch 86/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8995 - accuracy: 0.6054 - val_loss: 0.9983 - val_accuracy: 0.5000\n",
            "Epoch 87/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8926 - accuracy: 0.6020 - val_loss: 1.0246 - val_accuracy: 0.5000\n",
            "Epoch 88/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8959 - accuracy: 0.6020 - val_loss: 1.0255 - val_accuracy: 0.5135\n",
            "Epoch 89/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8870 - accuracy: 0.5952 - val_loss: 1.0645 - val_accuracy: 0.5135\n",
            "Epoch 90/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9021 - accuracy: 0.5986 - val_loss: 1.0049 - val_accuracy: 0.4865\n",
            "Epoch 91/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8943 - accuracy: 0.5986 - val_loss: 1.0169 - val_accuracy: 0.5000\n",
            "Epoch 92/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9019 - accuracy: 0.6020 - val_loss: 0.9995 - val_accuracy: 0.5000\n",
            "Epoch 93/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8982 - accuracy: 0.5986 - val_loss: 0.9896 - val_accuracy: 0.5000\n",
            "Epoch 94/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8993 - accuracy: 0.5986 - val_loss: 1.0062 - val_accuracy: 0.5000\n",
            "Epoch 95/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8946 - accuracy: 0.5986 - val_loss: 1.0107 - val_accuracy: 0.5000\n",
            "Epoch 96/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8965 - accuracy: 0.5986 - val_loss: 1.0096 - val_accuracy: 0.5000\n",
            "Epoch 97/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8949 - accuracy: 0.6054 - val_loss: 1.0041 - val_accuracy: 0.5000\n",
            "Epoch 98/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9085 - accuracy: 0.6054 - val_loss: 0.9893 - val_accuracy: 0.5000\n",
            "Epoch 99/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8866 - accuracy: 0.5952 - val_loss: 0.9835 - val_accuracy: 0.5270\n",
            "Epoch 100/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9035 - accuracy: 0.6054 - val_loss: 1.0028 - val_accuracy: 0.5000\n",
            "Epoch 101/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9089 - accuracy: 0.6054 - val_loss: 1.0718 - val_accuracy: 0.5000\n",
            "Epoch 102/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9040 - accuracy: 0.6020 - val_loss: 0.9957 - val_accuracy: 0.5000\n",
            "Epoch 103/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8900 - accuracy: 0.6020 - val_loss: 0.9923 - val_accuracy: 0.5405\n",
            "Epoch 104/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8826 - accuracy: 0.6088 - val_loss: 1.0109 - val_accuracy: 0.5000\n",
            "Epoch 105/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8842 - accuracy: 0.6054 - val_loss: 1.0315 - val_accuracy: 0.5000\n",
            "Epoch 106/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8861 - accuracy: 0.6088 - val_loss: 0.9970 - val_accuracy: 0.5000\n",
            "Epoch 107/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8857 - accuracy: 0.5986 - val_loss: 0.9985 - val_accuracy: 0.5135\n",
            "Epoch 108/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8720 - accuracy: 0.6020 - val_loss: 1.0222 - val_accuracy: 0.5000\n",
            "Epoch 109/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8882 - accuracy: 0.6088 - val_loss: 1.0819 - val_accuracy: 0.5000\n",
            "Epoch 110/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8877 - accuracy: 0.6020 - val_loss: 1.0163 - val_accuracy: 0.5000\n",
            "Epoch 111/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8632 - accuracy: 0.6088 - val_loss: 0.9688 - val_accuracy: 0.5541\n",
            "Epoch 112/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8628 - accuracy: 0.6020 - val_loss: 1.0449 - val_accuracy: 0.3514\n",
            "Epoch 113/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9266 - accuracy: 0.5136 - val_loss: 0.9894 - val_accuracy: 0.5135\n",
            "Epoch 114/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9055 - accuracy: 0.5952 - val_loss: 1.0056 - val_accuracy: 0.5000\n",
            "Epoch 115/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9068 - accuracy: 0.6054 - val_loss: 0.9881 - val_accuracy: 0.5000\n",
            "Epoch 116/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8966 - accuracy: 0.6088 - val_loss: 1.0335 - val_accuracy: 0.5000\n",
            "Epoch 117/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9077 - accuracy: 0.6020 - val_loss: 1.0465 - val_accuracy: 0.5000\n",
            "Epoch 118/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8967 - accuracy: 0.6020 - val_loss: 1.0191 - val_accuracy: 0.5000\n",
            "Epoch 119/800\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.8796 - accuracy: 0.6054 - val_loss: 1.0070 - val_accuracy: 0.5541\n",
            "Epoch 120/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8799 - accuracy: 0.6020 - val_loss: 1.0146 - val_accuracy: 0.5405\n",
            "Epoch 121/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8745 - accuracy: 0.5986 - val_loss: 1.0429 - val_accuracy: 0.5135\n",
            "Epoch 122/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8701 - accuracy: 0.6088 - val_loss: 1.0306 - val_accuracy: 0.5541\n",
            "Epoch 123/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8658 - accuracy: 0.6020 - val_loss: 1.0347 - val_accuracy: 0.5541\n",
            "Epoch 124/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8650 - accuracy: 0.6054 - val_loss: 1.0474 - val_accuracy: 0.5541\n",
            "Epoch 125/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8617 - accuracy: 0.6020 - val_loss: 1.0524 - val_accuracy: 0.5541\n",
            "Epoch 126/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8576 - accuracy: 0.6020 - val_loss: 1.0646 - val_accuracy: 0.5135\n",
            "Epoch 127/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8558 - accuracy: 0.6088 - val_loss: 1.0390 - val_accuracy: 0.5541\n",
            "Epoch 128/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8448 - accuracy: 0.6122 - val_loss: 1.0478 - val_accuracy: 0.5135\n",
            "Epoch 129/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8431 - accuracy: 0.5986 - val_loss: 1.0801 - val_accuracy: 0.5000\n",
            "Epoch 130/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8804 - accuracy: 0.6190 - val_loss: 1.0552 - val_accuracy: 0.5000\n",
            "Epoch 131/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8740 - accuracy: 0.6054 - val_loss: 1.0915 - val_accuracy: 0.4865\n",
            "Epoch 132/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8803 - accuracy: 0.6020 - val_loss: 1.0087 - val_accuracy: 0.5000\n",
            "Epoch 133/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8660 - accuracy: 0.6088 - val_loss: 0.9989 - val_accuracy: 0.5000\n",
            "Epoch 134/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8536 - accuracy: 0.6156 - val_loss: 1.0384 - val_accuracy: 0.5000\n",
            "Epoch 135/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8521 - accuracy: 0.6122 - val_loss: 1.0419 - val_accuracy: 0.5000\n",
            "Epoch 136/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8326 - accuracy: 0.6088 - val_loss: 1.0567 - val_accuracy: 0.5676\n",
            "Epoch 137/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8275 - accuracy: 0.6497 - val_loss: 1.0435 - val_accuracy: 0.5000\n",
            "Epoch 138/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8507 - accuracy: 0.6088 - val_loss: 1.0273 - val_accuracy: 0.5000\n",
            "Epoch 139/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8431 - accuracy: 0.6088 - val_loss: 1.0127 - val_accuracy: 0.5270\n",
            "Epoch 140/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8114 - accuracy: 0.6361 - val_loss: 1.1007 - val_accuracy: 0.5000\n",
            "Epoch 141/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8518 - accuracy: 0.6122 - val_loss: 1.1185 - val_accuracy: 0.5135\n",
            "Epoch 142/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8310 - accuracy: 0.5986 - val_loss: 1.0632 - val_accuracy: 0.5135\n",
            "Epoch 143/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8377 - accuracy: 0.6565 - val_loss: 1.0410 - val_accuracy: 0.5405\n",
            "Epoch 144/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8044 - accuracy: 0.6259 - val_loss: 1.0809 - val_accuracy: 0.5000\n",
            "Epoch 145/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7963 - accuracy: 0.6156 - val_loss: 1.1042 - val_accuracy: 0.5270\n",
            "Epoch 146/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8116 - accuracy: 0.6259 - val_loss: 1.0210 - val_accuracy: 0.5135\n",
            "Epoch 147/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8311 - accuracy: 0.6054 - val_loss: 1.0909 - val_accuracy: 0.5135\n",
            "Epoch 148/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8916 - accuracy: 0.6020 - val_loss: 0.9770 - val_accuracy: 0.5000\n",
            "Epoch 149/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8734 - accuracy: 0.6088 - val_loss: 0.9478 - val_accuracy: 0.5405\n",
            "Epoch 150/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8295 - accuracy: 0.5986 - val_loss: 1.0425 - val_accuracy: 0.5000\n",
            "Epoch 151/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8723 - accuracy: 0.6054 - val_loss: 0.9757 - val_accuracy: 0.5135\n",
            "Epoch 152/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8490 - accuracy: 0.6054 - val_loss: 0.9712 - val_accuracy: 0.5946\n",
            "Epoch 153/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.8697 - accuracy: 0.6088 - val_loss: 1.0061 - val_accuracy: 0.5135\n",
            "Epoch 154/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8563 - accuracy: 0.6156 - val_loss: 1.0524 - val_accuracy: 0.4865\n",
            "Epoch 155/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8722 - accuracy: 0.6054 - val_loss: 1.0267 - val_accuracy: 0.5000\n",
            "Epoch 156/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8524 - accuracy: 0.5986 - val_loss: 1.0160 - val_accuracy: 0.5000\n",
            "Epoch 157/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8656 - accuracy: 0.6190 - val_loss: 1.0409 - val_accuracy: 0.5000\n",
            "Epoch 158/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8663 - accuracy: 0.6054 - val_loss: 1.0546 - val_accuracy: 0.5000\n",
            "Epoch 159/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8667 - accuracy: 0.5952 - val_loss: 1.0464 - val_accuracy: 0.4865\n",
            "Epoch 160/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8473 - accuracy: 0.6020 - val_loss: 1.0359 - val_accuracy: 0.5000\n",
            "Epoch 161/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8464 - accuracy: 0.5986 - val_loss: 1.0566 - val_accuracy: 0.5000\n",
            "Epoch 162/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8546 - accuracy: 0.6020 - val_loss: 1.0074 - val_accuracy: 0.5405\n",
            "Epoch 163/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8311 - accuracy: 0.6122 - val_loss: 1.0089 - val_accuracy: 0.5135\n",
            "Epoch 164/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8138 - accuracy: 0.6531 - val_loss: 1.0557 - val_accuracy: 0.4054\n",
            "Epoch 165/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8032 - accuracy: 0.6667 - val_loss: 1.0775 - val_accuracy: 0.5000\n",
            "Epoch 166/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7863 - accuracy: 0.6224 - val_loss: 1.1172 - val_accuracy: 0.3919\n",
            "Epoch 167/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8552 - accuracy: 0.6259 - val_loss: 1.0186 - val_accuracy: 0.4865\n",
            "Epoch 168/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8471 - accuracy: 0.5986 - val_loss: 1.0355 - val_accuracy: 0.5000\n",
            "Epoch 169/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8405 - accuracy: 0.6088 - val_loss: 1.0121 - val_accuracy: 0.5135\n",
            "Epoch 170/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8412 - accuracy: 0.6361 - val_loss: 1.0385 - val_accuracy: 0.5000\n",
            "Epoch 171/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8340 - accuracy: 0.6156 - val_loss: 1.0238 - val_accuracy: 0.5135\n",
            "Epoch 172/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8136 - accuracy: 0.6259 - val_loss: 1.0392 - val_accuracy: 0.5000\n",
            "Epoch 173/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7946 - accuracy: 0.6327 - val_loss: 1.0992 - val_accuracy: 0.4459\n",
            "Epoch 174/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7898 - accuracy: 0.6361 - val_loss: 1.1169 - val_accuracy: 0.5135\n",
            "Epoch 175/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7843 - accuracy: 0.6361 - val_loss: 1.1080 - val_accuracy: 0.5135\n",
            "Epoch 176/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7639 - accuracy: 0.6531 - val_loss: 1.1173 - val_accuracy: 0.3784\n",
            "Epoch 177/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7790 - accuracy: 0.6803 - val_loss: 1.0840 - val_accuracy: 0.5541\n",
            "Epoch 178/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7834 - accuracy: 0.6293 - val_loss: 1.1547 - val_accuracy: 0.4324\n",
            "Epoch 179/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8274 - accuracy: 0.6020 - val_loss: 1.0590 - val_accuracy: 0.5541\n",
            "Epoch 180/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7951 - accuracy: 0.6259 - val_loss: 1.0350 - val_accuracy: 0.5405\n",
            "Epoch 181/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8130 - accuracy: 0.6020 - val_loss: 1.0012 - val_accuracy: 0.5135\n",
            "Epoch 182/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8134 - accuracy: 0.6395 - val_loss: 1.0048 - val_accuracy: 0.5541\n",
            "Epoch 183/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7928 - accuracy: 0.6020 - val_loss: 1.0644 - val_accuracy: 0.5000\n",
            "Epoch 184/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8074 - accuracy: 0.5986 - val_loss: 1.0335 - val_accuracy: 0.5270\n",
            "Epoch 185/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7793 - accuracy: 0.6599 - val_loss: 1.0443 - val_accuracy: 0.5135\n",
            "Epoch 186/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7657 - accuracy: 0.6735 - val_loss: 1.0574 - val_accuracy: 0.5270\n",
            "Epoch 187/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7601 - accuracy: 0.6259 - val_loss: 1.0741 - val_accuracy: 0.5000\n",
            "Epoch 188/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7415 - accuracy: 0.6429 - val_loss: 1.1189 - val_accuracy: 0.4459\n",
            "Epoch 189/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7313 - accuracy: 0.6803 - val_loss: 1.1319 - val_accuracy: 0.4324\n",
            "Epoch 190/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7313 - accuracy: 0.6633 - val_loss: 1.1251 - val_accuracy: 0.5000\n",
            "Epoch 191/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7250 - accuracy: 0.6769 - val_loss: 1.1225 - val_accuracy: 0.5000\n",
            "Epoch 192/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7369 - accuracy: 0.6735 - val_loss: 1.0859 - val_accuracy: 0.5135\n",
            "Epoch 193/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7655 - accuracy: 0.6939 - val_loss: 1.0591 - val_accuracy: 0.5270\n",
            "Epoch 194/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7572 - accuracy: 0.6429 - val_loss: 1.0681 - val_accuracy: 0.5000\n",
            "Epoch 195/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8013 - accuracy: 0.6395 - val_loss: 1.0197 - val_accuracy: 0.5270\n",
            "Epoch 196/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7890 - accuracy: 0.6395 - val_loss: 1.0207 - val_accuracy: 0.5270\n",
            "Epoch 197/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7858 - accuracy: 0.6224 - val_loss: 1.0359 - val_accuracy: 0.5405\n",
            "Epoch 198/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7809 - accuracy: 0.6293 - val_loss: 1.0076 - val_accuracy: 0.4865\n",
            "Epoch 199/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7686 - accuracy: 0.6293 - val_loss: 1.0211 - val_accuracy: 0.4459\n",
            "Epoch 200/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7485 - accuracy: 0.6497 - val_loss: 1.1291 - val_accuracy: 0.4324\n",
            "Epoch 201/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7255 - accuracy: 0.6735 - val_loss: 1.2427 - val_accuracy: 0.5000\n",
            "Epoch 202/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7910 - accuracy: 0.6599 - val_loss: 1.1504 - val_accuracy: 0.4054\n",
            "Epoch 203/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7283 - accuracy: 0.6803 - val_loss: 1.1345 - val_accuracy: 0.5405\n",
            "Epoch 204/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8647 - accuracy: 0.6259 - val_loss: 0.9479 - val_accuracy: 0.5811\n",
            "Epoch 205/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7716 - accuracy: 0.6735 - val_loss: 1.0066 - val_accuracy: 0.5541\n",
            "Epoch 206/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7933 - accuracy: 0.6565 - val_loss: 1.0572 - val_accuracy: 0.5000\n",
            "Epoch 207/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8007 - accuracy: 0.6224 - val_loss: 1.0997 - val_accuracy: 0.5135\n",
            "Epoch 208/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7893 - accuracy: 0.6361 - val_loss: 1.0991 - val_accuracy: 0.4865\n",
            "Epoch 209/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7658 - accuracy: 0.6395 - val_loss: 1.0900 - val_accuracy: 0.4730\n",
            "Epoch 210/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7675 - accuracy: 0.6497 - val_loss: 1.1167 - val_accuracy: 0.5270\n",
            "Epoch 211/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7511 - accuracy: 0.6361 - val_loss: 1.1555 - val_accuracy: 0.5270\n",
            "Epoch 212/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7566 - accuracy: 0.6463 - val_loss: 1.1409 - val_accuracy: 0.5541\n",
            "Epoch 213/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7169 - accuracy: 0.6973 - val_loss: 1.1830 - val_accuracy: 0.4324\n",
            "Epoch 214/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7334 - accuracy: 0.6871 - val_loss: 1.1867 - val_accuracy: 0.4730\n",
            "Epoch 215/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7212 - accuracy: 0.6735 - val_loss: 1.1363 - val_accuracy: 0.5811\n",
            "Epoch 216/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.7232 - accuracy: 0.6871 - val_loss: 1.1381 - val_accuracy: 0.5270\n",
            "Epoch 217/800\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6987 - accuracy: 0.6939 - val_loss: 1.1158 - val_accuracy: 0.5000\n",
            "Epoch 218/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6957 - accuracy: 0.7007 - val_loss: 1.1075 - val_accuracy: 0.4730\n",
            "Epoch 219/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6720 - accuracy: 0.7109 - val_loss: 1.1246 - val_accuracy: 0.4865\n",
            "Epoch 220/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6722 - accuracy: 0.7109 - val_loss: 1.1664 - val_accuracy: 0.5541\n",
            "Epoch 221/800\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.6821 - accuracy: 0.6701 - val_loss: 1.1679 - val_accuracy: 0.5000\n",
            "Epoch 222/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6756 - accuracy: 0.6973 - val_loss: 1.1040 - val_accuracy: 0.5541\n",
            "Epoch 223/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.6864 - accuracy: 0.7007 - val_loss: 1.1847 - val_accuracy: 0.3919\n",
            "Epoch 224/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7566 - accuracy: 0.6565 - val_loss: 1.1694 - val_accuracy: 0.4189\n",
            "Epoch 225/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7258 - accuracy: 0.6803 - val_loss: 1.3373 - val_accuracy: 0.4189\n",
            "Epoch 226/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7487 - accuracy: 0.6463 - val_loss: 1.2055 - val_accuracy: 0.5135\n",
            "Epoch 227/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7891 - accuracy: 0.6429 - val_loss: 1.0325 - val_accuracy: 0.5676\n",
            "Epoch 228/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7549 - accuracy: 0.6599 - val_loss: 1.0271 - val_accuracy: 0.5541\n",
            "Epoch 229/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7309 - accuracy: 0.6565 - val_loss: 1.0543 - val_accuracy: 0.5541\n",
            "Epoch 230/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7044 - accuracy: 0.6837 - val_loss: 1.1184 - val_accuracy: 0.4865\n",
            "Epoch 231/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6853 - accuracy: 0.7041 - val_loss: 1.2404 - val_accuracy: 0.4730\n",
            "Epoch 232/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6869 - accuracy: 0.7211 - val_loss: 1.1935 - val_accuracy: 0.5270\n",
            "Epoch 233/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6724 - accuracy: 0.7177 - val_loss: 1.1380 - val_accuracy: 0.4459\n",
            "Epoch 234/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6714 - accuracy: 0.7177 - val_loss: 1.1447 - val_accuracy: 0.5676\n",
            "Epoch 235/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6654 - accuracy: 0.7143 - val_loss: 1.1879 - val_accuracy: 0.5135\n",
            "Epoch 236/800\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.6527 - accuracy: 0.7279 - val_loss: 1.2116 - val_accuracy: 0.4459\n",
            "Epoch 237/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6476 - accuracy: 0.7245 - val_loss: 1.1918 - val_accuracy: 0.5541\n",
            "Epoch 238/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7212 - accuracy: 0.6633 - val_loss: 1.0899 - val_accuracy: 0.5946\n",
            "Epoch 239/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6678 - accuracy: 0.7177 - val_loss: 1.1771 - val_accuracy: 0.4595\n",
            "Epoch 240/800\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.6581 - accuracy: 0.7313 - val_loss: 1.1824 - val_accuracy: 0.5811\n",
            "Epoch 241/800\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.6559 - accuracy: 0.6939 - val_loss: 1.2661 - val_accuracy: 0.4865\n",
            "Epoch 242/800\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.6352 - accuracy: 0.7313 - val_loss: 1.2143 - val_accuracy: 0.4865\n",
            "Epoch 243/800\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.6355 - accuracy: 0.7415 - val_loss: 1.1705 - val_accuracy: 0.4730\n",
            "Epoch 244/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6813 - accuracy: 0.7007 - val_loss: 1.2693 - val_accuracy: 0.4730\n",
            "Epoch 245/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6799 - accuracy: 0.7245 - val_loss: 1.2229 - val_accuracy: 0.5405\n",
            "Epoch 246/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6962 - accuracy: 0.6973 - val_loss: 1.1827 - val_accuracy: 0.4459\n",
            "Epoch 247/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6312 - accuracy: 0.7381 - val_loss: 1.1171 - val_accuracy: 0.6081\n",
            "Epoch 248/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6430 - accuracy: 0.7585 - val_loss: 1.1843 - val_accuracy: 0.4730\n",
            "Epoch 249/800\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.6652 - accuracy: 0.7041 - val_loss: 1.2823 - val_accuracy: 0.3514\n",
            "Epoch 250/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6728 - accuracy: 0.7075 - val_loss: 1.1598 - val_accuracy: 0.5405\n",
            "Epoch 251/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6901 - accuracy: 0.6871 - val_loss: 1.0674 - val_accuracy: 0.5676\n",
            "Epoch 252/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6547 - accuracy: 0.7483 - val_loss: 1.1416 - val_accuracy: 0.5000\n",
            "Epoch 253/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6480 - accuracy: 0.7415 - val_loss: 1.2202 - val_accuracy: 0.4595\n",
            "Epoch 254/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6133 - accuracy: 0.7585 - val_loss: 1.2002 - val_accuracy: 0.5676\n",
            "Epoch 255/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.6647 - accuracy: 0.7177 - val_loss: 1.0046 - val_accuracy: 0.5946\n",
            "Epoch 256/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6433 - accuracy: 0.7415 - val_loss: 1.1428 - val_accuracy: 0.5000\n",
            "Epoch 257/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6236 - accuracy: 0.7619 - val_loss: 1.2334 - val_accuracy: 0.4865\n",
            "Epoch 258/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6519 - accuracy: 0.7245 - val_loss: 1.1666 - val_accuracy: 0.4459\n",
            "Epoch 259/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6314 - accuracy: 0.7551 - val_loss: 1.1875 - val_accuracy: 0.4189\n",
            "Epoch 260/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6292 - accuracy: 0.7415 - val_loss: 1.1921 - val_accuracy: 0.5541\n",
            "Epoch 261/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6008 - accuracy: 0.7483 - val_loss: 1.1211 - val_accuracy: 0.5811\n",
            "Epoch 262/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5819 - accuracy: 0.7721 - val_loss: 1.2613 - val_accuracy: 0.4459\n",
            "Epoch 263/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6018 - accuracy: 0.7721 - val_loss: 1.2511 - val_accuracy: 0.5676\n",
            "Epoch 264/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6424 - accuracy: 0.7483 - val_loss: 1.1026 - val_accuracy: 0.5946\n",
            "Epoch 265/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6955 - accuracy: 0.7211 - val_loss: 1.0642 - val_accuracy: 0.5135\n",
            "Epoch 266/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7446 - accuracy: 0.7109 - val_loss: 1.0772 - val_accuracy: 0.5541\n",
            "Epoch 267/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.7121 - accuracy: 0.6939 - val_loss: 1.0862 - val_accuracy: 0.5676\n",
            "Epoch 268/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6800 - accuracy: 0.7007 - val_loss: 1.1224 - val_accuracy: 0.5405\n",
            "Epoch 269/800\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.6670 - accuracy: 0.7415 - val_loss: 1.2047 - val_accuracy: 0.5000\n",
            "Epoch 270/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6564 - accuracy: 0.7177 - val_loss: 1.1914 - val_accuracy: 0.5135\n",
            "Epoch 271/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6099 - accuracy: 0.7347 - val_loss: 1.1572 - val_accuracy: 0.5541\n",
            "Epoch 272/800\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.6149 - accuracy: 0.7653 - val_loss: 1.1735 - val_accuracy: 0.4730\n",
            "Epoch 273/800\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.5925 - accuracy: 0.7313 - val_loss: 1.1872 - val_accuracy: 0.4459\n",
            "Epoch 274/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5901 - accuracy: 0.7517 - val_loss: 1.1625 - val_accuracy: 0.5000\n",
            "Epoch 275/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5826 - accuracy: 0.7653 - val_loss: 1.1394 - val_accuracy: 0.5676\n",
            "Epoch 276/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.5829 - accuracy: 0.7925 - val_loss: 1.1519 - val_accuracy: 0.5270\n",
            "Epoch 277/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6134 - accuracy: 0.7483 - val_loss: 1.2143 - val_accuracy: 0.5405\n",
            "Epoch 278/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5711 - accuracy: 0.7823 - val_loss: 1.2976 - val_accuracy: 0.4459\n",
            "Epoch 279/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5720 - accuracy: 0.7517 - val_loss: 1.1260 - val_accuracy: 0.5541\n",
            "Epoch 280/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5451 - accuracy: 0.7653 - val_loss: 1.2824 - val_accuracy: 0.5541\n",
            "Epoch 281/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5452 - accuracy: 0.7619 - val_loss: 1.2859 - val_accuracy: 0.5676\n",
            "Epoch 282/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5632 - accuracy: 0.7857 - val_loss: 1.1643 - val_accuracy: 0.5676\n",
            "Epoch 283/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5785 - accuracy: 0.7925 - val_loss: 1.1637 - val_accuracy: 0.5676\n",
            "Epoch 284/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5886 - accuracy: 0.7755 - val_loss: 1.1543 - val_accuracy: 0.5676\n",
            "Epoch 285/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5758 - accuracy: 0.7721 - val_loss: 1.1354 - val_accuracy: 0.5405\n",
            "Epoch 286/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5487 - accuracy: 0.7755 - val_loss: 1.2115 - val_accuracy: 0.4730\n",
            "Epoch 287/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5318 - accuracy: 0.7925 - val_loss: 1.3221 - val_accuracy: 0.5000\n",
            "Epoch 288/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5230 - accuracy: 0.7891 - val_loss: 1.3083 - val_accuracy: 0.4865\n",
            "Epoch 289/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5222 - accuracy: 0.7755 - val_loss: 1.3447 - val_accuracy: 0.4865\n",
            "Epoch 290/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5099 - accuracy: 0.8061 - val_loss: 1.3088 - val_accuracy: 0.5000\n",
            "Epoch 291/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5176 - accuracy: 0.7925 - val_loss: 1.2619 - val_accuracy: 0.5811\n",
            "Epoch 292/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5172 - accuracy: 0.7925 - val_loss: 1.3146 - val_accuracy: 0.5135\n",
            "Epoch 293/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5110 - accuracy: 0.8061 - val_loss: 1.2622 - val_accuracy: 0.5811\n",
            "Epoch 294/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5092 - accuracy: 0.7585 - val_loss: 1.2755 - val_accuracy: 0.5541\n",
            "Epoch 295/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4943 - accuracy: 0.8095 - val_loss: 1.4952 - val_accuracy: 0.5405\n",
            "Epoch 296/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4871 - accuracy: 0.7925 - val_loss: 1.4301 - val_accuracy: 0.4459\n",
            "Epoch 297/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5571 - accuracy: 0.8027 - val_loss: 1.3528 - val_accuracy: 0.5270\n",
            "Epoch 298/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5321 - accuracy: 0.7857 - val_loss: 1.1782 - val_accuracy: 0.5676\n",
            "Epoch 299/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5451 - accuracy: 0.7619 - val_loss: 1.3300 - val_accuracy: 0.5270\n",
            "Epoch 300/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5354 - accuracy: 0.7925 - val_loss: 1.2726 - val_accuracy: 0.5270\n",
            "Epoch 301/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4971 - accuracy: 0.7959 - val_loss: 1.3465 - val_accuracy: 0.5135\n",
            "Epoch 302/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4907 - accuracy: 0.8163 - val_loss: 1.4753 - val_accuracy: 0.4459\n",
            "Epoch 303/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4992 - accuracy: 0.8027 - val_loss: 1.2383 - val_accuracy: 0.5811\n",
            "Epoch 304/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5130 - accuracy: 0.7993 - val_loss: 1.1331 - val_accuracy: 0.5811\n",
            "Epoch 305/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5094 - accuracy: 0.8163 - val_loss: 1.2072 - val_accuracy: 0.6081\n",
            "Epoch 306/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5049 - accuracy: 0.8061 - val_loss: 1.3235 - val_accuracy: 0.5676\n",
            "Epoch 307/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4629 - accuracy: 0.8095 - val_loss: 1.3168 - val_accuracy: 0.5676\n",
            "Epoch 308/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4723 - accuracy: 0.8231 - val_loss: 1.4012 - val_accuracy: 0.5405\n",
            "Epoch 309/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4357 - accuracy: 0.8401 - val_loss: 1.2634 - val_accuracy: 0.5270\n",
            "Epoch 310/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4697 - accuracy: 0.7959 - val_loss: 1.3178 - val_accuracy: 0.5541\n",
            "Epoch 311/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4454 - accuracy: 0.8469 - val_loss: 1.5499 - val_accuracy: 0.4865\n",
            "Epoch 312/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4909 - accuracy: 0.7857 - val_loss: 1.2788 - val_accuracy: 0.5676\n",
            "Epoch 313/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4784 - accuracy: 0.8061 - val_loss: 1.2588 - val_accuracy: 0.5811\n",
            "Epoch 314/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4371 - accuracy: 0.8401 - val_loss: 1.4137 - val_accuracy: 0.4595\n",
            "Epoch 315/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4678 - accuracy: 0.8401 - val_loss: 1.4366 - val_accuracy: 0.5676\n",
            "Epoch 316/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4388 - accuracy: 0.8163 - val_loss: 1.4353 - val_accuracy: 0.5946\n",
            "Epoch 317/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4596 - accuracy: 0.8401 - val_loss: 1.3750 - val_accuracy: 0.4865\n",
            "Epoch 318/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4835 - accuracy: 0.7823 - val_loss: 1.3356 - val_accuracy: 0.4865\n",
            "Epoch 319/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5068 - accuracy: 0.7585 - val_loss: 1.2908 - val_accuracy: 0.5135\n",
            "Epoch 320/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4883 - accuracy: 0.7959 - val_loss: 1.1965 - val_accuracy: 0.5946\n",
            "Epoch 321/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4766 - accuracy: 0.7891 - val_loss: 1.2853 - val_accuracy: 0.5811\n",
            "Epoch 322/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4820 - accuracy: 0.8027 - val_loss: 1.3762 - val_accuracy: 0.5541\n",
            "Epoch 323/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4617 - accuracy: 0.7993 - val_loss: 1.4644 - val_accuracy: 0.5811\n",
            "Epoch 324/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4378 - accuracy: 0.8129 - val_loss: 1.2796 - val_accuracy: 0.5676\n",
            "Epoch 325/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4483 - accuracy: 0.8333 - val_loss: 1.3341 - val_accuracy: 0.5676\n",
            "Epoch 326/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4294 - accuracy: 0.8367 - val_loss: 1.4988 - val_accuracy: 0.5811\n",
            "Epoch 327/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4122 - accuracy: 0.8469 - val_loss: 1.3018 - val_accuracy: 0.5811\n",
            "Epoch 328/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4051 - accuracy: 0.8435 - val_loss: 1.4511 - val_accuracy: 0.5405\n",
            "Epoch 329/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4082 - accuracy: 0.8367 - val_loss: 1.3969 - val_accuracy: 0.5946\n",
            "Epoch 330/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4240 - accuracy: 0.8129 - val_loss: 1.5581 - val_accuracy: 0.4730\n",
            "Epoch 331/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4301 - accuracy: 0.8265 - val_loss: 1.4577 - val_accuracy: 0.5270\n",
            "Epoch 332/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4789 - accuracy: 0.7789 - val_loss: 1.6495 - val_accuracy: 0.5270\n",
            "Epoch 333/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5502 - accuracy: 0.7279 - val_loss: 1.3258 - val_accuracy: 0.5811\n",
            "Epoch 334/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5141 - accuracy: 0.7959 - val_loss: 1.5923 - val_accuracy: 0.5135\n",
            "Epoch 335/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5482 - accuracy: 0.7755 - val_loss: 1.5648 - val_accuracy: 0.5000\n",
            "Epoch 336/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4944 - accuracy: 0.7959 - val_loss: 1.3642 - val_accuracy: 0.5811\n",
            "Epoch 337/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4409 - accuracy: 0.8401 - val_loss: 1.4212 - val_accuracy: 0.6351\n",
            "Epoch 338/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4176 - accuracy: 0.8299 - val_loss: 1.5378 - val_accuracy: 0.5405\n",
            "Epoch 339/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3860 - accuracy: 0.8639 - val_loss: 1.5332 - val_accuracy: 0.5405\n",
            "Epoch 340/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4979 - accuracy: 0.8129 - val_loss: 1.3725 - val_accuracy: 0.5135\n",
            "Epoch 341/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4656 - accuracy: 0.8027 - val_loss: 1.2098 - val_accuracy: 0.5811\n",
            "Epoch 342/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4554 - accuracy: 0.8299 - val_loss: 1.2124 - val_accuracy: 0.5405\n",
            "Epoch 343/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4787 - accuracy: 0.7925 - val_loss: 1.3897 - val_accuracy: 0.5811\n",
            "Epoch 344/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4477 - accuracy: 0.8231 - val_loss: 1.3395 - val_accuracy: 0.6486\n",
            "Epoch 345/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4005 - accuracy: 0.8673 - val_loss: 1.3970 - val_accuracy: 0.6081\n",
            "Epoch 346/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4011 - accuracy: 0.8537 - val_loss: 1.4008 - val_accuracy: 0.6351\n",
            "Epoch 347/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3986 - accuracy: 0.8707 - val_loss: 1.5560 - val_accuracy: 0.5405\n",
            "Epoch 348/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3598 - accuracy: 0.8707 - val_loss: 1.4180 - val_accuracy: 0.5676\n",
            "Epoch 349/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3512 - accuracy: 0.8673 - val_loss: 1.4856 - val_accuracy: 0.5405\n",
            "Epoch 350/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3414 - accuracy: 0.8810 - val_loss: 1.4684 - val_accuracy: 0.5811\n",
            "Epoch 351/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3685 - accuracy: 0.8673 - val_loss: 1.4288 - val_accuracy: 0.5676\n",
            "Epoch 352/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3526 - accuracy: 0.8741 - val_loss: 1.3618 - val_accuracy: 0.5811\n",
            "Epoch 353/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3485 - accuracy: 0.8673 - val_loss: 1.5102 - val_accuracy: 0.5811\n",
            "Epoch 354/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3231 - accuracy: 0.8673 - val_loss: 1.4689 - val_accuracy: 0.5811\n",
            "Epoch 355/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3496 - accuracy: 0.8707 - val_loss: 1.4556 - val_accuracy: 0.5946\n",
            "Epoch 356/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3489 - accuracy: 0.8639 - val_loss: 1.6016 - val_accuracy: 0.6081\n",
            "Epoch 357/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3275 - accuracy: 0.8639 - val_loss: 1.6216 - val_accuracy: 0.5811\n",
            "Epoch 358/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3051 - accuracy: 0.9014 - val_loss: 1.5465 - val_accuracy: 0.5676\n",
            "Epoch 359/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3449 - accuracy: 0.8639 - val_loss: 1.6684 - val_accuracy: 0.5676\n",
            "Epoch 360/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3527 - accuracy: 0.8503 - val_loss: 1.7313 - val_accuracy: 0.5946\n",
            "Epoch 361/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3260 - accuracy: 0.8605 - val_loss: 1.5533 - val_accuracy: 0.6081\n",
            "Epoch 362/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3526 - accuracy: 0.8571 - val_loss: 1.5272 - val_accuracy: 0.5811\n",
            "Epoch 363/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3702 - accuracy: 0.8503 - val_loss: 1.6583 - val_accuracy: 0.5405\n",
            "Epoch 364/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3190 - accuracy: 0.8741 - val_loss: 1.5752 - val_accuracy: 0.5946\n",
            "Epoch 365/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2996 - accuracy: 0.9048 - val_loss: 1.7010 - val_accuracy: 0.5541\n",
            "Epoch 366/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3155 - accuracy: 0.8810 - val_loss: 1.5530 - val_accuracy: 0.5811\n",
            "Epoch 367/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2734 - accuracy: 0.9082 - val_loss: 1.4949 - val_accuracy: 0.5811\n",
            "Epoch 368/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2921 - accuracy: 0.9082 - val_loss: 1.6174 - val_accuracy: 0.5676\n",
            "Epoch 369/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2906 - accuracy: 0.8878 - val_loss: 1.6035 - val_accuracy: 0.5270\n",
            "Epoch 370/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2712 - accuracy: 0.8912 - val_loss: 1.5448 - val_accuracy: 0.5541\n",
            "Epoch 371/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2989 - accuracy: 0.8912 - val_loss: 1.6539 - val_accuracy: 0.5946\n",
            "Epoch 372/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3259 - accuracy: 0.8367 - val_loss: 1.7894 - val_accuracy: 0.5270\n",
            "Epoch 373/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3300 - accuracy: 0.8673 - val_loss: 1.5991 - val_accuracy: 0.5811\n",
            "Epoch 374/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3133 - accuracy: 0.8878 - val_loss: 1.7476 - val_accuracy: 0.6216\n",
            "Epoch 375/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2942 - accuracy: 0.8673 - val_loss: 1.6409 - val_accuracy: 0.5541\n",
            "Epoch 376/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3261 - accuracy: 0.8707 - val_loss: 1.7110 - val_accuracy: 0.6081\n",
            "Epoch 377/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3030 - accuracy: 0.8741 - val_loss: 1.6554 - val_accuracy: 0.5811\n",
            "Epoch 378/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3080 - accuracy: 0.8946 - val_loss: 1.5731 - val_accuracy: 0.5946\n",
            "Epoch 379/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2601 - accuracy: 0.9116 - val_loss: 1.6456 - val_accuracy: 0.5405\n",
            "Epoch 380/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2620 - accuracy: 0.8912 - val_loss: 1.6453 - val_accuracy: 0.6216\n",
            "Epoch 381/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2492 - accuracy: 0.9048 - val_loss: 1.9135 - val_accuracy: 0.5946\n",
            "Epoch 382/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2473 - accuracy: 0.9116 - val_loss: 1.8570 - val_accuracy: 0.5676\n",
            "Epoch 383/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2566 - accuracy: 0.9184 - val_loss: 1.7604 - val_accuracy: 0.6081\n",
            "Epoch 384/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2488 - accuracy: 0.9252 - val_loss: 1.7315 - val_accuracy: 0.5946\n",
            "Epoch 385/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2691 - accuracy: 0.8946 - val_loss: 1.9834 - val_accuracy: 0.5811\n",
            "Epoch 386/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2436 - accuracy: 0.9116 - val_loss: 1.9062 - val_accuracy: 0.6081\n",
            "Epoch 387/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2254 - accuracy: 0.9218 - val_loss: 1.7520 - val_accuracy: 0.5946\n",
            "Epoch 388/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2682 - accuracy: 0.9082 - val_loss: 1.8253 - val_accuracy: 0.5270\n",
            "Epoch 389/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2524 - accuracy: 0.9082 - val_loss: 1.8495 - val_accuracy: 0.5811\n",
            "Epoch 390/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2573 - accuracy: 0.9150 - val_loss: 1.6640 - val_accuracy: 0.5676\n",
            "Epoch 391/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2246 - accuracy: 0.9048 - val_loss: 1.6876 - val_accuracy: 0.5946\n",
            "Epoch 392/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2082 - accuracy: 0.9320 - val_loss: 1.8425 - val_accuracy: 0.5946\n",
            "Epoch 393/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2201 - accuracy: 0.9422 - val_loss: 1.8551 - val_accuracy: 0.5946\n",
            "Epoch 394/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2166 - accuracy: 0.9150 - val_loss: 2.0043 - val_accuracy: 0.6081\n",
            "Epoch 395/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2127 - accuracy: 0.9048 - val_loss: 1.8601 - val_accuracy: 0.6081\n",
            "Epoch 396/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2459 - accuracy: 0.9048 - val_loss: 1.9180 - val_accuracy: 0.5811\n",
            "Epoch 397/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2818 - accuracy: 0.8946 - val_loss: 2.0215 - val_accuracy: 0.5946\n",
            "Epoch 398/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3158 - accuracy: 0.8537 - val_loss: 1.5809 - val_accuracy: 0.6081\n",
            "Epoch 399/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3762 - accuracy: 0.8401 - val_loss: 1.8195 - val_accuracy: 0.6216\n",
            "Epoch 400/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3249 - accuracy: 0.8673 - val_loss: 1.8610 - val_accuracy: 0.5811\n",
            "Epoch 401/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3205 - accuracy: 0.8741 - val_loss: 1.5625 - val_accuracy: 0.6216\n",
            "Epoch 402/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4100 - accuracy: 0.8367 - val_loss: 1.7784 - val_accuracy: 0.6081\n",
            "Epoch 403/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2999 - accuracy: 0.8878 - val_loss: 1.6574 - val_accuracy: 0.5541\n",
            "Epoch 404/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2777 - accuracy: 0.9116 - val_loss: 1.6426 - val_accuracy: 0.6081\n",
            "Epoch 405/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2522 - accuracy: 0.9048 - val_loss: 1.7647 - val_accuracy: 0.5946\n",
            "Epoch 406/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2346 - accuracy: 0.9286 - val_loss: 1.8645 - val_accuracy: 0.5946\n",
            "Epoch 407/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2271 - accuracy: 0.9014 - val_loss: 1.9963 - val_accuracy: 0.5946\n",
            "Epoch 408/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2330 - accuracy: 0.9184 - val_loss: 2.0548 - val_accuracy: 0.6216\n",
            "Epoch 409/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2030 - accuracy: 0.9354 - val_loss: 2.0194 - val_accuracy: 0.5811\n",
            "Epoch 410/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2007 - accuracy: 0.9252 - val_loss: 1.8656 - val_accuracy: 0.5676\n",
            "Epoch 411/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2133 - accuracy: 0.9150 - val_loss: 1.7931 - val_accuracy: 0.5946\n",
            "Epoch 412/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1895 - accuracy: 0.9456 - val_loss: 2.0620 - val_accuracy: 0.6081\n",
            "Epoch 413/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2077 - accuracy: 0.9252 - val_loss: 2.1370 - val_accuracy: 0.6081\n",
            "Epoch 414/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2229 - accuracy: 0.9116 - val_loss: 1.8236 - val_accuracy: 0.5405\n",
            "Epoch 415/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2412 - accuracy: 0.8980 - val_loss: 1.6635 - val_accuracy: 0.5946\n",
            "Epoch 416/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2411 - accuracy: 0.9082 - val_loss: 1.8035 - val_accuracy: 0.5946\n",
            "Epoch 417/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2158 - accuracy: 0.9320 - val_loss: 2.0967 - val_accuracy: 0.5946\n",
            "Epoch 418/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2340 - accuracy: 0.9286 - val_loss: 2.0239 - val_accuracy: 0.5811\n",
            "Epoch 419/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1878 - accuracy: 0.9286 - val_loss: 1.8172 - val_accuracy: 0.6081\n",
            "Epoch 420/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2040 - accuracy: 0.9184 - val_loss: 1.8183 - val_accuracy: 0.6081\n",
            "Epoch 421/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2304 - accuracy: 0.9014 - val_loss: 1.8521 - val_accuracy: 0.6351\n",
            "Epoch 422/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1970 - accuracy: 0.9286 - val_loss: 1.9606 - val_accuracy: 0.5946\n",
            "Epoch 423/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2240 - accuracy: 0.9218 - val_loss: 1.9962 - val_accuracy: 0.5946\n",
            "Epoch 424/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1649 - accuracy: 0.9456 - val_loss: 2.0500 - val_accuracy: 0.5946\n",
            "Epoch 425/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1923 - accuracy: 0.9252 - val_loss: 2.1696 - val_accuracy: 0.5811\n",
            "Epoch 426/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1621 - accuracy: 0.9490 - val_loss: 2.2978 - val_accuracy: 0.5676\n",
            "Epoch 427/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1781 - accuracy: 0.9388 - val_loss: 2.3887 - val_accuracy: 0.6081\n",
            "Epoch 428/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2318 - accuracy: 0.9252 - val_loss: 2.0967 - val_accuracy: 0.5541\n",
            "Epoch 429/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1674 - accuracy: 0.9490 - val_loss: 1.9296 - val_accuracy: 0.5541\n",
            "Epoch 430/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2106 - accuracy: 0.9218 - val_loss: 1.9525 - val_accuracy: 0.5811\n",
            "Epoch 431/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1724 - accuracy: 0.9286 - val_loss: 1.8903 - val_accuracy: 0.5946\n",
            "Epoch 432/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2053 - accuracy: 0.9150 - val_loss: 1.8730 - val_accuracy: 0.6081\n",
            "Epoch 433/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1535 - accuracy: 0.9490 - val_loss: 2.0291 - val_accuracy: 0.6081\n",
            "Epoch 434/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1405 - accuracy: 0.9524 - val_loss: 2.0718 - val_accuracy: 0.5946\n",
            "Epoch 435/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2078 - accuracy: 0.9286 - val_loss: 2.0905 - val_accuracy: 0.6351\n",
            "Epoch 436/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2153 - accuracy: 0.9116 - val_loss: 1.9915 - val_accuracy: 0.5676\n",
            "Epoch 437/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1783 - accuracy: 0.9456 - val_loss: 1.9104 - val_accuracy: 0.5811\n",
            "Epoch 438/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1495 - accuracy: 0.9660 - val_loss: 2.0432 - val_accuracy: 0.5946\n",
            "Epoch 439/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1332 - accuracy: 0.9626 - val_loss: 2.2563 - val_accuracy: 0.6216\n",
            "Epoch 440/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1281 - accuracy: 0.9490 - val_loss: 2.4348 - val_accuracy: 0.6216\n",
            "Epoch 441/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1252 - accuracy: 0.9660 - val_loss: 2.2324 - val_accuracy: 0.6351\n",
            "Epoch 442/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1141 - accuracy: 0.9728 - val_loss: 2.1425 - val_accuracy: 0.6216\n",
            "Epoch 443/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1021 - accuracy: 0.9728 - val_loss: 2.1813 - val_accuracy: 0.6216\n",
            "Epoch 444/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1042 - accuracy: 0.9660 - val_loss: 2.2831 - val_accuracy: 0.6081\n",
            "Epoch 445/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1111 - accuracy: 0.9626 - val_loss: 2.2554 - val_accuracy: 0.6351\n",
            "Epoch 446/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0989 - accuracy: 0.9728 - val_loss: 2.2704 - val_accuracy: 0.6081\n",
            "Epoch 447/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0869 - accuracy: 0.9830 - val_loss: 2.2711 - val_accuracy: 0.6351\n",
            "Epoch 448/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0851 - accuracy: 0.9898 - val_loss: 2.3711 - val_accuracy: 0.5946\n",
            "Epoch 449/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0967 - accuracy: 0.9762 - val_loss: 2.3665 - val_accuracy: 0.6216\n",
            "Epoch 450/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0816 - accuracy: 0.9796 - val_loss: 2.2745 - val_accuracy: 0.6216\n",
            "Epoch 451/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0724 - accuracy: 0.9864 - val_loss: 2.3351 - val_accuracy: 0.6216\n",
            "Epoch 452/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0705 - accuracy: 0.9864 - val_loss: 2.4482 - val_accuracy: 0.6216\n",
            "Epoch 453/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0748 - accuracy: 0.9796 - val_loss: 2.5156 - val_accuracy: 0.6081\n",
            "Epoch 454/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0914 - accuracy: 0.9762 - val_loss: 2.4960 - val_accuracy: 0.5946\n",
            "Epoch 455/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0778 - accuracy: 0.9830 - val_loss: 2.5487 - val_accuracy: 0.6081\n",
            "Epoch 456/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0937 - accuracy: 0.9728 - val_loss: 2.6153 - val_accuracy: 0.6081\n",
            "Epoch 457/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2444 - accuracy: 0.9082 - val_loss: 2.3773 - val_accuracy: 0.6351\n",
            "Epoch 458/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2520 - accuracy: 0.9116 - val_loss: 2.4173 - val_accuracy: 0.5541\n",
            "Epoch 459/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1763 - accuracy: 0.9388 - val_loss: 2.5567 - val_accuracy: 0.6081\n",
            "Epoch 460/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2481 - accuracy: 0.8980 - val_loss: 2.0636 - val_accuracy: 0.6216\n",
            "Epoch 461/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1929 - accuracy: 0.9252 - val_loss: 2.0527 - val_accuracy: 0.6216\n",
            "Epoch 462/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2253 - accuracy: 0.9150 - val_loss: 2.0505 - val_accuracy: 0.6486\n",
            "Epoch 463/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1709 - accuracy: 0.9388 - val_loss: 2.1552 - val_accuracy: 0.6622\n",
            "Epoch 464/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1596 - accuracy: 0.9558 - val_loss: 2.2139 - val_accuracy: 0.5405\n",
            "Epoch 465/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1616 - accuracy: 0.9422 - val_loss: 2.4194 - val_accuracy: 0.5811\n",
            "Epoch 466/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1372 - accuracy: 0.9626 - val_loss: 2.5529 - val_accuracy: 0.5811\n",
            "Epoch 467/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1693 - accuracy: 0.9286 - val_loss: 2.4250 - val_accuracy: 0.6216\n",
            "Epoch 468/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1150 - accuracy: 0.9626 - val_loss: 2.1435 - val_accuracy: 0.5676\n",
            "Epoch 469/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1135 - accuracy: 0.9660 - val_loss: 2.1965 - val_accuracy: 0.5541\n",
            "Epoch 470/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0984 - accuracy: 0.9796 - val_loss: 2.3839 - val_accuracy: 0.6081\n",
            "Epoch 471/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0883 - accuracy: 0.9830 - val_loss: 2.4518 - val_accuracy: 0.6351\n",
            "Epoch 472/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0832 - accuracy: 0.9898 - val_loss: 2.5377 - val_accuracy: 0.6081\n",
            "Epoch 473/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0870 - accuracy: 0.9762 - val_loss: 2.4579 - val_accuracy: 0.5946\n",
            "Epoch 474/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0897 - accuracy: 0.9728 - val_loss: 2.3939 - val_accuracy: 0.5676\n",
            "Epoch 475/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0852 - accuracy: 0.9762 - val_loss: 2.3779 - val_accuracy: 0.6216\n",
            "Epoch 476/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1263 - accuracy: 0.9524 - val_loss: 2.6739 - val_accuracy: 0.5541\n",
            "Epoch 477/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1091 - accuracy: 0.9694 - val_loss: 2.6596 - val_accuracy: 0.5811\n",
            "Epoch 478/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1077 - accuracy: 0.9694 - val_loss: 2.4131 - val_accuracy: 0.5676\n",
            "Epoch 479/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0986 - accuracy: 0.9728 - val_loss: 2.4247 - val_accuracy: 0.6081\n",
            "Epoch 480/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1236 - accuracy: 0.9490 - val_loss: 2.6112 - val_accuracy: 0.6486\n",
            "Epoch 481/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0864 - accuracy: 0.9762 - val_loss: 2.6408 - val_accuracy: 0.5676\n",
            "Epoch 482/800\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0970 - accuracy: 0.9728 - val_loss: 2.5701 - val_accuracy: 0.6216\n",
            "Epoch 483/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1261 - accuracy: 0.9558 - val_loss: 2.9307 - val_accuracy: 0.6081\n",
            "Epoch 484/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2686 - accuracy: 0.8946 - val_loss: 2.9354 - val_accuracy: 0.5270\n",
            "Epoch 485/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2518 - accuracy: 0.9082 - val_loss: 2.5120 - val_accuracy: 0.6216\n",
            "Epoch 486/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3062 - accuracy: 0.8980 - val_loss: 2.7503 - val_accuracy: 0.5270\n",
            "Epoch 487/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2699 - accuracy: 0.8912 - val_loss: 2.3950 - val_accuracy: 0.5946\n",
            "Epoch 488/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3774 - accuracy: 0.8537 - val_loss: 2.2917 - val_accuracy: 0.6216\n",
            "Epoch 489/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3459 - accuracy: 0.8571 - val_loss: 2.0214 - val_accuracy: 0.5676\n",
            "Epoch 490/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3258 - accuracy: 0.8537 - val_loss: 1.9641 - val_accuracy: 0.5405\n",
            "Epoch 491/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2678 - accuracy: 0.8878 - val_loss: 2.1216 - val_accuracy: 0.5135\n",
            "Epoch 492/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2615 - accuracy: 0.8980 - val_loss: 2.1721 - val_accuracy: 0.5676\n",
            "Epoch 493/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2393 - accuracy: 0.8980 - val_loss: 2.1833 - val_accuracy: 0.5811\n",
            "Epoch 494/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2156 - accuracy: 0.9184 - val_loss: 2.1517 - val_accuracy: 0.6216\n",
            "Epoch 495/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2397 - accuracy: 0.9354 - val_loss: 2.2704 - val_accuracy: 0.5811\n",
            "Epoch 496/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1772 - accuracy: 0.9184 - val_loss: 2.3336 - val_accuracy: 0.5541\n",
            "Epoch 497/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1741 - accuracy: 0.9252 - val_loss: 2.3710 - val_accuracy: 0.5946\n",
            "Epoch 498/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1563 - accuracy: 0.9524 - val_loss: 2.2339 - val_accuracy: 0.6351\n",
            "Epoch 499/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1241 - accuracy: 0.9524 - val_loss: 2.2129 - val_accuracy: 0.5676\n",
            "Epoch 500/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1237 - accuracy: 0.9592 - val_loss: 2.3911 - val_accuracy: 0.6486\n",
            "Epoch 501/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0978 - accuracy: 0.9830 - val_loss: 2.5028 - val_accuracy: 0.5946\n",
            "Epoch 502/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0962 - accuracy: 0.9796 - val_loss: 2.4974 - val_accuracy: 0.6486\n",
            "Epoch 503/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0857 - accuracy: 0.9796 - val_loss: 2.4907 - val_accuracy: 0.6216\n",
            "Epoch 504/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0797 - accuracy: 0.9864 - val_loss: 2.3878 - val_accuracy: 0.6216\n",
            "Epoch 505/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0729 - accuracy: 0.9830 - val_loss: 2.4722 - val_accuracy: 0.6351\n",
            "Epoch 506/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0750 - accuracy: 0.9796 - val_loss: 2.4309 - val_accuracy: 0.5946\n",
            "Epoch 507/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0590 - accuracy: 0.9898 - val_loss: 2.4652 - val_accuracy: 0.6216\n",
            "Epoch 508/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0639 - accuracy: 0.9830 - val_loss: 2.5581 - val_accuracy: 0.5946\n",
            "Epoch 509/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0762 - accuracy: 0.9728 - val_loss: 2.6386 - val_accuracy: 0.6081\n",
            "Epoch 510/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0601 - accuracy: 0.9898 - val_loss: 2.6322 - val_accuracy: 0.6216\n",
            "Epoch 511/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0586 - accuracy: 0.9898 - val_loss: 2.6157 - val_accuracy: 0.6081\n",
            "Epoch 512/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0547 - accuracy: 0.9898 - val_loss: 2.7503 - val_accuracy: 0.6216\n",
            "Epoch 513/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0572 - accuracy: 0.9864 - val_loss: 2.8463 - val_accuracy: 0.6081\n",
            "Epoch 514/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0559 - accuracy: 0.9898 - val_loss: 2.7883 - val_accuracy: 0.6216\n",
            "Epoch 515/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0513 - accuracy: 0.9898 - val_loss: 2.7315 - val_accuracy: 0.6081\n",
            "Epoch 516/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0520 - accuracy: 0.9932 - val_loss: 2.7136 - val_accuracy: 0.6216\n",
            "Epoch 517/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0445 - accuracy: 0.9898 - val_loss: 2.7588 - val_accuracy: 0.6351\n",
            "Epoch 518/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0512 - accuracy: 0.9898 - val_loss: 2.7825 - val_accuracy: 0.6351\n",
            "Epoch 519/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0516 - accuracy: 0.9898 - val_loss: 2.7036 - val_accuracy: 0.6216\n",
            "Epoch 520/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0532 - accuracy: 0.9898 - val_loss: 2.7108 - val_accuracy: 0.6081\n",
            "Epoch 521/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0429 - accuracy: 0.9898 - val_loss: 2.7920 - val_accuracy: 0.6081\n",
            "Epoch 522/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0446 - accuracy: 0.9898 - val_loss: 2.8726 - val_accuracy: 0.5946\n",
            "Epoch 523/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0379 - accuracy: 0.9932 - val_loss: 2.9082 - val_accuracy: 0.5946\n",
            "Epoch 524/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0391 - accuracy: 0.9932 - val_loss: 2.9603 - val_accuracy: 0.5811\n",
            "Epoch 525/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0367 - accuracy: 0.9932 - val_loss: 2.9866 - val_accuracy: 0.6081\n",
            "Epoch 526/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0455 - accuracy: 0.9830 - val_loss: 2.9015 - val_accuracy: 0.6081\n",
            "Epoch 527/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0486 - accuracy: 0.9898 - val_loss: 2.6467 - val_accuracy: 0.6216\n",
            "Epoch 528/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0538 - accuracy: 0.9898 - val_loss: 2.5019 - val_accuracy: 0.6486\n",
            "Epoch 529/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0619 - accuracy: 0.9762 - val_loss: 2.6115 - val_accuracy: 0.6081\n",
            "Epoch 530/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0416 - accuracy: 0.9898 - val_loss: 2.9910 - val_accuracy: 0.6081\n",
            "Epoch 531/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0554 - accuracy: 0.9898 - val_loss: 3.1585 - val_accuracy: 0.5946\n",
            "Epoch 532/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0602 - accuracy: 0.9830 - val_loss: 2.9856 - val_accuracy: 0.6216\n",
            "Epoch 533/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0710 - accuracy: 0.9728 - val_loss: 2.8824 - val_accuracy: 0.5946\n",
            "Epoch 534/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1025 - accuracy: 0.9660 - val_loss: 2.7053 - val_accuracy: 0.5946\n",
            "Epoch 535/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0680 - accuracy: 0.9762 - val_loss: 2.6986 - val_accuracy: 0.6351\n",
            "Epoch 536/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0809 - accuracy: 0.9762 - val_loss: 2.8642 - val_accuracy: 0.6216\n",
            "Epoch 537/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1227 - accuracy: 0.9626 - val_loss: 3.1492 - val_accuracy: 0.6081\n",
            "Epoch 538/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1177 - accuracy: 0.9592 - val_loss: 2.9420 - val_accuracy: 0.5946\n",
            "Epoch 539/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1056 - accuracy: 0.9592 - val_loss: 2.7368 - val_accuracy: 0.5946\n",
            "Epoch 540/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0851 - accuracy: 0.9626 - val_loss: 3.1340 - val_accuracy: 0.5946\n",
            "Epoch 541/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0906 - accuracy: 0.9592 - val_loss: 3.1992 - val_accuracy: 0.6081\n",
            "Epoch 542/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1194 - accuracy: 0.9694 - val_loss: 3.0357 - val_accuracy: 0.5946\n",
            "Epoch 543/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1127 - accuracy: 0.9728 - val_loss: 2.8338 - val_accuracy: 0.5946\n",
            "Epoch 544/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1046 - accuracy: 0.9694 - val_loss: 2.7039 - val_accuracy: 0.6216\n",
            "Epoch 545/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1123 - accuracy: 0.9592 - val_loss: 3.0069 - val_accuracy: 0.6351\n",
            "Epoch 546/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0733 - accuracy: 0.9830 - val_loss: 3.1204 - val_accuracy: 0.6351\n",
            "Epoch 547/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0654 - accuracy: 0.9864 - val_loss: 3.0708 - val_accuracy: 0.6216\n",
            "Epoch 548/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0582 - accuracy: 0.9932 - val_loss: 2.9848 - val_accuracy: 0.6486\n",
            "Epoch 549/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0642 - accuracy: 0.9864 - val_loss: 2.9157 - val_accuracy: 0.6351\n",
            "Epoch 550/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0452 - accuracy: 0.9898 - val_loss: 2.8323 - val_accuracy: 0.6216\n",
            "Epoch 551/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0574 - accuracy: 0.9830 - val_loss: 2.9166 - val_accuracy: 0.6351\n",
            "Epoch 552/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0605 - accuracy: 0.9864 - val_loss: 2.8621 - val_accuracy: 0.6486\n",
            "Epoch 553/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0413 - accuracy: 0.9898 - val_loss: 2.9020 - val_accuracy: 0.6081\n",
            "Epoch 554/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0529 - accuracy: 0.9864 - val_loss: 2.9858 - val_accuracy: 0.6216\n",
            "Epoch 555/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0579 - accuracy: 0.9830 - val_loss: 2.9743 - val_accuracy: 0.6081\n",
            "Epoch 556/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0520 - accuracy: 0.9830 - val_loss: 2.9717 - val_accuracy: 0.5676\n",
            "Epoch 557/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0489 - accuracy: 0.9932 - val_loss: 2.9613 - val_accuracy: 0.6081\n",
            "Epoch 558/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0471 - accuracy: 0.9898 - val_loss: 3.0602 - val_accuracy: 0.6081\n",
            "Epoch 559/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0487 - accuracy: 0.9864 - val_loss: 3.0161 - val_accuracy: 0.6081\n",
            "Epoch 560/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0456 - accuracy: 0.9932 - val_loss: 2.9473 - val_accuracy: 0.6216\n",
            "Epoch 561/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0463 - accuracy: 0.9898 - val_loss: 3.0290 - val_accuracy: 0.6351\n",
            "Epoch 562/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0559 - accuracy: 0.9864 - val_loss: 3.0049 - val_accuracy: 0.6486\n",
            "Epoch 563/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 3.0848 - val_accuracy: 0.5811\n",
            "Epoch 564/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0545 - accuracy: 0.9864 - val_loss: 3.1163 - val_accuracy: 0.6351\n",
            "Epoch 565/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0582 - accuracy: 0.9864 - val_loss: 3.1245 - val_accuracy: 0.6216\n",
            "Epoch 566/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0448 - accuracy: 0.9864 - val_loss: 3.1716 - val_accuracy: 0.5946\n",
            "Epoch 567/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0615 - accuracy: 0.9830 - val_loss: 2.8058 - val_accuracy: 0.6216\n",
            "Epoch 568/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0669 - accuracy: 0.9796 - val_loss: 2.8669 - val_accuracy: 0.6081\n",
            "Epoch 569/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0861 - accuracy: 0.9762 - val_loss: 2.9102 - val_accuracy: 0.6081\n",
            "Epoch 570/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0657 - accuracy: 0.9728 - val_loss: 3.0421 - val_accuracy: 0.6216\n",
            "Epoch 571/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0715 - accuracy: 0.9864 - val_loss: 3.0099 - val_accuracy: 0.6351\n",
            "Epoch 572/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0750 - accuracy: 0.9626 - val_loss: 2.8858 - val_accuracy: 0.5946\n",
            "Epoch 573/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1339 - accuracy: 0.9592 - val_loss: 2.7133 - val_accuracy: 0.6081\n",
            "Epoch 574/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0936 - accuracy: 0.9762 - val_loss: 3.0206 - val_accuracy: 0.5811\n",
            "Epoch 575/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1100 - accuracy: 0.9660 - val_loss: 3.0299 - val_accuracy: 0.6351\n",
            "Epoch 576/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0834 - accuracy: 0.9694 - val_loss: 3.0980 - val_accuracy: 0.6216\n",
            "Epoch 577/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1138 - accuracy: 0.9524 - val_loss: 2.7263 - val_accuracy: 0.6486\n",
            "Epoch 578/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0777 - accuracy: 0.9796 - val_loss: 2.6280 - val_accuracy: 0.6081\n",
            "Epoch 579/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0733 - accuracy: 0.9796 - val_loss: 2.6729 - val_accuracy: 0.6081\n",
            "Epoch 580/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0743 - accuracy: 0.9762 - val_loss: 2.7728 - val_accuracy: 0.6351\n",
            "Epoch 581/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0543 - accuracy: 0.9898 - val_loss: 2.8968 - val_accuracy: 0.6081\n",
            "Epoch 582/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0748 - accuracy: 0.9762 - val_loss: 2.9854 - val_accuracy: 0.5946\n",
            "Epoch 583/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0560 - accuracy: 0.9830 - val_loss: 3.0541 - val_accuracy: 0.6351\n",
            "Epoch 584/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0469 - accuracy: 0.9898 - val_loss: 3.0098 - val_accuracy: 0.6351\n",
            "Epoch 585/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0546 - accuracy: 0.9864 - val_loss: 2.9345 - val_accuracy: 0.6351\n",
            "Epoch 586/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0546 - accuracy: 0.9796 - val_loss: 2.9624 - val_accuracy: 0.6351\n",
            "Epoch 587/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0393 - accuracy: 0.9932 - val_loss: 2.9124 - val_accuracy: 0.6486\n",
            "Epoch 588/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0483 - accuracy: 0.9864 - val_loss: 2.9313 - val_accuracy: 0.6216\n",
            "Epoch 589/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0409 - accuracy: 0.9932 - val_loss: 2.8549 - val_accuracy: 0.6622\n",
            "Epoch 590/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0408 - accuracy: 0.9932 - val_loss: 2.8321 - val_accuracy: 0.6486\n",
            "Epoch 591/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0322 - accuracy: 0.9966 - val_loss: 2.9317 - val_accuracy: 0.6351\n",
            "Epoch 592/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0341 - accuracy: 0.9932 - val_loss: 3.0705 - val_accuracy: 0.6216\n",
            "Epoch 593/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0330 - accuracy: 0.9932 - val_loss: 3.1289 - val_accuracy: 0.6081\n",
            "Epoch 594/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0332 - accuracy: 0.9932 - val_loss: 3.1585 - val_accuracy: 0.6351\n",
            "Epoch 595/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0279 - accuracy: 0.9932 - val_loss: 3.1548 - val_accuracy: 0.6216\n",
            "Epoch 596/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0283 - accuracy: 0.9898 - val_loss: 3.0607 - val_accuracy: 0.6351\n",
            "Epoch 597/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0335 - accuracy: 0.9932 - val_loss: 3.0817 - val_accuracy: 0.6486\n",
            "Epoch 598/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0254 - accuracy: 0.9932 - val_loss: 3.1415 - val_accuracy: 0.6351\n",
            "Epoch 599/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0311 - accuracy: 0.9932 - val_loss: 3.2607 - val_accuracy: 0.6486\n",
            "Epoch 600/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0256 - accuracy: 0.9932 - val_loss: 3.2429 - val_accuracy: 0.6351\n",
            "Epoch 601/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0309 - accuracy: 0.9932 - val_loss: 3.2182 - val_accuracy: 0.6351\n",
            "Epoch 602/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0308 - accuracy: 0.9932 - val_loss: 3.2161 - val_accuracy: 0.6081\n",
            "Epoch 603/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0702 - accuracy: 0.9694 - val_loss: 3.1438 - val_accuracy: 0.6486\n",
            "Epoch 604/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0387 - accuracy: 0.9864 - val_loss: 3.1742 - val_accuracy: 0.6081\n",
            "Epoch 605/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0383 - accuracy: 0.9932 - val_loss: 3.3371 - val_accuracy: 0.6216\n",
            "Epoch 606/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0338 - accuracy: 0.9966 - val_loss: 3.3706 - val_accuracy: 0.6216\n",
            "Epoch 607/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0330 - accuracy: 0.9898 - val_loss: 3.2863 - val_accuracy: 0.6081\n",
            "Epoch 608/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0324 - accuracy: 0.9898 - val_loss: 3.1788 - val_accuracy: 0.6081\n",
            "Epoch 609/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0256 - accuracy: 0.9932 - val_loss: 3.1509 - val_accuracy: 0.6216\n",
            "Epoch 610/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0282 - accuracy: 0.9932 - val_loss: 3.1637 - val_accuracy: 0.6351\n",
            "Epoch 611/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0254 - accuracy: 0.9966 - val_loss: 3.2305 - val_accuracy: 0.6486\n",
            "Epoch 612/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0311 - accuracy: 0.9864 - val_loss: 3.2253 - val_accuracy: 0.6486\n",
            "Epoch 613/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0225 - accuracy: 0.9932 - val_loss: 3.2381 - val_accuracy: 0.6216\n",
            "Epoch 614/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0252 - accuracy: 0.9966 - val_loss: 3.3084 - val_accuracy: 0.6351\n",
            "Epoch 615/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0203 - accuracy: 0.9932 - val_loss: 3.3656 - val_accuracy: 0.6351\n",
            "Epoch 616/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0223 - accuracy: 0.9932 - val_loss: 3.3489 - val_accuracy: 0.6351\n",
            "Epoch 617/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0223 - accuracy: 0.9966 - val_loss: 3.3504 - val_accuracy: 0.6351\n",
            "Epoch 618/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0179 - accuracy: 0.9932 - val_loss: 3.3575 - val_accuracy: 0.6351\n",
            "Epoch 619/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0312 - accuracy: 0.9898 - val_loss: 3.3696 - val_accuracy: 0.6486\n",
            "Epoch 620/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0217 - accuracy: 0.9898 - val_loss: 3.3727 - val_accuracy: 0.6216\n",
            "Epoch 621/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 3.4225 - val_accuracy: 0.6216\n",
            "Epoch 622/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0267 - accuracy: 0.9898 - val_loss: 3.4916 - val_accuracy: 0.6216\n",
            "Epoch 623/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0230 - accuracy: 0.9966 - val_loss: 3.5092 - val_accuracy: 0.6081\n",
            "Epoch 624/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0440 - accuracy: 0.9796 - val_loss: 3.3439 - val_accuracy: 0.6351\n",
            "Epoch 625/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0313 - accuracy: 0.9932 - val_loss: 3.3770 - val_accuracy: 0.6351\n",
            "Epoch 626/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0296 - accuracy: 0.9898 - val_loss: 3.5647 - val_accuracy: 0.5811\n",
            "Epoch 627/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0530 - accuracy: 0.9796 - val_loss: 3.2668 - val_accuracy: 0.6351\n",
            "Epoch 628/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0702 - accuracy: 0.9762 - val_loss: 3.0833 - val_accuracy: 0.6622\n",
            "Epoch 629/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0438 - accuracy: 0.9898 - val_loss: 3.0664 - val_accuracy: 0.6081\n",
            "Epoch 630/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0479 - accuracy: 0.9796 - val_loss: 3.2147 - val_accuracy: 0.5946\n",
            "Epoch 631/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0425 - accuracy: 0.9898 - val_loss: 3.3000 - val_accuracy: 0.6081\n",
            "Epoch 632/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0438 - accuracy: 0.9898 - val_loss: 3.2126 - val_accuracy: 0.6081\n",
            "Epoch 633/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0347 - accuracy: 0.9966 - val_loss: 3.3334 - val_accuracy: 0.6351\n",
            "Epoch 634/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0540 - accuracy: 0.9762 - val_loss: 3.3199 - val_accuracy: 0.6216\n",
            "Epoch 635/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0482 - accuracy: 0.9864 - val_loss: 3.0620 - val_accuracy: 0.6216\n",
            "Epoch 636/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0399 - accuracy: 0.9932 - val_loss: 2.9765 - val_accuracy: 0.6216\n",
            "Epoch 637/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0450 - accuracy: 0.9898 - val_loss: 3.1028 - val_accuracy: 0.6081\n",
            "Epoch 638/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1911 - accuracy: 0.9456 - val_loss: 3.6089 - val_accuracy: 0.5811\n",
            "Epoch 639/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1977 - accuracy: 0.9286 - val_loss: 3.0346 - val_accuracy: 0.5405\n",
            "Epoch 640/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3124 - accuracy: 0.9014 - val_loss: 3.0202 - val_accuracy: 0.6216\n",
            "Epoch 641/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2649 - accuracy: 0.9218 - val_loss: 3.8130 - val_accuracy: 0.6081\n",
            "Epoch 642/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2180 - accuracy: 0.9218 - val_loss: 3.2854 - val_accuracy: 0.6081\n",
            "Epoch 643/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3023 - accuracy: 0.8810 - val_loss: 2.8616 - val_accuracy: 0.6486\n",
            "Epoch 644/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3051 - accuracy: 0.8810 - val_loss: 3.0800 - val_accuracy: 0.5405\n",
            "Epoch 645/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1599 - accuracy: 0.9218 - val_loss: 3.4552 - val_accuracy: 0.5946\n",
            "Epoch 646/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3207 - accuracy: 0.9014 - val_loss: 2.6532 - val_accuracy: 0.5676\n",
            "Epoch 647/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2316 - accuracy: 0.9116 - val_loss: 2.8900 - val_accuracy: 0.5676\n",
            "Epoch 648/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1805 - accuracy: 0.9490 - val_loss: 2.9453 - val_accuracy: 0.6081\n",
            "Epoch 649/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1239 - accuracy: 0.9626 - val_loss: 3.0892 - val_accuracy: 0.6081\n",
            "Epoch 650/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1222 - accuracy: 0.9694 - val_loss: 3.1077 - val_accuracy: 0.6216\n",
            "Epoch 651/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0945 - accuracy: 0.9728 - val_loss: 3.1344 - val_accuracy: 0.5946\n",
            "Epoch 652/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0956 - accuracy: 0.9660 - val_loss: 3.0061 - val_accuracy: 0.6486\n",
            "Epoch 653/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.0709 - accuracy: 0.9830 - val_loss: 2.7434 - val_accuracy: 0.6216\n",
            "Epoch 654/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0811 - accuracy: 0.9762 - val_loss: 2.7549 - val_accuracy: 0.6351\n",
            "Epoch 655/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0633 - accuracy: 0.9864 - val_loss: 2.9837 - val_accuracy: 0.6351\n",
            "Epoch 656/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0490 - accuracy: 0.9932 - val_loss: 3.1090 - val_accuracy: 0.6351\n",
            "Epoch 657/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0517 - accuracy: 0.9830 - val_loss: 3.1511 - val_accuracy: 0.5811\n",
            "Epoch 658/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0544 - accuracy: 0.9898 - val_loss: 3.2175 - val_accuracy: 0.6351\n",
            "Epoch 659/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0465 - accuracy: 0.9898 - val_loss: 3.2624 - val_accuracy: 0.6351\n",
            "Epoch 660/800\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0527 - accuracy: 0.9830 - val_loss: 3.1080 - val_accuracy: 0.6081\n",
            "Epoch 661/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0469 - accuracy: 0.9898 - val_loss: 3.0104 - val_accuracy: 0.6081\n",
            "Epoch 662/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0401 - accuracy: 0.9932 - val_loss: 2.9972 - val_accuracy: 0.6351\n",
            "Epoch 663/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0364 - accuracy: 0.9966 - val_loss: 2.9704 - val_accuracy: 0.6486\n",
            "Epoch 664/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0310 - accuracy: 0.9966 - val_loss: 2.9940 - val_accuracy: 0.6486\n",
            "Epoch 665/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0347 - accuracy: 0.9898 - val_loss: 3.0066 - val_accuracy: 0.6486\n",
            "Epoch 666/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0319 - accuracy: 0.9932 - val_loss: 3.0530 - val_accuracy: 0.6081\n",
            "Epoch 667/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0339 - accuracy: 0.9966 - val_loss: 3.1230 - val_accuracy: 0.6081\n",
            "Epoch 668/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0318 - accuracy: 0.9966 - val_loss: 3.1924 - val_accuracy: 0.6216\n",
            "Epoch 669/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0273 - accuracy: 0.9966 - val_loss: 3.2140 - val_accuracy: 0.6351\n",
            "Epoch 670/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0247 - accuracy: 0.9966 - val_loss: 3.1897 - val_accuracy: 0.6486\n",
            "Epoch 671/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0250 - accuracy: 0.9966 - val_loss: 3.0583 - val_accuracy: 0.6216\n",
            "Epoch 672/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0496 - accuracy: 0.9830 - val_loss: 3.1974 - val_accuracy: 0.6757\n",
            "Epoch 673/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0561 - accuracy: 0.9830 - val_loss: 3.2010 - val_accuracy: 0.6486\n",
            "Epoch 674/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0390 - accuracy: 0.9898 - val_loss: 3.2485 - val_accuracy: 0.6216\n",
            "Epoch 675/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0636 - accuracy: 0.9796 - val_loss: 3.1574 - val_accuracy: 0.6351\n",
            "Epoch 676/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0472 - accuracy: 0.9898 - val_loss: 3.1182 - val_accuracy: 0.6486\n",
            "Epoch 677/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1032 - accuracy: 0.9694 - val_loss: 3.1363 - val_accuracy: 0.6351\n",
            "Epoch 678/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0439 - accuracy: 0.9864 - val_loss: 3.1325 - val_accuracy: 0.6351\n",
            "Epoch 679/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0424 - accuracy: 0.9864 - val_loss: 3.3620 - val_accuracy: 0.6081\n",
            "Epoch 680/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0609 - accuracy: 0.9864 - val_loss: 3.3531 - val_accuracy: 0.6081\n",
            "Epoch 681/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0394 - accuracy: 0.9932 - val_loss: 3.2923 - val_accuracy: 0.6351\n",
            "Epoch 682/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0343 - accuracy: 0.9864 - val_loss: 3.2114 - val_accuracy: 0.6486\n",
            "Epoch 683/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0269 - accuracy: 0.9932 - val_loss: 3.0777 - val_accuracy: 0.6216\n",
            "Epoch 684/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0295 - accuracy: 0.9966 - val_loss: 3.1054 - val_accuracy: 0.6486\n",
            "Epoch 685/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0339 - accuracy: 0.9932 - val_loss: 3.2142 - val_accuracy: 0.6486\n",
            "Epoch 686/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0298 - accuracy: 0.9966 - val_loss: 3.2871 - val_accuracy: 0.6486\n",
            "Epoch 687/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0265 - accuracy: 0.9966 - val_loss: 3.3899 - val_accuracy: 0.6486\n",
            "Epoch 688/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0246 - accuracy: 0.9932 - val_loss: 3.3404 - val_accuracy: 0.6081\n",
            "Epoch 689/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0332 - accuracy: 0.9932 - val_loss: 3.3121 - val_accuracy: 0.6351\n",
            "Epoch 690/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 3.3350 - val_accuracy: 0.6486\n",
            "Epoch 691/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0240 - accuracy: 0.9966 - val_loss: 3.3909 - val_accuracy: 0.6486\n",
            "Epoch 692/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0251 - accuracy: 0.9898 - val_loss: 3.4390 - val_accuracy: 0.6351\n",
            "Epoch 693/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 3.4427 - val_accuracy: 0.6351\n",
            "Epoch 694/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0202 - accuracy: 0.9966 - val_loss: 3.4454 - val_accuracy: 0.6216\n",
            "Epoch 695/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 3.4888 - val_accuracy: 0.6351\n",
            "Epoch 696/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0203 - accuracy: 0.9932 - val_loss: 3.5255 - val_accuracy: 0.6351\n",
            "Epoch 697/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0189 - accuracy: 0.9966 - val_loss: 3.5152 - val_accuracy: 0.6351\n",
            "Epoch 698/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0180 - accuracy: 0.9966 - val_loss: 3.4547 - val_accuracy: 0.6486\n",
            "Epoch 699/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0191 - accuracy: 0.9966 - val_loss: 3.4481 - val_accuracy: 0.6486\n",
            "Epoch 700/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0166 - accuracy: 0.9966 - val_loss: 3.4604 - val_accuracy: 0.6486\n",
            "Epoch 701/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0197 - accuracy: 0.9932 - val_loss: 3.4669 - val_accuracy: 0.6486\n",
            "Epoch 702/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0176 - accuracy: 0.9932 - val_loss: 3.4669 - val_accuracy: 0.6216\n",
            "Epoch 703/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0174 - accuracy: 0.9966 - val_loss: 3.4914 - val_accuracy: 0.6351\n",
            "Epoch 704/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0163 - accuracy: 0.9966 - val_loss: 3.5254 - val_accuracy: 0.6486\n",
            "Epoch 705/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0165 - accuracy: 0.9966 - val_loss: 3.5717 - val_accuracy: 0.6486\n",
            "Epoch 706/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0162 - accuracy: 0.9932 - val_loss: 3.5823 - val_accuracy: 0.6216\n",
            "Epoch 707/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0155 - accuracy: 0.9966 - val_loss: 3.5735 - val_accuracy: 0.6216\n",
            "Epoch 708/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0246 - accuracy: 0.9898 - val_loss: 3.5055 - val_accuracy: 0.6351\n",
            "Epoch 709/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0142 - accuracy: 0.9966 - val_loss: 3.4409 - val_accuracy: 0.6486\n",
            "Epoch 710/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0234 - accuracy: 0.9932 - val_loss: 3.3418 - val_accuracy: 0.6351\n",
            "Epoch 711/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0470 - accuracy: 0.9830 - val_loss: 3.4453 - val_accuracy: 0.6486\n",
            "Epoch 712/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0498 - accuracy: 0.9694 - val_loss: 3.6380 - val_accuracy: 0.6486\n",
            "Epoch 713/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0426 - accuracy: 0.9898 - val_loss: 3.4737 - val_accuracy: 0.6486\n",
            "Epoch 714/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0445 - accuracy: 0.9830 - val_loss: 3.2821 - val_accuracy: 0.6081\n",
            "Epoch 715/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0360 - accuracy: 0.9932 - val_loss: 3.4022 - val_accuracy: 0.5946\n",
            "Epoch 716/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0435 - accuracy: 0.9864 - val_loss: 3.6724 - val_accuracy: 0.6486\n",
            "Epoch 717/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0329 - accuracy: 0.9932 - val_loss: 3.5427 - val_accuracy: 0.6351\n",
            "Epoch 718/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0286 - accuracy: 0.9898 - val_loss: 3.4259 - val_accuracy: 0.6216\n",
            "Epoch 719/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0478 - accuracy: 0.9898 - val_loss: 3.8669 - val_accuracy: 0.6081\n",
            "Epoch 720/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0759 - accuracy: 0.9626 - val_loss: 3.5349 - val_accuracy: 0.5946\n",
            "Epoch 721/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0672 - accuracy: 0.9762 - val_loss: 3.3425 - val_accuracy: 0.6216\n",
            "Epoch 722/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1011 - accuracy: 0.9592 - val_loss: 3.3746 - val_accuracy: 0.6757\n",
            "Epoch 723/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0690 - accuracy: 0.9728 - val_loss: 3.5600 - val_accuracy: 0.6351\n",
            "Epoch 724/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0671 - accuracy: 0.9728 - val_loss: 3.5032 - val_accuracy: 0.6486\n",
            "Epoch 725/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0516 - accuracy: 0.9898 - val_loss: 3.3672 - val_accuracy: 0.6216\n",
            "Epoch 726/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0836 - accuracy: 0.9626 - val_loss: 3.3328 - val_accuracy: 0.6486\n",
            "Epoch 727/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0509 - accuracy: 0.9830 - val_loss: 3.3657 - val_accuracy: 0.6351\n",
            "Epoch 728/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0386 - accuracy: 0.9864 - val_loss: 3.2561 - val_accuracy: 0.6351\n",
            "Epoch 729/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0502 - accuracy: 0.9864 - val_loss: 3.3894 - val_accuracy: 0.6081\n",
            "Epoch 730/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0318 - accuracy: 0.9966 - val_loss: 3.4341 - val_accuracy: 0.6486\n",
            "Epoch 731/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0346 - accuracy: 0.9864 - val_loss: 3.3934 - val_accuracy: 0.6486\n",
            "Epoch 732/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0344 - accuracy: 0.9898 - val_loss: 3.4288 - val_accuracy: 0.6216\n",
            "Epoch 733/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0247 - accuracy: 0.9966 - val_loss: 3.3805 - val_accuracy: 0.6351\n",
            "Epoch 734/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0267 - accuracy: 0.9898 - val_loss: 3.3530 - val_accuracy: 0.6216\n",
            "Epoch 735/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0212 - accuracy: 0.9966 - val_loss: 3.4113 - val_accuracy: 0.6351\n",
            "Epoch 736/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 3.5086 - val_accuracy: 0.6216\n",
            "Epoch 737/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0182 - accuracy: 0.9932 - val_loss: 3.6136 - val_accuracy: 0.6081\n",
            "Epoch 738/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0254 - accuracy: 0.9932 - val_loss: 3.6992 - val_accuracy: 0.6081\n",
            "Epoch 739/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0208 - accuracy: 0.9966 - val_loss: 3.7620 - val_accuracy: 0.5946\n",
            "Epoch 740/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0392 - accuracy: 0.9864 - val_loss: 3.6380 - val_accuracy: 0.6081\n",
            "Epoch 741/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0307 - accuracy: 0.9898 - val_loss: 3.6133 - val_accuracy: 0.5946\n",
            "Epoch 742/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0270 - accuracy: 0.9898 - val_loss: 3.5660 - val_accuracy: 0.6081\n",
            "Epoch 743/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0215 - accuracy: 0.9898 - val_loss: 3.5732 - val_accuracy: 0.6216\n",
            "Epoch 744/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0241 - accuracy: 0.9932 - val_loss: 3.6411 - val_accuracy: 0.6351\n",
            "Epoch 745/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0297 - accuracy: 0.9932 - val_loss: 3.5524 - val_accuracy: 0.6351\n",
            "Epoch 746/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0225 - accuracy: 0.9932 - val_loss: 3.3883 - val_accuracy: 0.6486\n",
            "Epoch 747/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0272 - accuracy: 0.9898 - val_loss: 3.4104 - val_accuracy: 0.6351\n",
            "Epoch 748/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0166 - accuracy: 0.9966 - val_loss: 3.6595 - val_accuracy: 0.6081\n",
            "Epoch 749/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0246 - accuracy: 0.9966 - val_loss: 3.6193 - val_accuracy: 0.6622\n",
            "Epoch 750/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0315 - accuracy: 0.9932 - val_loss: 3.4827 - val_accuracy: 0.6486\n",
            "Epoch 751/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0517 - accuracy: 0.9694 - val_loss: 3.2426 - val_accuracy: 0.6351\n",
            "Epoch 752/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0267 - accuracy: 0.9932 - val_loss: 3.3311 - val_accuracy: 0.6216\n",
            "Epoch 753/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0291 - accuracy: 0.9932 - val_loss: 3.5023 - val_accuracy: 0.6216\n",
            "Epoch 754/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0550 - accuracy: 0.9864 - val_loss: 3.5740 - val_accuracy: 0.6351\n",
            "Epoch 755/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0310 - accuracy: 0.9932 - val_loss: 3.5359 - val_accuracy: 0.6351\n",
            "Epoch 756/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0423 - accuracy: 0.9864 - val_loss: 3.3915 - val_accuracy: 0.6486\n",
            "Epoch 757/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0271 - accuracy: 0.9966 - val_loss: 3.3866 - val_accuracy: 0.6216\n",
            "Epoch 758/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0309 - accuracy: 0.9898 - val_loss: 3.4432 - val_accuracy: 0.6351\n",
            "Epoch 759/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0227 - accuracy: 0.9966 - val_loss: 3.4497 - val_accuracy: 0.6351\n",
            "Epoch 760/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0252 - accuracy: 0.9932 - val_loss: 3.4910 - val_accuracy: 0.6216\n",
            "Epoch 761/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0220 - accuracy: 0.9966 - val_loss: 3.5457 - val_accuracy: 0.6216\n",
            "Epoch 762/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0225 - accuracy: 0.9932 - val_loss: 3.4920 - val_accuracy: 0.6216\n",
            "Epoch 763/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0223 - accuracy: 0.9966 - val_loss: 3.4382 - val_accuracy: 0.6216\n",
            "Epoch 764/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0256 - accuracy: 0.9898 - val_loss: 3.3418 - val_accuracy: 0.6351\n",
            "Epoch 765/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0193 - accuracy: 0.9932 - val_loss: 3.4239 - val_accuracy: 0.6351\n",
            "Epoch 766/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 3.5429 - val_accuracy: 0.6081\n",
            "Epoch 767/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0254 - accuracy: 0.9932 - val_loss: 3.5603 - val_accuracy: 0.6081\n",
            "Epoch 768/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0207 - accuracy: 0.9932 - val_loss: 3.6321 - val_accuracy: 0.6216\n",
            "Epoch 769/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0176 - accuracy: 0.9966 - val_loss: 3.7010 - val_accuracy: 0.6081\n",
            "Epoch 770/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0196 - accuracy: 0.9966 - val_loss: 3.6918 - val_accuracy: 0.6081\n",
            "Epoch 771/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0173 - accuracy: 0.9966 - val_loss: 3.6342 - val_accuracy: 0.6081\n",
            "Epoch 772/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0159 - accuracy: 0.9966 - val_loss: 3.5287 - val_accuracy: 0.6216\n",
            "Epoch 773/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0255 - accuracy: 0.9898 - val_loss: 3.5619 - val_accuracy: 0.6351\n",
            "Epoch 774/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0169 - accuracy: 0.9898 - val_loss: 3.6222 - val_accuracy: 0.6351\n",
            "Epoch 775/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0142 - accuracy: 0.9966 - val_loss: 3.7268 - val_accuracy: 0.6351\n",
            "Epoch 776/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0175 - accuracy: 0.9966 - val_loss: 3.8156 - val_accuracy: 0.6216\n",
            "Epoch 777/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0181 - accuracy: 0.9966 - val_loss: 3.8577 - val_accuracy: 0.6216\n",
            "Epoch 778/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0176 - accuracy: 0.9932 - val_loss: 3.8360 - val_accuracy: 0.6216\n",
            "Epoch 779/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0160 - accuracy: 0.9932 - val_loss: 3.7572 - val_accuracy: 0.6486\n",
            "Epoch 780/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0144 - accuracy: 0.9932 - val_loss: 3.6827 - val_accuracy: 0.6351\n",
            "Epoch 781/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 3.6700 - val_accuracy: 0.6216\n",
            "Epoch 782/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 3.7188 - val_accuracy: 0.6351\n",
            "Epoch 783/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0148 - accuracy: 0.9932 - val_loss: 3.7122 - val_accuracy: 0.6351\n",
            "Epoch 784/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0216 - accuracy: 0.9898 - val_loss: 3.6668 - val_accuracy: 0.6351\n",
            "Epoch 785/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0125 - accuracy: 0.9966 - val_loss: 3.7729 - val_accuracy: 0.6486\n",
            "Epoch 786/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0348 - accuracy: 0.9898 - val_loss: 3.7119 - val_accuracy: 0.6486\n",
            "Epoch 787/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0234 - accuracy: 0.9932 - val_loss: 3.6400 - val_accuracy: 0.6351\n",
            "Epoch 788/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0226 - accuracy: 0.9966 - val_loss: 3.6135 - val_accuracy: 0.5946\n",
            "Epoch 789/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0178 - accuracy: 0.9932 - val_loss: 3.5338 - val_accuracy: 0.6081\n",
            "Epoch 790/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0213 - accuracy: 0.9932 - val_loss: 3.7175 - val_accuracy: 0.6081\n",
            "Epoch 791/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0249 - accuracy: 0.9932 - val_loss: 3.8375 - val_accuracy: 0.6216\n",
            "Epoch 792/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0140 - accuracy: 0.9966 - val_loss: 3.8616 - val_accuracy: 0.6081\n",
            "Epoch 793/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0333 - accuracy: 0.9830 - val_loss: 3.9114 - val_accuracy: 0.5946\n",
            "Epoch 794/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0208 - accuracy: 0.9966 - val_loss: 3.8599 - val_accuracy: 0.6351\n",
            "Epoch 795/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.0288 - accuracy: 0.9898 - val_loss: 3.7649 - val_accuracy: 0.6622\n",
            "Epoch 796/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0201 - accuracy: 0.9932 - val_loss: 3.7515 - val_accuracy: 0.6351\n",
            "Epoch 797/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0250 - accuracy: 0.9932 - val_loss: 4.1245 - val_accuracy: 0.6351\n",
            "Epoch 798/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1141 - accuracy: 0.9524 - val_loss: 3.5796 - val_accuracy: 0.6216\n",
            "Epoch 799/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1611 - accuracy: 0.9422 - val_loss: 3.3700 - val_accuracy: 0.5676\n",
            "Epoch 800/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1537 - accuracy: 0.9456 - val_loss: 3.5352 - val_accuracy: 0.6081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_RNN_model():\n",
        "    RNN = Sequential()\n",
        "    RNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False))\n",
        "\n",
        "    RNN.add(Bidirectional(LSTM(word_dimension)))\n",
        "    RNN.add(Dense(word_dimension, activation='relu'))\n",
        "    RNN.add(Dense(3, activation='softmax'))\n",
        "    RNN.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "    return RNN\n",
        "\n",
        "RNN_model = create_RNN_model()\n",
        "RNN_history = RNN_model.fit(feature_train, label_train_y, epochs=800, batch_size=128, validation_data=(feature_valid, label_valid_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RONOSfBzoqGI",
        "outputId": "32415d1d-6bab-494f-f5ea-7568de7e5fe2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "3/3 [==============================] - 4s 432ms/step - loss: 1.0432 - accuracy: 0.4388 - val_loss: 1.0469 - val_accuracy: 0.4865\n",
            "Epoch 2/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.9820 - accuracy: 0.5714 - val_loss: 1.0850 - val_accuracy: 0.4865\n",
            "Epoch 3/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9896 - accuracy: 0.5714 - val_loss: 1.0943 - val_accuracy: 0.4865\n",
            "Epoch 4/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9840 - accuracy: 0.5714 - val_loss: 1.0665 - val_accuracy: 0.4865\n",
            "Epoch 5/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9853 - accuracy: 0.5714 - val_loss: 1.0526 - val_accuracy: 0.4865\n",
            "Epoch 6/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9785 - accuracy: 0.5714 - val_loss: 1.0431 - val_accuracy: 0.4865\n",
            "Epoch 7/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9764 - accuracy: 0.5714 - val_loss: 1.0456 - val_accuracy: 0.4865\n",
            "Epoch 8/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9764 - accuracy: 0.5714 - val_loss: 1.0466 - val_accuracy: 0.4865\n",
            "Epoch 9/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9742 - accuracy: 0.5714 - val_loss: 1.0524 - val_accuracy: 0.4865\n",
            "Epoch 10/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9723 - accuracy: 0.5714 - val_loss: 1.0578 - val_accuracy: 0.4865\n",
            "Epoch 11/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.9730 - accuracy: 0.5714 - val_loss: 1.0584 - val_accuracy: 0.4865\n",
            "Epoch 12/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9754 - accuracy: 0.5714 - val_loss: 1.0543 - val_accuracy: 0.4865\n",
            "Epoch 13/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9722 - accuracy: 0.5714 - val_loss: 1.0584 - val_accuracy: 0.4865\n",
            "Epoch 14/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9720 - accuracy: 0.5714 - val_loss: 1.0562 - val_accuracy: 0.4865\n",
            "Epoch 15/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9708 - accuracy: 0.5714 - val_loss: 1.0507 - val_accuracy: 0.4865\n",
            "Epoch 16/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9709 - accuracy: 0.5714 - val_loss: 1.0509 - val_accuracy: 0.4865\n",
            "Epoch 17/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9708 - accuracy: 0.5714 - val_loss: 1.0520 - val_accuracy: 0.4865\n",
            "Epoch 18/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9704 - accuracy: 0.5714 - val_loss: 1.0520 - val_accuracy: 0.4865\n",
            "Epoch 19/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9730 - accuracy: 0.5714 - val_loss: 1.0496 - val_accuracy: 0.4865\n",
            "Epoch 20/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9717 - accuracy: 0.5714 - val_loss: 1.0437 - val_accuracy: 0.4865\n",
            "Epoch 21/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9700 - accuracy: 0.5714 - val_loss: 1.0426 - val_accuracy: 0.4865\n",
            "Epoch 22/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9689 - accuracy: 0.5714 - val_loss: 1.0464 - val_accuracy: 0.4865\n",
            "Epoch 23/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9702 - accuracy: 0.5714 - val_loss: 1.0541 - val_accuracy: 0.4865\n",
            "Epoch 24/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9655 - accuracy: 0.5714 - val_loss: 1.0472 - val_accuracy: 0.4865\n",
            "Epoch 25/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9683 - accuracy: 0.5714 - val_loss: 1.0421 - val_accuracy: 0.4865\n",
            "Epoch 26/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9683 - accuracy: 0.5714 - val_loss: 1.0437 - val_accuracy: 0.4865\n",
            "Epoch 27/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9659 - accuracy: 0.5714 - val_loss: 1.0554 - val_accuracy: 0.4865\n",
            "Epoch 28/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9661 - accuracy: 0.5714 - val_loss: 1.0505 - val_accuracy: 0.4865\n",
            "Epoch 29/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9635 - accuracy: 0.5714 - val_loss: 1.0357 - val_accuracy: 0.4865\n",
            "Epoch 30/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9676 - accuracy: 0.5714 - val_loss: 1.0292 - val_accuracy: 0.4865\n",
            "Epoch 31/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9671 - accuracy: 0.5714 - val_loss: 1.0482 - val_accuracy: 0.4865\n",
            "Epoch 32/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9636 - accuracy: 0.5714 - val_loss: 1.0694 - val_accuracy: 0.4865\n",
            "Epoch 33/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9674 - accuracy: 0.5714 - val_loss: 1.0599 - val_accuracy: 0.4865\n",
            "Epoch 34/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9626 - accuracy: 0.5714 - val_loss: 1.0391 - val_accuracy: 0.4865\n",
            "Epoch 35/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9594 - accuracy: 0.5714 - val_loss: 1.0281 - val_accuracy: 0.4865\n",
            "Epoch 36/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9618 - accuracy: 0.5714 - val_loss: 1.0294 - val_accuracy: 0.4865\n",
            "Epoch 37/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9589 - accuracy: 0.5714 - val_loss: 1.0415 - val_accuracy: 0.4865\n",
            "Epoch 38/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9586 - accuracy: 0.5714 - val_loss: 1.0431 - val_accuracy: 0.4865\n",
            "Epoch 39/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9562 - accuracy: 0.5714 - val_loss: 1.0430 - val_accuracy: 0.4865\n",
            "Epoch 40/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9533 - accuracy: 0.5714 - val_loss: 1.0244 - val_accuracy: 0.4865\n",
            "Epoch 41/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9541 - accuracy: 0.5714 - val_loss: 1.0214 - val_accuracy: 0.4865\n",
            "Epoch 42/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9459 - accuracy: 0.5714 - val_loss: 1.0440 - val_accuracy: 0.4865\n",
            "Epoch 43/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9509 - accuracy: 0.5714 - val_loss: 1.0249 - val_accuracy: 0.4865\n",
            "Epoch 44/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9446 - accuracy: 0.5714 - val_loss: 1.0041 - val_accuracy: 0.4865\n",
            "Epoch 45/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9439 - accuracy: 0.5714 - val_loss: 1.0170 - val_accuracy: 0.4865\n",
            "Epoch 46/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9555 - accuracy: 0.5714 - val_loss: 1.0413 - val_accuracy: 0.4865\n",
            "Epoch 47/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9478 - accuracy: 0.5714 - val_loss: 1.0012 - val_accuracy: 0.4865\n",
            "Epoch 48/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9508 - accuracy: 0.5816 - val_loss: 1.0041 - val_accuracy: 0.5000\n",
            "Epoch 49/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9466 - accuracy: 0.5884 - val_loss: 1.0198 - val_accuracy: 0.4865\n",
            "Epoch 50/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9399 - accuracy: 0.5714 - val_loss: 1.0405 - val_accuracy: 0.4865\n",
            "Epoch 51/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9425 - accuracy: 0.5714 - val_loss: 1.0180 - val_accuracy: 0.4865\n",
            "Epoch 52/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9338 - accuracy: 0.5986 - val_loss: 1.0107 - val_accuracy: 0.5000\n",
            "Epoch 53/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9314 - accuracy: 0.5986 - val_loss: 1.0124 - val_accuracy: 0.5000\n",
            "Epoch 54/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9308 - accuracy: 0.5986 - val_loss: 1.0089 - val_accuracy: 0.5000\n",
            "Epoch 55/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9265 - accuracy: 0.5986 - val_loss: 0.9862 - val_accuracy: 0.5000\n",
            "Epoch 56/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9341 - accuracy: 0.5986 - val_loss: 1.0027 - val_accuracy: 0.5000\n",
            "Epoch 57/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9368 - accuracy: 0.5952 - val_loss: 1.0480 - val_accuracy: 0.4865\n",
            "Epoch 58/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9354 - accuracy: 0.5952 - val_loss: 0.9920 - val_accuracy: 0.5000\n",
            "Epoch 59/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9219 - accuracy: 0.5952 - val_loss: 0.9785 - val_accuracy: 0.5135\n",
            "Epoch 60/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9306 - accuracy: 0.6054 - val_loss: 0.9931 - val_accuracy: 0.5000\n",
            "Epoch 61/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9204 - accuracy: 0.6054 - val_loss: 1.0309 - val_accuracy: 0.5000\n",
            "Epoch 62/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9216 - accuracy: 0.5986 - val_loss: 1.0071 - val_accuracy: 0.5000\n",
            "Epoch 63/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9106 - accuracy: 0.6054 - val_loss: 0.9837 - val_accuracy: 0.5270\n",
            "Epoch 64/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9203 - accuracy: 0.5986 - val_loss: 0.9861 - val_accuracy: 0.5270\n",
            "Epoch 65/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9222 - accuracy: 0.6020 - val_loss: 1.0163 - val_accuracy: 0.5000\n",
            "Epoch 66/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9111 - accuracy: 0.6054 - val_loss: 1.0082 - val_accuracy: 0.5000\n",
            "Epoch 67/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9098 - accuracy: 0.5986 - val_loss: 1.0082 - val_accuracy: 0.5000\n",
            "Epoch 68/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9070 - accuracy: 0.6020 - val_loss: 1.0079 - val_accuracy: 0.5000\n",
            "Epoch 69/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9019 - accuracy: 0.6122 - val_loss: 0.9963 - val_accuracy: 0.5135\n",
            "Epoch 70/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9082 - accuracy: 0.5986 - val_loss: 1.0143 - val_accuracy: 0.5000\n",
            "Epoch 71/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9110 - accuracy: 0.6054 - val_loss: 0.9986 - val_accuracy: 0.5000\n",
            "Epoch 72/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9028 - accuracy: 0.6054 - val_loss: 0.9722 - val_accuracy: 0.5135\n",
            "Epoch 73/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9055 - accuracy: 0.5918 - val_loss: 0.9785 - val_accuracy: 0.5135\n",
            "Epoch 74/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8996 - accuracy: 0.5952 - val_loss: 0.9979 - val_accuracy: 0.5135\n",
            "Epoch 75/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9000 - accuracy: 0.5986 - val_loss: 0.9983 - val_accuracy: 0.5135\n",
            "Epoch 76/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8984 - accuracy: 0.6020 - val_loss: 0.9918 - val_accuracy: 0.5000\n",
            "Epoch 77/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8953 - accuracy: 0.5986 - val_loss: 0.9916 - val_accuracy: 0.5000\n",
            "Epoch 78/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8953 - accuracy: 0.6088 - val_loss: 1.0112 - val_accuracy: 0.5000\n",
            "Epoch 79/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8939 - accuracy: 0.6088 - val_loss: 0.9983 - val_accuracy: 0.5135\n",
            "Epoch 80/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8859 - accuracy: 0.5952 - val_loss: 0.9856 - val_accuracy: 0.5270\n",
            "Epoch 81/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8865 - accuracy: 0.5986 - val_loss: 0.9990 - val_accuracy: 0.5270\n",
            "Epoch 82/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8789 - accuracy: 0.6088 - val_loss: 0.9951 - val_accuracy: 0.5135\n",
            "Epoch 83/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9060 - accuracy: 0.6054 - val_loss: 1.0138 - val_accuracy: 0.4865\n",
            "Epoch 84/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9148 - accuracy: 0.6020 - val_loss: 0.9836 - val_accuracy: 0.4865\n",
            "Epoch 85/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9166 - accuracy: 0.6020 - val_loss: 1.0060 - val_accuracy: 0.5000\n",
            "Epoch 86/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9094 - accuracy: 0.6054 - val_loss: 0.9950 - val_accuracy: 0.5000\n",
            "Epoch 87/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8961 - accuracy: 0.5986 - val_loss: 0.9705 - val_accuracy: 0.5135\n",
            "Epoch 88/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9087 - accuracy: 0.5748 - val_loss: 1.0059 - val_accuracy: 0.5000\n",
            "Epoch 89/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9057 - accuracy: 0.6020 - val_loss: 0.9959 - val_accuracy: 0.5135\n",
            "Epoch 90/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8929 - accuracy: 0.5986 - val_loss: 0.9806 - val_accuracy: 0.5270\n",
            "Epoch 91/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9061 - accuracy: 0.5850 - val_loss: 1.0047 - val_accuracy: 0.5000\n",
            "Epoch 92/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8969 - accuracy: 0.6054 - val_loss: 1.0475 - val_accuracy: 0.5000\n",
            "Epoch 93/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9035 - accuracy: 0.6054 - val_loss: 0.9956 - val_accuracy: 0.5000\n",
            "Epoch 94/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8948 - accuracy: 0.6054 - val_loss: 0.9880 - val_accuracy: 0.5000\n",
            "Epoch 95/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8858 - accuracy: 0.6054 - val_loss: 1.0043 - val_accuracy: 0.5000\n",
            "Epoch 96/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8864 - accuracy: 0.6054 - val_loss: 1.0008 - val_accuracy: 0.5135\n",
            "Epoch 97/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8861 - accuracy: 0.6020 - val_loss: 0.9874 - val_accuracy: 0.5135\n",
            "Epoch 98/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8819 - accuracy: 0.5952 - val_loss: 0.9796 - val_accuracy: 0.5135\n",
            "Epoch 99/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8801 - accuracy: 0.5918 - val_loss: 0.9772 - val_accuracy: 0.5135\n",
            "Epoch 100/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8766 - accuracy: 0.5986 - val_loss: 0.9945 - val_accuracy: 0.5135\n",
            "Epoch 101/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8750 - accuracy: 0.6020 - val_loss: 0.9667 - val_accuracy: 0.5270\n",
            "Epoch 102/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8747 - accuracy: 0.6054 - val_loss: 0.9744 - val_accuracy: 0.5135\n",
            "Epoch 103/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8708 - accuracy: 0.6054 - val_loss: 1.0170 - val_accuracy: 0.5000\n",
            "Epoch 104/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8814 - accuracy: 0.6054 - val_loss: 0.9899 - val_accuracy: 0.5135\n",
            "Epoch 105/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8686 - accuracy: 0.5986 - val_loss: 0.9866 - val_accuracy: 0.5000\n",
            "Epoch 106/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8677 - accuracy: 0.5918 - val_loss: 1.0230 - val_accuracy: 0.5000\n",
            "Epoch 107/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8787 - accuracy: 0.6054 - val_loss: 1.0052 - val_accuracy: 0.5000\n",
            "Epoch 108/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8624 - accuracy: 0.6054 - val_loss: 0.9820 - val_accuracy: 0.5541\n",
            "Epoch 109/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8771 - accuracy: 0.6020 - val_loss: 1.0092 - val_accuracy: 0.5270\n",
            "Epoch 110/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8626 - accuracy: 0.6054 - val_loss: 1.0470 - val_accuracy: 0.5135\n",
            "Epoch 111/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8632 - accuracy: 0.6156 - val_loss: 1.0391 - val_accuracy: 0.5135\n",
            "Epoch 112/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9137 - accuracy: 0.6054 - val_loss: 0.9595 - val_accuracy: 0.5135\n",
            "Epoch 113/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9006 - accuracy: 0.6156 - val_loss: 0.9947 - val_accuracy: 0.4865\n",
            "Epoch 114/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9175 - accuracy: 0.6020 - val_loss: 1.0310 - val_accuracy: 0.4865\n",
            "Epoch 115/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8932 - accuracy: 0.5918 - val_loss: 0.9701 - val_accuracy: 0.5000\n",
            "Epoch 116/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.9111 - accuracy: 0.6088 - val_loss: 0.9791 - val_accuracy: 0.5135\n",
            "Epoch 117/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8976 - accuracy: 0.6054 - val_loss: 1.0072 - val_accuracy: 0.5000\n",
            "Epoch 118/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8885 - accuracy: 0.6054 - val_loss: 1.0435 - val_accuracy: 0.5135\n",
            "Epoch 119/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8856 - accuracy: 0.6054 - val_loss: 0.9788 - val_accuracy: 0.5000\n",
            "Epoch 120/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8757 - accuracy: 0.5918 - val_loss: 0.9679 - val_accuracy: 0.5270\n",
            "Epoch 121/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8818 - accuracy: 0.5918 - val_loss: 0.9777 - val_accuracy: 0.5000\n",
            "Epoch 122/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8633 - accuracy: 0.5952 - val_loss: 0.9969 - val_accuracy: 0.5135\n",
            "Epoch 123/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8685 - accuracy: 0.6054 - val_loss: 0.9853 - val_accuracy: 0.5270\n",
            "Epoch 124/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8615 - accuracy: 0.5986 - val_loss: 0.9702 - val_accuracy: 0.5541\n",
            "Epoch 125/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8575 - accuracy: 0.6054 - val_loss: 0.9830 - val_accuracy: 0.5270\n",
            "Epoch 126/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8551 - accuracy: 0.6020 - val_loss: 1.0032 - val_accuracy: 0.5270\n",
            "Epoch 127/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8579 - accuracy: 0.6054 - val_loss: 1.0007 - val_accuracy: 0.5135\n",
            "Epoch 128/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8447 - accuracy: 0.6020 - val_loss: 1.0114 - val_accuracy: 0.5135\n",
            "Epoch 129/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8529 - accuracy: 0.6088 - val_loss: 0.9850 - val_accuracy: 0.5405\n",
            "Epoch 130/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8618 - accuracy: 0.6054 - val_loss: 0.9924 - val_accuracy: 0.5541\n",
            "Epoch 131/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8571 - accuracy: 0.6088 - val_loss: 1.0396 - val_accuracy: 0.5270\n",
            "Epoch 132/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8461 - accuracy: 0.6020 - val_loss: 1.0074 - val_accuracy: 0.5135\n",
            "Epoch 133/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8391 - accuracy: 0.6088 - val_loss: 0.9944 - val_accuracy: 0.5135\n",
            "Epoch 134/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8332 - accuracy: 0.6190 - val_loss: 1.0064 - val_accuracy: 0.5405\n",
            "Epoch 135/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8411 - accuracy: 0.6088 - val_loss: 1.0160 - val_accuracy: 0.5270\n",
            "Epoch 136/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8580 - accuracy: 0.5952 - val_loss: 0.9731 - val_accuracy: 0.5135\n",
            "Epoch 137/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8316 - accuracy: 0.6122 - val_loss: 0.9921 - val_accuracy: 0.5000\n",
            "Epoch 138/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8366 - accuracy: 0.5952 - val_loss: 0.9833 - val_accuracy: 0.5000\n",
            "Epoch 139/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8516 - accuracy: 0.5986 - val_loss: 0.9893 - val_accuracy: 0.5135\n",
            "Epoch 140/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8361 - accuracy: 0.5952 - val_loss: 1.0044 - val_accuracy: 0.5405\n",
            "Epoch 141/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8311 - accuracy: 0.6054 - val_loss: 0.9702 - val_accuracy: 0.5135\n",
            "Epoch 142/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8433 - accuracy: 0.5952 - val_loss: 0.9632 - val_accuracy: 0.5270\n",
            "Epoch 143/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8755 - accuracy: 0.6361 - val_loss: 0.9797 - val_accuracy: 0.5270\n",
            "Epoch 144/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8242 - accuracy: 0.6224 - val_loss: 1.0895 - val_accuracy: 0.5000\n",
            "Epoch 145/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8586 - accuracy: 0.6156 - val_loss: 1.0870 - val_accuracy: 0.5000\n",
            "Epoch 146/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8801 - accuracy: 0.6327 - val_loss: 1.0038 - val_accuracy: 0.5270\n",
            "Epoch 147/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9297 - accuracy: 0.6224 - val_loss: 0.9879 - val_accuracy: 0.4865\n",
            "Epoch 148/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8489 - accuracy: 0.6122 - val_loss: 1.0618 - val_accuracy: 0.5135\n",
            "Epoch 149/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8743 - accuracy: 0.5952 - val_loss: 1.0733 - val_accuracy: 0.5135\n",
            "Epoch 150/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8682 - accuracy: 0.5952 - val_loss: 1.0110 - val_accuracy: 0.5135\n",
            "Epoch 151/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8403 - accuracy: 0.6054 - val_loss: 0.9652 - val_accuracy: 0.5000\n",
            "Epoch 152/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8369 - accuracy: 0.6122 - val_loss: 0.9593 - val_accuracy: 0.5135\n",
            "Epoch 153/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8275 - accuracy: 0.6293 - val_loss: 0.9829 - val_accuracy: 0.5135\n",
            "Epoch 154/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8196 - accuracy: 0.6293 - val_loss: 0.9798 - val_accuracy: 0.5270\n",
            "Epoch 155/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8221 - accuracy: 0.6361 - val_loss: 0.9768 - val_accuracy: 0.5405\n",
            "Epoch 156/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8156 - accuracy: 0.6327 - val_loss: 0.9868 - val_accuracy: 0.5135\n",
            "Epoch 157/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8083 - accuracy: 0.6429 - val_loss: 0.9702 - val_accuracy: 0.5135\n",
            "Epoch 158/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8168 - accuracy: 0.6497 - val_loss: 0.9526 - val_accuracy: 0.5135\n",
            "Epoch 159/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8165 - accuracy: 0.6361 - val_loss: 0.9611 - val_accuracy: 0.5270\n",
            "Epoch 160/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7994 - accuracy: 0.6395 - val_loss: 0.9864 - val_accuracy: 0.5270\n",
            "Epoch 161/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8143 - accuracy: 0.6224 - val_loss: 1.1493 - val_accuracy: 0.4324\n",
            "Epoch 162/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8758 - accuracy: 0.5952 - val_loss: 0.9862 - val_accuracy: 0.5135\n",
            "Epoch 163/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8143 - accuracy: 0.6361 - val_loss: 1.0601 - val_accuracy: 0.5000\n",
            "Epoch 164/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8559 - accuracy: 0.5918 - val_loss: 1.0172 - val_accuracy: 0.5135\n",
            "Epoch 165/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8261 - accuracy: 0.6054 - val_loss: 0.9462 - val_accuracy: 0.5270\n",
            "Epoch 166/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8292 - accuracy: 0.6293 - val_loss: 0.9579 - val_accuracy: 0.5405\n",
            "Epoch 167/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8042 - accuracy: 0.6054 - val_loss: 1.0283 - val_accuracy: 0.5135\n",
            "Epoch 168/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8278 - accuracy: 0.5952 - val_loss: 0.9730 - val_accuracy: 0.5135\n",
            "Epoch 169/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8073 - accuracy: 0.6088 - val_loss: 0.9566 - val_accuracy: 0.5270\n",
            "Epoch 170/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8230 - accuracy: 0.6497 - val_loss: 0.9716 - val_accuracy: 0.5405\n",
            "Epoch 171/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.7892 - accuracy: 0.6361 - val_loss: 1.0430 - val_accuracy: 0.5405\n",
            "Epoch 172/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8067 - accuracy: 0.6327 - val_loss: 1.0057 - val_accuracy: 0.5405\n",
            "Epoch 173/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7732 - accuracy: 0.6667 - val_loss: 0.9771 - val_accuracy: 0.5135\n",
            "Epoch 174/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7911 - accuracy: 0.6224 - val_loss: 1.0466 - val_accuracy: 0.4865\n",
            "Epoch 175/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8011 - accuracy: 0.6054 - val_loss: 1.1392 - val_accuracy: 0.5000\n",
            "Epoch 176/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8220 - accuracy: 0.6190 - val_loss: 1.0327 - val_accuracy: 0.4865\n",
            "Epoch 177/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7910 - accuracy: 0.6497 - val_loss: 0.9773 - val_accuracy: 0.5405\n",
            "Epoch 178/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7785 - accuracy: 0.6701 - val_loss: 1.0254 - val_accuracy: 0.5676\n",
            "Epoch 179/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7727 - accuracy: 0.6565 - val_loss: 1.0507 - val_accuracy: 0.5405\n",
            "Epoch 180/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7739 - accuracy: 0.6531 - val_loss: 0.9970 - val_accuracy: 0.5270\n",
            "Epoch 181/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7674 - accuracy: 0.6497 - val_loss: 0.9785 - val_accuracy: 0.5270\n",
            "Epoch 182/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7658 - accuracy: 0.6701 - val_loss: 1.0380 - val_accuracy: 0.5135\n",
            "Epoch 183/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7538 - accuracy: 0.6531 - val_loss: 1.0196 - val_accuracy: 0.5135\n",
            "Epoch 184/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7569 - accuracy: 0.6905 - val_loss: 1.0354 - val_accuracy: 0.5135\n",
            "Epoch 185/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7305 - accuracy: 0.6837 - val_loss: 0.9813 - val_accuracy: 0.5135\n",
            "Epoch 186/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7372 - accuracy: 0.6803 - val_loss: 1.0212 - val_accuracy: 0.5000\n",
            "Epoch 187/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7078 - accuracy: 0.6667 - val_loss: 1.1374 - val_accuracy: 0.4865\n",
            "Epoch 188/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7770 - accuracy: 0.6531 - val_loss: 1.0199 - val_accuracy: 0.5135\n",
            "Epoch 189/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7748 - accuracy: 0.6599 - val_loss: 0.9285 - val_accuracy: 0.5270\n",
            "Epoch 190/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8200 - accuracy: 0.5952 - val_loss: 0.9826 - val_accuracy: 0.6081\n",
            "Epoch 191/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8185 - accuracy: 0.6361 - val_loss: 1.0290 - val_accuracy: 0.5541\n",
            "Epoch 192/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8264 - accuracy: 0.6361 - val_loss: 1.0612 - val_accuracy: 0.5541\n",
            "Epoch 193/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8423 - accuracy: 0.6395 - val_loss: 1.0558 - val_accuracy: 0.5676\n",
            "Epoch 194/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.8249 - accuracy: 0.6395 - val_loss: 0.9672 - val_accuracy: 0.5541\n",
            "Epoch 195/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7828 - accuracy: 0.6735 - val_loss: 1.0096 - val_accuracy: 0.5000\n",
            "Epoch 196/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7843 - accuracy: 0.6327 - val_loss: 1.0220 - val_accuracy: 0.5135\n",
            "Epoch 197/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7618 - accuracy: 0.6633 - val_loss: 1.0254 - val_accuracy: 0.4865\n",
            "Epoch 198/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7718 - accuracy: 0.6667 - val_loss: 1.0378 - val_accuracy: 0.5000\n",
            "Epoch 199/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7511 - accuracy: 0.6735 - val_loss: 1.0559 - val_accuracy: 0.5135\n",
            "Epoch 200/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7568 - accuracy: 0.6769 - val_loss: 1.0180 - val_accuracy: 0.4865\n",
            "Epoch 201/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7479 - accuracy: 0.6667 - val_loss: 1.0302 - val_accuracy: 0.5000\n",
            "Epoch 202/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7423 - accuracy: 0.6633 - val_loss: 1.0121 - val_accuracy: 0.5135\n",
            "Epoch 203/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7240 - accuracy: 0.6973 - val_loss: 1.0531 - val_accuracy: 0.4595\n",
            "Epoch 204/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7380 - accuracy: 0.6633 - val_loss: 1.0432 - val_accuracy: 0.4865\n",
            "Epoch 205/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7067 - accuracy: 0.6667 - val_loss: 1.0293 - val_accuracy: 0.5135\n",
            "Epoch 206/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7380 - accuracy: 0.6599 - val_loss: 1.0160 - val_accuracy: 0.5000\n",
            "Epoch 207/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7059 - accuracy: 0.6667 - val_loss: 1.0461 - val_accuracy: 0.5000\n",
            "Epoch 208/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6904 - accuracy: 0.6769 - val_loss: 1.0633 - val_accuracy: 0.4865\n",
            "Epoch 209/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6792 - accuracy: 0.7279 - val_loss: 1.0052 - val_accuracy: 0.5135\n",
            "Epoch 210/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7034 - accuracy: 0.7075 - val_loss: 1.0791 - val_accuracy: 0.4865\n",
            "Epoch 211/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6646 - accuracy: 0.7313 - val_loss: 1.0383 - val_accuracy: 0.5270\n",
            "Epoch 212/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6812 - accuracy: 0.6905 - val_loss: 1.0006 - val_accuracy: 0.5135\n",
            "Epoch 213/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6727 - accuracy: 0.7177 - val_loss: 1.0745 - val_accuracy: 0.5135\n",
            "Epoch 214/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7051 - accuracy: 0.6769 - val_loss: 1.3419 - val_accuracy: 0.5000\n",
            "Epoch 215/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7211 - accuracy: 0.7041 - val_loss: 0.9885 - val_accuracy: 0.5270\n",
            "Epoch 216/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7094 - accuracy: 0.6939 - val_loss: 1.0043 - val_accuracy: 0.5676\n",
            "Epoch 217/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7365 - accuracy: 0.6803 - val_loss: 1.1424 - val_accuracy: 0.5135\n",
            "Epoch 218/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7388 - accuracy: 0.6735 - val_loss: 1.1156 - val_accuracy: 0.5000\n",
            "Epoch 219/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7379 - accuracy: 0.6565 - val_loss: 1.0162 - val_accuracy: 0.4730\n",
            "Epoch 220/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7074 - accuracy: 0.7075 - val_loss: 0.9930 - val_accuracy: 0.5135\n",
            "Epoch 221/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6934 - accuracy: 0.7041 - val_loss: 1.1220 - val_accuracy: 0.4865\n",
            "Epoch 222/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6635 - accuracy: 0.7313 - val_loss: 1.0994 - val_accuracy: 0.5000\n",
            "Epoch 223/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6578 - accuracy: 0.7279 - val_loss: 1.0309 - val_accuracy: 0.5000\n",
            "Epoch 224/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6608 - accuracy: 0.7245 - val_loss: 1.0399 - val_accuracy: 0.5000\n",
            "Epoch 225/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6585 - accuracy: 0.7279 - val_loss: 1.0048 - val_accuracy: 0.5135\n",
            "Epoch 226/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6500 - accuracy: 0.7415 - val_loss: 1.1793 - val_accuracy: 0.4865\n",
            "Epoch 227/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6947 - accuracy: 0.7109 - val_loss: 1.1383 - val_accuracy: 0.5000\n",
            "Epoch 228/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7205 - accuracy: 0.7007 - val_loss: 1.1067 - val_accuracy: 0.4865\n",
            "Epoch 229/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6707 - accuracy: 0.7381 - val_loss: 0.9971 - val_accuracy: 0.5270\n",
            "Epoch 230/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6901 - accuracy: 0.7143 - val_loss: 1.1009 - val_accuracy: 0.4865\n",
            "Epoch 231/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7253 - accuracy: 0.6973 - val_loss: 1.0982 - val_accuracy: 0.4865\n",
            "Epoch 232/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6991 - accuracy: 0.6837 - val_loss: 1.0425 - val_accuracy: 0.5270\n",
            "Epoch 233/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6868 - accuracy: 0.7177 - val_loss: 1.0540 - val_accuracy: 0.5135\n",
            "Epoch 234/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6716 - accuracy: 0.7381 - val_loss: 1.2149 - val_accuracy: 0.4865\n",
            "Epoch 235/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6938 - accuracy: 0.7109 - val_loss: 1.0754 - val_accuracy: 0.5135\n",
            "Epoch 236/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6515 - accuracy: 0.7347 - val_loss: 1.1130 - val_accuracy: 0.4595\n",
            "Epoch 237/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7930 - accuracy: 0.6701 - val_loss: 0.9765 - val_accuracy: 0.5541\n",
            "Epoch 238/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6939 - accuracy: 0.7211 - val_loss: 0.9412 - val_accuracy: 0.5541\n",
            "Epoch 239/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7480 - accuracy: 0.6701 - val_loss: 0.9313 - val_accuracy: 0.5676\n",
            "Epoch 240/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7531 - accuracy: 0.7007 - val_loss: 0.9685 - val_accuracy: 0.5270\n",
            "Epoch 241/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7444 - accuracy: 0.7041 - val_loss: 0.9868 - val_accuracy: 0.5541\n",
            "Epoch 242/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7755 - accuracy: 0.6327 - val_loss: 0.9780 - val_accuracy: 0.5405\n",
            "Epoch 243/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7516 - accuracy: 0.6599 - val_loss: 0.9653 - val_accuracy: 0.5270\n",
            "Epoch 244/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7329 - accuracy: 0.6735 - val_loss: 0.9973 - val_accuracy: 0.5541\n",
            "Epoch 245/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7297 - accuracy: 0.6633 - val_loss: 0.9864 - val_accuracy: 0.5405\n",
            "Epoch 246/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.7112 - accuracy: 0.7177 - val_loss: 0.9991 - val_accuracy: 0.5405\n",
            "Epoch 247/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6943 - accuracy: 0.7245 - val_loss: 1.0265 - val_accuracy: 0.4730\n",
            "Epoch 248/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6747 - accuracy: 0.6973 - val_loss: 0.9937 - val_accuracy: 0.5135\n",
            "Epoch 249/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6615 - accuracy: 0.7347 - val_loss: 1.0286 - val_accuracy: 0.5000\n",
            "Epoch 250/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6378 - accuracy: 0.7449 - val_loss: 1.1295 - val_accuracy: 0.4865\n",
            "Epoch 251/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6363 - accuracy: 0.7517 - val_loss: 1.2219 - val_accuracy: 0.3919\n",
            "Epoch 252/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6699 - accuracy: 0.7381 - val_loss: 1.1129 - val_accuracy: 0.4459\n",
            "Epoch 253/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6720 - accuracy: 0.7177 - val_loss: 1.1040 - val_accuracy: 0.4595\n",
            "Epoch 254/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6590 - accuracy: 0.7347 - val_loss: 1.0982 - val_accuracy: 0.4189\n",
            "Epoch 255/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6202 - accuracy: 0.7415 - val_loss: 1.1555 - val_accuracy: 0.4595\n",
            "Epoch 256/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.6316 - accuracy: 0.7619 - val_loss: 1.2105 - val_accuracy: 0.4865\n",
            "Epoch 257/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6212 - accuracy: 0.7347 - val_loss: 1.1728 - val_accuracy: 0.4459\n",
            "Epoch 258/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6472 - accuracy: 0.7313 - val_loss: 1.0864 - val_accuracy: 0.4865\n",
            "Epoch 259/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6579 - accuracy: 0.7279 - val_loss: 1.0122 - val_accuracy: 0.5541\n",
            "Epoch 260/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6323 - accuracy: 0.7483 - val_loss: 1.0798 - val_accuracy: 0.5000\n",
            "Epoch 261/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6565 - accuracy: 0.7109 - val_loss: 1.0485 - val_accuracy: 0.5405\n",
            "Epoch 262/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6128 - accuracy: 0.7585 - val_loss: 1.1119 - val_accuracy: 0.4730\n",
            "Epoch 263/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6023 - accuracy: 0.7721 - val_loss: 1.1972 - val_accuracy: 0.5000\n",
            "Epoch 264/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6102 - accuracy: 0.7619 - val_loss: 1.1527 - val_accuracy: 0.5000\n",
            "Epoch 265/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5816 - accuracy: 0.7925 - val_loss: 1.2095 - val_accuracy: 0.4324\n",
            "Epoch 266/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6061 - accuracy: 0.7551 - val_loss: 1.1207 - val_accuracy: 0.4459\n",
            "Epoch 267/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6011 - accuracy: 0.7551 - val_loss: 1.0358 - val_accuracy: 0.5405\n",
            "Epoch 268/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6325 - accuracy: 0.7449 - val_loss: 1.0732 - val_accuracy: 0.5270\n",
            "Epoch 269/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6008 - accuracy: 0.7415 - val_loss: 1.1447 - val_accuracy: 0.4730\n",
            "Epoch 270/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6215 - accuracy: 0.7483 - val_loss: 1.0590 - val_accuracy: 0.5676\n",
            "Epoch 271/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6450 - accuracy: 0.7585 - val_loss: 1.1399 - val_accuracy: 0.4865\n",
            "Epoch 272/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5579 - accuracy: 0.7823 - val_loss: 1.2931 - val_accuracy: 0.4865\n",
            "Epoch 273/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6392 - accuracy: 0.7449 - val_loss: 1.1555 - val_accuracy: 0.5000\n",
            "Epoch 274/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5832 - accuracy: 0.7517 - val_loss: 1.0696 - val_accuracy: 0.5135\n",
            "Epoch 275/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5819 - accuracy: 0.7653 - val_loss: 1.1836 - val_accuracy: 0.5135\n",
            "Epoch 276/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5754 - accuracy: 0.7653 - val_loss: 1.1880 - val_accuracy: 0.5135\n",
            "Epoch 277/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5703 - accuracy: 0.7823 - val_loss: 1.2616 - val_accuracy: 0.5270\n",
            "Epoch 278/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5821 - accuracy: 0.7619 - val_loss: 1.2802 - val_accuracy: 0.4459\n",
            "Epoch 279/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5512 - accuracy: 0.7891 - val_loss: 1.1586 - val_accuracy: 0.4595\n",
            "Epoch 280/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5651 - accuracy: 0.7891 - val_loss: 1.1007 - val_accuracy: 0.4865\n",
            "Epoch 281/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5546 - accuracy: 0.7925 - val_loss: 1.3739 - val_accuracy: 0.5270\n",
            "Epoch 282/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5467 - accuracy: 0.7891 - val_loss: 1.2206 - val_accuracy: 0.4459\n",
            "Epoch 283/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5171 - accuracy: 0.7857 - val_loss: 1.1948 - val_accuracy: 0.5135\n",
            "Epoch 284/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5302 - accuracy: 0.7959 - val_loss: 1.3042 - val_accuracy: 0.4865\n",
            "Epoch 285/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5301 - accuracy: 0.7857 - val_loss: 1.2210 - val_accuracy: 0.5000\n",
            "Epoch 286/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5167 - accuracy: 0.7891 - val_loss: 1.1827 - val_accuracy: 0.5405\n",
            "Epoch 287/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5211 - accuracy: 0.7823 - val_loss: 1.2458 - val_accuracy: 0.4865\n",
            "Epoch 288/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5229 - accuracy: 0.7721 - val_loss: 1.1075 - val_accuracy: 0.4865\n",
            "Epoch 289/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5268 - accuracy: 0.7823 - val_loss: 1.1348 - val_accuracy: 0.5676\n",
            "Epoch 290/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5717 - accuracy: 0.7755 - val_loss: 1.1773 - val_accuracy: 0.4730\n",
            "Epoch 291/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5363 - accuracy: 0.7687 - val_loss: 1.1063 - val_accuracy: 0.6081\n",
            "Epoch 292/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6307 - accuracy: 0.7415 - val_loss: 1.1935 - val_accuracy: 0.5405\n",
            "Epoch 293/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.7636 - accuracy: 0.7041 - val_loss: 1.0472 - val_accuracy: 0.5811\n",
            "Epoch 294/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6105 - accuracy: 0.7551 - val_loss: 1.0585 - val_accuracy: 0.5000\n",
            "Epoch 295/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6566 - accuracy: 0.7109 - val_loss: 1.1758 - val_accuracy: 0.5135\n",
            "Epoch 296/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6064 - accuracy: 0.7177 - val_loss: 1.2721 - val_accuracy: 0.5000\n",
            "Epoch 297/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6191 - accuracy: 0.7313 - val_loss: 1.0759 - val_accuracy: 0.5541\n",
            "Epoch 298/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5773 - accuracy: 0.7517 - val_loss: 1.0753 - val_accuracy: 0.6216\n",
            "Epoch 299/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5869 - accuracy: 0.7517 - val_loss: 1.0837 - val_accuracy: 0.5135\n",
            "Epoch 300/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.5575 - accuracy: 0.7551 - val_loss: 1.1283 - val_accuracy: 0.5270\n",
            "Epoch 301/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5450 - accuracy: 0.7925 - val_loss: 1.2458 - val_accuracy: 0.5270\n",
            "Epoch 302/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5948 - accuracy: 0.7551 - val_loss: 1.0795 - val_accuracy: 0.5405\n",
            "Epoch 303/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5575 - accuracy: 0.7687 - val_loss: 1.0565 - val_accuracy: 0.5541\n",
            "Epoch 304/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6120 - accuracy: 0.7449 - val_loss: 1.0845 - val_accuracy: 0.5405\n",
            "Epoch 305/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5895 - accuracy: 0.7585 - val_loss: 1.1003 - val_accuracy: 0.5676\n",
            "Epoch 306/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5559 - accuracy: 0.7483 - val_loss: 1.1912 - val_accuracy: 0.5541\n",
            "Epoch 307/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5297 - accuracy: 0.7891 - val_loss: 1.2125 - val_accuracy: 0.4730\n",
            "Epoch 308/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5437 - accuracy: 0.7755 - val_loss: 1.2052 - val_accuracy: 0.4595\n",
            "Epoch 309/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5321 - accuracy: 0.7449 - val_loss: 1.2047 - val_accuracy: 0.4865\n",
            "Epoch 310/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4991 - accuracy: 0.7789 - val_loss: 1.1986 - val_accuracy: 0.4865\n",
            "Epoch 311/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5062 - accuracy: 0.8163 - val_loss: 1.1869 - val_accuracy: 0.5541\n",
            "Epoch 312/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4991 - accuracy: 0.7823 - val_loss: 1.2737 - val_accuracy: 0.4595\n",
            "Epoch 313/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4876 - accuracy: 0.7857 - val_loss: 1.1746 - val_accuracy: 0.5405\n",
            "Epoch 314/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4926 - accuracy: 0.7755 - val_loss: 1.2765 - val_accuracy: 0.4865\n",
            "Epoch 315/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5198 - accuracy: 0.7993 - val_loss: 1.2046 - val_accuracy: 0.5270\n",
            "Epoch 316/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5199 - accuracy: 0.7891 - val_loss: 1.2369 - val_accuracy: 0.4865\n",
            "Epoch 317/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4776 - accuracy: 0.8027 - val_loss: 1.1119 - val_accuracy: 0.5676\n",
            "Epoch 318/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5440 - accuracy: 0.7925 - val_loss: 1.1365 - val_accuracy: 0.5541\n",
            "Epoch 319/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5399 - accuracy: 0.7789 - val_loss: 1.2541 - val_accuracy: 0.5135\n",
            "Epoch 320/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.5378 - accuracy: 0.7585 - val_loss: 1.2792 - val_accuracy: 0.5000\n",
            "Epoch 321/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5729 - accuracy: 0.7619 - val_loss: 1.1991 - val_accuracy: 0.5811\n",
            "Epoch 322/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5299 - accuracy: 0.7959 - val_loss: 1.2188 - val_accuracy: 0.5270\n",
            "Epoch 323/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5105 - accuracy: 0.7823 - val_loss: 1.2452 - val_accuracy: 0.5270\n",
            "Epoch 324/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5672 - accuracy: 0.7687 - val_loss: 1.2020 - val_accuracy: 0.5405\n",
            "Epoch 325/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5115 - accuracy: 0.8027 - val_loss: 1.1097 - val_accuracy: 0.5405\n",
            "Epoch 326/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5193 - accuracy: 0.7959 - val_loss: 1.1467 - val_accuracy: 0.5405\n",
            "Epoch 327/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4854 - accuracy: 0.8061 - val_loss: 1.2416 - val_accuracy: 0.5000\n",
            "Epoch 328/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4859 - accuracy: 0.7993 - val_loss: 1.1380 - val_accuracy: 0.5000\n",
            "Epoch 329/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4378 - accuracy: 0.8333 - val_loss: 1.1669 - val_accuracy: 0.5135\n",
            "Epoch 330/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4492 - accuracy: 0.7857 - val_loss: 1.2683 - val_accuracy: 0.5541\n",
            "Epoch 331/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4411 - accuracy: 0.8503 - val_loss: 1.3003 - val_accuracy: 0.5135\n",
            "Epoch 332/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4612 - accuracy: 0.7891 - val_loss: 1.2270 - val_accuracy: 0.5270\n",
            "Epoch 333/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4407 - accuracy: 0.8469 - val_loss: 1.4245 - val_accuracy: 0.4865\n",
            "Epoch 334/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4581 - accuracy: 0.8197 - val_loss: 1.1194 - val_accuracy: 0.5811\n",
            "Epoch 335/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4940 - accuracy: 0.8061 - val_loss: 1.2096 - val_accuracy: 0.5541\n",
            "Epoch 336/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5040 - accuracy: 0.8231 - val_loss: 1.3212 - val_accuracy: 0.4730\n",
            "Epoch 337/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4526 - accuracy: 0.8265 - val_loss: 1.2215 - val_accuracy: 0.4865\n",
            "Epoch 338/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5012 - accuracy: 0.7585 - val_loss: 1.1320 - val_accuracy: 0.5541\n",
            "Epoch 339/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4455 - accuracy: 0.8401 - val_loss: 1.1631 - val_accuracy: 0.5405\n",
            "Epoch 340/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5418 - accuracy: 0.7857 - val_loss: 1.2329 - val_accuracy: 0.5811\n",
            "Epoch 341/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5301 - accuracy: 0.7959 - val_loss: 1.2007 - val_accuracy: 0.5541\n",
            "Epoch 342/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6020 - accuracy: 0.7483 - val_loss: 1.0789 - val_accuracy: 0.6351\n",
            "Epoch 343/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4824 - accuracy: 0.8367 - val_loss: 1.1675 - val_accuracy: 0.5405\n",
            "Epoch 344/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4764 - accuracy: 0.8095 - val_loss: 1.2441 - val_accuracy: 0.5541\n",
            "Epoch 345/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4549 - accuracy: 0.7993 - val_loss: 1.1189 - val_accuracy: 0.5541\n",
            "Epoch 346/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4929 - accuracy: 0.7891 - val_loss: 1.1663 - val_accuracy: 0.5676\n",
            "Epoch 347/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4568 - accuracy: 0.8333 - val_loss: 1.2900 - val_accuracy: 0.4865\n",
            "Epoch 348/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4373 - accuracy: 0.7959 - val_loss: 1.2482 - val_accuracy: 0.5135\n",
            "Epoch 349/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4295 - accuracy: 0.8435 - val_loss: 1.3388 - val_accuracy: 0.5270\n",
            "Epoch 350/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4250 - accuracy: 0.8401 - val_loss: 1.3053 - val_accuracy: 0.5541\n",
            "Epoch 351/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4117 - accuracy: 0.8469 - val_loss: 1.2316 - val_accuracy: 0.5946\n",
            "Epoch 352/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3844 - accuracy: 0.8639 - val_loss: 1.2481 - val_accuracy: 0.5541\n",
            "Epoch 353/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4734 - accuracy: 0.7959 - val_loss: 1.4045 - val_accuracy: 0.4865\n",
            "Epoch 354/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5610 - accuracy: 0.7857 - val_loss: 1.2073 - val_accuracy: 0.5676\n",
            "Epoch 355/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5633 - accuracy: 0.7687 - val_loss: 1.2301 - val_accuracy: 0.5000\n",
            "Epoch 356/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4912 - accuracy: 0.7789 - val_loss: 1.1369 - val_accuracy: 0.5541\n",
            "Epoch 357/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.5343 - accuracy: 0.7687 - val_loss: 1.1230 - val_accuracy: 0.5541\n",
            "Epoch 358/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.5263 - accuracy: 0.7925 - val_loss: 1.2961 - val_accuracy: 0.4730\n",
            "Epoch 359/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4175 - accuracy: 0.8469 - val_loss: 1.3295 - val_accuracy: 0.5270\n",
            "Epoch 360/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4497 - accuracy: 0.7789 - val_loss: 1.3093 - val_accuracy: 0.5676\n",
            "Epoch 361/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4500 - accuracy: 0.8299 - val_loss: 1.1791 - val_accuracy: 0.5676\n",
            "Epoch 362/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4317 - accuracy: 0.8265 - val_loss: 1.1397 - val_accuracy: 0.5811\n",
            "Epoch 363/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4132 - accuracy: 0.8537 - val_loss: 1.2448 - val_accuracy: 0.5811\n",
            "Epoch 364/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4160 - accuracy: 0.8469 - val_loss: 1.3893 - val_accuracy: 0.5000\n",
            "Epoch 365/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3996 - accuracy: 0.8435 - val_loss: 1.2947 - val_accuracy: 0.5541\n",
            "Epoch 366/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3759 - accuracy: 0.8605 - val_loss: 1.2873 - val_accuracy: 0.5676\n",
            "Epoch 367/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3951 - accuracy: 0.8673 - val_loss: 1.2231 - val_accuracy: 0.5135\n",
            "Epoch 368/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3719 - accuracy: 0.8469 - val_loss: 1.4915 - val_accuracy: 0.4595\n",
            "Epoch 369/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4421 - accuracy: 0.8333 - val_loss: 1.2218 - val_accuracy: 0.6216\n",
            "Epoch 370/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4508 - accuracy: 0.8333 - val_loss: 1.2121 - val_accuracy: 0.5676\n",
            "Epoch 371/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4405 - accuracy: 0.8503 - val_loss: 1.2572 - val_accuracy: 0.5405\n",
            "Epoch 372/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3913 - accuracy: 0.8810 - val_loss: 1.3490 - val_accuracy: 0.5135\n",
            "Epoch 373/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3550 - accuracy: 0.8537 - val_loss: 1.3220 - val_accuracy: 0.5541\n",
            "Epoch 374/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3893 - accuracy: 0.8401 - val_loss: 1.3114 - val_accuracy: 0.5541\n",
            "Epoch 375/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3739 - accuracy: 0.8435 - val_loss: 1.3329 - val_accuracy: 0.5135\n",
            "Epoch 376/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3829 - accuracy: 0.8503 - val_loss: 1.2788 - val_accuracy: 0.5676\n",
            "Epoch 377/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3597 - accuracy: 0.8776 - val_loss: 1.2679 - val_accuracy: 0.6081\n",
            "Epoch 378/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3465 - accuracy: 0.8503 - val_loss: 1.4485 - val_accuracy: 0.5135\n",
            "Epoch 379/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3941 - accuracy: 0.8605 - val_loss: 1.1643 - val_accuracy: 0.5541\n",
            "Epoch 380/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4720 - accuracy: 0.7891 - val_loss: 1.2378 - val_accuracy: 0.5811\n",
            "Epoch 381/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4038 - accuracy: 0.8401 - val_loss: 1.2842 - val_accuracy: 0.6486\n",
            "Epoch 382/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3548 - accuracy: 0.8776 - val_loss: 1.1577 - val_accuracy: 0.6351\n",
            "Epoch 383/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3610 - accuracy: 0.8605 - val_loss: 1.3121 - val_accuracy: 0.6081\n",
            "Epoch 384/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4118 - accuracy: 0.8367 - val_loss: 1.1841 - val_accuracy: 0.5946\n",
            "Epoch 385/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4285 - accuracy: 0.8197 - val_loss: 1.2142 - val_accuracy: 0.6081\n",
            "Epoch 386/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3814 - accuracy: 0.8503 - val_loss: 1.3975 - val_accuracy: 0.5405\n",
            "Epoch 387/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3566 - accuracy: 0.8435 - val_loss: 1.1349 - val_accuracy: 0.6622\n",
            "Epoch 388/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3587 - accuracy: 0.8707 - val_loss: 1.2765 - val_accuracy: 0.6216\n",
            "Epoch 389/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3598 - accuracy: 0.8435 - val_loss: 1.4832 - val_accuracy: 0.5270\n",
            "Epoch 390/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3086 - accuracy: 0.8776 - val_loss: 1.2792 - val_accuracy: 0.5270\n",
            "Epoch 391/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3488 - accuracy: 0.8469 - val_loss: 1.3356 - val_accuracy: 0.6216\n",
            "Epoch 392/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3241 - accuracy: 0.8707 - val_loss: 1.3835 - val_accuracy: 0.6216\n",
            "Epoch 393/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3094 - accuracy: 0.8844 - val_loss: 1.4167 - val_accuracy: 0.5946\n",
            "Epoch 394/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2939 - accuracy: 0.8946 - val_loss: 1.4943 - val_accuracy: 0.6622\n",
            "Epoch 395/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3127 - accuracy: 0.8776 - val_loss: 1.5540 - val_accuracy: 0.5541\n",
            "Epoch 396/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3370 - accuracy: 0.8946 - val_loss: 1.3735 - val_accuracy: 0.6351\n",
            "Epoch 397/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3496 - accuracy: 0.8673 - val_loss: 1.5373 - val_accuracy: 0.5676\n",
            "Epoch 398/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3846 - accuracy: 0.8707 - val_loss: 1.4012 - val_accuracy: 0.6486\n",
            "Epoch 399/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3192 - accuracy: 0.8844 - val_loss: 1.4866 - val_accuracy: 0.6081\n",
            "Epoch 400/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2755 - accuracy: 0.9184 - val_loss: 1.5495 - val_accuracy: 0.5946\n",
            "Epoch 401/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2749 - accuracy: 0.9116 - val_loss: 1.5226 - val_accuracy: 0.5811\n",
            "Epoch 402/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2478 - accuracy: 0.9048 - val_loss: 1.4906 - val_accuracy: 0.6351\n",
            "Epoch 403/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2738 - accuracy: 0.8946 - val_loss: 1.6481 - val_accuracy: 0.5135\n",
            "Epoch 404/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3260 - accuracy: 0.8810 - val_loss: 1.4972 - val_accuracy: 0.5946\n",
            "Epoch 405/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3064 - accuracy: 0.8912 - val_loss: 1.4831 - val_accuracy: 0.6351\n",
            "Epoch 406/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3433 - accuracy: 0.8673 - val_loss: 1.3618 - val_accuracy: 0.5676\n",
            "Epoch 407/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5830 - accuracy: 0.7449 - val_loss: 1.3081 - val_accuracy: 0.5946\n",
            "Epoch 408/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4919 - accuracy: 0.7857 - val_loss: 1.6138 - val_accuracy: 0.5000\n",
            "Epoch 409/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4553 - accuracy: 0.7857 - val_loss: 1.3878 - val_accuracy: 0.5811\n",
            "Epoch 410/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5340 - accuracy: 0.7891 - val_loss: 1.8297 - val_accuracy: 0.4595\n",
            "Epoch 411/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5171 - accuracy: 0.7823 - val_loss: 1.4438 - val_accuracy: 0.5541\n",
            "Epoch 412/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6050 - accuracy: 0.7415 - val_loss: 1.4837 - val_accuracy: 0.5000\n",
            "Epoch 413/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4762 - accuracy: 0.8231 - val_loss: 1.2549 - val_accuracy: 0.5676\n",
            "Epoch 414/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4287 - accuracy: 0.8367 - val_loss: 1.3179 - val_accuracy: 0.5270\n",
            "Epoch 415/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3696 - accuracy: 0.8537 - val_loss: 1.4672 - val_accuracy: 0.5811\n",
            "Epoch 416/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3731 - accuracy: 0.8401 - val_loss: 1.3247 - val_accuracy: 0.5811\n",
            "Epoch 417/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3367 - accuracy: 0.8741 - val_loss: 1.2774 - val_accuracy: 0.5676\n",
            "Epoch 418/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3427 - accuracy: 0.8946 - val_loss: 1.3636 - val_accuracy: 0.5676\n",
            "Epoch 419/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3615 - accuracy: 0.8401 - val_loss: 1.3607 - val_accuracy: 0.5811\n",
            "Epoch 420/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3089 - accuracy: 0.8946 - val_loss: 1.4008 - val_accuracy: 0.5811\n",
            "Epoch 421/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3112 - accuracy: 0.9116 - val_loss: 1.3905 - val_accuracy: 0.5270\n",
            "Epoch 422/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2943 - accuracy: 0.9082 - val_loss: 1.3934 - val_accuracy: 0.5946\n",
            "Epoch 423/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2602 - accuracy: 0.9014 - val_loss: 1.5105 - val_accuracy: 0.5541\n",
            "Epoch 424/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2879 - accuracy: 0.8741 - val_loss: 1.5221 - val_accuracy: 0.5270\n",
            "Epoch 425/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3298 - accuracy: 0.8810 - val_loss: 1.6184 - val_accuracy: 0.5135\n",
            "Epoch 426/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3108 - accuracy: 0.8741 - val_loss: 1.4066 - val_accuracy: 0.5946\n",
            "Epoch 427/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3001 - accuracy: 0.8810 - val_loss: 1.6191 - val_accuracy: 0.5811\n",
            "Epoch 428/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2757 - accuracy: 0.9116 - val_loss: 1.4227 - val_accuracy: 0.6081\n",
            "Epoch 429/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3152 - accuracy: 0.8707 - val_loss: 1.5145 - val_accuracy: 0.5676\n",
            "Epoch 430/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3859 - accuracy: 0.8673 - val_loss: 1.2561 - val_accuracy: 0.5946\n",
            "Epoch 431/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3348 - accuracy: 0.8776 - val_loss: 1.3194 - val_accuracy: 0.6622\n",
            "Epoch 432/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2813 - accuracy: 0.9048 - val_loss: 1.5946 - val_accuracy: 0.6081\n",
            "Epoch 433/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2750 - accuracy: 0.9184 - val_loss: 1.4978 - val_accuracy: 0.5811\n",
            "Epoch 434/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2643 - accuracy: 0.9184 - val_loss: 1.4143 - val_accuracy: 0.5946\n",
            "Epoch 435/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2755 - accuracy: 0.9218 - val_loss: 1.6092 - val_accuracy: 0.5676\n",
            "Epoch 436/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2520 - accuracy: 0.9184 - val_loss: 1.5510 - val_accuracy: 0.6486\n",
            "Epoch 437/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2268 - accuracy: 0.9456 - val_loss: 1.7296 - val_accuracy: 0.5946\n",
            "Epoch 438/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2379 - accuracy: 0.9252 - val_loss: 1.5477 - val_accuracy: 0.6081\n",
            "Epoch 439/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2253 - accuracy: 0.9082 - val_loss: 1.6620 - val_accuracy: 0.6081\n",
            "Epoch 440/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3359 - accuracy: 0.8776 - val_loss: 1.6749 - val_accuracy: 0.5541\n",
            "Epoch 441/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3647 - accuracy: 0.8673 - val_loss: 1.4089 - val_accuracy: 0.5541\n",
            "Epoch 442/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3110 - accuracy: 0.8605 - val_loss: 1.5502 - val_accuracy: 0.5676\n",
            "Epoch 443/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3760 - accuracy: 0.8571 - val_loss: 1.3033 - val_accuracy: 0.6486\n",
            "Epoch 444/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3655 - accuracy: 0.8639 - val_loss: 1.5419 - val_accuracy: 0.5811\n",
            "Epoch 445/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4001 - accuracy: 0.8401 - val_loss: 1.4894 - val_accuracy: 0.6081\n",
            "Epoch 446/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2893 - accuracy: 0.8946 - val_loss: 1.5078 - val_accuracy: 0.5541\n",
            "Epoch 447/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3706 - accuracy: 0.8333 - val_loss: 1.5305 - val_accuracy: 0.6081\n",
            "Epoch 448/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2444 - accuracy: 0.9320 - val_loss: 1.7919 - val_accuracy: 0.5676\n",
            "Epoch 449/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2900 - accuracy: 0.8912 - val_loss: 1.5610 - val_accuracy: 0.5946\n",
            "Epoch 450/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2247 - accuracy: 0.9150 - val_loss: 1.5066 - val_accuracy: 0.6351\n",
            "Epoch 451/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2096 - accuracy: 0.9388 - val_loss: 1.5688 - val_accuracy: 0.6081\n",
            "Epoch 452/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2054 - accuracy: 0.9082 - val_loss: 1.5453 - val_accuracy: 0.5946\n",
            "Epoch 453/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2219 - accuracy: 0.9354 - val_loss: 1.5854 - val_accuracy: 0.6216\n",
            "Epoch 454/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1897 - accuracy: 0.9388 - val_loss: 1.7361 - val_accuracy: 0.5541\n",
            "Epoch 455/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2553 - accuracy: 0.8912 - val_loss: 1.5629 - val_accuracy: 0.6351\n",
            "Epoch 456/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2783 - accuracy: 0.8810 - val_loss: 1.6596 - val_accuracy: 0.5676\n",
            "Epoch 457/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3068 - accuracy: 0.8776 - val_loss: 1.4694 - val_accuracy: 0.6351\n",
            "Epoch 458/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3185 - accuracy: 0.8741 - val_loss: 1.9273 - val_accuracy: 0.5405\n",
            "Epoch 459/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4991 - accuracy: 0.8231 - val_loss: 1.3838 - val_accuracy: 0.5946\n",
            "Epoch 460/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4731 - accuracy: 0.7993 - val_loss: 1.3706 - val_accuracy: 0.5811\n",
            "Epoch 461/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3748 - accuracy: 0.8571 - val_loss: 1.5828 - val_accuracy: 0.5811\n",
            "Epoch 462/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2985 - accuracy: 0.8912 - val_loss: 1.7172 - val_accuracy: 0.5811\n",
            "Epoch 463/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2669 - accuracy: 0.8946 - val_loss: 1.5212 - val_accuracy: 0.6081\n",
            "Epoch 464/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2433 - accuracy: 0.9116 - val_loss: 1.5735 - val_accuracy: 0.6081\n",
            "Epoch 465/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2420 - accuracy: 0.9184 - val_loss: 1.4567 - val_accuracy: 0.6486\n",
            "Epoch 466/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2380 - accuracy: 0.9014 - val_loss: 1.4477 - val_accuracy: 0.6486\n",
            "Epoch 467/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2327 - accuracy: 0.9252 - val_loss: 1.5160 - val_accuracy: 0.6486\n",
            "Epoch 468/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2223 - accuracy: 0.9286 - val_loss: 1.7581 - val_accuracy: 0.6081\n",
            "Epoch 469/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2143 - accuracy: 0.9218 - val_loss: 1.9067 - val_accuracy: 0.5405\n",
            "Epoch 470/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2980 - accuracy: 0.8912 - val_loss: 1.7134 - val_accuracy: 0.5946\n",
            "Epoch 471/800\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3261 - accuracy: 0.8810 - val_loss: 1.8061 - val_accuracy: 0.5135\n",
            "Epoch 472/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2993 - accuracy: 0.8878 - val_loss: 1.5580 - val_accuracy: 0.5946\n",
            "Epoch 473/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2432 - accuracy: 0.9150 - val_loss: 1.7286 - val_accuracy: 0.6081\n",
            "Epoch 474/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2408 - accuracy: 0.9116 - val_loss: 1.9976 - val_accuracy: 0.5811\n",
            "Epoch 475/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2466 - accuracy: 0.9184 - val_loss: 1.6543 - val_accuracy: 0.6216\n",
            "Epoch 476/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2019 - accuracy: 0.9388 - val_loss: 1.5735 - val_accuracy: 0.6216\n",
            "Epoch 477/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2070 - accuracy: 0.9524 - val_loss: 1.6429 - val_accuracy: 0.6081\n",
            "Epoch 478/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1536 - accuracy: 0.9660 - val_loss: 1.7213 - val_accuracy: 0.6351\n",
            "Epoch 479/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1989 - accuracy: 0.9252 - val_loss: 1.6839 - val_accuracy: 0.5676\n",
            "Epoch 480/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1948 - accuracy: 0.9388 - val_loss: 1.6692 - val_accuracy: 0.5946\n",
            "Epoch 481/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2138 - accuracy: 0.9218 - val_loss: 1.5563 - val_accuracy: 0.5541\n",
            "Epoch 482/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2834 - accuracy: 0.9048 - val_loss: 1.6296 - val_accuracy: 0.6081\n",
            "Epoch 483/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2348 - accuracy: 0.9082 - val_loss: 1.9499 - val_accuracy: 0.5676\n",
            "Epoch 484/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2378 - accuracy: 0.9082 - val_loss: 1.9739 - val_accuracy: 0.5405\n",
            "Epoch 485/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2035 - accuracy: 0.9252 - val_loss: 1.6469 - val_accuracy: 0.6081\n",
            "Epoch 486/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2498 - accuracy: 0.9150 - val_loss: 1.6815 - val_accuracy: 0.5811\n",
            "Epoch 487/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2038 - accuracy: 0.9354 - val_loss: 1.8869 - val_accuracy: 0.5676\n",
            "Epoch 488/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2063 - accuracy: 0.9184 - val_loss: 2.0059 - val_accuracy: 0.5541\n",
            "Epoch 489/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2114 - accuracy: 0.9218 - val_loss: 1.7114 - val_accuracy: 0.6081\n",
            "Epoch 490/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2455 - accuracy: 0.8878 - val_loss: 2.0045 - val_accuracy: 0.5541\n",
            "Epoch 491/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2153 - accuracy: 0.9354 - val_loss: 1.7489 - val_accuracy: 0.5676\n",
            "Epoch 492/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1633 - accuracy: 0.9388 - val_loss: 1.8002 - val_accuracy: 0.6081\n",
            "Epoch 493/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1926 - accuracy: 0.9524 - val_loss: 1.7710 - val_accuracy: 0.6081\n",
            "Epoch 494/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1552 - accuracy: 0.9456 - val_loss: 1.7305 - val_accuracy: 0.5946\n",
            "Epoch 495/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1667 - accuracy: 0.9422 - val_loss: 1.8757 - val_accuracy: 0.5946\n",
            "Epoch 496/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1482 - accuracy: 0.9592 - val_loss: 1.9162 - val_accuracy: 0.6081\n",
            "Epoch 497/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1436 - accuracy: 0.9626 - val_loss: 1.8229 - val_accuracy: 0.5811\n",
            "Epoch 498/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1672 - accuracy: 0.9490 - val_loss: 1.8070 - val_accuracy: 0.5676\n",
            "Epoch 499/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1426 - accuracy: 0.9626 - val_loss: 1.7979 - val_accuracy: 0.5676\n",
            "Epoch 500/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3634 - accuracy: 0.8401 - val_loss: 1.7625 - val_accuracy: 0.5270\n",
            "Epoch 501/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4181 - accuracy: 0.8503 - val_loss: 1.8422 - val_accuracy: 0.5811\n",
            "Epoch 502/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2602 - accuracy: 0.8810 - val_loss: 1.7328 - val_accuracy: 0.5946\n",
            "Epoch 503/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2776 - accuracy: 0.9116 - val_loss: 1.7908 - val_accuracy: 0.5541\n",
            "Epoch 504/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4527 - accuracy: 0.8027 - val_loss: 1.8022 - val_accuracy: 0.5946\n",
            "Epoch 505/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3874 - accuracy: 0.8333 - val_loss: 1.7021 - val_accuracy: 0.5541\n",
            "Epoch 506/800\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2811 - accuracy: 0.9082 - val_loss: 1.6195 - val_accuracy: 0.5676\n",
            "Epoch 507/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2446 - accuracy: 0.8878 - val_loss: 1.8044 - val_accuracy: 0.5676\n",
            "Epoch 508/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1904 - accuracy: 0.9354 - val_loss: 1.6497 - val_accuracy: 0.5946\n",
            "Epoch 509/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1865 - accuracy: 0.9456 - val_loss: 1.7159 - val_accuracy: 0.5811\n",
            "Epoch 510/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1660 - accuracy: 0.9422 - val_loss: 1.6721 - val_accuracy: 0.6216\n",
            "Epoch 511/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1636 - accuracy: 0.9456 - val_loss: 1.6567 - val_accuracy: 0.6081\n",
            "Epoch 512/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1611 - accuracy: 0.9524 - val_loss: 1.7326 - val_accuracy: 0.5811\n",
            "Epoch 513/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1768 - accuracy: 0.9456 - val_loss: 1.8770 - val_accuracy: 0.5946\n",
            "Epoch 514/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2069 - accuracy: 0.9320 - val_loss: 1.8274 - val_accuracy: 0.6081\n",
            "Epoch 515/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2476 - accuracy: 0.9150 - val_loss: 1.6443 - val_accuracy: 0.5946\n",
            "Epoch 516/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1596 - accuracy: 0.9558 - val_loss: 1.7369 - val_accuracy: 0.5811\n",
            "Epoch 517/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1903 - accuracy: 0.9354 - val_loss: 1.8501 - val_accuracy: 0.5541\n",
            "Epoch 518/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2480 - accuracy: 0.9150 - val_loss: 1.7155 - val_accuracy: 0.6351\n",
            "Epoch 519/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1938 - accuracy: 0.9252 - val_loss: 1.8276 - val_accuracy: 0.5676\n",
            "Epoch 520/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1979 - accuracy: 0.9184 - val_loss: 1.8671 - val_accuracy: 0.5811\n",
            "Epoch 521/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1950 - accuracy: 0.9252 - val_loss: 1.9634 - val_accuracy: 0.5946\n",
            "Epoch 522/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3951 - accuracy: 0.8435 - val_loss: 1.8813 - val_accuracy: 0.5135\n",
            "Epoch 523/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3000 - accuracy: 0.8776 - val_loss: 1.6364 - val_accuracy: 0.5811\n",
            "Epoch 524/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3225 - accuracy: 0.8503 - val_loss: 1.7050 - val_accuracy: 0.5811\n",
            "Epoch 525/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2585 - accuracy: 0.8912 - val_loss: 1.7488 - val_accuracy: 0.6081\n",
            "Epoch 526/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1954 - accuracy: 0.9252 - val_loss: 1.9349 - val_accuracy: 0.6216\n",
            "Epoch 527/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2339 - accuracy: 0.9082 - val_loss: 1.9331 - val_accuracy: 0.5676\n",
            "Epoch 528/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1898 - accuracy: 0.9252 - val_loss: 1.9569 - val_accuracy: 0.5946\n",
            "Epoch 529/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1615 - accuracy: 0.9388 - val_loss: 1.9418 - val_accuracy: 0.5676\n",
            "Epoch 530/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1784 - accuracy: 0.9524 - val_loss: 1.7884 - val_accuracy: 0.5946\n",
            "Epoch 531/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1292 - accuracy: 0.9660 - val_loss: 2.0477 - val_accuracy: 0.5541\n",
            "Epoch 532/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1272 - accuracy: 0.9558 - val_loss: 1.9158 - val_accuracy: 0.6351\n",
            "Epoch 533/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1471 - accuracy: 0.9558 - val_loss: 1.9271 - val_accuracy: 0.6351\n",
            "Epoch 534/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1569 - accuracy: 0.9626 - val_loss: 1.8729 - val_accuracy: 0.6081\n",
            "Epoch 535/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1667 - accuracy: 0.9592 - val_loss: 1.7687 - val_accuracy: 0.6081\n",
            "Epoch 536/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1575 - accuracy: 0.9456 - val_loss: 2.0779 - val_accuracy: 0.5946\n",
            "Epoch 537/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1257 - accuracy: 0.9660 - val_loss: 2.0163 - val_accuracy: 0.5946\n",
            "Epoch 538/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1309 - accuracy: 0.9592 - val_loss: 1.9439 - val_accuracy: 0.6216\n",
            "Epoch 539/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1453 - accuracy: 0.9558 - val_loss: 2.2592 - val_accuracy: 0.5676\n",
            "Epoch 540/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1521 - accuracy: 0.9558 - val_loss: 1.8565 - val_accuracy: 0.5946\n",
            "Epoch 541/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1287 - accuracy: 0.9660 - val_loss: 1.9881 - val_accuracy: 0.5811\n",
            "Epoch 542/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1074 - accuracy: 0.9796 - val_loss: 2.1145 - val_accuracy: 0.6081\n",
            "Epoch 543/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1356 - accuracy: 0.9490 - val_loss: 2.0552 - val_accuracy: 0.6081\n",
            "Epoch 544/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1082 - accuracy: 0.9830 - val_loss: 2.0356 - val_accuracy: 0.5946\n",
            "Epoch 545/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1207 - accuracy: 0.9694 - val_loss: 2.1191 - val_accuracy: 0.5811\n",
            "Epoch 546/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1037 - accuracy: 0.9762 - val_loss: 2.1494 - val_accuracy: 0.5946\n",
            "Epoch 547/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0842 - accuracy: 0.9864 - val_loss: 1.9227 - val_accuracy: 0.5946\n",
            "Epoch 548/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0968 - accuracy: 0.9830 - val_loss: 2.0619 - val_accuracy: 0.6351\n",
            "Epoch 549/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0874 - accuracy: 0.9830 - val_loss: 2.0058 - val_accuracy: 0.6486\n",
            "Epoch 550/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0895 - accuracy: 0.9796 - val_loss: 2.2483 - val_accuracy: 0.5946\n",
            "Epoch 551/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1282 - accuracy: 0.9524 - val_loss: 2.2405 - val_accuracy: 0.5946\n",
            "Epoch 552/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1082 - accuracy: 0.9660 - val_loss: 2.0217 - val_accuracy: 0.6216\n",
            "Epoch 553/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1342 - accuracy: 0.9660 - val_loss: 2.4254 - val_accuracy: 0.5676\n",
            "Epoch 554/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1396 - accuracy: 0.9592 - val_loss: 2.0668 - val_accuracy: 0.6081\n",
            "Epoch 555/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1909 - accuracy: 0.9116 - val_loss: 2.4045 - val_accuracy: 0.4865\n",
            "Epoch 556/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2055 - accuracy: 0.9048 - val_loss: 2.4391 - val_accuracy: 0.5541\n",
            "Epoch 557/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2394 - accuracy: 0.9116 - val_loss: 1.8618 - val_accuracy: 0.5811\n",
            "Epoch 558/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2514 - accuracy: 0.9048 - val_loss: 2.1620 - val_accuracy: 0.5270\n",
            "Epoch 559/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2875 - accuracy: 0.8810 - val_loss: 1.8222 - val_accuracy: 0.6081\n",
            "Epoch 560/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2523 - accuracy: 0.8878 - val_loss: 2.1567 - val_accuracy: 0.5946\n",
            "Epoch 561/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2311 - accuracy: 0.9116 - val_loss: 2.0075 - val_accuracy: 0.5676\n",
            "Epoch 562/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1852 - accuracy: 0.9150 - val_loss: 1.7334 - val_accuracy: 0.5676\n",
            "Epoch 563/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2134 - accuracy: 0.9048 - val_loss: 1.9248 - val_accuracy: 0.5676\n",
            "Epoch 564/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1797 - accuracy: 0.9286 - val_loss: 2.0463 - val_accuracy: 0.6081\n",
            "Epoch 565/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2176 - accuracy: 0.9150 - val_loss: 2.0921 - val_accuracy: 0.6081\n",
            "Epoch 566/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1961 - accuracy: 0.9422 - val_loss: 1.9372 - val_accuracy: 0.5946\n",
            "Epoch 567/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1565 - accuracy: 0.9456 - val_loss: 2.0613 - val_accuracy: 0.5946\n",
            "Epoch 568/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1349 - accuracy: 0.9592 - val_loss: 2.1269 - val_accuracy: 0.5676\n",
            "Epoch 569/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1025 - accuracy: 0.9694 - val_loss: 2.2602 - val_accuracy: 0.5946\n",
            "Epoch 570/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1021 - accuracy: 0.9694 - val_loss: 2.1912 - val_accuracy: 0.5811\n",
            "Epoch 571/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0944 - accuracy: 0.9762 - val_loss: 2.1497 - val_accuracy: 0.5946\n",
            "Epoch 572/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0783 - accuracy: 0.9864 - val_loss: 2.1274 - val_accuracy: 0.6081\n",
            "Epoch 573/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0924 - accuracy: 0.9694 - val_loss: 2.0490 - val_accuracy: 0.6216\n",
            "Epoch 574/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0771 - accuracy: 0.9864 - val_loss: 2.2868 - val_accuracy: 0.5676\n",
            "Epoch 575/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0955 - accuracy: 0.9762 - val_loss: 2.2023 - val_accuracy: 0.6216\n",
            "Epoch 576/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0795 - accuracy: 0.9728 - val_loss: 2.2952 - val_accuracy: 0.5811\n",
            "Epoch 577/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0804 - accuracy: 0.9830 - val_loss: 2.0328 - val_accuracy: 0.6081\n",
            "Epoch 578/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1368 - accuracy: 0.9286 - val_loss: 2.0881 - val_accuracy: 0.6081\n",
            "Epoch 579/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0866 - accuracy: 0.9864 - val_loss: 2.0904 - val_accuracy: 0.5811\n",
            "Epoch 580/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0722 - accuracy: 0.9932 - val_loss: 2.1472 - val_accuracy: 0.6216\n",
            "Epoch 581/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0813 - accuracy: 0.9830 - val_loss: 2.4057 - val_accuracy: 0.5811\n",
            "Epoch 582/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0838 - accuracy: 0.9796 - val_loss: 2.1660 - val_accuracy: 0.5811\n",
            "Epoch 583/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0765 - accuracy: 0.9830 - val_loss: 2.2184 - val_accuracy: 0.6216\n",
            "Epoch 584/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0578 - accuracy: 0.9898 - val_loss: 2.2738 - val_accuracy: 0.6081\n",
            "Epoch 585/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0582 - accuracy: 0.9830 - val_loss: 2.1979 - val_accuracy: 0.5811\n",
            "Epoch 586/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0659 - accuracy: 0.9796 - val_loss: 2.3112 - val_accuracy: 0.6081\n",
            "Epoch 587/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0620 - accuracy: 0.9932 - val_loss: 2.2732 - val_accuracy: 0.6351\n",
            "Epoch 588/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0551 - accuracy: 0.9932 - val_loss: 2.2187 - val_accuracy: 0.6216\n",
            "Epoch 589/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0623 - accuracy: 0.9864 - val_loss: 2.2144 - val_accuracy: 0.6216\n",
            "Epoch 590/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0548 - accuracy: 0.9864 - val_loss: 2.2900 - val_accuracy: 0.5811\n",
            "Epoch 591/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0616 - accuracy: 0.9898 - val_loss: 2.3455 - val_accuracy: 0.5676\n",
            "Epoch 592/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0665 - accuracy: 0.9864 - val_loss: 2.2717 - val_accuracy: 0.6351\n",
            "Epoch 593/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0640 - accuracy: 0.9796 - val_loss: 2.3587 - val_accuracy: 0.6081\n",
            "Epoch 594/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0658 - accuracy: 0.9898 - val_loss: 2.2710 - val_accuracy: 0.6081\n",
            "Epoch 595/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1018 - accuracy: 0.9694 - val_loss: 2.3213 - val_accuracy: 0.5676\n",
            "Epoch 596/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0769 - accuracy: 0.9796 - val_loss: 2.4159 - val_accuracy: 0.5946\n",
            "Epoch 597/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0675 - accuracy: 0.9864 - val_loss: 2.3496 - val_accuracy: 0.6081\n",
            "Epoch 598/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0637 - accuracy: 0.9898 - val_loss: 2.4791 - val_accuracy: 0.5676\n",
            "Epoch 599/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0577 - accuracy: 0.9830 - val_loss: 2.3048 - val_accuracy: 0.6216\n",
            "Epoch 600/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0642 - accuracy: 0.9728 - val_loss: 2.3574 - val_accuracy: 0.5676\n",
            "Epoch 601/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0530 - accuracy: 0.9898 - val_loss: 2.4326 - val_accuracy: 0.5676\n",
            "Epoch 602/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0691 - accuracy: 0.9864 - val_loss: 2.3106 - val_accuracy: 0.6216\n",
            "Epoch 603/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0583 - accuracy: 0.9932 - val_loss: 2.3329 - val_accuracy: 0.5946\n",
            "Epoch 604/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0907 - accuracy: 0.9728 - val_loss: 2.2892 - val_accuracy: 0.6081\n",
            "Epoch 605/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1229 - accuracy: 0.9660 - val_loss: 2.3364 - val_accuracy: 0.6216\n",
            "Epoch 606/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1347 - accuracy: 0.9456 - val_loss: 2.3690 - val_accuracy: 0.5811\n",
            "Epoch 607/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1301 - accuracy: 0.9490 - val_loss: 2.2626 - val_accuracy: 0.5946\n",
            "Epoch 608/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1603 - accuracy: 0.9422 - val_loss: 2.2326 - val_accuracy: 0.6081\n",
            "Epoch 609/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1288 - accuracy: 0.9354 - val_loss: 2.6144 - val_accuracy: 0.5811\n",
            "Epoch 610/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1896 - accuracy: 0.9252 - val_loss: 2.2979 - val_accuracy: 0.6351\n",
            "Epoch 611/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1353 - accuracy: 0.9490 - val_loss: 2.0553 - val_accuracy: 0.6216\n",
            "Epoch 612/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2265 - accuracy: 0.9116 - val_loss: 2.1512 - val_accuracy: 0.6216\n",
            "Epoch 613/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1583 - accuracy: 0.9320 - val_loss: 2.6218 - val_accuracy: 0.5676\n",
            "Epoch 614/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1887 - accuracy: 0.9490 - val_loss: 2.1495 - val_accuracy: 0.6216\n",
            "Epoch 615/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1614 - accuracy: 0.9388 - val_loss: 2.3340 - val_accuracy: 0.5946\n",
            "Epoch 616/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1641 - accuracy: 0.9490 - val_loss: 2.3504 - val_accuracy: 0.5541\n",
            "Epoch 617/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1784 - accuracy: 0.9354 - val_loss: 2.6201 - val_accuracy: 0.5270\n",
            "Epoch 618/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1236 - accuracy: 0.9490 - val_loss: 2.3592 - val_accuracy: 0.6216\n",
            "Epoch 619/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0870 - accuracy: 0.9762 - val_loss: 2.2829 - val_accuracy: 0.6757\n",
            "Epoch 620/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0848 - accuracy: 0.9796 - val_loss: 2.2464 - val_accuracy: 0.6081\n",
            "Epoch 621/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1015 - accuracy: 0.9762 - val_loss: 2.1369 - val_accuracy: 0.5946\n",
            "Epoch 622/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0824 - accuracy: 0.9728 - val_loss: 2.2914 - val_accuracy: 0.6081\n",
            "Epoch 623/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1006 - accuracy: 0.9660 - val_loss: 2.1698 - val_accuracy: 0.5946\n",
            "Epoch 624/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0702 - accuracy: 0.9830 - val_loss: 2.1602 - val_accuracy: 0.6216\n",
            "Epoch 625/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0765 - accuracy: 0.9796 - val_loss: 2.2922 - val_accuracy: 0.6351\n",
            "Epoch 626/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0796 - accuracy: 0.9762 - val_loss: 2.4856 - val_accuracy: 0.5676\n",
            "Epoch 627/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0988 - accuracy: 0.9728 - val_loss: 2.2549 - val_accuracy: 0.5946\n",
            "Epoch 628/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0573 - accuracy: 0.9898 - val_loss: 2.2280 - val_accuracy: 0.5811\n",
            "Epoch 629/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0603 - accuracy: 0.9932 - val_loss: 2.3643 - val_accuracy: 0.6081\n",
            "Epoch 630/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0517 - accuracy: 0.9932 - val_loss: 2.4003 - val_accuracy: 0.6081\n",
            "Epoch 631/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0473 - accuracy: 0.9898 - val_loss: 2.3032 - val_accuracy: 0.6351\n",
            "Epoch 632/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0493 - accuracy: 0.9898 - val_loss: 2.2578 - val_accuracy: 0.6216\n",
            "Epoch 633/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0554 - accuracy: 0.9864 - val_loss: 2.3288 - val_accuracy: 0.6351\n",
            "Epoch 634/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0467 - accuracy: 0.9898 - val_loss: 2.4347 - val_accuracy: 0.5946\n",
            "Epoch 635/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0533 - accuracy: 0.9864 - val_loss: 2.4773 - val_accuracy: 0.5946\n",
            "Epoch 636/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0476 - accuracy: 0.9898 - val_loss: 2.4093 - val_accuracy: 0.6216\n",
            "Epoch 637/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0550 - accuracy: 0.9932 - val_loss: 2.3934 - val_accuracy: 0.6216\n",
            "Epoch 638/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0525 - accuracy: 0.9898 - val_loss: 2.5113 - val_accuracy: 0.5946\n",
            "Epoch 639/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0467 - accuracy: 0.9932 - val_loss: 2.4481 - val_accuracy: 0.6622\n",
            "Epoch 640/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0690 - accuracy: 0.9592 - val_loss: 2.4773 - val_accuracy: 0.6216\n",
            "Epoch 641/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0533 - accuracy: 0.9864 - val_loss: 2.4478 - val_accuracy: 0.6216\n",
            "Epoch 642/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0612 - accuracy: 0.9898 - val_loss: 2.2225 - val_accuracy: 0.6081\n",
            "Epoch 643/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0834 - accuracy: 0.9694 - val_loss: 2.4761 - val_accuracy: 0.5946\n",
            "Epoch 644/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0653 - accuracy: 0.9694 - val_loss: 2.5406 - val_accuracy: 0.6081\n",
            "Epoch 645/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0481 - accuracy: 0.9932 - val_loss: 2.5397 - val_accuracy: 0.5676\n",
            "Epoch 646/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0482 - accuracy: 0.9898 - val_loss: 2.4514 - val_accuracy: 0.6216\n",
            "Epoch 647/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0432 - accuracy: 0.9932 - val_loss: 2.5096 - val_accuracy: 0.6081\n",
            "Epoch 648/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0511 - accuracy: 0.9864 - val_loss: 2.3903 - val_accuracy: 0.5811\n",
            "Epoch 649/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0687 - accuracy: 0.9830 - val_loss: 2.5102 - val_accuracy: 0.5811\n",
            "Epoch 650/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0446 - accuracy: 0.9932 - val_loss: 2.7625 - val_accuracy: 0.6081\n",
            "Epoch 651/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0634 - accuracy: 0.9830 - val_loss: 2.5022 - val_accuracy: 0.5811\n",
            "Epoch 652/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0825 - accuracy: 0.9660 - val_loss: 2.5648 - val_accuracy: 0.5811\n",
            "Epoch 653/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0832 - accuracy: 0.9762 - val_loss: 2.6197 - val_accuracy: 0.6081\n",
            "Epoch 654/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1190 - accuracy: 0.9694 - val_loss: 2.5433 - val_accuracy: 0.5405\n",
            "Epoch 655/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1541 - accuracy: 0.9388 - val_loss: 2.4035 - val_accuracy: 0.5946\n",
            "Epoch 656/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0768 - accuracy: 0.9694 - val_loss: 2.5434 - val_accuracy: 0.5946\n",
            "Epoch 657/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0696 - accuracy: 0.9796 - val_loss: 2.5025 - val_accuracy: 0.5946\n",
            "Epoch 658/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1272 - accuracy: 0.9592 - val_loss: 2.3702 - val_accuracy: 0.5541\n",
            "Epoch 659/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1133 - accuracy: 0.9524 - val_loss: 2.2687 - val_accuracy: 0.6081\n",
            "Epoch 660/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1044 - accuracy: 0.9694 - val_loss: 2.3329 - val_accuracy: 0.6216\n",
            "Epoch 661/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0685 - accuracy: 0.9796 - val_loss: 2.5078 - val_accuracy: 0.5946\n",
            "Epoch 662/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1278 - accuracy: 0.9592 - val_loss: 2.4459 - val_accuracy: 0.6081\n",
            "Epoch 663/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0696 - accuracy: 0.9898 - val_loss: 2.2633 - val_accuracy: 0.6081\n",
            "Epoch 664/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0800 - accuracy: 0.9762 - val_loss: 2.4412 - val_accuracy: 0.5676\n",
            "Epoch 665/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0630 - accuracy: 0.9830 - val_loss: 2.4923 - val_accuracy: 0.6081\n",
            "Epoch 666/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1321 - accuracy: 0.9490 - val_loss: 2.3781 - val_accuracy: 0.6216\n",
            "Epoch 667/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0891 - accuracy: 0.9694 - val_loss: 2.5467 - val_accuracy: 0.5946\n",
            "Epoch 668/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0890 - accuracy: 0.9762 - val_loss: 2.4940 - val_accuracy: 0.5946\n",
            "Epoch 669/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0636 - accuracy: 0.9830 - val_loss: 2.4015 - val_accuracy: 0.6081\n",
            "Epoch 670/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0464 - accuracy: 0.9898 - val_loss: 2.4686 - val_accuracy: 0.6081\n",
            "Epoch 671/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0475 - accuracy: 0.9830 - val_loss: 2.3041 - val_accuracy: 0.6216\n",
            "Epoch 672/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1688 - accuracy: 0.9388 - val_loss: 2.6142 - val_accuracy: 0.5676\n",
            "Epoch 673/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1747 - accuracy: 0.9388 - val_loss: 2.8095 - val_accuracy: 0.5541\n",
            "Epoch 674/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1112 - accuracy: 0.9558 - val_loss: 2.5364 - val_accuracy: 0.5676\n",
            "Epoch 675/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1332 - accuracy: 0.9490 - val_loss: 2.5608 - val_accuracy: 0.5946\n",
            "Epoch 676/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0853 - accuracy: 0.9694 - val_loss: 2.5658 - val_accuracy: 0.5811\n",
            "Epoch 677/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0446 - accuracy: 0.9932 - val_loss: 2.4678 - val_accuracy: 0.5946\n",
            "Epoch 678/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1107 - accuracy: 0.9660 - val_loss: 2.6252 - val_accuracy: 0.5946\n",
            "Epoch 679/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0758 - accuracy: 0.9830 - val_loss: 2.8471 - val_accuracy: 0.5676\n",
            "Epoch 680/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2109 - accuracy: 0.9320 - val_loss: 2.2648 - val_accuracy: 0.6216\n",
            "Epoch 681/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2702 - accuracy: 0.8980 - val_loss: 2.4067 - val_accuracy: 0.6216\n",
            "Epoch 682/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1276 - accuracy: 0.9592 - val_loss: 2.7009 - val_accuracy: 0.5270\n",
            "Epoch 683/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1540 - accuracy: 0.9388 - val_loss: 2.4213 - val_accuracy: 0.5541\n",
            "Epoch 684/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2840 - accuracy: 0.8776 - val_loss: 2.4036 - val_accuracy: 0.6216\n",
            "Epoch 685/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2262 - accuracy: 0.9116 - val_loss: 2.5390 - val_accuracy: 0.5946\n",
            "Epoch 686/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2224 - accuracy: 0.9354 - val_loss: 2.2238 - val_accuracy: 0.6351\n",
            "Epoch 687/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1728 - accuracy: 0.9252 - val_loss: 2.2277 - val_accuracy: 0.5676\n",
            "Epoch 688/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1482 - accuracy: 0.9524 - val_loss: 2.6311 - val_accuracy: 0.5811\n",
            "Epoch 689/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1031 - accuracy: 0.9660 - val_loss: 2.8820 - val_accuracy: 0.5811\n",
            "Epoch 690/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1136 - accuracy: 0.9660 - val_loss: 2.4959 - val_accuracy: 0.5811\n",
            "Epoch 691/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1725 - accuracy: 0.9252 - val_loss: 2.1594 - val_accuracy: 0.5946\n",
            "Epoch 692/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1959 - accuracy: 0.9252 - val_loss: 2.2631 - val_accuracy: 0.5676\n",
            "Epoch 693/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1539 - accuracy: 0.9456 - val_loss: 2.4582 - val_accuracy: 0.5676\n",
            "Epoch 694/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1692 - accuracy: 0.9388 - val_loss: 2.5292 - val_accuracy: 0.5676\n",
            "Epoch 695/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1396 - accuracy: 0.9626 - val_loss: 2.4065 - val_accuracy: 0.5946\n",
            "Epoch 696/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1038 - accuracy: 0.9694 - val_loss: 2.1607 - val_accuracy: 0.6081\n",
            "Epoch 697/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1277 - accuracy: 0.9490 - val_loss: 2.2124 - val_accuracy: 0.6216\n",
            "Epoch 698/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0887 - accuracy: 0.9762 - val_loss: 2.4994 - val_accuracy: 0.5811\n",
            "Epoch 699/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1214 - accuracy: 0.9524 - val_loss: 2.4291 - val_accuracy: 0.5811\n",
            "Epoch 700/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0780 - accuracy: 0.9762 - val_loss: 2.3494 - val_accuracy: 0.5135\n",
            "Epoch 701/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0682 - accuracy: 0.9830 - val_loss: 2.3702 - val_accuracy: 0.5541\n",
            "Epoch 702/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0603 - accuracy: 0.9898 - val_loss: 2.5456 - val_accuracy: 0.5541\n",
            "Epoch 703/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0558 - accuracy: 0.9898 - val_loss: 2.5551 - val_accuracy: 0.5946\n",
            "Epoch 704/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0448 - accuracy: 0.9932 - val_loss: 2.4415 - val_accuracy: 0.6351\n",
            "Epoch 705/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0458 - accuracy: 0.9864 - val_loss: 2.4319 - val_accuracy: 0.6081\n",
            "Epoch 706/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0454 - accuracy: 0.9864 - val_loss: 2.3594 - val_accuracy: 0.6081\n",
            "Epoch 707/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0378 - accuracy: 0.9932 - val_loss: 2.4241 - val_accuracy: 0.6081\n",
            "Epoch 708/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0339 - accuracy: 0.9932 - val_loss: 2.5083 - val_accuracy: 0.5946\n",
            "Epoch 709/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0371 - accuracy: 0.9932 - val_loss: 2.4865 - val_accuracy: 0.6216\n",
            "Epoch 710/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0312 - accuracy: 0.9932 - val_loss: 2.5027 - val_accuracy: 0.6351\n",
            "Epoch 711/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0321 - accuracy: 0.9932 - val_loss: 2.4761 - val_accuracy: 0.6216\n",
            "Epoch 712/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0311 - accuracy: 0.9932 - val_loss: 2.4517 - val_accuracy: 0.6216\n",
            "Epoch 713/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0298 - accuracy: 0.9932 - val_loss: 2.5188 - val_accuracy: 0.6351\n",
            "Epoch 714/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0276 - accuracy: 0.9932 - val_loss: 2.5949 - val_accuracy: 0.6351\n",
            "Epoch 715/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0284 - accuracy: 0.9932 - val_loss: 2.5782 - val_accuracy: 0.6351\n",
            "Epoch 716/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0271 - accuracy: 0.9932 - val_loss: 2.5428 - val_accuracy: 0.6486\n",
            "Epoch 717/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0264 - accuracy: 0.9932 - val_loss: 2.5184 - val_accuracy: 0.6216\n",
            "Epoch 718/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0262 - accuracy: 0.9932 - val_loss: 2.5089 - val_accuracy: 0.6216\n",
            "Epoch 719/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0267 - accuracy: 0.9898 - val_loss: 2.5095 - val_accuracy: 0.6216\n",
            "Epoch 720/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0242 - accuracy: 0.9932 - val_loss: 2.6295 - val_accuracy: 0.6216\n",
            "Epoch 721/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0278 - accuracy: 0.9932 - val_loss: 2.6199 - val_accuracy: 0.6351\n",
            "Epoch 722/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0250 - accuracy: 0.9932 - val_loss: 2.4724 - val_accuracy: 0.6351\n",
            "Epoch 723/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0275 - accuracy: 0.9864 - val_loss: 2.4832 - val_accuracy: 0.6486\n",
            "Epoch 724/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0286 - accuracy: 0.9864 - val_loss: 2.6255 - val_accuracy: 0.6081\n",
            "Epoch 725/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0255 - accuracy: 0.9898 - val_loss: 2.6727 - val_accuracy: 0.6081\n",
            "Epoch 726/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0255 - accuracy: 0.9898 - val_loss: 2.6406 - val_accuracy: 0.6081\n",
            "Epoch 727/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0296 - accuracy: 0.9864 - val_loss: 2.5603 - val_accuracy: 0.6351\n",
            "Epoch 728/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0252 - accuracy: 0.9932 - val_loss: 2.5518 - val_accuracy: 0.6351\n",
            "Epoch 729/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0291 - accuracy: 0.9932 - val_loss: 2.5006 - val_accuracy: 0.6216\n",
            "Epoch 730/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0289 - accuracy: 0.9966 - val_loss: 2.4211 - val_accuracy: 0.6351\n",
            "Epoch 731/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0320 - accuracy: 0.9898 - val_loss: 2.5241 - val_accuracy: 0.6081\n",
            "Epoch 732/800\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.0334 - accuracy: 0.9898 - val_loss: 2.5762 - val_accuracy: 0.6081\n",
            "Epoch 733/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0251 - accuracy: 0.9932 - val_loss: 2.6451 - val_accuracy: 0.6081\n",
            "Epoch 734/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0273 - accuracy: 0.9932 - val_loss: 2.6694 - val_accuracy: 0.6081\n",
            "Epoch 735/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0283 - accuracy: 0.9932 - val_loss: 2.7182 - val_accuracy: 0.5811\n",
            "Epoch 736/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0288 - accuracy: 0.9932 - val_loss: 2.5898 - val_accuracy: 0.6216\n",
            "Epoch 737/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0236 - accuracy: 0.9932 - val_loss: 2.5495 - val_accuracy: 0.6757\n",
            "Epoch 738/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0285 - accuracy: 0.9898 - val_loss: 2.5927 - val_accuracy: 0.6486\n",
            "Epoch 739/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0256 - accuracy: 0.9932 - val_loss: 2.6412 - val_accuracy: 0.6216\n",
            "Epoch 740/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0240 - accuracy: 0.9932 - val_loss: 2.5359 - val_accuracy: 0.5946\n",
            "Epoch 741/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0309 - accuracy: 0.9898 - val_loss: 2.6172 - val_accuracy: 0.5946\n",
            "Epoch 742/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0255 - accuracy: 0.9898 - val_loss: 2.7626 - val_accuracy: 0.5946\n",
            "Epoch 743/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0288 - accuracy: 0.9932 - val_loss: 2.7230 - val_accuracy: 0.6081\n",
            "Epoch 744/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0427 - accuracy: 0.9830 - val_loss: 2.5779 - val_accuracy: 0.6216\n",
            "Epoch 745/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0267 - accuracy: 0.9932 - val_loss: 2.5023 - val_accuracy: 0.5811\n",
            "Epoch 746/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0390 - accuracy: 0.9932 - val_loss: 2.4277 - val_accuracy: 0.6216\n",
            "Epoch 747/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0310 - accuracy: 0.9932 - val_loss: 2.6773 - val_accuracy: 0.6081\n",
            "Epoch 748/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0351 - accuracy: 0.9864 - val_loss: 2.5152 - val_accuracy: 0.6216\n",
            "Epoch 749/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0377 - accuracy: 0.9898 - val_loss: 2.7113 - val_accuracy: 0.6081\n",
            "Epoch 750/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0485 - accuracy: 0.9864 - val_loss: 2.6820 - val_accuracy: 0.6216\n",
            "Epoch 751/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0486 - accuracy: 0.9864 - val_loss: 2.5795 - val_accuracy: 0.6216\n",
            "Epoch 752/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0360 - accuracy: 0.9898 - val_loss: 2.6497 - val_accuracy: 0.5676\n",
            "Epoch 753/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0495 - accuracy: 0.9864 - val_loss: 2.5481 - val_accuracy: 0.6351\n",
            "Epoch 754/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0457 - accuracy: 0.9762 - val_loss: 2.5238 - val_accuracy: 0.6351\n",
            "Epoch 755/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0455 - accuracy: 0.9830 - val_loss: 2.6922 - val_accuracy: 0.5946\n",
            "Epoch 756/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0416 - accuracy: 0.9864 - val_loss: 2.6225 - val_accuracy: 0.6351\n",
            "Epoch 757/800\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0268 - accuracy: 0.9932 - val_loss: 2.5067 - val_accuracy: 0.6486\n",
            "Epoch 758/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0446 - accuracy: 0.9898 - val_loss: 2.6298 - val_accuracy: 0.6216\n",
            "Epoch 759/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0304 - accuracy: 0.9932 - val_loss: 2.7293 - val_accuracy: 0.6216\n",
            "Epoch 760/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0292 - accuracy: 0.9932 - val_loss: 2.7594 - val_accuracy: 0.5946\n",
            "Epoch 761/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0329 - accuracy: 0.9932 - val_loss: 2.7387 - val_accuracy: 0.5811\n",
            "Epoch 762/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0273 - accuracy: 0.9932 - val_loss: 2.7321 - val_accuracy: 0.5946\n",
            "Epoch 763/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0264 - accuracy: 0.9932 - val_loss: 2.7249 - val_accuracy: 0.6081\n",
            "Epoch 764/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0259 - accuracy: 0.9932 - val_loss: 2.7339 - val_accuracy: 0.6081\n",
            "Epoch 765/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0265 - accuracy: 0.9932 - val_loss: 2.6709 - val_accuracy: 0.6216\n",
            "Epoch 766/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0242 - accuracy: 0.9898 - val_loss: 2.7492 - val_accuracy: 0.5946\n",
            "Epoch 767/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0271 - accuracy: 0.9932 - val_loss: 2.8058 - val_accuracy: 0.6081\n",
            "Epoch 768/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0242 - accuracy: 0.9932 - val_loss: 2.8319 - val_accuracy: 0.6081\n",
            "Epoch 769/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0284 - accuracy: 0.9932 - val_loss: 2.8642 - val_accuracy: 0.5946\n",
            "Epoch 770/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 2.7111 - val_accuracy: 0.6216\n",
            "Epoch 771/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.0284 - accuracy: 0.9864 - val_loss: 2.7037 - val_accuracy: 0.6216\n",
            "Epoch 772/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0220 - accuracy: 0.9966 - val_loss: 2.7835 - val_accuracy: 0.6081\n",
            "Epoch 773/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0252 - accuracy: 0.9932 - val_loss: 2.7361 - val_accuracy: 0.6351\n",
            "Epoch 774/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0356 - accuracy: 0.9898 - val_loss: 2.8052 - val_accuracy: 0.6351\n",
            "Epoch 775/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0243 - accuracy: 0.9932 - val_loss: 2.8530 - val_accuracy: 0.6081\n",
            "Epoch 776/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0230 - accuracy: 0.9932 - val_loss: 2.8118 - val_accuracy: 0.6081\n",
            "Epoch 777/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0257 - accuracy: 0.9932 - val_loss: 2.7274 - val_accuracy: 0.6486\n",
            "Epoch 778/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0303 - accuracy: 0.9830 - val_loss: 2.6521 - val_accuracy: 0.6486\n",
            "Epoch 779/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0230 - accuracy: 0.9864 - val_loss: 2.6350 - val_accuracy: 0.6351\n",
            "Epoch 780/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0279 - accuracy: 0.9932 - val_loss: 2.6669 - val_accuracy: 0.6216\n",
            "Epoch 781/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0263 - accuracy: 0.9932 - val_loss: 2.6947 - val_accuracy: 0.6216\n",
            "Epoch 782/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 2.7106 - val_accuracy: 0.6216\n",
            "Epoch 783/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0199 - accuracy: 0.9932 - val_loss: 2.7856 - val_accuracy: 0.5946\n",
            "Epoch 784/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0204 - accuracy: 0.9898 - val_loss: 2.8971 - val_accuracy: 0.5676\n",
            "Epoch 785/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0224 - accuracy: 0.9932 - val_loss: 2.9318 - val_accuracy: 0.5541\n",
            "Epoch 786/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.0261 - accuracy: 0.9932 - val_loss: 2.8163 - val_accuracy: 0.6081\n",
            "Epoch 787/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0215 - accuracy: 0.9932 - val_loss: 2.7899 - val_accuracy: 0.6081\n",
            "Epoch 788/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0201 - accuracy: 0.9932 - val_loss: 2.8518 - val_accuracy: 0.6216\n",
            "Epoch 789/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.0236 - accuracy: 0.9932 - val_loss: 2.8123 - val_accuracy: 0.6216\n",
            "Epoch 790/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.0305 - accuracy: 0.9898 - val_loss: 2.8147 - val_accuracy: 0.5946\n",
            "Epoch 791/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0310 - accuracy: 0.9898 - val_loss: 2.9636 - val_accuracy: 0.5270\n",
            "Epoch 792/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.0893 - accuracy: 0.9660 - val_loss: 2.8020 - val_accuracy: 0.5811\n",
            "Epoch 793/800\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1094 - accuracy: 0.9558 - val_loss: 3.0180 - val_accuracy: 0.5135\n",
            "Epoch 794/800\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5340 - accuracy: 0.8503 - val_loss: 1.8658 - val_accuracy: 0.6351\n",
            "Epoch 795/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.8582 - accuracy: 0.6973 - val_loss: 2.1403 - val_accuracy: 0.5405\n",
            "Epoch 796/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.0371 - accuracy: 0.7279 - val_loss: 2.7843 - val_accuracy: 0.6081\n",
            "Epoch 797/800\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.2311 - accuracy: 0.7211 - val_loss: 2.3330 - val_accuracy: 0.5270\n",
            "Epoch 798/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8592 - accuracy: 0.7313 - val_loss: 1.5316 - val_accuracy: 0.5000\n",
            "Epoch 799/800\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.6334 - accuracy: 0.7483 - val_loss: 1.2387 - val_accuracy: 0.5946\n",
            "Epoch 800/800\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6510 - accuracy: 0.7245 - val_loss: 1.1962 - val_accuracy: 0.6216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation with the RNN\n",
        "y_valid_RNN = RNN_model.predict(feature_valid)\n",
        "valid_y_RNN = y_valid_RNN.copy()\n",
        "for i in range(len(y_valid_RNN)):\n",
        "    j = np.where(y_valid_RNN[i] == np.amax(y_valid_RNN[i]))\n",
        "    valid_y_RNN[i] = [0, 0, 0]\n",
        "    valid_y_RNN[i][j] = 1\n",
        "\n",
        "# print acc and report\n",
        "print(accuracy_score(label_valid_y,valid_y_RNN))\n",
        "print(classification_report(label_valid_y,valid_y_RNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_RNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH_wAo-aj7pp",
        "outputId": "b997e874-3bac-48f5-aad5-ce5414e694e4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 7ms/step\n",
            "0.6081081081081081\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.53      0.64        15\n",
            "           1       0.61      0.75      0.67        36\n",
            "           2       0.50      0.43      0.47        23\n",
            "\n",
            "   micro avg       0.61      0.61      0.61        74\n",
            "   macro avg       0.64      0.57      0.59        74\n",
            "weighted avg       0.62      0.61      0.60        74\n",
            " samples avg       0.61      0.61      0.61        74\n",
            "\n",
            "auc score:  0.6734617974198431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_RNN = RNN_model.predict(feature_test)\n",
        "# convert the test vector\n",
        "test_y_RNN = y_test_RNN.copy()\n",
        "for i in range(len(y_test_RNN)):\n",
        "    j = np.where(y_test_RNN[i] == np.amax(y_test_RNN[i]))\n",
        "    test_y_RNN[i] = [0, 0, 0]\n",
        "    test_y_RNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_test_y,test_y_RNN))\n",
        "print(classification_report(label_test_y,test_y_RNN))\n",
        "print(\"auc score: \",roc_auc_score(label_test_y,test_y_RNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE9_m0yUj9e-",
        "outputId": "0a5d6b09-8220-4320-a934-88f8b524ba9c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 7ms/step\n",
            "0.6451612903225806\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.47      0.53        17\n",
            "           1       0.67      0.86      0.76        43\n",
            "           2       0.60      0.45      0.52        33\n",
            "\n",
            "   micro avg       0.65      0.65      0.65        93\n",
            "   macro avg       0.63      0.60      0.60        93\n",
            "weighted avg       0.64      0.65      0.63        93\n",
            " samples avg       0.65      0.65      0.65        93\n",
            "\n",
            "auc score:  0.6988571109612941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Bidirectional, GRU, LSTM, Attention, GlobalMaxPooling1D, Dense, Dropout, Input, Concatenate, Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "\n",
        "def create_CNN_RNN_model():\n",
        "    inputs = Input(shape=(maxlen,))\n",
        "    embeddings = Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False)(inputs)\n",
        "\n",
        "    conv1 = Conv1D(128, 3, activation='relu')(embeddings)\n",
        "    pool1 = MaxPooling1D(3)(conv1)\n",
        "    conv2 = Conv1D(128, 3, activation='relu')(pool1)\n",
        "    pool2 = MaxPooling1D(3)(conv2)\n",
        "\n",
        "    gru1 = Bidirectional(GRU(128, return_sequences=True))(pool2)\n",
        "    gru2 = Bidirectional(GRU(64, return_sequences=True))(gru1)\n",
        "\n",
        "    lstm1 = Bidirectional(LSTM(128, return_sequences=True))(pool2)\n",
        "    lstm2 = Bidirectional(LSTM(64, return_sequences=True))(lstm1)\n",
        "\n",
        "    concat = Concatenate(axis=-1)([gru2, lstm2])\n",
        "\n",
        "    attention = Attention()([concat, concat])\n",
        "\n",
        "    pool = GlobalMaxPooling1D()(attention)\n",
        "\n",
        "    dense1 = Dense(128, activation='relu')(pool)\n",
        "    dropout1 = Dropout(0.5)(dense1)\n",
        "\n",
        "    dense2 = Dense(64, activation='relu')(dropout1)\n",
        "    dropout2 = Dropout(0.5)(dense2)\n",
        "\n",
        "    outputs = Dense(3, activation='softmax')(dropout2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "J_ylya-Sj_8a"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_RNN_model = create_CNN_RNN_model()\n",
        "CNN_RNN_history = CNN_RNN_model.fit(feature_train, label_train_y, epochs=750, batch_size=128,validation_data=(feature_valid, label_valid_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETMdc-cvkHBR",
        "outputId": "56981ea4-b156-4f5f-ae6c-b7874ab10520"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/750\n",
            "3/3 [==============================] - 16s 1s/step - loss: 1.0204 - accuracy: 0.5374 - val_loss: 1.0792 - val_accuracy: 0.4865\n",
            "Epoch 2/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.0279 - accuracy: 0.5306 - val_loss: 1.0389 - val_accuracy: 0.4865\n",
            "Epoch 3/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.0366 - accuracy: 0.5170 - val_loss: 1.0408 - val_accuracy: 0.4865\n",
            "Epoch 4/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.0300 - accuracy: 0.5578 - val_loss: 1.0419 - val_accuracy: 0.4865\n",
            "Epoch 5/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.0017 - accuracy: 0.5714 - val_loss: 1.0471 - val_accuracy: 0.4865\n",
            "Epoch 6/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9766 - accuracy: 0.5612 - val_loss: 1.0561 - val_accuracy: 0.4865\n",
            "Epoch 7/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 1.0067 - accuracy: 0.5748 - val_loss: 1.0432 - val_accuracy: 0.4865\n",
            "Epoch 8/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9928 - accuracy: 0.5714 - val_loss: 1.0420 - val_accuracy: 0.4865\n",
            "Epoch 9/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9869 - accuracy: 0.5510 - val_loss: 1.0414 - val_accuracy: 0.4865\n",
            "Epoch 10/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9727 - accuracy: 0.5748 - val_loss: 1.0675 - val_accuracy: 0.4865\n",
            "Epoch 11/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9978 - accuracy: 0.5680 - val_loss: 1.0457 - val_accuracy: 0.4865\n",
            "Epoch 12/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.0129 - accuracy: 0.5646 - val_loss: 1.0438 - val_accuracy: 0.4865\n",
            "Epoch 13/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.0149 - accuracy: 0.5646 - val_loss: 1.0446 - val_accuracy: 0.4865\n",
            "Epoch 14/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.9784 - accuracy: 0.5714 - val_loss: 1.0463 - val_accuracy: 0.4865\n",
            "Epoch 15/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.0012 - accuracy: 0.5680 - val_loss: 1.0552 - val_accuracy: 0.4865\n",
            "Epoch 16/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9892 - accuracy: 0.5714 - val_loss: 1.0459 - val_accuracy: 0.4865\n",
            "Epoch 17/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9783 - accuracy: 0.5680 - val_loss: 1.0437 - val_accuracy: 0.4865\n",
            "Epoch 18/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 1.0118 - accuracy: 0.5646 - val_loss: 1.0459 - val_accuracy: 0.4865\n",
            "Epoch 19/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9871 - accuracy: 0.5748 - val_loss: 1.0555 - val_accuracy: 0.4865\n",
            "Epoch 20/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9906 - accuracy: 0.5714 - val_loss: 1.0739 - val_accuracy: 0.4865\n",
            "Epoch 21/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9608 - accuracy: 0.5748 - val_loss: 1.0706 - val_accuracy: 0.4865\n",
            "Epoch 22/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9932 - accuracy: 0.5646 - val_loss: 1.0554 - val_accuracy: 0.4865\n",
            "Epoch 23/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9723 - accuracy: 0.5714 - val_loss: 1.0524 - val_accuracy: 0.4865\n",
            "Epoch 24/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9707 - accuracy: 0.5714 - val_loss: 1.0730 - val_accuracy: 0.4865\n",
            "Epoch 25/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9587 - accuracy: 0.5680 - val_loss: 1.0776 - val_accuracy: 0.4865\n",
            "Epoch 26/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9609 - accuracy: 0.5714 - val_loss: 1.0710 - val_accuracy: 0.4865\n",
            "Epoch 27/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9574 - accuracy: 0.5714 - val_loss: 1.0517 - val_accuracy: 0.4865\n",
            "Epoch 28/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9480 - accuracy: 0.5714 - val_loss: 1.0857 - val_accuracy: 0.4865\n",
            "Epoch 29/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9538 - accuracy: 0.5714 - val_loss: 1.0960 - val_accuracy: 0.4865\n",
            "Epoch 30/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.9370 - accuracy: 0.5714 - val_loss: 1.0511 - val_accuracy: 0.4865\n",
            "Epoch 31/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9717 - accuracy: 0.5782 - val_loss: 1.0497 - val_accuracy: 0.4865\n",
            "Epoch 32/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9476 - accuracy: 0.5714 - val_loss: 1.1042 - val_accuracy: 0.4865\n",
            "Epoch 33/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9392 - accuracy: 0.5680 - val_loss: 1.0642 - val_accuracy: 0.4865\n",
            "Epoch 34/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9300 - accuracy: 0.5714 - val_loss: 1.0447 - val_accuracy: 0.4865\n",
            "Epoch 35/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9225 - accuracy: 0.5816 - val_loss: 1.0762 - val_accuracy: 0.4865\n",
            "Epoch 36/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.9272 - accuracy: 0.5782 - val_loss: 1.0879 - val_accuracy: 0.4865\n",
            "Epoch 37/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.9345 - accuracy: 0.5782 - val_loss: 1.0480 - val_accuracy: 0.4189\n",
            "Epoch 38/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.9138 - accuracy: 0.5884 - val_loss: 1.0837 - val_accuracy: 0.4865\n",
            "Epoch 39/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.8789 - accuracy: 0.6088 - val_loss: 1.0742 - val_accuracy: 0.4730\n",
            "Epoch 40/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.8957 - accuracy: 0.6020 - val_loss: 1.1379 - val_accuracy: 0.4865\n",
            "Epoch 41/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.8844 - accuracy: 0.6122 - val_loss: 1.0783 - val_accuracy: 0.4865\n",
            "Epoch 42/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.8755 - accuracy: 0.6088 - val_loss: 1.0454 - val_accuracy: 0.4595\n",
            "Epoch 43/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.8508 - accuracy: 0.6327 - val_loss: 1.1070 - val_accuracy: 0.5000\n",
            "Epoch 44/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.8799 - accuracy: 0.6224 - val_loss: 1.0818 - val_accuracy: 0.4865\n",
            "Epoch 45/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.8434 - accuracy: 0.6190 - val_loss: 1.0704 - val_accuracy: 0.4865\n",
            "Epoch 46/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8353 - accuracy: 0.6224 - val_loss: 1.0529 - val_accuracy: 0.4730\n",
            "Epoch 47/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8346 - accuracy: 0.6190 - val_loss: 1.0704 - val_accuracy: 0.4865\n",
            "Epoch 48/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8057 - accuracy: 0.6224 - val_loss: 1.0625 - val_accuracy: 0.4459\n",
            "Epoch 49/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8170 - accuracy: 0.6293 - val_loss: 1.1283 - val_accuracy: 0.5000\n",
            "Epoch 50/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8523 - accuracy: 0.6224 - val_loss: 1.0323 - val_accuracy: 0.4595\n",
            "Epoch 51/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.8084 - accuracy: 0.6463 - val_loss: 1.0320 - val_accuracy: 0.5000\n",
            "Epoch 52/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7942 - accuracy: 0.6327 - val_loss: 1.0552 - val_accuracy: 0.4459\n",
            "Epoch 53/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.7891 - accuracy: 0.6361 - val_loss: 1.1050 - val_accuracy: 0.5000\n",
            "Epoch 54/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.8031 - accuracy: 0.6259 - val_loss: 1.0397 - val_accuracy: 0.4865\n",
            "Epoch 55/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7640 - accuracy: 0.6463 - val_loss: 1.0809 - val_accuracy: 0.5000\n",
            "Epoch 56/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7735 - accuracy: 0.6361 - val_loss: 1.1346 - val_accuracy: 0.4865\n",
            "Epoch 57/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8158 - accuracy: 0.6293 - val_loss: 1.1467 - val_accuracy: 0.4730\n",
            "Epoch 58/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.8872 - accuracy: 0.5850 - val_loss: 0.9947 - val_accuracy: 0.4865\n",
            "Epoch 59/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.8114 - accuracy: 0.6463 - val_loss: 1.0133 - val_accuracy: 0.4459\n",
            "Epoch 60/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.8073 - accuracy: 0.6633 - val_loss: 1.1163 - val_accuracy: 0.5000\n",
            "Epoch 61/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7571 - accuracy: 0.6599 - val_loss: 1.0700 - val_accuracy: 0.4595\n",
            "Epoch 62/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7465 - accuracy: 0.6395 - val_loss: 1.1080 - val_accuracy: 0.5135\n",
            "Epoch 63/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7403 - accuracy: 0.6463 - val_loss: 1.1145 - val_accuracy: 0.4324\n",
            "Epoch 64/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7559 - accuracy: 0.6497 - val_loss: 1.2282 - val_accuracy: 0.5000\n",
            "Epoch 65/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.8097 - accuracy: 0.6429 - val_loss: 1.0690 - val_accuracy: 0.4459\n",
            "Epoch 66/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7812 - accuracy: 0.6395 - val_loss: 1.0692 - val_accuracy: 0.5135\n",
            "Epoch 67/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.7360 - accuracy: 0.6361 - val_loss: 1.0695 - val_accuracy: 0.5135\n",
            "Epoch 68/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7370 - accuracy: 0.6633 - val_loss: 1.0530 - val_accuracy: 0.5000\n",
            "Epoch 69/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7378 - accuracy: 0.6599 - val_loss: 1.0857 - val_accuracy: 0.5135\n",
            "Epoch 70/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7046 - accuracy: 0.6599 - val_loss: 1.0466 - val_accuracy: 0.5135\n",
            "Epoch 71/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.7003 - accuracy: 0.6871 - val_loss: 1.1218 - val_accuracy: 0.5135\n",
            "Epoch 72/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7001 - accuracy: 0.6667 - val_loss: 1.1018 - val_accuracy: 0.4865\n",
            "Epoch 73/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7004 - accuracy: 0.7007 - val_loss: 1.2045 - val_accuracy: 0.5270\n",
            "Epoch 74/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7203 - accuracy: 0.6701 - val_loss: 1.2507 - val_accuracy: 0.4324\n",
            "Epoch 75/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.7162 - accuracy: 0.7041 - val_loss: 1.1803 - val_accuracy: 0.5000\n",
            "Epoch 76/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.7261 - accuracy: 0.6905 - val_loss: 1.0117 - val_accuracy: 0.5000\n",
            "Epoch 77/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7086 - accuracy: 0.7245 - val_loss: 1.1398 - val_accuracy: 0.4865\n",
            "Epoch 78/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6946 - accuracy: 0.6837 - val_loss: 1.1980 - val_accuracy: 0.4865\n",
            "Epoch 79/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6534 - accuracy: 0.7211 - val_loss: 1.1379 - val_accuracy: 0.4865\n",
            "Epoch 80/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6512 - accuracy: 0.7041 - val_loss: 1.1359 - val_accuracy: 0.5000\n",
            "Epoch 81/750\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.6586 - accuracy: 0.7041 - val_loss: 1.1012 - val_accuracy: 0.4865\n",
            "Epoch 82/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6657 - accuracy: 0.7143 - val_loss: 1.1284 - val_accuracy: 0.4865\n",
            "Epoch 83/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6548 - accuracy: 0.6973 - val_loss: 1.1268 - val_accuracy: 0.5676\n",
            "Epoch 84/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6524 - accuracy: 0.7177 - val_loss: 1.1029 - val_accuracy: 0.5270\n",
            "Epoch 85/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5946 - accuracy: 0.7347 - val_loss: 1.2101 - val_accuracy: 0.5270\n",
            "Epoch 86/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6056 - accuracy: 0.7211 - val_loss: 1.2803 - val_accuracy: 0.5000\n",
            "Epoch 87/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5903 - accuracy: 0.7449 - val_loss: 1.2470 - val_accuracy: 0.5541\n",
            "Epoch 88/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5718 - accuracy: 0.7483 - val_loss: 1.1522 - val_accuracy: 0.4595\n",
            "Epoch 89/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6222 - accuracy: 0.7551 - val_loss: 1.2024 - val_accuracy: 0.4865\n",
            "Epoch 90/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5691 - accuracy: 0.7687 - val_loss: 1.2898 - val_accuracy: 0.5405\n",
            "Epoch 91/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5349 - accuracy: 0.7823 - val_loss: 1.5999 - val_accuracy: 0.5135\n",
            "Epoch 92/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.6109 - accuracy: 0.7483 - val_loss: 1.5680 - val_accuracy: 0.5405\n",
            "Epoch 93/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6755 - accuracy: 0.7211 - val_loss: 1.2028 - val_accuracy: 0.4730\n",
            "Epoch 94/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6159 - accuracy: 0.7415 - val_loss: 1.1539 - val_accuracy: 0.5000\n",
            "Epoch 95/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5793 - accuracy: 0.7483 - val_loss: 1.1864 - val_accuracy: 0.5270\n",
            "Epoch 96/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5747 - accuracy: 0.7483 - val_loss: 1.1234 - val_accuracy: 0.5270\n",
            "Epoch 97/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5559 - accuracy: 0.7653 - val_loss: 1.2429 - val_accuracy: 0.5405\n",
            "Epoch 98/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5203 - accuracy: 0.7823 - val_loss: 1.2115 - val_accuracy: 0.5135\n",
            "Epoch 99/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5077 - accuracy: 0.7823 - val_loss: 1.4274 - val_accuracy: 0.5405\n",
            "Epoch 100/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5505 - accuracy: 0.7857 - val_loss: 1.2446 - val_accuracy: 0.5405\n",
            "Epoch 101/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5256 - accuracy: 0.7891 - val_loss: 1.5216 - val_accuracy: 0.5270\n",
            "Epoch 102/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5405 - accuracy: 0.7517 - val_loss: 1.2400 - val_accuracy: 0.5135\n",
            "Epoch 103/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5078 - accuracy: 0.7891 - val_loss: 1.4549 - val_accuracy: 0.5135\n",
            "Epoch 104/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5193 - accuracy: 0.7755 - val_loss: 1.3224 - val_accuracy: 0.5135\n",
            "Epoch 105/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5007 - accuracy: 0.8061 - val_loss: 1.2759 - val_accuracy: 0.5541\n",
            "Epoch 106/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4582 - accuracy: 0.8095 - val_loss: 1.3804 - val_accuracy: 0.5270\n",
            "Epoch 107/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4565 - accuracy: 0.7993 - val_loss: 1.3828 - val_accuracy: 0.5000\n",
            "Epoch 108/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4296 - accuracy: 0.7925 - val_loss: 1.3697 - val_accuracy: 0.5405\n",
            "Epoch 109/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4511 - accuracy: 0.8061 - val_loss: 1.4273 - val_accuracy: 0.5811\n",
            "Epoch 110/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4576 - accuracy: 0.8265 - val_loss: 1.3682 - val_accuracy: 0.5541\n",
            "Epoch 111/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4340 - accuracy: 0.8197 - val_loss: 1.5888 - val_accuracy: 0.5541\n",
            "Epoch 112/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4945 - accuracy: 0.7857 - val_loss: 1.4442 - val_accuracy: 0.5270\n",
            "Epoch 113/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5082 - accuracy: 0.7857 - val_loss: 1.5402 - val_accuracy: 0.5270\n",
            "Epoch 114/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4854 - accuracy: 0.7789 - val_loss: 1.3497 - val_accuracy: 0.5676\n",
            "Epoch 115/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4231 - accuracy: 0.8333 - val_loss: 1.3653 - val_accuracy: 0.5811\n",
            "Epoch 116/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4187 - accuracy: 0.8333 - val_loss: 1.5556 - val_accuracy: 0.5946\n",
            "Epoch 117/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4185 - accuracy: 0.8061 - val_loss: 1.5209 - val_accuracy: 0.5676\n",
            "Epoch 118/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3749 - accuracy: 0.8469 - val_loss: 1.4296 - val_accuracy: 0.6351\n",
            "Epoch 119/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3948 - accuracy: 0.8401 - val_loss: 1.4850 - val_accuracy: 0.5811\n",
            "Epoch 120/750\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4209 - accuracy: 0.8299 - val_loss: 1.4558 - val_accuracy: 0.5946\n",
            "Epoch 121/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3542 - accuracy: 0.8537 - val_loss: 1.5492 - val_accuracy: 0.5541\n",
            "Epoch 122/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3580 - accuracy: 0.8367 - val_loss: 1.5376 - val_accuracy: 0.5676\n",
            "Epoch 123/750\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3421 - accuracy: 0.8605 - val_loss: 1.5743 - val_accuracy: 0.6486\n",
            "Epoch 124/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3403 - accuracy: 0.8639 - val_loss: 1.7216 - val_accuracy: 0.6081\n",
            "Epoch 125/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3250 - accuracy: 0.8673 - val_loss: 1.9663 - val_accuracy: 0.5811\n",
            "Epoch 126/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3426 - accuracy: 0.8605 - val_loss: 1.9432 - val_accuracy: 0.5946\n",
            "Epoch 127/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3883 - accuracy: 0.8367 - val_loss: 1.9546 - val_accuracy: 0.5676\n",
            "Epoch 128/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3995 - accuracy: 0.8299 - val_loss: 1.8973 - val_accuracy: 0.5405\n",
            "Epoch 129/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3953 - accuracy: 0.8333 - val_loss: 1.7840 - val_accuracy: 0.5946\n",
            "Epoch 130/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3505 - accuracy: 0.8469 - val_loss: 1.8012 - val_accuracy: 0.5811\n",
            "Epoch 131/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4184 - accuracy: 0.8129 - val_loss: 1.6750 - val_accuracy: 0.6081\n",
            "Epoch 132/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3459 - accuracy: 0.8469 - val_loss: 1.7346 - val_accuracy: 0.5946\n",
            "Epoch 133/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3761 - accuracy: 0.8435 - val_loss: 1.6896 - val_accuracy: 0.6081\n",
            "Epoch 134/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3661 - accuracy: 0.8537 - val_loss: 1.7470 - val_accuracy: 0.5811\n",
            "Epoch 135/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3462 - accuracy: 0.8503 - val_loss: 1.7317 - val_accuracy: 0.5676\n",
            "Epoch 136/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3259 - accuracy: 0.8639 - val_loss: 1.6212 - val_accuracy: 0.5811\n",
            "Epoch 137/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3146 - accuracy: 0.8673 - val_loss: 1.6491 - val_accuracy: 0.5946\n",
            "Epoch 138/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3009 - accuracy: 0.8673 - val_loss: 1.9307 - val_accuracy: 0.5811\n",
            "Epoch 139/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2852 - accuracy: 0.8707 - val_loss: 1.8639 - val_accuracy: 0.5946\n",
            "Epoch 140/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3054 - accuracy: 0.8673 - val_loss: 1.5717 - val_accuracy: 0.5811\n",
            "Epoch 141/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3253 - accuracy: 0.8605 - val_loss: 1.6813 - val_accuracy: 0.5946\n",
            "Epoch 142/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2638 - accuracy: 0.8912 - val_loss: 1.8702 - val_accuracy: 0.6216\n",
            "Epoch 143/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2947 - accuracy: 0.8707 - val_loss: 1.9348 - val_accuracy: 0.6216\n",
            "Epoch 144/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2690 - accuracy: 0.8810 - val_loss: 1.9258 - val_accuracy: 0.6081\n",
            "Epoch 145/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2811 - accuracy: 0.8810 - val_loss: 1.9374 - val_accuracy: 0.5946\n",
            "Epoch 146/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2665 - accuracy: 0.8741 - val_loss: 1.9843 - val_accuracy: 0.6081\n",
            "Epoch 147/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2757 - accuracy: 0.8605 - val_loss: 1.9682 - val_accuracy: 0.5946\n",
            "Epoch 148/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2513 - accuracy: 0.8946 - val_loss: 1.9801 - val_accuracy: 0.6351\n",
            "Epoch 149/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2568 - accuracy: 0.8741 - val_loss: 2.0127 - val_accuracy: 0.6216\n",
            "Epoch 150/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2536 - accuracy: 0.8912 - val_loss: 2.1036 - val_accuracy: 0.5676\n",
            "Epoch 151/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2920 - accuracy: 0.8810 - val_loss: 2.0063 - val_accuracy: 0.6216\n",
            "Epoch 152/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2401 - accuracy: 0.8912 - val_loss: 2.1111 - val_accuracy: 0.6351\n",
            "Epoch 153/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2567 - accuracy: 0.8810 - val_loss: 2.3459 - val_accuracy: 0.6216\n",
            "Epoch 154/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2546 - accuracy: 0.8878 - val_loss: 2.4384 - val_accuracy: 0.6081\n",
            "Epoch 155/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2341 - accuracy: 0.8946 - val_loss: 2.3519 - val_accuracy: 0.6081\n",
            "Epoch 156/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2248 - accuracy: 0.8912 - val_loss: 2.3700 - val_accuracy: 0.6351\n",
            "Epoch 157/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2184 - accuracy: 0.9014 - val_loss: 2.4375 - val_accuracy: 0.6216\n",
            "Epoch 158/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2363 - accuracy: 0.9048 - val_loss: 2.4773 - val_accuracy: 0.6081\n",
            "Epoch 159/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2178 - accuracy: 0.9048 - val_loss: 2.6568 - val_accuracy: 0.5541\n",
            "Epoch 160/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2457 - accuracy: 0.8844 - val_loss: 2.6085 - val_accuracy: 0.5946\n",
            "Epoch 161/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2471 - accuracy: 0.8980 - val_loss: 2.6233 - val_accuracy: 0.6081\n",
            "Epoch 162/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2301 - accuracy: 0.8980 - val_loss: 2.6038 - val_accuracy: 0.5946\n",
            "Epoch 163/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2429 - accuracy: 0.9014 - val_loss: 2.6635 - val_accuracy: 0.5811\n",
            "Epoch 164/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2613 - accuracy: 0.8946 - val_loss: 2.9192 - val_accuracy: 0.6081\n",
            "Epoch 165/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3132 - accuracy: 0.8776 - val_loss: 2.5534 - val_accuracy: 0.5811\n",
            "Epoch 166/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2805 - accuracy: 0.8707 - val_loss: 2.5657 - val_accuracy: 0.6081\n",
            "Epoch 167/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2528 - accuracy: 0.8810 - val_loss: 2.6835 - val_accuracy: 0.5946\n",
            "Epoch 168/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2361 - accuracy: 0.8946 - val_loss: 2.8216 - val_accuracy: 0.5811\n",
            "Epoch 169/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2493 - accuracy: 0.8980 - val_loss: 2.6499 - val_accuracy: 0.5811\n",
            "Epoch 170/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2474 - accuracy: 0.8980 - val_loss: 2.5506 - val_accuracy: 0.5946\n",
            "Epoch 171/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2317 - accuracy: 0.9048 - val_loss: 2.5663 - val_accuracy: 0.5946\n",
            "Epoch 172/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2212 - accuracy: 0.9082 - val_loss: 2.6010 - val_accuracy: 0.5946\n",
            "Epoch 173/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2369 - accuracy: 0.9014 - val_loss: 2.7087 - val_accuracy: 0.6081\n",
            "Epoch 174/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2089 - accuracy: 0.9048 - val_loss: 2.7850 - val_accuracy: 0.5946\n",
            "Epoch 175/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2294 - accuracy: 0.9014 - val_loss: 2.6604 - val_accuracy: 0.6216\n",
            "Epoch 176/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2190 - accuracy: 0.9014 - val_loss: 2.7673 - val_accuracy: 0.5676\n",
            "Epoch 177/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2536 - accuracy: 0.8912 - val_loss: 3.3037 - val_accuracy: 0.5676\n",
            "Epoch 178/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2584 - accuracy: 0.8844 - val_loss: 3.0124 - val_accuracy: 0.5676\n",
            "Epoch 179/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2217 - accuracy: 0.8912 - val_loss: 2.8709 - val_accuracy: 0.5946\n",
            "Epoch 180/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2179 - accuracy: 0.8980 - val_loss: 2.8940 - val_accuracy: 0.5405\n",
            "Epoch 181/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2384 - accuracy: 0.8946 - val_loss: 3.0250 - val_accuracy: 0.5270\n",
            "Epoch 182/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2208 - accuracy: 0.8980 - val_loss: 2.9745 - val_accuracy: 0.5541\n",
            "Epoch 183/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2310 - accuracy: 0.8980 - val_loss: 2.9356 - val_accuracy: 0.5811\n",
            "Epoch 184/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2369 - accuracy: 0.9014 - val_loss: 2.8440 - val_accuracy: 0.5676\n",
            "Epoch 185/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2225 - accuracy: 0.9014 - val_loss: 2.8272 - val_accuracy: 0.5270\n",
            "Epoch 186/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2395 - accuracy: 0.8980 - val_loss: 2.7948 - val_accuracy: 0.5541\n",
            "Epoch 187/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2217 - accuracy: 0.8980 - val_loss: 2.7962 - val_accuracy: 0.5811\n",
            "Epoch 188/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2248 - accuracy: 0.8878 - val_loss: 2.8794 - val_accuracy: 0.5676\n",
            "Epoch 189/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2080 - accuracy: 0.8912 - val_loss: 2.9262 - val_accuracy: 0.5541\n",
            "Epoch 190/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2239 - accuracy: 0.9082 - val_loss: 2.8606 - val_accuracy: 0.5541\n",
            "Epoch 191/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2173 - accuracy: 0.9048 - val_loss: 2.8654 - val_accuracy: 0.6081\n",
            "Epoch 192/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1962 - accuracy: 0.9082 - val_loss: 2.9214 - val_accuracy: 0.6081\n",
            "Epoch 193/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1984 - accuracy: 0.9048 - val_loss: 2.9901 - val_accuracy: 0.6081\n",
            "Epoch 194/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2024 - accuracy: 0.9116 - val_loss: 3.0710 - val_accuracy: 0.6216\n",
            "Epoch 195/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2095 - accuracy: 0.9048 - val_loss: 3.1127 - val_accuracy: 0.6081\n",
            "Epoch 196/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1884 - accuracy: 0.9116 - val_loss: 3.1053 - val_accuracy: 0.5946\n",
            "Epoch 197/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2130 - accuracy: 0.9048 - val_loss: 3.1763 - val_accuracy: 0.5946\n",
            "Epoch 198/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2077 - accuracy: 0.9082 - val_loss: 3.3341 - val_accuracy: 0.5946\n",
            "Epoch 199/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1908 - accuracy: 0.9082 - val_loss: 3.4133 - val_accuracy: 0.6351\n",
            "Epoch 200/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2233 - accuracy: 0.8980 - val_loss: 3.2746 - val_accuracy: 0.6081\n",
            "Epoch 201/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1881 - accuracy: 0.9184 - val_loss: 3.1013 - val_accuracy: 0.5946\n",
            "Epoch 202/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2307 - accuracy: 0.8980 - val_loss: 3.2158 - val_accuracy: 0.5811\n",
            "Epoch 203/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2028 - accuracy: 0.9014 - val_loss: 2.9454 - val_accuracy: 0.5946\n",
            "Epoch 204/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2181 - accuracy: 0.9082 - val_loss: 2.9066 - val_accuracy: 0.6351\n",
            "Epoch 205/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2061 - accuracy: 0.9014 - val_loss: 2.8963 - val_accuracy: 0.5811\n",
            "Epoch 206/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2150 - accuracy: 0.8980 - val_loss: 3.0627 - val_accuracy: 0.5405\n",
            "Epoch 207/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2181 - accuracy: 0.9048 - val_loss: 3.0289 - val_accuracy: 0.5405\n",
            "Epoch 208/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2230 - accuracy: 0.9014 - val_loss: 2.9845 - val_accuracy: 0.5405\n",
            "Epoch 209/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2122 - accuracy: 0.9014 - val_loss: 2.9720 - val_accuracy: 0.5811\n",
            "Epoch 210/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1993 - accuracy: 0.9048 - val_loss: 2.9534 - val_accuracy: 0.5676\n",
            "Epoch 211/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1997 - accuracy: 0.9048 - val_loss: 2.9330 - val_accuracy: 0.5541\n",
            "Epoch 212/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2043 - accuracy: 0.9048 - val_loss: 2.9778 - val_accuracy: 0.5405\n",
            "Epoch 213/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2037 - accuracy: 0.9048 - val_loss: 3.0373 - val_accuracy: 0.5676\n",
            "Epoch 214/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2285 - accuracy: 0.9048 - val_loss: 3.0090 - val_accuracy: 0.5946\n",
            "Epoch 215/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2044 - accuracy: 0.9082 - val_loss: 2.9487 - val_accuracy: 0.6081\n",
            "Epoch 216/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2069 - accuracy: 0.9150 - val_loss: 3.0229 - val_accuracy: 0.5946\n",
            "Epoch 217/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2053 - accuracy: 0.9014 - val_loss: 2.9323 - val_accuracy: 0.5946\n",
            "Epoch 218/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2042 - accuracy: 0.9014 - val_loss: 2.9033 - val_accuracy: 0.6081\n",
            "Epoch 219/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2070 - accuracy: 0.9048 - val_loss: 2.9432 - val_accuracy: 0.6351\n",
            "Epoch 220/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2068 - accuracy: 0.9048 - val_loss: 2.9875 - val_accuracy: 0.6486\n",
            "Epoch 221/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2122 - accuracy: 0.9014 - val_loss: 3.1274 - val_accuracy: 0.6216\n",
            "Epoch 222/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1870 - accuracy: 0.9116 - val_loss: 3.2971 - val_accuracy: 0.5811\n",
            "Epoch 223/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2162 - accuracy: 0.9014 - val_loss: 3.2620 - val_accuracy: 0.6216\n",
            "Epoch 224/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2023 - accuracy: 0.9116 - val_loss: 3.2912 - val_accuracy: 0.5405\n",
            "Epoch 225/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2013 - accuracy: 0.9116 - val_loss: 3.2685 - val_accuracy: 0.5811\n",
            "Epoch 226/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1882 - accuracy: 0.9082 - val_loss: 3.1667 - val_accuracy: 0.6216\n",
            "Epoch 227/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1972 - accuracy: 0.9082 - val_loss: 3.0512 - val_accuracy: 0.6486\n",
            "Epoch 228/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2249 - accuracy: 0.9014 - val_loss: 3.1564 - val_accuracy: 0.5946\n",
            "Epoch 229/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2005 - accuracy: 0.9116 - val_loss: 3.1929 - val_accuracy: 0.5946\n",
            "Epoch 230/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2561 - accuracy: 0.8912 - val_loss: 3.1135 - val_accuracy: 0.5946\n",
            "Epoch 231/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2181 - accuracy: 0.8980 - val_loss: 3.2692 - val_accuracy: 0.5946\n",
            "Epoch 232/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2110 - accuracy: 0.9014 - val_loss: 3.5261 - val_accuracy: 0.5946\n",
            "Epoch 233/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2191 - accuracy: 0.8980 - val_loss: 3.4485 - val_accuracy: 0.5405\n",
            "Epoch 234/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2104 - accuracy: 0.9014 - val_loss: 3.5026 - val_accuracy: 0.5811\n",
            "Epoch 235/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2015 - accuracy: 0.9150 - val_loss: 3.7294 - val_accuracy: 0.5541\n",
            "Epoch 236/750\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2371 - accuracy: 0.8980 - val_loss: 3.6234 - val_accuracy: 0.5541\n",
            "Epoch 237/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2313 - accuracy: 0.8946 - val_loss: 3.5047 - val_accuracy: 0.5270\n",
            "Epoch 238/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2188 - accuracy: 0.9116 - val_loss: 3.3032 - val_accuracy: 0.5811\n",
            "Epoch 239/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2189 - accuracy: 0.9048 - val_loss: 3.2429 - val_accuracy: 0.5811\n",
            "Epoch 240/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2041 - accuracy: 0.9048 - val_loss: 3.0200 - val_accuracy: 0.6216\n",
            "Epoch 241/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2211 - accuracy: 0.9048 - val_loss: 3.0331 - val_accuracy: 0.5946\n",
            "Epoch 242/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1995 - accuracy: 0.9048 - val_loss: 3.1319 - val_accuracy: 0.5946\n",
            "Epoch 243/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1903 - accuracy: 0.9048 - val_loss: 3.2225 - val_accuracy: 0.6081\n",
            "Epoch 244/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1990 - accuracy: 0.9014 - val_loss: 3.2441 - val_accuracy: 0.5541\n",
            "Epoch 245/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2154 - accuracy: 0.9014 - val_loss: 3.2029 - val_accuracy: 0.5405\n",
            "Epoch 246/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1983 - accuracy: 0.9082 - val_loss: 3.2813 - val_accuracy: 0.5541\n",
            "Epoch 247/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2008 - accuracy: 0.9116 - val_loss: 3.4678 - val_accuracy: 0.5946\n",
            "Epoch 248/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1980 - accuracy: 0.9116 - val_loss: 3.5948 - val_accuracy: 0.5946\n",
            "Epoch 249/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1963 - accuracy: 0.9150 - val_loss: 3.7200 - val_accuracy: 0.5946\n",
            "Epoch 250/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1977 - accuracy: 0.9082 - val_loss: 3.7958 - val_accuracy: 0.5946\n",
            "Epoch 251/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1921 - accuracy: 0.9116 - val_loss: 3.7828 - val_accuracy: 0.5811\n",
            "Epoch 252/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1951 - accuracy: 0.9218 - val_loss: 3.7342 - val_accuracy: 0.5811\n",
            "Epoch 253/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2019 - accuracy: 0.9048 - val_loss: 3.6830 - val_accuracy: 0.5541\n",
            "Epoch 254/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1817 - accuracy: 0.9184 - val_loss: 3.6827 - val_accuracy: 0.5676\n",
            "Epoch 255/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1821 - accuracy: 0.9150 - val_loss: 3.7054 - val_accuracy: 0.5811\n",
            "Epoch 256/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2004 - accuracy: 0.9116 - val_loss: 3.7662 - val_accuracy: 0.5676\n",
            "Epoch 257/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1955 - accuracy: 0.9082 - val_loss: 3.7518 - val_accuracy: 0.5811\n",
            "Epoch 258/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1952 - accuracy: 0.9082 - val_loss: 3.7676 - val_accuracy: 0.5946\n",
            "Epoch 259/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1983 - accuracy: 0.9082 - val_loss: 3.8185 - val_accuracy: 0.5946\n",
            "Epoch 260/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2046 - accuracy: 0.9014 - val_loss: 3.7397 - val_accuracy: 0.5541\n",
            "Epoch 261/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1974 - accuracy: 0.9048 - val_loss: 3.8298 - val_accuracy: 0.5811\n",
            "Epoch 262/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2065 - accuracy: 0.9048 - val_loss: 3.8047 - val_accuracy: 0.5946\n",
            "Epoch 263/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1938 - accuracy: 0.9082 - val_loss: 3.6744 - val_accuracy: 0.5811\n",
            "Epoch 264/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2173 - accuracy: 0.9048 - val_loss: 3.4887 - val_accuracy: 0.5811\n",
            "Epoch 265/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2081 - accuracy: 0.9014 - val_loss: 3.5634 - val_accuracy: 0.5676\n",
            "Epoch 266/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2234 - accuracy: 0.8946 - val_loss: 3.2742 - val_accuracy: 0.6216\n",
            "Epoch 267/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2129 - accuracy: 0.9048 - val_loss: 2.7938 - val_accuracy: 0.6216\n",
            "Epoch 268/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1956 - accuracy: 0.9048 - val_loss: 2.6887 - val_accuracy: 0.6081\n",
            "Epoch 269/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1965 - accuracy: 0.9116 - val_loss: 2.8291 - val_accuracy: 0.6081\n",
            "Epoch 270/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2078 - accuracy: 0.9082 - val_loss: 2.9205 - val_accuracy: 0.5946\n",
            "Epoch 271/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2028 - accuracy: 0.9116 - val_loss: 2.9765 - val_accuracy: 0.5676\n",
            "Epoch 272/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2070 - accuracy: 0.9082 - val_loss: 3.0841 - val_accuracy: 0.5270\n",
            "Epoch 273/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2104 - accuracy: 0.9014 - val_loss: 3.3263 - val_accuracy: 0.5135\n",
            "Epoch 274/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2007 - accuracy: 0.9116 - val_loss: 3.4357 - val_accuracy: 0.5811\n",
            "Epoch 275/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2021 - accuracy: 0.9116 - val_loss: 3.5283 - val_accuracy: 0.5946\n",
            "Epoch 276/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1907 - accuracy: 0.9116 - val_loss: 3.5169 - val_accuracy: 0.5811\n",
            "Epoch 277/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1916 - accuracy: 0.9116 - val_loss: 3.4496 - val_accuracy: 0.5811\n",
            "Epoch 278/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2007 - accuracy: 0.9150 - val_loss: 3.3899 - val_accuracy: 0.5811\n",
            "Epoch 279/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1890 - accuracy: 0.9082 - val_loss: 3.3007 - val_accuracy: 0.5676\n",
            "Epoch 280/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1860 - accuracy: 0.9116 - val_loss: 3.2361 - val_accuracy: 0.5676\n",
            "Epoch 281/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1836 - accuracy: 0.9150 - val_loss: 3.2917 - val_accuracy: 0.5811\n",
            "Epoch 282/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2049 - accuracy: 0.9048 - val_loss: 3.4301 - val_accuracy: 0.5676\n",
            "Epoch 283/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1969 - accuracy: 0.9082 - val_loss: 3.4584 - val_accuracy: 0.5405\n",
            "Epoch 284/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1892 - accuracy: 0.9116 - val_loss: 3.4479 - val_accuracy: 0.5811\n",
            "Epoch 285/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1964 - accuracy: 0.9082 - val_loss: 3.4973 - val_accuracy: 0.5541\n",
            "Epoch 286/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1925 - accuracy: 0.9116 - val_loss: 3.4703 - val_accuracy: 0.6081\n",
            "Epoch 287/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2446 - accuracy: 0.8878 - val_loss: 3.5349 - val_accuracy: 0.5811\n",
            "Epoch 288/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3062 - accuracy: 0.8980 - val_loss: 2.8350 - val_accuracy: 0.5946\n",
            "Epoch 289/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4073 - accuracy: 0.8741 - val_loss: 2.7655 - val_accuracy: 0.5000\n",
            "Epoch 290/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2493 - accuracy: 0.8912 - val_loss: 2.9092 - val_accuracy: 0.5676\n",
            "Epoch 291/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2528 - accuracy: 0.8946 - val_loss: 2.6692 - val_accuracy: 0.5811\n",
            "Epoch 292/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2492 - accuracy: 0.8980 - val_loss: 2.1750 - val_accuracy: 0.6351\n",
            "Epoch 293/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3726 - accuracy: 0.8673 - val_loss: 1.7946 - val_accuracy: 0.5811\n",
            "Epoch 294/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.5209 - accuracy: 0.8197 - val_loss: 1.3247 - val_accuracy: 0.6216\n",
            "Epoch 295/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.6177 - accuracy: 0.7789 - val_loss: 1.3355 - val_accuracy: 0.5541\n",
            "Epoch 296/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.5586 - accuracy: 0.7823 - val_loss: 1.1596 - val_accuracy: 0.6081\n",
            "Epoch 297/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4711 - accuracy: 0.8129 - val_loss: 1.0612 - val_accuracy: 0.5676\n",
            "Epoch 298/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4372 - accuracy: 0.8197 - val_loss: 1.0087 - val_accuracy: 0.6081\n",
            "Epoch 299/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3913 - accuracy: 0.8367 - val_loss: 1.2572 - val_accuracy: 0.6081\n",
            "Epoch 300/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3718 - accuracy: 0.8435 - val_loss: 1.2398 - val_accuracy: 0.6081\n",
            "Epoch 301/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3289 - accuracy: 0.8503 - val_loss: 1.2357 - val_accuracy: 0.5946\n",
            "Epoch 302/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3122 - accuracy: 0.8741 - val_loss: 1.3172 - val_accuracy: 0.5946\n",
            "Epoch 303/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2877 - accuracy: 0.8707 - val_loss: 1.5763 - val_accuracy: 0.5946\n",
            "Epoch 304/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2917 - accuracy: 0.8844 - val_loss: 1.5247 - val_accuracy: 0.5811\n",
            "Epoch 305/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2559 - accuracy: 0.8878 - val_loss: 1.5541 - val_accuracy: 0.6081\n",
            "Epoch 306/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2642 - accuracy: 0.8878 - val_loss: 1.7070 - val_accuracy: 0.5946\n",
            "Epoch 307/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2326 - accuracy: 0.8980 - val_loss: 1.9168 - val_accuracy: 0.6081\n",
            "Epoch 308/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2203 - accuracy: 0.8946 - val_loss: 2.0037 - val_accuracy: 0.6081\n",
            "Epoch 309/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2095 - accuracy: 0.9082 - val_loss: 2.0832 - val_accuracy: 0.5811\n",
            "Epoch 310/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2086 - accuracy: 0.9116 - val_loss: 2.2359 - val_accuracy: 0.5676\n",
            "Epoch 311/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2227 - accuracy: 0.9048 - val_loss: 2.3112 - val_accuracy: 0.5811\n",
            "Epoch 312/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2113 - accuracy: 0.9048 - val_loss: 2.3211 - val_accuracy: 0.5811\n",
            "Epoch 313/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2094 - accuracy: 0.9048 - val_loss: 2.3067 - val_accuracy: 0.5811\n",
            "Epoch 314/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2110 - accuracy: 0.9048 - val_loss: 2.3136 - val_accuracy: 0.5811\n",
            "Epoch 315/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2024 - accuracy: 0.9048 - val_loss: 2.3314 - val_accuracy: 0.5946\n",
            "Epoch 316/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1935 - accuracy: 0.9048 - val_loss: 2.4013 - val_accuracy: 0.5946\n",
            "Epoch 317/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1842 - accuracy: 0.9116 - val_loss: 2.4702 - val_accuracy: 0.5946\n",
            "Epoch 318/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2046 - accuracy: 0.9048 - val_loss: 2.5028 - val_accuracy: 0.5946\n",
            "Epoch 319/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1955 - accuracy: 0.8980 - val_loss: 2.4933 - val_accuracy: 0.6081\n",
            "Epoch 320/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1923 - accuracy: 0.9150 - val_loss: 2.5493 - val_accuracy: 0.5405\n",
            "Epoch 321/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2051 - accuracy: 0.9048 - val_loss: 2.6073 - val_accuracy: 0.5946\n",
            "Epoch 322/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2112 - accuracy: 0.9048 - val_loss: 2.6816 - val_accuracy: 0.5946\n",
            "Epoch 323/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2026 - accuracy: 0.9048 - val_loss: 2.6264 - val_accuracy: 0.5946\n",
            "Epoch 324/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2036 - accuracy: 0.9048 - val_loss: 2.5379 - val_accuracy: 0.6216\n",
            "Epoch 325/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2011 - accuracy: 0.9048 - val_loss: 2.5372 - val_accuracy: 0.6081\n",
            "Epoch 326/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1945 - accuracy: 0.9082 - val_loss: 2.5806 - val_accuracy: 0.5811\n",
            "Epoch 327/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2022 - accuracy: 0.9082 - val_loss: 2.5955 - val_accuracy: 0.5811\n",
            "Epoch 328/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2006 - accuracy: 0.9116 - val_loss: 2.5953 - val_accuracy: 0.5946\n",
            "Epoch 329/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1955 - accuracy: 0.9116 - val_loss: 2.6091 - val_accuracy: 0.5946\n",
            "Epoch 330/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1899 - accuracy: 0.9082 - val_loss: 2.6232 - val_accuracy: 0.5946\n",
            "Epoch 331/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1925 - accuracy: 0.9116 - val_loss: 2.6467 - val_accuracy: 0.5811\n",
            "Epoch 332/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1860 - accuracy: 0.9150 - val_loss: 2.6853 - val_accuracy: 0.5811\n",
            "Epoch 333/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1964 - accuracy: 0.9082 - val_loss: 2.7191 - val_accuracy: 0.5811\n",
            "Epoch 334/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1940 - accuracy: 0.9116 - val_loss: 2.7373 - val_accuracy: 0.5811\n",
            "Epoch 335/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1946 - accuracy: 0.9116 - val_loss: 2.7476 - val_accuracy: 0.5811\n",
            "Epoch 336/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2038 - accuracy: 0.9116 - val_loss: 2.7426 - val_accuracy: 0.5811\n",
            "Epoch 337/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1907 - accuracy: 0.9116 - val_loss: 2.7297 - val_accuracy: 0.5811\n",
            "Epoch 338/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1902 - accuracy: 0.9116 - val_loss: 2.7202 - val_accuracy: 0.5946\n",
            "Epoch 339/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1860 - accuracy: 0.9116 - val_loss: 2.7228 - val_accuracy: 0.6081\n",
            "Epoch 340/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1991 - accuracy: 0.9116 - val_loss: 2.7515 - val_accuracy: 0.6081\n",
            "Epoch 341/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1848 - accuracy: 0.9116 - val_loss: 2.7837 - val_accuracy: 0.6081\n",
            "Epoch 342/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1850 - accuracy: 0.9150 - val_loss: 2.8027 - val_accuracy: 0.6081\n",
            "Epoch 343/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1879 - accuracy: 0.9116 - val_loss: 2.8189 - val_accuracy: 0.6216\n",
            "Epoch 344/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1891 - accuracy: 0.9150 - val_loss: 2.8435 - val_accuracy: 0.6216\n",
            "Epoch 345/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1913 - accuracy: 0.9116 - val_loss: 2.8562 - val_accuracy: 0.6216\n",
            "Epoch 346/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1909 - accuracy: 0.9116 - val_loss: 2.8590 - val_accuracy: 0.6216\n",
            "Epoch 347/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1933 - accuracy: 0.9116 - val_loss: 2.8674 - val_accuracy: 0.6216\n",
            "Epoch 348/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1935 - accuracy: 0.9116 - val_loss: 2.8856 - val_accuracy: 0.6351\n",
            "Epoch 349/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1969 - accuracy: 0.9082 - val_loss: 2.8967 - val_accuracy: 0.6351\n",
            "Epoch 350/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1884 - accuracy: 0.9116 - val_loss: 2.9073 - val_accuracy: 0.6351\n",
            "Epoch 351/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1910 - accuracy: 0.9116 - val_loss: 2.9152 - val_accuracy: 0.6081\n",
            "Epoch 352/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2036 - accuracy: 0.9048 - val_loss: 2.9087 - val_accuracy: 0.5811\n",
            "Epoch 353/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1925 - accuracy: 0.9048 - val_loss: 2.9241 - val_accuracy: 0.5811\n",
            "Epoch 354/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1928 - accuracy: 0.9082 - val_loss: 2.9831 - val_accuracy: 0.5946\n",
            "Epoch 355/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1936 - accuracy: 0.9082 - val_loss: 3.0465 - val_accuracy: 0.5946\n",
            "Epoch 356/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1848 - accuracy: 0.9082 - val_loss: 3.0359 - val_accuracy: 0.5946\n",
            "Epoch 357/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1899 - accuracy: 0.9150 - val_loss: 3.0378 - val_accuracy: 0.5946\n",
            "Epoch 358/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1963 - accuracy: 0.9116 - val_loss: 3.0117 - val_accuracy: 0.6351\n",
            "Epoch 359/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1946 - accuracy: 0.9082 - val_loss: 3.0225 - val_accuracy: 0.6081\n",
            "Epoch 360/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1912 - accuracy: 0.9048 - val_loss: 3.0148 - val_accuracy: 0.6216\n",
            "Epoch 361/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1874 - accuracy: 0.9116 - val_loss: 3.0215 - val_accuracy: 0.6216\n",
            "Epoch 362/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1904 - accuracy: 0.9082 - val_loss: 3.0241 - val_accuracy: 0.6081\n",
            "Epoch 363/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1923 - accuracy: 0.9116 - val_loss: 3.0298 - val_accuracy: 0.6081\n",
            "Epoch 364/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1899 - accuracy: 0.9150 - val_loss: 3.0365 - val_accuracy: 0.6081\n",
            "Epoch 365/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1870 - accuracy: 0.9116 - val_loss: 3.0496 - val_accuracy: 0.6081\n",
            "Epoch 366/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1946 - accuracy: 0.9048 - val_loss: 3.0792 - val_accuracy: 0.6081\n",
            "Epoch 367/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1843 - accuracy: 0.9150 - val_loss: 3.1101 - val_accuracy: 0.6081\n",
            "Epoch 368/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1874 - accuracy: 0.9116 - val_loss: 3.1628 - val_accuracy: 0.5946\n",
            "Epoch 369/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1898 - accuracy: 0.9082 - val_loss: 3.2516 - val_accuracy: 0.6216\n",
            "Epoch 370/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1836 - accuracy: 0.9116 - val_loss: 3.2927 - val_accuracy: 0.6216\n",
            "Epoch 371/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2015 - accuracy: 0.9116 - val_loss: 3.2873 - val_accuracy: 0.6081\n",
            "Epoch 372/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1908 - accuracy: 0.9116 - val_loss: 3.2436 - val_accuracy: 0.6081\n",
            "Epoch 373/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1866 - accuracy: 0.9116 - val_loss: 3.2501 - val_accuracy: 0.6081\n",
            "Epoch 374/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1912 - accuracy: 0.9116 - val_loss: 3.2740 - val_accuracy: 0.5946\n",
            "Epoch 375/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1952 - accuracy: 0.9048 - val_loss: 3.3113 - val_accuracy: 0.5946\n",
            "Epoch 376/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1849 - accuracy: 0.9116 - val_loss: 3.3417 - val_accuracy: 0.5946\n",
            "Epoch 377/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1879 - accuracy: 0.9150 - val_loss: 3.3631 - val_accuracy: 0.5946\n",
            "Epoch 378/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1862 - accuracy: 0.9116 - val_loss: 3.3677 - val_accuracy: 0.5811\n",
            "Epoch 379/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1953 - accuracy: 0.9116 - val_loss: 3.3683 - val_accuracy: 0.5811\n",
            "Epoch 380/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1831 - accuracy: 0.9150 - val_loss: 3.3524 - val_accuracy: 0.5811\n",
            "Epoch 381/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1847 - accuracy: 0.9116 - val_loss: 3.3410 - val_accuracy: 0.5946\n",
            "Epoch 382/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1862 - accuracy: 0.9116 - val_loss: 3.3410 - val_accuracy: 0.6081\n",
            "Epoch 383/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1873 - accuracy: 0.9116 - val_loss: 3.3571 - val_accuracy: 0.6216\n",
            "Epoch 384/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1866 - accuracy: 0.9116 - val_loss: 3.3718 - val_accuracy: 0.6216\n",
            "Epoch 385/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1841 - accuracy: 0.9150 - val_loss: 3.3935 - val_accuracy: 0.6216\n",
            "Epoch 386/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1870 - accuracy: 0.9082 - val_loss: 3.4170 - val_accuracy: 0.6216\n",
            "Epoch 387/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1832 - accuracy: 0.9116 - val_loss: 3.4672 - val_accuracy: 0.6081\n",
            "Epoch 388/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1858 - accuracy: 0.9116 - val_loss: 3.5130 - val_accuracy: 0.5946\n",
            "Epoch 389/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1825 - accuracy: 0.9116 - val_loss: 3.5287 - val_accuracy: 0.5946\n",
            "Epoch 390/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1858 - accuracy: 0.9116 - val_loss: 3.5389 - val_accuracy: 0.5946\n",
            "Epoch 391/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1878 - accuracy: 0.9150 - val_loss: 3.5373 - val_accuracy: 0.5811\n",
            "Epoch 392/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1924 - accuracy: 0.9116 - val_loss: 3.5148 - val_accuracy: 0.6081\n",
            "Epoch 393/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1855 - accuracy: 0.9116 - val_loss: 3.5023 - val_accuracy: 0.6081\n",
            "Epoch 394/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1998 - accuracy: 0.9082 - val_loss: 3.4956 - val_accuracy: 0.5946\n",
            "Epoch 395/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1899 - accuracy: 0.9048 - val_loss: 3.4831 - val_accuracy: 0.5946\n",
            "Epoch 396/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1776 - accuracy: 0.9116 - val_loss: 3.4735 - val_accuracy: 0.5946\n",
            "Epoch 397/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1904 - accuracy: 0.9116 - val_loss: 3.4788 - val_accuracy: 0.5946\n",
            "Epoch 398/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1874 - accuracy: 0.9082 - val_loss: 3.4818 - val_accuracy: 0.5946\n",
            "Epoch 399/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1846 - accuracy: 0.9116 - val_loss: 3.4826 - val_accuracy: 0.5811\n",
            "Epoch 400/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1832 - accuracy: 0.9116 - val_loss: 3.4865 - val_accuracy: 0.5811\n",
            "Epoch 401/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1961 - accuracy: 0.9048 - val_loss: 3.4986 - val_accuracy: 0.5811\n",
            "Epoch 402/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1896 - accuracy: 0.9082 - val_loss: 3.5053 - val_accuracy: 0.5811\n",
            "Epoch 403/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1902 - accuracy: 0.9116 - val_loss: 3.4965 - val_accuracy: 0.5811\n",
            "Epoch 404/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1942 - accuracy: 0.9082 - val_loss: 3.4872 - val_accuracy: 0.5676\n",
            "Epoch 405/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1828 - accuracy: 0.9150 - val_loss: 3.4775 - val_accuracy: 0.5676\n",
            "Epoch 406/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1873 - accuracy: 0.9184 - val_loss: 3.4802 - val_accuracy: 0.5676\n",
            "Epoch 407/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1922 - accuracy: 0.9082 - val_loss: 3.4962 - val_accuracy: 0.5676\n",
            "Epoch 408/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1907 - accuracy: 0.9116 - val_loss: 3.5282 - val_accuracy: 0.5811\n",
            "Epoch 409/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1820 - accuracy: 0.9116 - val_loss: 3.5732 - val_accuracy: 0.5811\n",
            "Epoch 410/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1914 - accuracy: 0.9116 - val_loss: 3.6003 - val_accuracy: 0.5811\n",
            "Epoch 411/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1894 - accuracy: 0.9116 - val_loss: 3.6894 - val_accuracy: 0.5676\n",
            "Epoch 412/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1957 - accuracy: 0.9116 - val_loss: 3.5988 - val_accuracy: 0.6081\n",
            "Epoch 413/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2094 - accuracy: 0.9116 - val_loss: 3.4247 - val_accuracy: 0.5811\n",
            "Epoch 414/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1919 - accuracy: 0.9116 - val_loss: 3.3534 - val_accuracy: 0.5946\n",
            "Epoch 415/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1893 - accuracy: 0.9116 - val_loss: 3.4734 - val_accuracy: 0.5946\n",
            "Epoch 416/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2003 - accuracy: 0.9116 - val_loss: 3.4349 - val_accuracy: 0.5676\n",
            "Epoch 417/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1949 - accuracy: 0.9116 - val_loss: 3.5929 - val_accuracy: 0.5676\n",
            "Epoch 418/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2138 - accuracy: 0.9048 - val_loss: 3.4671 - val_accuracy: 0.6081\n",
            "Epoch 419/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1934 - accuracy: 0.9116 - val_loss: 3.7213 - val_accuracy: 0.6081\n",
            "Epoch 420/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2064 - accuracy: 0.9082 - val_loss: 3.5904 - val_accuracy: 0.5946\n",
            "Epoch 421/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2014 - accuracy: 0.9082 - val_loss: 3.2321 - val_accuracy: 0.6216\n",
            "Epoch 422/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1896 - accuracy: 0.9116 - val_loss: 3.1391 - val_accuracy: 0.5676\n",
            "Epoch 423/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1902 - accuracy: 0.9048 - val_loss: 3.2089 - val_accuracy: 0.6081\n",
            "Epoch 424/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1961 - accuracy: 0.9082 - val_loss: 3.2922 - val_accuracy: 0.5811\n",
            "Epoch 425/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1866 - accuracy: 0.9116 - val_loss: 3.4258 - val_accuracy: 0.5811\n",
            "Epoch 426/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1898 - accuracy: 0.9082 - val_loss: 3.5362 - val_accuracy: 0.5676\n",
            "Epoch 427/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1853 - accuracy: 0.9082 - val_loss: 3.6267 - val_accuracy: 0.5676\n",
            "Epoch 428/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1931 - accuracy: 0.9116 - val_loss: 3.6944 - val_accuracy: 0.5676\n",
            "Epoch 429/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1954 - accuracy: 0.9048 - val_loss: 3.7342 - val_accuracy: 0.5676\n",
            "Epoch 430/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1911 - accuracy: 0.9082 - val_loss: 3.6257 - val_accuracy: 0.5676\n",
            "Epoch 431/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1813 - accuracy: 0.9150 - val_loss: 3.6079 - val_accuracy: 0.5676\n",
            "Epoch 432/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1855 - accuracy: 0.9116 - val_loss: 3.6372 - val_accuracy: 0.5676\n",
            "Epoch 433/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1952 - accuracy: 0.9116 - val_loss: 3.6680 - val_accuracy: 0.5811\n",
            "Epoch 434/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1874 - accuracy: 0.9150 - val_loss: 3.6957 - val_accuracy: 0.5811\n",
            "Epoch 435/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1839 - accuracy: 0.9116 - val_loss: 3.7186 - val_accuracy: 0.5946\n",
            "Epoch 436/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1839 - accuracy: 0.9150 - val_loss: 3.7474 - val_accuracy: 0.6081\n",
            "Epoch 437/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1949 - accuracy: 0.9116 - val_loss: 3.7975 - val_accuracy: 0.5676\n",
            "Epoch 438/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1924 - accuracy: 0.9116 - val_loss: 3.9283 - val_accuracy: 0.5541\n",
            "Epoch 439/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2087 - accuracy: 0.9082 - val_loss: 3.6965 - val_accuracy: 0.5676\n",
            "Epoch 440/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1963 - accuracy: 0.9116 - val_loss: 3.4550 - val_accuracy: 0.5811\n",
            "Epoch 441/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1947 - accuracy: 0.9082 - val_loss: 3.3863 - val_accuracy: 0.6081\n",
            "Epoch 442/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1884 - accuracy: 0.9116 - val_loss: 3.3198 - val_accuracy: 0.6351\n",
            "Epoch 443/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2112 - accuracy: 0.9082 - val_loss: 3.1001 - val_accuracy: 0.6216\n",
            "Epoch 444/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1887 - accuracy: 0.9116 - val_loss: 3.2515 - val_accuracy: 0.5946\n",
            "Epoch 445/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2360 - accuracy: 0.9048 - val_loss: 2.9670 - val_accuracy: 0.6081\n",
            "Epoch 446/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1869 - accuracy: 0.9116 - val_loss: 2.6678 - val_accuracy: 0.6351\n",
            "Epoch 447/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2019 - accuracy: 0.9082 - val_loss: 2.6315 - val_accuracy: 0.6216\n",
            "Epoch 448/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2154 - accuracy: 0.9014 - val_loss: 2.6898 - val_accuracy: 0.6216\n",
            "Epoch 449/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1923 - accuracy: 0.9150 - val_loss: 2.8352 - val_accuracy: 0.5811\n",
            "Epoch 450/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2468 - accuracy: 0.9014 - val_loss: 2.7467 - val_accuracy: 0.5676\n",
            "Epoch 451/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2194 - accuracy: 0.9014 - val_loss: 2.6577 - val_accuracy: 0.5946\n",
            "Epoch 452/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2195 - accuracy: 0.8946 - val_loss: 2.7398 - val_accuracy: 0.5946\n",
            "Epoch 453/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2012 - accuracy: 0.9082 - val_loss: 2.8188 - val_accuracy: 0.5946\n",
            "Epoch 454/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1984 - accuracy: 0.9082 - val_loss: 2.9084 - val_accuracy: 0.5946\n",
            "Epoch 455/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2005 - accuracy: 0.9048 - val_loss: 2.9073 - val_accuracy: 0.5946\n",
            "Epoch 456/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2007 - accuracy: 0.9048 - val_loss: 2.8924 - val_accuracy: 0.5811\n",
            "Epoch 457/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1898 - accuracy: 0.9116 - val_loss: 2.8806 - val_accuracy: 0.5676\n",
            "Epoch 458/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1913 - accuracy: 0.9116 - val_loss: 2.8893 - val_accuracy: 0.5811\n",
            "Epoch 459/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1952 - accuracy: 0.9116 - val_loss: 2.9137 - val_accuracy: 0.5811\n",
            "Epoch 460/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1929 - accuracy: 0.9116 - val_loss: 2.9465 - val_accuracy: 0.5946\n",
            "Epoch 461/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1915 - accuracy: 0.9116 - val_loss: 2.9932 - val_accuracy: 0.5946\n",
            "Epoch 462/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1877 - accuracy: 0.9116 - val_loss: 3.0428 - val_accuracy: 0.5946\n",
            "Epoch 463/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1916 - accuracy: 0.9116 - val_loss: 3.0872 - val_accuracy: 0.5946\n",
            "Epoch 464/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1870 - accuracy: 0.9116 - val_loss: 3.1272 - val_accuracy: 0.5946\n",
            "Epoch 465/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1883 - accuracy: 0.9116 - val_loss: 3.1477 - val_accuracy: 0.5946\n",
            "Epoch 466/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1904 - accuracy: 0.9116 - val_loss: 3.1333 - val_accuracy: 0.5946\n",
            "Epoch 467/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1859 - accuracy: 0.9116 - val_loss: 3.1357 - val_accuracy: 0.6216\n",
            "Epoch 468/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1891 - accuracy: 0.9150 - val_loss: 3.1484 - val_accuracy: 0.5811\n",
            "Epoch 469/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1853 - accuracy: 0.9116 - val_loss: 3.1642 - val_accuracy: 0.5676\n",
            "Epoch 470/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1884 - accuracy: 0.9150 - val_loss: 3.1791 - val_accuracy: 0.5676\n",
            "Epoch 471/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1820 - accuracy: 0.9150 - val_loss: 3.2074 - val_accuracy: 0.5676\n",
            "Epoch 472/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1848 - accuracy: 0.9116 - val_loss: 3.2403 - val_accuracy: 0.5676\n",
            "Epoch 473/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1941 - accuracy: 0.9116 - val_loss: 3.2328 - val_accuracy: 0.5946\n",
            "Epoch 474/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1916 - accuracy: 0.9082 - val_loss: 3.2426 - val_accuracy: 0.6081\n",
            "Epoch 475/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1837 - accuracy: 0.9116 - val_loss: 3.2644 - val_accuracy: 0.5946\n",
            "Epoch 476/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1855 - accuracy: 0.9150 - val_loss: 3.3296 - val_accuracy: 0.5811\n",
            "Epoch 477/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1842 - accuracy: 0.9116 - val_loss: 3.3412 - val_accuracy: 0.5811\n",
            "Epoch 478/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1982 - accuracy: 0.9082 - val_loss: 3.3325 - val_accuracy: 0.5811\n",
            "Epoch 479/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1901 - accuracy: 0.9150 - val_loss: 3.3230 - val_accuracy: 0.5811\n",
            "Epoch 480/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1851 - accuracy: 0.9082 - val_loss: 3.3313 - val_accuracy: 0.6081\n",
            "Epoch 481/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1848 - accuracy: 0.9150 - val_loss: 3.3753 - val_accuracy: 0.6081\n",
            "Epoch 482/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1874 - accuracy: 0.9082 - val_loss: 3.4260 - val_accuracy: 0.6081\n",
            "Epoch 483/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1910 - accuracy: 0.9116 - val_loss: 3.4645 - val_accuracy: 0.6081\n",
            "Epoch 484/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1921 - accuracy: 0.9082 - val_loss: 3.5576 - val_accuracy: 0.5946\n",
            "Epoch 485/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1852 - accuracy: 0.9116 - val_loss: 3.6233 - val_accuracy: 0.5946\n",
            "Epoch 486/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1963 - accuracy: 0.9048 - val_loss: 3.6394 - val_accuracy: 0.6081\n",
            "Epoch 487/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1819 - accuracy: 0.9116 - val_loss: 3.6541 - val_accuracy: 0.6216\n",
            "Epoch 488/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1800 - accuracy: 0.9184 - val_loss: 3.6476 - val_accuracy: 0.6081\n",
            "Epoch 489/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1884 - accuracy: 0.9150 - val_loss: 3.6542 - val_accuracy: 0.6081\n",
            "Epoch 490/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1996 - accuracy: 0.8980 - val_loss: 3.6207 - val_accuracy: 0.6081\n",
            "Epoch 491/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1890 - accuracy: 0.9082 - val_loss: 3.5755 - val_accuracy: 0.5811\n",
            "Epoch 492/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1949 - accuracy: 0.9116 - val_loss: 3.5480 - val_accuracy: 0.5811\n",
            "Epoch 493/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1934 - accuracy: 0.9116 - val_loss: 3.5249 - val_accuracy: 0.5811\n",
            "Epoch 494/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1969 - accuracy: 0.9116 - val_loss: 3.4994 - val_accuracy: 0.5676\n",
            "Epoch 495/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1881 - accuracy: 0.9116 - val_loss: 3.4699 - val_accuracy: 0.5811\n",
            "Epoch 496/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1862 - accuracy: 0.9150 - val_loss: 3.4619 - val_accuracy: 0.5811\n",
            "Epoch 497/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1882 - accuracy: 0.9116 - val_loss: 3.4671 - val_accuracy: 0.5811\n",
            "Epoch 498/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1885 - accuracy: 0.9116 - val_loss: 3.4878 - val_accuracy: 0.5811\n",
            "Epoch 499/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1796 - accuracy: 0.9150 - val_loss: 3.5088 - val_accuracy: 0.5811\n",
            "Epoch 500/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1909 - accuracy: 0.9082 - val_loss: 3.5365 - val_accuracy: 0.5811\n",
            "Epoch 501/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1884 - accuracy: 0.9150 - val_loss: 3.5692 - val_accuracy: 0.5676\n",
            "Epoch 502/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1911 - accuracy: 0.9048 - val_loss: 3.5949 - val_accuracy: 0.5676\n",
            "Epoch 503/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1926 - accuracy: 0.9082 - val_loss: 3.6329 - val_accuracy: 0.5676\n",
            "Epoch 504/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1846 - accuracy: 0.9116 - val_loss: 3.6620 - val_accuracy: 0.5676\n",
            "Epoch 505/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1864 - accuracy: 0.9116 - val_loss: 3.6838 - val_accuracy: 0.5811\n",
            "Epoch 506/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1869 - accuracy: 0.9116 - val_loss: 3.6684 - val_accuracy: 0.5811\n",
            "Epoch 507/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2076 - accuracy: 0.9116 - val_loss: 3.5181 - val_accuracy: 0.5811\n",
            "Epoch 508/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2183 - accuracy: 0.9116 - val_loss: 3.4201 - val_accuracy: 0.5946\n",
            "Epoch 509/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1994 - accuracy: 0.9116 - val_loss: 3.4300 - val_accuracy: 0.5946\n",
            "Epoch 510/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1874 - accuracy: 0.9116 - val_loss: 3.4312 - val_accuracy: 0.5946\n",
            "Epoch 511/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1892 - accuracy: 0.9082 - val_loss: 3.4471 - val_accuracy: 0.6081\n",
            "Epoch 512/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1866 - accuracy: 0.9116 - val_loss: 3.4630 - val_accuracy: 0.5946\n",
            "Epoch 513/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1927 - accuracy: 0.9116 - val_loss: 3.4945 - val_accuracy: 0.5946\n",
            "Epoch 514/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1868 - accuracy: 0.9116 - val_loss: 3.5286 - val_accuracy: 0.5946\n",
            "Epoch 515/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1865 - accuracy: 0.9116 - val_loss: 3.5613 - val_accuracy: 0.5946\n",
            "Epoch 516/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1902 - accuracy: 0.9082 - val_loss: 3.5872 - val_accuracy: 0.5946\n",
            "Epoch 517/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1857 - accuracy: 0.9150 - val_loss: 3.6102 - val_accuracy: 0.5946\n",
            "Epoch 518/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1837 - accuracy: 0.9150 - val_loss: 3.6488 - val_accuracy: 0.5946\n",
            "Epoch 519/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1848 - accuracy: 0.9150 - val_loss: 3.6903 - val_accuracy: 0.5946\n",
            "Epoch 520/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2036 - accuracy: 0.9082 - val_loss: 3.7066 - val_accuracy: 0.5946\n",
            "Epoch 521/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1836 - accuracy: 0.9116 - val_loss: 3.7153 - val_accuracy: 0.5946\n",
            "Epoch 522/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1886 - accuracy: 0.9116 - val_loss: 3.6893 - val_accuracy: 0.5946\n",
            "Epoch 523/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1975 - accuracy: 0.9048 - val_loss: 3.6483 - val_accuracy: 0.6081\n",
            "Epoch 524/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1879 - accuracy: 0.9082 - val_loss: 3.6537 - val_accuracy: 0.5946\n",
            "Epoch 525/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1901 - accuracy: 0.9082 - val_loss: 3.6690 - val_accuracy: 0.5946\n",
            "Epoch 526/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1885 - accuracy: 0.9116 - val_loss: 3.6470 - val_accuracy: 0.5946\n",
            "Epoch 527/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1824 - accuracy: 0.9082 - val_loss: 3.6292 - val_accuracy: 0.5946\n",
            "Epoch 528/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1831 - accuracy: 0.9150 - val_loss: 3.6024 - val_accuracy: 0.6081\n",
            "Epoch 529/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1854 - accuracy: 0.9116 - val_loss: 3.6069 - val_accuracy: 0.6081\n",
            "Epoch 530/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1837 - accuracy: 0.9116 - val_loss: 3.6161 - val_accuracy: 0.6081\n",
            "Epoch 531/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1924 - accuracy: 0.9116 - val_loss: 3.6301 - val_accuracy: 0.5946\n",
            "Epoch 532/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1815 - accuracy: 0.9116 - val_loss: 3.6583 - val_accuracy: 0.5946\n",
            "Epoch 533/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1918 - accuracy: 0.9116 - val_loss: 3.6903 - val_accuracy: 0.5946\n",
            "Epoch 534/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1804 - accuracy: 0.9184 - val_loss: 3.7417 - val_accuracy: 0.5946\n",
            "Epoch 535/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1830 - accuracy: 0.9116 - val_loss: 3.7561 - val_accuracy: 0.5946\n",
            "Epoch 536/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1947 - accuracy: 0.9116 - val_loss: 3.7732 - val_accuracy: 0.5946\n",
            "Epoch 537/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1990 - accuracy: 0.9116 - val_loss: 3.8034 - val_accuracy: 0.5946\n",
            "Epoch 538/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1858 - accuracy: 0.9116 - val_loss: 3.8292 - val_accuracy: 0.5946\n",
            "Epoch 539/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1844 - accuracy: 0.9116 - val_loss: 3.8401 - val_accuracy: 0.5946\n",
            "Epoch 540/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1915 - accuracy: 0.9082 - val_loss: 3.8371 - val_accuracy: 0.5811\n",
            "Epoch 541/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1851 - accuracy: 0.9116 - val_loss: 3.8365 - val_accuracy: 0.5946\n",
            "Epoch 542/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1948 - accuracy: 0.9082 - val_loss: 3.8420 - val_accuracy: 0.5946\n",
            "Epoch 543/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1852 - accuracy: 0.9116 - val_loss: 3.8464 - val_accuracy: 0.5946\n",
            "Epoch 544/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1834 - accuracy: 0.9150 - val_loss: 3.8591 - val_accuracy: 0.5946\n",
            "Epoch 545/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1923 - accuracy: 0.9048 - val_loss: 3.8752 - val_accuracy: 0.6081\n",
            "Epoch 546/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1816 - accuracy: 0.9184 - val_loss: 3.8987 - val_accuracy: 0.6081\n",
            "Epoch 547/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1805 - accuracy: 0.9150 - val_loss: 3.9252 - val_accuracy: 0.5946\n",
            "Epoch 548/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1911 - accuracy: 0.9082 - val_loss: 3.9582 - val_accuracy: 0.5946\n",
            "Epoch 549/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1874 - accuracy: 0.9082 - val_loss: 3.9958 - val_accuracy: 0.5811\n",
            "Epoch 550/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1835 - accuracy: 0.9116 - val_loss: 4.0234 - val_accuracy: 0.5811\n",
            "Epoch 551/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1878 - accuracy: 0.9082 - val_loss: 4.0483 - val_accuracy: 0.5811\n",
            "Epoch 552/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1867 - accuracy: 0.9116 - val_loss: 4.0613 - val_accuracy: 0.5811\n",
            "Epoch 553/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1835 - accuracy: 0.9116 - val_loss: 4.0824 - val_accuracy: 0.5811\n",
            "Epoch 554/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1845 - accuracy: 0.9150 - val_loss: 4.0879 - val_accuracy: 0.5811\n",
            "Epoch 555/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1871 - accuracy: 0.9082 - val_loss: 4.1064 - val_accuracy: 0.5946\n",
            "Epoch 556/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1824 - accuracy: 0.9116 - val_loss: 4.1241 - val_accuracy: 0.5946\n",
            "Epoch 557/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1887 - accuracy: 0.9150 - val_loss: 4.1344 - val_accuracy: 0.6081\n",
            "Epoch 558/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1856 - accuracy: 0.9116 - val_loss: 4.1080 - val_accuracy: 0.5946\n",
            "Epoch 559/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1825 - accuracy: 0.9150 - val_loss: 4.1074 - val_accuracy: 0.6081\n",
            "Epoch 560/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1814 - accuracy: 0.9150 - val_loss: 4.1269 - val_accuracy: 0.6081\n",
            "Epoch 561/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1846 - accuracy: 0.9116 - val_loss: 4.1391 - val_accuracy: 0.6081\n",
            "Epoch 562/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1957 - accuracy: 0.9048 - val_loss: 4.1534 - val_accuracy: 0.6081\n",
            "Epoch 563/750\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1930 - accuracy: 0.9082 - val_loss: 3.9382 - val_accuracy: 0.6216\n",
            "Epoch 564/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2023 - accuracy: 0.9082 - val_loss: 3.7307 - val_accuracy: 0.6081\n",
            "Epoch 565/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1799 - accuracy: 0.9116 - val_loss: 3.6064 - val_accuracy: 0.6216\n",
            "Epoch 566/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1909 - accuracy: 0.9116 - val_loss: 3.5962 - val_accuracy: 0.6216\n",
            "Epoch 567/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1899 - accuracy: 0.9116 - val_loss: 3.6832 - val_accuracy: 0.6216\n",
            "Epoch 568/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1885 - accuracy: 0.9116 - val_loss: 3.7208 - val_accuracy: 0.6216\n",
            "Epoch 569/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1928 - accuracy: 0.9116 - val_loss: 3.8177 - val_accuracy: 0.5946\n",
            "Epoch 570/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.1875 - accuracy: 0.9116 - val_loss: 3.9246 - val_accuracy: 0.5946\n",
            "Epoch 571/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1836 - accuracy: 0.9116 - val_loss: 4.0244 - val_accuracy: 0.5946\n",
            "Epoch 572/750\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1985 - accuracy: 0.9048 - val_loss: 4.0204 - val_accuracy: 0.5811\n",
            "Epoch 573/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2006 - accuracy: 0.9082 - val_loss: 3.8401 - val_accuracy: 0.5946\n",
            "Epoch 574/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1866 - accuracy: 0.9116 - val_loss: 3.6497 - val_accuracy: 0.5946\n",
            "Epoch 575/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1778 - accuracy: 0.9150 - val_loss: 3.5179 - val_accuracy: 0.6081\n",
            "Epoch 576/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1953 - accuracy: 0.9082 - val_loss: 3.4566 - val_accuracy: 0.6216\n",
            "Epoch 577/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1821 - accuracy: 0.9116 - val_loss: 3.4224 - val_accuracy: 0.6216\n",
            "Epoch 578/750\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1907 - accuracy: 0.9116 - val_loss: 3.4131 - val_accuracy: 0.6081\n",
            "Epoch 579/750\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.1890 - accuracy: 0.9082 - val_loss: 3.4069 - val_accuracy: 0.6081\n",
            "Epoch 580/750\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.1858 - accuracy: 0.9150 - val_loss: 3.3896 - val_accuracy: 0.6081\n",
            "Epoch 581/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1908 - accuracy: 0.9116 - val_loss: 3.3877 - val_accuracy: 0.6081\n",
            "Epoch 582/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1852 - accuracy: 0.9116 - val_loss: 3.4041 - val_accuracy: 0.6081\n",
            "Epoch 583/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1949 - accuracy: 0.9082 - val_loss: 3.5048 - val_accuracy: 0.5946\n",
            "Epoch 584/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1864 - accuracy: 0.9150 - val_loss: 3.6025 - val_accuracy: 0.6081\n",
            "Epoch 585/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1918 - accuracy: 0.9116 - val_loss: 3.6481 - val_accuracy: 0.6081\n",
            "Epoch 586/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1872 - accuracy: 0.9116 - val_loss: 3.6576 - val_accuracy: 0.5946\n",
            "Epoch 587/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1824 - accuracy: 0.9082 - val_loss: 3.6771 - val_accuracy: 0.5946\n",
            "Epoch 588/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1877 - accuracy: 0.9116 - val_loss: 3.6999 - val_accuracy: 0.6081\n",
            "Epoch 589/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1847 - accuracy: 0.9150 - val_loss: 3.7272 - val_accuracy: 0.6216\n",
            "Epoch 590/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1888 - accuracy: 0.9116 - val_loss: 3.7471 - val_accuracy: 0.6216\n",
            "Epoch 591/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1832 - accuracy: 0.9150 - val_loss: 3.7587 - val_accuracy: 0.6216\n",
            "Epoch 592/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1808 - accuracy: 0.9150 - val_loss: 3.7699 - val_accuracy: 0.6081\n",
            "Epoch 593/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1877 - accuracy: 0.9150 - val_loss: 3.7828 - val_accuracy: 0.5946\n",
            "Epoch 594/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1914 - accuracy: 0.9116 - val_loss: 3.8106 - val_accuracy: 0.6081\n",
            "Epoch 595/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1940 - accuracy: 0.9116 - val_loss: 3.8307 - val_accuracy: 0.6081\n",
            "Epoch 596/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1902 - accuracy: 0.9116 - val_loss: 3.8349 - val_accuracy: 0.5946\n",
            "Epoch 597/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1876 - accuracy: 0.9150 - val_loss: 3.8357 - val_accuracy: 0.5946\n",
            "Epoch 598/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1880 - accuracy: 0.9048 - val_loss: 3.8415 - val_accuracy: 0.6081\n",
            "Epoch 599/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1815 - accuracy: 0.9150 - val_loss: 3.8626 - val_accuracy: 0.5946\n",
            "Epoch 600/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1834 - accuracy: 0.9116 - val_loss: 3.8638 - val_accuracy: 0.6081\n",
            "Epoch 601/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1777 - accuracy: 0.9150 - val_loss: 3.8815 - val_accuracy: 0.6216\n",
            "Epoch 602/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1814 - accuracy: 0.9150 - val_loss: 3.9223 - val_accuracy: 0.6081\n",
            "Epoch 603/750\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1886 - accuracy: 0.9116 - val_loss: 3.9431 - val_accuracy: 0.6081\n",
            "Epoch 604/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1788 - accuracy: 0.9150 - val_loss: 3.9440 - val_accuracy: 0.5946\n",
            "Epoch 605/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1854 - accuracy: 0.9116 - val_loss: 3.9330 - val_accuracy: 0.5946\n",
            "Epoch 606/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1815 - accuracy: 0.9150 - val_loss: 3.9708 - val_accuracy: 0.6081\n",
            "Epoch 607/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1850 - accuracy: 0.9116 - val_loss: 3.9906 - val_accuracy: 0.6081\n",
            "Epoch 608/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1783 - accuracy: 0.9150 - val_loss: 3.9948 - val_accuracy: 0.6081\n",
            "Epoch 609/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1842 - accuracy: 0.9150 - val_loss: 4.0054 - val_accuracy: 0.6081\n",
            "Epoch 610/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1835 - accuracy: 0.9150 - val_loss: 4.0391 - val_accuracy: 0.6216\n",
            "Epoch 611/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1862 - accuracy: 0.9150 - val_loss: 4.0444 - val_accuracy: 0.5946\n",
            "Epoch 612/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1755 - accuracy: 0.9150 - val_loss: 4.0530 - val_accuracy: 0.6081\n",
            "Epoch 613/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1879 - accuracy: 0.9116 - val_loss: 4.0952 - val_accuracy: 0.6081\n",
            "Epoch 614/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1870 - accuracy: 0.9116 - val_loss: 4.0876 - val_accuracy: 0.5946\n",
            "Epoch 615/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1851 - accuracy: 0.9150 - val_loss: 4.0847 - val_accuracy: 0.6081\n",
            "Epoch 616/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1747 - accuracy: 0.9184 - val_loss: 4.1666 - val_accuracy: 0.6081\n",
            "Epoch 617/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1882 - accuracy: 0.9116 - val_loss: 4.1658 - val_accuracy: 0.5946\n",
            "Epoch 618/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1863 - accuracy: 0.9116 - val_loss: 4.1371 - val_accuracy: 0.5811\n",
            "Epoch 619/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1807 - accuracy: 0.9150 - val_loss: 4.1775 - val_accuracy: 0.6081\n",
            "Epoch 620/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1860 - accuracy: 0.9082 - val_loss: 4.1091 - val_accuracy: 0.5946\n",
            "Epoch 621/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1831 - accuracy: 0.9116 - val_loss: 4.2188 - val_accuracy: 0.5946\n",
            "Epoch 622/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2033 - accuracy: 0.9116 - val_loss: 4.0317 - val_accuracy: 0.5946\n",
            "Epoch 623/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2186 - accuracy: 0.9048 - val_loss: 4.2940 - val_accuracy: 0.5811\n",
            "Epoch 624/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1976 - accuracy: 0.9048 - val_loss: 3.8082 - val_accuracy: 0.6081\n",
            "Epoch 625/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2382 - accuracy: 0.9014 - val_loss: 2.8707 - val_accuracy: 0.6216\n",
            "Epoch 626/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3062 - accuracy: 0.8912 - val_loss: 3.1675 - val_accuracy: 0.6351\n",
            "Epoch 627/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4445 - accuracy: 0.8605 - val_loss: 2.5490 - val_accuracy: 0.6216\n",
            "Epoch 628/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2693 - accuracy: 0.8946 - val_loss: 2.3353 - val_accuracy: 0.6216\n",
            "Epoch 629/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2242 - accuracy: 0.9048 - val_loss: 2.4416 - val_accuracy: 0.6216\n",
            "Epoch 630/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2408 - accuracy: 0.9014 - val_loss: 2.3499 - val_accuracy: 0.6351\n",
            "Epoch 631/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1961 - accuracy: 0.9082 - val_loss: 2.3536 - val_accuracy: 0.6081\n",
            "Epoch 632/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2060 - accuracy: 0.9082 - val_loss: 2.4835 - val_accuracy: 0.6081\n",
            "Epoch 633/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2032 - accuracy: 0.9014 - val_loss: 2.5832 - val_accuracy: 0.5946\n",
            "Epoch 634/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2238 - accuracy: 0.9014 - val_loss: 2.6029 - val_accuracy: 0.5946\n",
            "Epoch 635/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2041 - accuracy: 0.9048 - val_loss: 2.6428 - val_accuracy: 0.6081\n",
            "Epoch 636/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2284 - accuracy: 0.9048 - val_loss: 2.6609 - val_accuracy: 0.6081\n",
            "Epoch 637/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1953 - accuracy: 0.9116 - val_loss: 2.7290 - val_accuracy: 0.6081\n",
            "Epoch 638/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1851 - accuracy: 0.9116 - val_loss: 2.8248 - val_accuracy: 0.6216\n",
            "Epoch 639/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1920 - accuracy: 0.9082 - val_loss: 2.9024 - val_accuracy: 0.5946\n",
            "Epoch 640/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1902 - accuracy: 0.9082 - val_loss: 2.9415 - val_accuracy: 0.6081\n",
            "Epoch 641/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1907 - accuracy: 0.9116 - val_loss: 2.9823 - val_accuracy: 0.5811\n",
            "Epoch 642/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1917 - accuracy: 0.9082 - val_loss: 3.0196 - val_accuracy: 0.5811\n",
            "Epoch 643/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1949 - accuracy: 0.9116 - val_loss: 3.0167 - val_accuracy: 0.5811\n",
            "Epoch 644/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1860 - accuracy: 0.9116 - val_loss: 3.0366 - val_accuracy: 0.5946\n",
            "Epoch 645/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1905 - accuracy: 0.9116 - val_loss: 3.0907 - val_accuracy: 0.6081\n",
            "Epoch 646/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1859 - accuracy: 0.9116 - val_loss: 3.1497 - val_accuracy: 0.6081\n",
            "Epoch 647/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1937 - accuracy: 0.9116 - val_loss: 3.1977 - val_accuracy: 0.6081\n",
            "Epoch 648/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1821 - accuracy: 0.9116 - val_loss: 3.2036 - val_accuracy: 0.6216\n",
            "Epoch 649/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1874 - accuracy: 0.9116 - val_loss: 3.2020 - val_accuracy: 0.6216\n",
            "Epoch 650/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1904 - accuracy: 0.9082 - val_loss: 3.2069 - val_accuracy: 0.6216\n",
            "Epoch 651/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1896 - accuracy: 0.9116 - val_loss: 3.2029 - val_accuracy: 0.6081\n",
            "Epoch 652/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1841 - accuracy: 0.9150 - val_loss: 3.1837 - val_accuracy: 0.5946\n",
            "Epoch 653/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1869 - accuracy: 0.9082 - val_loss: 3.1639 - val_accuracy: 0.6081\n",
            "Epoch 654/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1816 - accuracy: 0.9116 - val_loss: 3.1518 - val_accuracy: 0.6216\n",
            "Epoch 655/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1868 - accuracy: 0.9116 - val_loss: 3.1511 - val_accuracy: 0.6081\n",
            "Epoch 656/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1862 - accuracy: 0.9082 - val_loss: 3.1631 - val_accuracy: 0.6216\n",
            "Epoch 657/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1864 - accuracy: 0.9116 - val_loss: 3.1794 - val_accuracy: 0.6216\n",
            "Epoch 658/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1887 - accuracy: 0.9116 - val_loss: 3.2004 - val_accuracy: 0.6216\n",
            "Epoch 659/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1870 - accuracy: 0.9116 - val_loss: 3.2436 - val_accuracy: 0.6081\n",
            "Epoch 660/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1761 - accuracy: 0.9184 - val_loss: 3.2906 - val_accuracy: 0.6216\n",
            "Epoch 661/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1878 - accuracy: 0.9082 - val_loss: 3.3228 - val_accuracy: 0.6216\n",
            "Epoch 662/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1877 - accuracy: 0.9116 - val_loss: 3.3580 - val_accuracy: 0.6216\n",
            "Epoch 663/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1916 - accuracy: 0.9116 - val_loss: 3.3964 - val_accuracy: 0.6216\n",
            "Epoch 664/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1852 - accuracy: 0.9150 - val_loss: 3.4454 - val_accuracy: 0.6351\n",
            "Epoch 665/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1810 - accuracy: 0.9116 - val_loss: 3.4882 - val_accuracy: 0.6216\n",
            "Epoch 666/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1886 - accuracy: 0.9150 - val_loss: 3.5217 - val_accuracy: 0.6081\n",
            "Epoch 667/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1853 - accuracy: 0.9116 - val_loss: 3.5119 - val_accuracy: 0.6081\n",
            "Epoch 668/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1821 - accuracy: 0.9150 - val_loss: 3.5105 - val_accuracy: 0.5946\n",
            "Epoch 669/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1876 - accuracy: 0.9116 - val_loss: 3.5061 - val_accuracy: 0.5946\n",
            "Epoch 670/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1862 - accuracy: 0.9150 - val_loss: 3.5164 - val_accuracy: 0.5946\n",
            "Epoch 671/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1925 - accuracy: 0.9116 - val_loss: 3.5162 - val_accuracy: 0.5946\n",
            "Epoch 672/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1812 - accuracy: 0.9116 - val_loss: 3.4936 - val_accuracy: 0.5946\n",
            "Epoch 673/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1903 - accuracy: 0.9014 - val_loss: 3.4348 - val_accuracy: 0.5811\n",
            "Epoch 674/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1821 - accuracy: 0.9116 - val_loss: 3.4376 - val_accuracy: 0.5946\n",
            "Epoch 675/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1862 - accuracy: 0.9116 - val_loss: 3.4267 - val_accuracy: 0.5946\n",
            "Epoch 676/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1926 - accuracy: 0.9116 - val_loss: 3.4164 - val_accuracy: 0.5946\n",
            "Epoch 677/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1946 - accuracy: 0.9116 - val_loss: 3.4023 - val_accuracy: 0.5946\n",
            "Epoch 678/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1920 - accuracy: 0.9116 - val_loss: 3.3761 - val_accuracy: 0.5811\n",
            "Epoch 679/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1885 - accuracy: 0.9116 - val_loss: 3.3508 - val_accuracy: 0.6081\n",
            "Epoch 680/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1818 - accuracy: 0.9116 - val_loss: 3.3552 - val_accuracy: 0.5946\n",
            "Epoch 681/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1816 - accuracy: 0.9150 - val_loss: 3.3665 - val_accuracy: 0.6081\n",
            "Epoch 682/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1906 - accuracy: 0.9048 - val_loss: 3.3801 - val_accuracy: 0.6081\n",
            "Epoch 683/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1880 - accuracy: 0.9082 - val_loss: 3.3951 - val_accuracy: 0.6081\n",
            "Epoch 684/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1903 - accuracy: 0.9082 - val_loss: 3.4188 - val_accuracy: 0.5946\n",
            "Epoch 685/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1893 - accuracy: 0.9116 - val_loss: 3.4793 - val_accuracy: 0.5946\n",
            "Epoch 686/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1919 - accuracy: 0.9116 - val_loss: 3.5402 - val_accuracy: 0.5811\n",
            "Epoch 687/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2035 - accuracy: 0.9116 - val_loss: 3.5536 - val_accuracy: 0.5811\n",
            "Epoch 688/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1880 - accuracy: 0.9116 - val_loss: 3.5379 - val_accuracy: 0.5811\n",
            "Epoch 689/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1857 - accuracy: 0.9116 - val_loss: 3.5332 - val_accuracy: 0.5811\n",
            "Epoch 690/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1854 - accuracy: 0.9116 - val_loss: 3.5357 - val_accuracy: 0.5811\n",
            "Epoch 691/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1843 - accuracy: 0.9082 - val_loss: 3.5468 - val_accuracy: 0.5946\n",
            "Epoch 692/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1837 - accuracy: 0.9116 - val_loss: 3.5643 - val_accuracy: 0.5946\n",
            "Epoch 693/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1808 - accuracy: 0.9184 - val_loss: 3.5817 - val_accuracy: 0.5946\n",
            "Epoch 694/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1873 - accuracy: 0.9116 - val_loss: 3.6074 - val_accuracy: 0.5946\n",
            "Epoch 695/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1890 - accuracy: 0.9116 - val_loss: 3.6552 - val_accuracy: 0.5811\n",
            "Epoch 696/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1823 - accuracy: 0.9150 - val_loss: 3.6915 - val_accuracy: 0.5811\n",
            "Epoch 697/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1846 - accuracy: 0.9150 - val_loss: 3.7580 - val_accuracy: 0.5811\n",
            "Epoch 698/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1898 - accuracy: 0.9150 - val_loss: 3.8536 - val_accuracy: 0.5676\n",
            "Epoch 699/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1878 - accuracy: 0.9116 - val_loss: 3.8619 - val_accuracy: 0.5811\n",
            "Epoch 700/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1864 - accuracy: 0.9116 - val_loss: 3.8528 - val_accuracy: 0.5811\n",
            "Epoch 701/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1882 - accuracy: 0.9150 - val_loss: 3.8275 - val_accuracy: 0.5811\n",
            "Epoch 702/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1918 - accuracy: 0.9082 - val_loss: 3.9740 - val_accuracy: 0.5676\n",
            "Epoch 703/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1843 - accuracy: 0.9116 - val_loss: 3.8911 - val_accuracy: 0.5811\n",
            "Epoch 704/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2088 - accuracy: 0.9082 - val_loss: 3.9257 - val_accuracy: 0.5541\n",
            "Epoch 705/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1909 - accuracy: 0.9116 - val_loss: 4.1018 - val_accuracy: 0.6216\n",
            "Epoch 706/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2487 - accuracy: 0.9048 - val_loss: 3.9186 - val_accuracy: 0.5541\n",
            "Epoch 707/750\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.2280 - accuracy: 0.9014 - val_loss: 3.7234 - val_accuracy: 0.5946\n",
            "Epoch 708/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3020 - accuracy: 0.8810 - val_loss: 3.1342 - val_accuracy: 0.6216\n",
            "Epoch 709/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3138 - accuracy: 0.8912 - val_loss: 2.8781 - val_accuracy: 0.5811\n",
            "Epoch 710/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3150 - accuracy: 0.8878 - val_loss: 2.9150 - val_accuracy: 0.6216\n",
            "Epoch 711/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4978 - accuracy: 0.8469 - val_loss: 2.7645 - val_accuracy: 0.5270\n",
            "Epoch 712/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3305 - accuracy: 0.8469 - val_loss: 2.1050 - val_accuracy: 0.5811\n",
            "Epoch 713/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3390 - accuracy: 0.8776 - val_loss: 1.8211 - val_accuracy: 0.6216\n",
            "Epoch 714/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2959 - accuracy: 0.8776 - val_loss: 1.5737 - val_accuracy: 0.6622\n",
            "Epoch 715/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2589 - accuracy: 0.8912 - val_loss: 1.5660 - val_accuracy: 0.6216\n",
            "Epoch 716/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2260 - accuracy: 0.9014 - val_loss: 1.7306 - val_accuracy: 0.6216\n",
            "Epoch 717/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2324 - accuracy: 0.8980 - val_loss: 1.9037 - val_accuracy: 0.6216\n",
            "Epoch 718/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2252 - accuracy: 0.8946 - val_loss: 1.9869 - val_accuracy: 0.5811\n",
            "Epoch 719/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2401 - accuracy: 0.8912 - val_loss: 2.1861 - val_accuracy: 0.5676\n",
            "Epoch 720/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2113 - accuracy: 0.9048 - val_loss: 2.2193 - val_accuracy: 0.5676\n",
            "Epoch 721/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2392 - accuracy: 0.8980 - val_loss: 2.4900 - val_accuracy: 0.5676\n",
            "Epoch 722/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2427 - accuracy: 0.8912 - val_loss: 2.8467 - val_accuracy: 0.5946\n",
            "Epoch 723/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2429 - accuracy: 0.8946 - val_loss: 2.9117 - val_accuracy: 0.6216\n",
            "Epoch 724/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2452 - accuracy: 0.9014 - val_loss: 2.6329 - val_accuracy: 0.5811\n",
            "Epoch 725/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2231 - accuracy: 0.8980 - val_loss: 2.4486 - val_accuracy: 0.5811\n",
            "Epoch 726/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1981 - accuracy: 0.9116 - val_loss: 2.3733 - val_accuracy: 0.5811\n",
            "Epoch 727/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2066 - accuracy: 0.9116 - val_loss: 2.4916 - val_accuracy: 0.5946\n",
            "Epoch 728/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2111 - accuracy: 0.9048 - val_loss: 2.6720 - val_accuracy: 0.5811\n",
            "Epoch 729/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1960 - accuracy: 0.9116 - val_loss: 2.7067 - val_accuracy: 0.5811\n",
            "Epoch 730/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1914 - accuracy: 0.9082 - val_loss: 2.7130 - val_accuracy: 0.5811\n",
            "Epoch 731/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1886 - accuracy: 0.9150 - val_loss: 2.6627 - val_accuracy: 0.5946\n",
            "Epoch 732/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2044 - accuracy: 0.8946 - val_loss: 2.6411 - val_accuracy: 0.5946\n",
            "Epoch 733/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1922 - accuracy: 0.9150 - val_loss: 2.5871 - val_accuracy: 0.6081\n",
            "Epoch 734/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1935 - accuracy: 0.9150 - val_loss: 2.6040 - val_accuracy: 0.6081\n",
            "Epoch 735/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1908 - accuracy: 0.9116 - val_loss: 2.5842 - val_accuracy: 0.6216\n",
            "Epoch 736/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1921 - accuracy: 0.9082 - val_loss: 2.5980 - val_accuracy: 0.6081\n",
            "Epoch 737/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1884 - accuracy: 0.9082 - val_loss: 2.6318 - val_accuracy: 0.5946\n",
            "Epoch 738/750\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.1939 - accuracy: 0.9082 - val_loss: 2.6938 - val_accuracy: 0.5946\n",
            "Epoch 739/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1857 - accuracy: 0.9116 - val_loss: 2.7519 - val_accuracy: 0.5811\n",
            "Epoch 740/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1830 - accuracy: 0.9150 - val_loss: 2.7937 - val_accuracy: 0.5811\n",
            "Epoch 741/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1892 - accuracy: 0.9150 - val_loss: 2.8427 - val_accuracy: 0.5811\n",
            "Epoch 742/750\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.1905 - accuracy: 0.9116 - val_loss: 2.8886 - val_accuracy: 0.5811\n",
            "Epoch 743/750\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1923 - accuracy: 0.9116 - val_loss: 2.8790 - val_accuracy: 0.5676\n",
            "Epoch 744/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1832 - accuracy: 0.9150 - val_loss: 2.7971 - val_accuracy: 0.5676\n",
            "Epoch 745/750\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.1831 - accuracy: 0.9116 - val_loss: 2.7158 - val_accuracy: 0.5811\n",
            "Epoch 746/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1789 - accuracy: 0.9184 - val_loss: 2.7046 - val_accuracy: 0.5811\n",
            "Epoch 747/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1902 - accuracy: 0.9116 - val_loss: 2.6830 - val_accuracy: 0.5946\n",
            "Epoch 748/750\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.1820 - accuracy: 0.9082 - val_loss: 2.6552 - val_accuracy: 0.5946\n",
            "Epoch 749/750\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1897 - accuracy: 0.9082 - val_loss: 2.6427 - val_accuracy: 0.5946\n",
            "Epoch 750/750\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1928 - accuracy: 0.9082 - val_loss: 2.6548 - val_accuracy: 0.6081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation with the RNN\n",
        "y_valid_CNN_RNN = CNN_RNN_model.predict(feature_valid)\n",
        "valid_y_CNN_RNN = y_valid_CNN_RNN.copy()\n",
        "for i in range(len(y_valid_CNN_RNN)):\n",
        "    j = np.where(y_valid_CNN_RNN[i] == np.amax(y_valid_CNN_RNN[i]))\n",
        "    valid_y_CNN_RNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN_RNN[i][j] = 1\n",
        "\n",
        "# print acc and report\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN_RNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN_RNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN_RNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv77kyW5kKti",
        "outputId": "5882d104-6dbf-42bb-b141-02b64fd69494"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 7ms/step\n",
            "0.6081081081081081\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.53      0.67        15\n",
            "           1       0.57      0.81      0.67        36\n",
            "           2       0.57      0.35      0.43        23\n",
            "\n",
            "   micro avg       0.61      0.61      0.61        74\n",
            "   macro avg       0.68      0.56      0.59        74\n",
            "weighted avg       0.63      0.61      0.59        74\n",
            " samples avg       0.61      0.61      0.61        74\n",
            "\n",
            "auc score:  0.6621952326764093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_CNN_RNN = CNN_RNN_model.predict(feature_test)\n",
        "# convert the test vector\n",
        "test_y_CNN_RNN = y_test_CNN_RNN.copy()\n",
        "for i in range(len(y_test_CNN_RNN)):\n",
        "    j = np.where(y_test_CNN_RNN[i] == np.amax(y_test_CNN_RNN[i]))\n",
        "    test_y_CNN_RNN[i] = [0, 0, 0]\n",
        "    test_y_CNN_RNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_test_y,test_y_CNN_RNN))\n",
        "print(classification_report(label_test_y,test_y_CNN_RNN))\n",
        "print(\"auc score: \",roc_auc_score(label_test_y,test_y_CNN_RNN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKxdxkT1kMhz",
        "outputId": "0fbd85da-4c8a-4106-8511-bf561bb2ae9b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 7ms/step\n",
            "0.5806451612903226\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.35      0.46        17\n",
            "           1       0.57      0.81      0.67        43\n",
            "           2       0.57      0.39      0.46        33\n",
            "\n",
            "   micro avg       0.58      0.58      0.58        93\n",
            "   macro avg       0.60      0.52      0.53        93\n",
            "weighted avg       0.59      0.58      0.56        93\n",
            " samples avg       0.58      0.58      0.58        93\n",
            "\n",
            "auc score:  0.6391156179841472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zFizuxTssBik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 预训练"
      ],
      "metadata": {
        "id": "0VPJnCA57ONW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# 加载预训练的 BERT 模型和分词器\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "# 创建一个字典来存储词及其对应的向量表示\n",
        "BERT = {}\n",
        "\n",
        "# 将 train_text_words 拆分成批次\n",
        "batch_size = 512\n",
        "num_batches = len(train_text_words) // batch_size + 1\n",
        "\n",
        "for i in range(num_batches):\n",
        "    # 获取当前批次的词\n",
        "    start_index = i * batch_size\n",
        "    end_index = min((i + 1) * batch_size, len(train_text_words))\n",
        "    batch_words = train_text_words[start_index:end_index]\n",
        "\n",
        "    # 将词转换为字符串列表\n",
        "    batch_sentences = [' '.join(batch_words)]\n",
        "\n",
        "    # 将字符串列表编码为 BERT 的输入格式\n",
        "    encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    # 使用 BERT 模型获取词的向量表示\n",
        "    with torch.no_grad():\n",
        "        model_outputs = bert_model(**encoded_inputs)\n",
        "        word_embeddings = model_outputs.last_hidden_state\n",
        "\n",
        "    # 获取每个词的向量表示\n",
        "    for sentence in batch_sentences:\n",
        "        # 将句子编码为 BERT 的输入格式\n",
        "        encoded_inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
        "        # 获取句子中的每个词\n",
        "        words = tokenizer.convert_ids_to_tokens(encoded_inputs['input_ids'][0])\n",
        "\n",
        "        for j, word in enumerate(words):\n",
        "            # 如果词是子词(以 ## 开头),则跳过\n",
        "            if word.startswith('##'):\n",
        "                continue\n",
        "\n",
        "            # 如果词还没有在字典中,则将其添加到字典中\n",
        "            if word not in BERT:\n",
        "                # 获取词的向量表示\n",
        "                word_vector = word_embeddings[0][j].cpu().numpy()\n",
        "                # 将词及其对应的向量表示添加到字典中\n",
        "                BERT[word] = word_vector\n",
        "\n",
        "# 打印词向量字典的大小\n",
        "print(\"词向量字典的大小:\", len(BERT))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652,
          "referenced_widgets": [
            "b07b4199b8d64178acef2910f3908e1f",
            "9ebf2a9a641a4a6e925119c2eb511f20",
            "f69191df65464684b4d09327385b975e",
            "0f8c80f5bfb54f35960e8e9a6e97a22e",
            "4b5dbab8171a45a8b43a230ca870321d",
            "5d41668d3603484a93d34767bf98442a",
            "77e9ddbec4194df6ab0641582b040034",
            "d720b2d313a44df384d35ced93e94c3c",
            "cbabbbde5c8c481ea29b5f6b5a8db4b3",
            "783e0bf2f66c4180aa4d1b6446ba3205",
            "6b5afafcfafc4669a3cc5fd7eaf0fe5d",
            "910e630657194385b1b260f15cade8a7",
            "12c8c500bf66422e95081607e85d0626",
            "85a732ce94c44d91b690a3330d85db7f",
            "2659c008781d4c9b99b2f2c488c51541",
            "b960f17aedd34de9ac1811e7d409d5b0",
            "21704c4e2926479baf6e311739e01bd9",
            "e27a9cbb9d3643b382fa8c3f8f81a174",
            "c158a2221ca14ed8bfff558e59ca9a41",
            "8eb3cace2fd14a179d13df1794ab273a",
            "de6650abbaea4f4d9b7d25ccc413f518",
            "8e38aab262b64ee593fd6b797c704f58",
            "e4cb53a4042843c69bad09739d30ce79",
            "84221b8dbfdb4252b4d1d334f7543ce9",
            "3733a8d2476c461e8fb8e8d355db03d8",
            "b320d6cfdddb4286a616ee9f05996e1d",
            "eb72a27acc3d404baf8ab82f55369fe5",
            "eb81dd1666ec4f8f852571f34e7ee965",
            "fc3384d778e842e9a299b06101368e46",
            "64f4f3263271499291952060d8ea89f8",
            "ccb65119fc874a83b5c5d57a2897234c",
            "a4f85c98a6b941489fbe02860f220ef9",
            "ac65e73df9234067899f33affa669219",
            "7c4a9e16a2384bc79b4721031cd2d103",
            "4b0af796114d4a4ab737047acc663eac",
            "796d55f88cc246dfbe936a83e16e5bb6",
            "f786a5fa7f614d51803b32093d5a47b7",
            "bb98b04dd5104614853b89dbad25e292",
            "cde2c17c43984dacb09e6232a2c6a69d",
            "43588aa756e44402aaf237c6178fcbd0",
            "a7c8448d91564699af670d8871cec092",
            "cc99cf52329e47e0abfb657860023835",
            "1c29c8c0dde248f6a92fdd0bf21d5adf",
            "5c0dffb3bf86461d8c1fb8c19d7aefb1",
            "51f24b6462d4402289d72c102858f052",
            "a49328cb681c4b58b06e228ff15bf1ae",
            "4a94b8c1747f45db942846c4665e3887",
            "88cd5cbb2efd4495abd04d62adb72f6e",
            "8b38c2da61ea4004a0524a2f9a4d3cbd",
            "45512c6ec39048e183bb88d7fbd547ab",
            "6820f0ff66894ef08549ac7326e1482b",
            "bc306a92790541a3acccdd20c1006043",
            "53097f73bb4544aca158b80fa791eab4",
            "033ec1d612384b8b8dffe1c5f1ef1742",
            "11dcad8f588a4a5eae89795a18411157"
          ]
        },
        "id": "JDu50tTeay-q",
        "outputId": "6f6e3748-791c-4a37-bc2f-3b22d0d19d68"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b07b4199b8d64178acef2910f3908e1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "910e630657194385b1b260f15cade8a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4cb53a4042843c69bad09739d30ce79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c4a9e16a2384bc79b4721031cd2d103"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51f24b6462d4402289d72c102858f052"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-93a00e4ba055>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# 使用 BERT 模型获取词的向量表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         )\n\u001b[0;32m--> 988\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    989\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 )\n\u001b[1;32m    581\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    515\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将BERT存储到文件\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/BERT.pkl', 'wb') as f:\n",
        "    pickle.dump(BERT, f)"
      ],
      "metadata": {
        "id": "yrcl63EzsPrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 从文件中加载BERT\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP_app/BERT.pkl', 'rb') as f:\n",
        "    loaded_BERT = pickle.load(f)\n",
        "\n",
        "# 现在可以使用加载后的BERT进行相关操作\n",
        "print(loaded_BERT['applies'])"
      ],
      "metadata": {
        "id": "MiefNFvysQf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BERT['applies']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diJhYpOGtZbY",
        "outputId": "0b2bdea3-4314-4ef6-e958-81939446ad95"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5.15429616e-01,  1.27438772e+00,  8.94290507e-01,  2.61702716e-01,\n",
              "        4.52505797e-01,  4.34847206e-01,  7.73696721e-01,  2.98062805e-02,\n",
              "        7.60203227e-02, -3.31532001e-01,  2.21079975e-01, -1.15030102e-01,\n",
              "        1.09592281e-01, -2.62939394e-01,  4.41898890e-02,  1.45336089e-03,\n",
              "        2.71368295e-01, -4.67662454e-01, -9.24305201e-01, -2.42684379e-01,\n",
              "        2.80374855e-01,  2.42467627e-01, -7.09719360e-01, -4.72268388e-02,\n",
              "        1.59784108e-01,  3.77158731e-01,  4.65178996e-01, -8.84478152e-01,\n",
              "       -2.99527436e-01, -5.44966638e-01,  7.43205547e-01,  4.52126145e-01,\n",
              "        3.93232852e-02, -2.94771850e-01, -7.91562617e-01, -6.51823699e-01,\n",
              "       -6.50480315e-02,  3.92983973e-01, -5.51143527e-01,  9.05102730e-01,\n",
              "       -9.19793487e-01, -4.46531802e-01,  8.16921443e-02, -1.17146127e-01,\n",
              "        4.46869671e-01, -3.59070063e-01, -7.50101030e-01, -8.96791458e-01,\n",
              "       -2.45118782e-01, -1.06866136e-01, -7.80152008e-02, -5.15955627e-01,\n",
              "        3.47108781e-01, -3.72703046e-01, -6.28207505e-01,  7.07743645e-01,\n",
              "        2.84336805e-01, -5.23704708e-01,  8.01809132e-01, -7.10895896e-01,\n",
              "       -4.68968242e-01, -3.29327822e-01,  3.61566283e-02, -7.47637451e-01,\n",
              "        1.02699302e-01,  2.73083508e-01, -4.91446406e-01,  1.68392509e-01,\n",
              "        1.98823228e-01,  2.07746968e-01,  3.36523682e-01, -3.65729332e-02,\n",
              "        5.58941960e-01,  2.21521914e-01,  2.14098483e-01,  3.05428684e-01,\n",
              "        1.20567933e-01,  1.13972902e+00,  6.26927972e-01, -8.07295516e-02,\n",
              "        1.96291968e-01,  1.21901476e+00, -3.42940927e-01,  2.20103532e-01,\n",
              "       -6.55952916e-02,  3.03533673e-02,  1.96657270e-01,  8.90809834e-01,\n",
              "       -6.44151866e-01,  8.05963933e-01,  5.85658967e-01, -8.73300195e-01,\n",
              "        3.11077505e-01, -6.67929649e-01,  4.54187989e-01,  4.98464346e-01,\n",
              "       -7.29705453e-01,  5.40533781e-01, -7.72380054e-01,  6.98700547e-01,\n",
              "       -1.94878578e-01,  9.44693863e-01,  4.08989012e-01, -2.47430280e-02,\n",
              "        4.81536776e-01, -2.19718352e-01, -4.55240339e-01,  8.35547209e-01,\n",
              "       -4.94185001e-01,  7.13259816e-01,  1.24818891e-01, -4.91307169e-01,\n",
              "       -1.32281169e-01, -5.25164366e-01,  4.60732192e-01, -3.06060910e-02,\n",
              "        2.48015419e-01,  9.68881547e-02, -2.97346622e-01,  4.82535362e-01,\n",
              "        1.85666960e-02, -5.61782539e-01, -1.89650595e-01,  3.04498553e-01,\n",
              "       -2.44915366e-01, -3.23554188e-01, -4.60699350e-01,  4.44484860e-01,\n",
              "        6.17993832e-01, -3.86092067e-02,  2.43713439e-01, -4.76648718e-01,\n",
              "       -1.14387097e-02,  4.56326127e-01, -4.70065296e-01,  2.36623779e-01,\n",
              "        6.10108256e-01, -1.01175614e-01, -3.57080877e-01,  7.59528697e-01,\n",
              "       -7.40090832e-02,  3.71431857e-01,  1.10277331e+00, -1.48609233e+00,\n",
              "        2.57528901e-01, -8.34469736e-01, -4.36835200e-01,  2.78023779e-01,\n",
              "       -8.26344967e-01,  2.07026601e-01,  7.49003664e-02,  4.42926735e-02,\n",
              "        1.55936241e-01,  1.10423222e-01, -2.46831905e-02, -4.54042405e-01,\n",
              "        2.17637554e-01, -2.99051404e-01, -5.75160235e-03, -7.01217413e-01,\n",
              "       -2.83816963e-01,  4.06633705e-01, -3.97639841e-01,  7.03901947e-02,\n",
              "        4.46018785e-01, -8.13365519e-01, -4.53447342e-01,  5.68988740e-01,\n",
              "        7.64851794e-02,  3.74304980e-01,  8.81779552e-01,  7.90390790e-01,\n",
              "        7.59369910e-01, -5.75423658e-01,  3.32286179e-01, -2.41254214e-02,\n",
              "        4.70328778e-02, -2.28910208e-01,  8.78685832e-01,  5.48728824e-01,\n",
              "        3.52134585e-01,  1.56015784e-01, -3.57002646e-01,  1.15454674e-01,\n",
              "        1.85256898e-01,  4.20858026e-01, -4.66622442e-01,  6.37383223e-01,\n",
              "        1.60052395e+00, -3.01441550e-01,  1.21292107e-01, -4.27078575e-01,\n",
              "       -4.26682472e-01,  2.45465249e-01,  6.03874803e-01,  2.68411040e-01,\n",
              "       -1.32223114e-01,  2.00969577e-01,  4.37638834e-02, -4.53684866e-01,\n",
              "        6.46153331e-01,  6.23619735e-01, -3.54697317e-01,  7.52676368e-01,\n",
              "       -6.30893409e-01, -3.76626998e-01,  1.01078069e+00, -3.27532113e-01,\n",
              "       -4.24829394e-01, -1.75808948e-02,  9.60313380e-02,  3.35433096e-01,\n",
              "       -2.16579391e-03, -9.59565163e-01, -4.22807127e-01, -2.54363090e-01,\n",
              "       -2.36873627e-01, -8.37804079e-02, -9.59966704e-02,  3.30039889e-01,\n",
              "       -2.54522022e-02, -3.27426940e-01, -6.67941272e-01,  6.33031249e-01,\n",
              "       -1.09458125e+00,  3.72298032e-01,  1.11170006e+00, -8.17625999e-01,\n",
              "       -4.18535799e-01, -4.15967822e-01, -2.23853439e-02, -1.27291277e-01,\n",
              "       -6.96096197e-02, -2.78532118e-01, -4.86809909e-01,  7.63407946e-02,\n",
              "        8.29854384e-02, -3.05093944e-01,  8.04174066e-01,  2.59657264e-01,\n",
              "        2.42940634e-01,  4.84179139e-01,  4.49371934e-01, -1.07981376e-01,\n",
              "        3.26026231e-01,  1.69728070e-01, -1.27209127e+00, -2.53662020e-01,\n",
              "       -5.71019948e-01,  1.64879963e-01, -6.67507410e-01,  3.05668294e-01,\n",
              "        6.48233414e-01, -2.19418466e-01, -2.26879343e-01,  5.16580462e-01,\n",
              "       -2.76614279e-02, -2.16628745e-01,  4.85333264e-01, -1.40340462e-01,\n",
              "        3.57108451e-02,  3.17854248e-02,  2.63971597e-01,  5.98222136e-01,\n",
              "        1.26531929e-01, -1.03320396e+00,  3.17725599e-01,  4.34594393e-01,\n",
              "        1.84866145e-01,  5.29683173e-01,  5.83162725e-01, -5.73071599e-01,\n",
              "        5.02897561e-01,  9.54223275e-01, -3.61032449e-02, -3.04407743e-03,\n",
              "        6.32522762e-01,  1.18078135e-01,  1.87454090e-01,  9.49497372e-02,\n",
              "       -1.46647930e-01, -7.75022134e-02, -7.74630249e-01,  5.43223321e-01,\n",
              "       -2.43189186e-01, -4.27593261e-01,  2.16243267e-01, -3.61667305e-01,\n",
              "       -1.83477812e-02, -3.64265889e-01, -9.95588899e-01,  1.19762290e+00,\n",
              "       -6.02281630e-01, -7.51312077e-01,  1.93257987e-01,  3.44937257e-02,\n",
              "       -4.34321254e-01, -1.67297572e-01, -7.03420877e-01, -1.16228998e+00,\n",
              "       -6.72991335e-01, -2.36017749e-01,  7.03683496e-01,  1.54037789e-01,\n",
              "        4.45614725e-01,  4.85685050e-01, -6.41290486e-01, -1.05015922e+00,\n",
              "       -3.92602849e+00, -1.49379164e-01, -6.55946851e-01,  2.85455883e-01,\n",
              "        1.35962522e+00, -4.34902519e-01,  1.44051635e+00,  4.68802989e-01,\n",
              "        4.08720002e-02, -5.62162399e-01,  6.31715238e-01,  1.24286182e-01,\n",
              "       -1.95691466e-01, -2.50454605e-01, -2.53612906e-01,  5.07652164e-01,\n",
              "       -1.04456675e+00,  1.63341641e-01, -7.98681937e-03,  8.62041056e-01,\n",
              "        2.47814715e-01, -8.53826702e-01,  2.09987834e-01,  2.33947090e-03,\n",
              "       -1.34638011e-01,  7.58538663e-01, -8.08296680e-01,  6.07984364e-01,\n",
              "        4.64762114e-02, -1.24029851e+00,  6.22241020e-01,  6.08824015e-01,\n",
              "       -5.68868637e-01,  4.02585305e-02, -2.88359284e-01,  5.84267020e-01,\n",
              "       -1.25934049e-01,  6.89441413e-02,  2.90588140e-01, -8.20525825e-01,\n",
              "        9.68465954e-02, -7.94378281e-01, -7.20170289e-02,  7.04540372e-01,\n",
              "        3.96264136e-01, -1.51580289e-01, -2.62117505e-01,  9.09429640e-02,\n",
              "        1.73580408e-01,  6.36513174e-01,  4.64370340e-01, -4.00589764e-01,\n",
              "       -3.57914776e-01, -5.46387844e-02, -3.43966424e-01, -6.76531792e-01,\n",
              "        8.14404547e-01,  4.43916589e-01, -1.09357738e+00, -6.32343769e-01,\n",
              "        7.99259245e-01, -3.62432331e-01, -1.02252758e+00,  2.64397085e-01,\n",
              "       -4.67857391e-01,  2.20016956e-01, -5.81888795e-01, -3.78637433e-01,\n",
              "       -9.97710764e-01, -3.79561126e-01,  1.83352157e-01,  1.03723729e+00,\n",
              "       -2.45299488e-01, -7.11398900e-01, -1.67212918e-01, -4.47914064e-01,\n",
              "        1.57837886e-02, -1.09657490e+00, -1.56872496e-01, -9.66786921e-01,\n",
              "       -2.24869639e-01,  9.64466929e-02,  5.73512077e-01, -1.44149721e-01,\n",
              "       -8.96683753e-01, -1.07400745e-01, -7.40187526e-01,  4.93134677e-01,\n",
              "       -5.93226790e-01, -4.34555739e-01,  2.05074519e-01,  2.30851665e-01,\n",
              "       -8.77412334e-02, -9.83160362e-02, -1.44026056e-01,  5.75717427e-02,\n",
              "        1.14908926e-01,  1.57570332e-01,  4.60185319e-01, -3.14675063e-01,\n",
              "        9.03355181e-01,  8.73906985e-02,  1.38454068e+00, -1.76693037e-01,\n",
              "        1.62395978e+00, -6.90535367e-01,  3.43434840e-01,  7.69995451e-02,\n",
              "        7.59812891e-01, -7.25748301e-01, -4.47827190e-01, -6.52937770e-01,\n",
              "        3.57576221e-01, -1.22515820e-01, -3.28931093e-01,  5.75343847e-01,\n",
              "       -5.10275722e-01,  5.87499738e-01, -5.79576716e-02, -1.26523003e-01,\n",
              "        6.59823239e-01, -3.76901656e-01, -5.77975571e-01, -7.65153229e-01,\n",
              "       -1.10889745e+00, -5.56214333e-01, -2.47171655e-01,  6.71468496e-01,\n",
              "        2.19606861e-01,  5.56385875e-01, -2.06253916e-01, -5.02563357e-01,\n",
              "        1.25699356e-01,  2.11469144e-01,  4.23130244e-01,  2.74833947e-01,\n",
              "        6.37118340e-01,  1.67256549e-01,  7.82936156e-01, -3.03383055e-03,\n",
              "        5.75650558e-02, -3.13681185e-01, -2.25006953e-01, -2.12990507e-01,\n",
              "       -6.44780040e-01,  9.20816302e-01, -1.13939993e-01,  4.01917011e-01,\n",
              "       -8.84449124e-01, -2.08026528e-01, -9.45925415e-01, -5.22234797e-01,\n",
              "       -3.99548769e-01, -1.00561118e+00, -9.06913757e-01,  2.11167678e-01,\n",
              "       -2.60535896e-01, -6.85346007e-01, -3.34294736e-02, -8.41842294e-02,\n",
              "       -3.25680405e-01, -6.84223056e-01, -5.67510426e-01,  4.83452350e-01,\n",
              "       -6.17016554e-01,  5.33632874e-01, -3.78792509e-02, -7.14107633e-01,\n",
              "        6.67284429e-01,  1.59576267e-01, -2.87067056e-01, -1.43386096e-01,\n",
              "       -2.39515126e-01, -1.05104506e+00,  2.22843379e-01,  1.67277706e+00,\n",
              "       -2.85416376e-02,  2.52768338e-01,  4.55673695e-01, -5.81267849e-02,\n",
              "       -6.30736589e-01, -3.48676741e-01, -5.04610002e-01, -6.56588793e-01,\n",
              "       -9.80216980e-01,  3.06419641e-01, -3.91211808e-02, -4.49052453e-01,\n",
              "        2.51749098e-01, -2.27287441e-01, -3.42646018e-02,  3.74441087e-01,\n",
              "        2.15618145e-02,  2.91017964e-02,  4.27574873e-01, -2.01344222e-01,\n",
              "        5.05009532e-01,  1.41669750e+00,  7.19403625e-02,  4.66302127e-01,\n",
              "       -5.60363233e-01,  8.56324434e-01,  7.36967981e-01,  2.05798119e-01,\n",
              "       -5.71766347e-02, -1.93720102e-01, -8.37669432e-01, -9.42012072e-02,\n",
              "        1.30463630e-01, -5.01515508e-01, -1.25810012e-01, -9.65543926e-01,\n",
              "        7.24549353e-01,  2.16131732e-01, -7.89266050e-01, -5.45134187e-01,\n",
              "        2.37243116e-01, -2.89238006e-01, -4.77485687e-01,  6.87911391e-01,\n",
              "       -1.26159579e-01,  5.03686130e-01, -1.38701007e-01,  1.41637802e-01,\n",
              "        1.80466130e-01, -4.12561625e-01,  2.30209529e-01, -2.32353136e-01,\n",
              "        5.40292859e-01,  2.87747800e-01,  1.01707943e-01, -3.93453211e-01,\n",
              "        6.58254698e-02,  3.68034661e-01, -6.93298042e-01,  5.34152031e-01,\n",
              "       -2.41551042e-01, -9.25664306e-01,  1.02045739e+00, -3.40796769e-01,\n",
              "       -8.36307287e-01, -2.43419752e-01,  1.29314408e-01, -9.05896723e-02,\n",
              "        8.67065609e-01, -2.80851692e-01, -4.24176306e-01, -4.99031216e-01,\n",
              "       -8.40023696e-01,  4.77497518e-01,  1.69034570e-01,  3.54059309e-01,\n",
              "       -1.03408523e-01,  5.14780521e-01,  6.89131200e-01, -2.41927907e-01,\n",
              "       -6.18860304e-01,  1.67190060e-01, -3.41150343e-01, -5.19552231e-01,\n",
              "       -4.28583354e-01,  7.78621852e-01, -4.40295666e-01, -2.30263412e-01,\n",
              "        6.13088906e-01,  5.80371499e-01,  1.58948228e-01,  6.81570709e-01,\n",
              "        3.44297558e-01, -7.50701964e-01, -2.04355404e-01, -9.63933647e-01,\n",
              "        4.82161790e-01,  7.36618936e-01,  1.40428394e-01, -6.22847155e-02,\n",
              "       -4.72380333e-02,  8.77624631e-01,  3.37914914e-01,  2.63187528e-01,\n",
              "       -1.69759184e-01, -1.52409092e-01, -1.16671681e+00,  1.20730013e-01,\n",
              "        9.76880014e-01, -1.35777965e-01, -4.52714413e-01,  8.87920499e-01,\n",
              "       -1.70424342e-01, -2.29320452e-01, -7.83661455e-02,  1.17587030e-01,\n",
              "       -4.17296410e-01, -1.85002744e-01, -7.73516774e-01,  2.28549093e-01,\n",
              "       -1.08388849e-01,  3.41458023e-01,  8.86932313e-01, -5.09415567e-01,\n",
              "        5.28118968e-01,  4.30206716e-01,  9.02699471e-01,  1.30011380e-01,\n",
              "       -5.54562151e-01,  7.47857928e-01,  9.09237266e-02, -1.85576990e-01,\n",
              "       -4.74499971e-01,  5.23365378e-01, -5.66210866e-01, -1.06861019e+00,\n",
              "        3.34032744e-01,  2.70418942e-01, -6.06236309e-02, -2.52813637e-01,\n",
              "       -8.06493282e-01, -5.41254640e-01,  6.46165669e-01,  1.19819212e+00,\n",
              "        3.91963005e-01,  8.03199291e-01,  6.88080966e-01, -2.52524972e-01,\n",
              "        9.37641084e-01,  4.04115349e-01,  1.79039314e-02, -2.64638126e-01,\n",
              "       -7.80760705e-01, -2.07159683e-01,  6.81420863e-01,  5.62093556e-01,\n",
              "       -8.09627235e-01, -3.60371359e-02,  2.01077595e-01,  4.61071372e-01,\n",
              "        1.46307603e-01, -2.68485606e-01,  4.64424610e-01, -1.03104603e+00,\n",
              "       -1.40423656e-01,  1.36101767e-01,  7.15742111e-01,  2.41124570e-01,\n",
              "       -2.33447745e-01, -3.76183748e-01, -2.41887063e-01, -1.23813473e-01,\n",
              "       -3.31135243e-01,  7.71579325e-01, -4.83166501e-02, -9.69866663e-02,\n",
              "       -3.12648833e-01, -9.10886168e-01,  2.69058645e-01, -1.19077694e+00,\n",
              "        3.86179209e-01,  3.49237651e-01,  8.45256031e-01,  4.83069569e-01,\n",
              "        1.21074058e-01,  9.33780372e-01, -5.52265763e-01, -6.43575549e-01,\n",
              "       -2.96485866e-03,  7.33924657e-02,  1.36326742e+00, -1.55536890e-01,\n",
              "        1.38782500e-03,  1.49750570e-02, -3.36363435e-01, -8.95889044e-01,\n",
              "       -5.40435553e-01, -5.71525514e-01,  3.22050422e-01, -1.96729258e-01,\n",
              "       -9.82671604e-02, -1.11538815e+00, -1.88680813e-01,  7.88076162e-01,\n",
              "        4.79810178e-01, -1.11351264e+00, -3.19509536e-01, -9.47599053e-01,\n",
              "        2.90205330e-01,  2.04636097e-01,  7.46272728e-02,  3.14506739e-01,\n",
              "       -2.89210051e-01, -4.86677349e-01, -4.83002007e-01, -1.85028717e-01,\n",
              "       -2.08541110e-01, -3.44603688e-01, -1.20368076e-03, -7.40467250e-01,\n",
              "        1.39108017e-01,  3.67467493e-01,  3.14689636e-01,  7.55518079e-02,\n",
              "       -4.95484509e-02, -1.01295829e+00, -2.74261206e-01,  3.95658100e-03,\n",
              "        1.22791074e-01,  8.24718773e-01,  1.25071660e-01,  5.39886594e-01,\n",
              "       -4.30819333e-01, -3.88839811e-01,  7.76671588e-01,  4.00860578e-01,\n",
              "       -4.07537490e-01, -1.15249991e+00, -3.88975441e-01,  4.01166260e-01,\n",
              "        2.77136505e-01,  3.56216937e-01, -3.31363618e-01, -1.85604662e-01,\n",
              "       -8.09686407e-02,  1.71042338e-01,  4.80471373e-01,  6.53838098e-01,\n",
              "       -5.95630229e-01,  2.37535194e-01,  7.95012593e-01, -1.27954453e-01,\n",
              "        3.31698924e-01, -6.21574104e-01, -4.41167830e-03,  7.98856139e-01,\n",
              "        1.67085767e-01,  7.15406775e-01,  1.39110148e-01, -9.53506008e-02,\n",
              "       -3.99826795e-01,  1.18921816e+00, -1.37165800e-01,  5.84468186e-01,\n",
              "        2.45328397e-01, -9.82560068e-02, -2.83710770e-02,  4.60188717e-01,\n",
              "        6.77428365e-01,  2.10723907e-01, -7.50535190e-01,  4.56789322e-02,\n",
              "       -7.02289522e-01, -6.37302101e-02, -6.37749374e-01,  3.50728661e-01,\n",
              "       -2.13623613e-01, -8.00529271e-02, -7.05066264e-01, -8.67534161e-01,\n",
              "        3.67807180e-01, -7.23968923e-01, -1.44770950e-01, -4.62057954e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key_words = ['asset', 'diversification', 'risk', 'allocation', 'investing', 'income', 'passive', 'active', 'fundamental', 'technical', 'equities', 'bonds', 'funds', 'ETFs', 'tolerance', 'portfolio', 'returns']"
      ],
      "metadata": {
        "id": "3iDvmG0TDAvA"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_n_closer(word, n, word2vec):\n",
        "    vect = word2vec[word]\n",
        "    dist_dict = {k: cosine(v, vect) for k, v in word2vec.items()}\n",
        "    closer_words = []\n",
        "    for _ in range(n):\n",
        "        min_key = min(dist_dict.keys(), key=lambda k: dist_dict[k])\n",
        "        closer_words.append(min_key)\n",
        "        del dist_dict[min_key]\n",
        "    return closer_words\n",
        "\n",
        "##knowledge base\n",
        "def create_knowledge_base(num_neighbors, word2vec, key_words):\n",
        "    knowledge_base = set()\n",
        "    out = display(progress(0, len(key_words)-1), display_id=True)\n",
        "    for ii, key_word in enumerate(key_words) :\n",
        "        knowledge_base.add(key_word)\n",
        "        neighbors = []\n",
        "        try :\n",
        "            neighbors = get_n_closer(key_word, num_neighbors, word2vec)\n",
        "        except :\n",
        "            print(key_word + ' not in BERT')\n",
        "\n",
        "        knowledge_base.update(neighbors)\n",
        "\n",
        "        out.update(progress(ii, len(key_words)-1))\n",
        "    return knowledge_base\n",
        "\n",
        "knowledge_base = create_knowledge_base(10, BERT, key_words)\n",
        "print(knowledge_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "KONLKfO9hqWh",
        "outputId": "d2db8a79-52d0-451c-adeb-0f71bd776016"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='16'\n",
              "            max='16',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            16\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diversification not in BERT\n",
            "equities not in BERT\n",
            "ETFs not in BERT\n",
            "{'concentrate', 'appropriate', 'percentage', 'engaging', 'maintenance', 'returns', 'certificates', 'offset', 'requirements', 'investor', 'reaction', 'directional', 'marketing', 'invest', 'expense', 'ETFs', 'assist', 'trusts', 'tolerance', 'interest', 'neutral', 'detailed', 'loans', 'share', 'practical', 'equities', 'transactions', 'bond', 'statutory', 'quantitative', 'savings', 'return', 'portfolio', 'wealth', 'investors', 'investments', 'heading', 'speculative', 'income', 'passive', 'legal', 'technical', 'diversification', 'risks', 'aggressive', 'asset', 'funding', 'accounts', 'yields', 'account', 'accepts', 'contractual', 'premium', 'prediction', 'complex', 'renewal', 'transition', 'fund', 'value', 'shares', 'capital', 'funds', 'investment', 'withdrawing', 'bills', 'dependency', 'exclusive', 'earnings', 'annual', 'invested', 'concentration', 'risk', 'guarantees', 'open', 'researching', 'calculations', 'norm', 'investing', 'allocation', 'harder', 'bonds', 'debts', 'negotiation', 'none', 'securities', 'proficiency', 'organized', 'encounters', 'authorized', 'current', 'incurred', 'sensitivity', 'weakness', 'extension', 'explaining', 'reports', 'free', 'tax', 'skill', 'estate', 'rigorous', 'curve', 'fundamental', 'revenue', 'appreciation', 'updates', 'basic', 'policies', 'active', 'combine', 'focus'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(knowledge_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADobXfjxYYow",
        "outputId": "ef3cb44f-0286-4bc9-bd29-97a324139983"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes a summary, the knowledge base and some hyper parameters and returns the \"num_sent\" sentences\n",
        "# of the summary that are closer to the the knowledge base in term of spacial distances.\n",
        "def extract_sentence_distance(summary, knowledge, n_closer, n_reject, num_sent):\n",
        "    # Split the summary into sentences.\n",
        "    sentences = sent_tokenize(summary)\n",
        "    sentence_scores = []\n",
        "    # Loop over the sentences.\n",
        "    for j, sentence in enumerate(sentences):\n",
        "        # we tokenize and clean the sentence\n",
        "        tokens = tokenizer(sentence)\n",
        "\n",
        "        sentence_barycentre = np.zeros(embedding_size)\n",
        "        effective_len = 0\n",
        "        # Compute the barycentre of the sentence\n",
        "        for token in tokens :\n",
        "            try :\n",
        "                sentence_barycentre += np.array(word2vec[token])\n",
        "                effective_len += 1\n",
        "            except KeyError :\n",
        "                pass\n",
        "            except :\n",
        "                raise\n",
        "\n",
        "        # Reject sentences with less than n_reject words in our word2vec map\n",
        "        if effective_len <= n_reject :\n",
        "            sentence_scores.append(1)\n",
        "\n",
        "        else :\n",
        "            sentence_barycentre = sentence_barycentre/effective_len\n",
        "            # Compute the distance sentece_barycentre -> words in our knowledge base\n",
        "            barycentre_distance = [cosine(sentence_barycentre, word2vec[key_word]) for key_word in knowledge]\n",
        "            barycentre_distance.sort()\n",
        "            # Create the score of the sentence by averaging the \"n_closer\" smallest distances\n",
        "            score = np.mean(barycentre_distance[:n_closer])\n",
        "            sentence_scores.append(score)\n",
        "    # Select the \"num_sent\" sentences that have the smallest score (smallest distance score with the knowledge base)\n",
        "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
        "    top_sentences = sentences[:num_sent]\n",
        "    return ' '.join(top_sentences)\n",
        "\n",
        "#prepare the train,validation and test dataframe\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "X_valid_df = pd.DataFrame(X_valid)\n",
        "X_test_df = pd.DataFrame(X_test)\n",
        "\n",
        "X_train_df['sentences_distance'] = X_train_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
        "X_valid_df['sentences_distance'] = X_valid_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
        "X_test_df['sentences_distance'] = X_test_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)"
      ],
      "metadata": {
        "id": "9lXShspr7UIu"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY0LAcQxmYWs",
        "outputId": "1e3d2ef4-2c87-410f-b864-1c607664b7d0"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149    MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...\n",
              "436    INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...\n",
              "394    Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...\n",
              "440    INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...\n",
              "330    Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...\n",
              "                             ...                        \n",
              "152    MainStay VP MacKay International Equity Portfo...\n",
              "158    MainStay VP MacKay Small Cap Core Portfolio\\n\\...\n",
              "374    Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...\n",
              "308    PIMCO Gurtin California Municipal Intermediate...\n",
              "287    Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...\n",
              "Name: summary, Length: 294, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "nYK_gIIZmmDq",
        "outputId": "d50b63be-97cb-4834-c032-7eb3c45e489f"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               summary  \\\n",
              "149  MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...   \n",
              "436  INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...   \n",
              "394  Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...   \n",
              "440  INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...   \n",
              "330  Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...   \n",
              "..                                                 ...   \n",
              "152  MainStay VP MacKay International Equity Portfo...   \n",
              "158  MainStay VP MacKay Small Cap Core Portfolio\\n\\...   \n",
              "374  Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...   \n",
              "308  PIMCO Gurtin California Municipal Intermediate...   \n",
              "287  Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...   \n",
              "\n",
              "                                    sentences_distance  \n",
              "149  19\\n\\nMainStay VP Epoch U.S. Small Cap Portfol...  \n",
              "436  1 Year\\t3 Years\\t5 Years\\t10 Years\\n$93\\t$290\\...  \n",
              "394  2. 3. A higher portfolio turnover rate may ind...  \n",
              "440  1 Year\\t3 Years\\t5 Years\\t10 Years\\nFund Share...  \n",
              "330  \"Growth\" Investing. \"Growth\" stocks can perfor...  \n",
              "..                                                 ...  \n",
              "152  35\\n\\nMainStay VP MacKay International Equity ...  \n",
              "158  47\\n\\nMainStay VP MacKay Small Cap Core Portfo...  \n",
              "374  A figure of 1.00 would indicate perfect correl...  \n",
              "308  A higher portfolio turnover rate may indicate ...  \n",
              "287  2 \\tWith limited exceptions, for Class A share...  \n",
              "\n",
              "[294 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e3c7097-d70e-428e-ad37-71b9d894e19e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>sentences_distance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...</td>\n",
              "      <td>19\\n\\nMainStay VP Epoch U.S. Small Cap Portfol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...</td>\n",
              "      <td>1 Year\\t3 Years\\t5 Years\\t10 Years\\n$93\\t$290\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...</td>\n",
              "      <td>2. 3. A higher portfolio turnover rate may ind...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...</td>\n",
              "      <td>1 Year\\t3 Years\\t5 Years\\t10 Years\\nFund Share...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...</td>\n",
              "      <td>\"Growth\" Investing. \"Growth\" stocks can perfor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>MainStay VP MacKay International Equity Portfo...</td>\n",
              "      <td>35\\n\\nMainStay VP MacKay International Equity ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>MainStay VP MacKay Small Cap Core Portfolio\\n\\...</td>\n",
              "      <td>47\\n\\nMainStay VP MacKay Small Cap Core Portfo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...</td>\n",
              "      <td>A figure of 1.00 would indicate perfect correl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>PIMCO Gurtin California Municipal Intermediate...</td>\n",
              "      <td>A higher portfolio turnover rate may indicate ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...</td>\n",
              "      <td>2 \\tWith limited exceptions, for Class A share...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e3c7097-d70e-428e-ad37-71b9d894e19e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e3c7097-d70e-428e-ad37-71b9d894e19e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e3c7097-d70e-428e-ad37-71b9d894e19e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0f99e8dd-7d06-4051-ba71-6c4060511492\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0f99e8dd-7d06-4051-ba71-6c4060511492')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0f99e8dd-7d06-4051-ba71-6c4060511492 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df",
              "summary": "{\n  \"name\": \"X_train_df\",\n  \"rows\": 294,\n  \"fields\": [\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 292,\n        \"samples\": [\n          \"Vanguard Short-Term Treasury Index Fund\\n\\nInvestment Objective\\n\\nThe Fund seeks to track the performance of a market-weighted Treasury index with a short-term dollar-weighted average maturity.\\n\\nFees and Expenses\\n\\nThe following table describes the fees and expenses you may pay if you buy and hold Admiral Shares of the Fund.\\n\\nShareholder Fees\\t \\n(Fees paid directly from your investment)\\t \\n \\nSales Charge (Load) Imposed on Purchases\\tNone\\nPurchase Fee\\tNone\\nSales Charge (Load) Imposed on Reinvested Dividends\\tNone\\nRedemption Fee\\tNone\\nAccount Service Fee (for certain fund account balances below $10,000)\\t$20/year\\n \\nAnnual Fund Operating Expenses\\t \\n(Expenses that you pay each year as a percentage of the value of your investment)\\t \\n \\nManagement Fees\\t0.06%\\n12b-1 Distribution Fee\\tNone\\nOther Expenses\\t0.01%\\nTotal Annual Fund Operating Expenses\\t0.07%\\n \\n\\nExample\\n\\nThe following example is intended to help you compare the cost of investing in the Fund\\u2019s Admiral Shares with the cost of investing in other mutual funds. It illustrates the hypothetical expenses that you would incur over various periods if you were to invest $10,000 in the Fund\\u2019s shares. This example assumes that the shares provide a return of 5% each year and that total annual fund operating expenses remain as stated in the preceding table. You would incur these hypothetical expenses whether or not you redeem your investment at the end of the given period. Although your actual costs may be higher or lower, based on these assumptions your costs would be:\\n\\n1 Year\\t3 Years\\t5 Years\\t10 Years\\n$7\\t$23\\t$40\\t$90\\n \\n\\n1\\n\\n \\n\\nPortfolio Turnover\\n\\nThe Fund pays transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d its portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in more taxes when Fund shares are held in a taxable account. These costs, which are not reflected in annual fund operating expenses or in the previous expense example, reduce the Fund\\u2019s performance. During the most recent fiscal year, the Fund\\u2019s portfolio turnover rate was 67% of the average value of its portfolio.\\n\\nPrincipal Investment Strategies\\n\\nThe Fund employs an indexing investment approach designed to track the performance of the Bloomberg Barclays US Treasury 1\\u20133 Year Bond Index. This Index includes fixed income securities issued by the U.S. Treasury (not including inflation-protected securities), all with maturities between 1 and 3 years.\\n\\nThe Fund invests by sampling the Index, meaning that it holds a range of securities that, in the aggregate, approximates the full Index in terms of key risk factors and other characteristics. All of the Fund\\u2019s investments will be selected through the sampling process, and under normal circumstances, at least 80% of the Fund\\u2019s assets will be invested in bonds included in the Index. The Fund maintains a dollar-weighted average maturity consistent with that of the Index. As of August 31, 2018, the dollar-weighted average maturity of the Index was 2.0 years.\\n\\nPrincipal Risks\\n\\nThe Fund is designed for investors with a low tolerance for risk, but you could still lose money by investing in it. The Fund is subject to the following risks, which could affect the Fund\\u2019s performance:\\n\\n\\u2022 Interest rate risk, which is the chance that bond prices will decline because of rising interest rates. Interest rate risk should be low for the Fund because it invests primarily in short-term bonds, whose prices are less sensitive to interest rate changes than are the prices of longer-term bonds.\\n\\n\\u2022 Income risk, which is the chance that the Fund\\u2019s income will decline because of falling interest rates. Income risk is generally high for short-term bond funds, so investors should expect the Fund\\u2019s monthly income to fluctuate.\\n\\n\\u2022 Index sampling risk, which is the chance that the securities selected for the Fund, in the aggregate, will not provide investment performance matching that of the Fund\\u2018s target index. Index sampling risk for the Fund is expected to be low.\\n\\nAn investment in the Fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency.\\n\\n2\\n\\n \\n\\nAnnual Total Returns\\n\\nThe following bar chart and table are intended to help you understand the risks of investing in the Fund. The bar chart shows how the performance of the Fund\\u2019s Admiral Shares has varied from one calendar year to another over the periods shown. The table shows how the average annual total returns of the Admiral Shares compare with those of the Fund\\u2019s target index, which has investment characteristics similar to those of the Fund. The Fund\\u2019s Signal\\u00ae Shares were renamed Admiral Shares on October 16, 2013. Keep in mind that the Fund\\u2019s past performance (before and after taxes) does not indicate how the Fund will perform in the future. Updated performance information is available on our website at vanguard.com/performance or by calling Vanguard toll-free at 800-662-7447.\\n\\nAnnual Total Returns \\u2014 Vanguard Short-Term Treasury Index Fund Admiral Shares1\\n\\n\\n1 The year-to-date return as of the most recent calendar quarter, which ended on September 30, 2018, was 0.17%.\\n\\nDuring the periods shown in the bar chart, the highest return for a calendar quarter was 1.16% (quarter ended June 30, 2010), and the lowest return for a quarter was \\u20130.45% (quarter ended December 31, 2016).\\n\\n3\\n\\n \\n\\nAverage Annual Total Returns for Periods Ended December 31, 2017\\t \\t \\n \\t \\t \\tSince\\n \\t \\t \\tInception\\n \\t \\t \\t(Dec. 28,\\n \\t1 Year\\t5 Years\\t2009)\\nVanguard Short-Term Treasury Index Fund Admiral Shares\\t \\t \\t \\nReturn Before Taxes\\t0.40%\\t0.50%\\t0.81%\\nReturn After Taxes on Distributions\\t\\u20130.08\\t0.20\\t0.54\\nReturn After Taxes on Distributions and Sale of Fund Shares\\t0.22\\t0.25\\t0.51\\nBloomberg Barclays US Treasury 1-3 Year Bond Index\\t \\t \\t \\n(reflects no deduction for fees, expenses, or taxes)\\t0.42%\\t0.57%\\t0.89%\\n \\n\\nActual after-tax returns depend on your tax situation and may differ from those shown in the preceding table. When after-tax returns are calculated, it is assumed that the shareholder was in the highest individual federal marginal income tax bracket at the time of each distribution of income or capital gains or upon redemption. State and local income taxes are not reflected in the calculations. Please note that after-tax returns are not relevant for a shareholder who holds fund shares in a tax-deferred account, such as an individual retirement account or a 401(k) plan. Also, figures captioned Return After Taxes on Distributions and Sale of Fund Shares may be higher than other figures for the same period if a capital loss occurs upon redemption and results in an assumed tax deduction for the shareholder.\\n\\nInvestment Advisor\\nThe Vanguard Group, Inc. (Vanguard)\\n\\nPortfolio Manager\\n\\nJoshua C. Barrickman, CFA, Principal of Vanguard and head of Vanguard\\u2019s Fixed Income Indexing Americas. He has managed the Fund since 2013.\\n\\n4\\n\\n \\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase or redeem shares online through our website (vanguard.com), by mail (The Vanguard Group, P.O. Box 1110, Valley Forge, PA 19482-1110), or by telephone (800-662-2739). The minimum investment amount required to open and maintain a Fund account for Admiral Shares is $3,000. The minimum investment amount required to add to an existing Fund account is generally $1. Financial intermediaries, institutional, and Vanguard-advised clients should contact Vanguard for information on special eligibility rules that may apply to them regarding Admiral Shares. If you are investing through an intermediary, please contact that firm directly for more information regarding your eligibility. If you are investing through an employer-sponsored retirement or savings plan, your plan administrator or your benefits office can provide you with detailed information on how you can invest through your plan.\\n\\nTax Information\\n\\nThe Fund\\u2019s distributions may be taxable as ordinary income or capital gain. If you are investing through a tax-advantaged account, such as an IRA or an employer-sponsored retirement or savings plan, special tax rules apply.\\n\\nPayments to Financial Intermediaries\\n\\nThe Fund and its investment advisor do not pay financial intermediaries for sales of Fund shares.\",\n          \"Eaton Vance TABS Intermediate-Term Municipal Bond Fund\\n\\nInvestment Objective\\n\\nThe Fund\\u2019s investment objective is to seek after-tax total return.\\n\\nFees and Expenses of the Fund\\n\\nThis table describes the fees and expenses that you may pay if you buy and hold shares of the Fund. Investors may also pay commissions or other fees to their financial intermediary when they buy and hold shares of the Fund, which are not reflected below. You may qualify for a reduced sales charge on purchases of Class A shares if you invest, or agree to invest over a 13-month period, at least $100,000 in Eaton Vance funds. Certain financial intermediaries also may offer variations in Fund sales charges to their customers as described in Appendix A \\u2013 Financial Intermediary Sales Charge Variations in this Prospectus. More information about these and other discounts is available from your financial intermediary and in Sales Charges beginning on page 20 of this Prospectus and page 21 of the Fund\\u2019s Statement of Additional Information.\\n\\nShareholder Fees (fees paid directly from your investment)\\tClass A\\tClass C\\tClass I\\nMaximum Sales Charge (Load) Imposed on Purchases (as a percentage of offering price)\\t2.25%\\tNone\\tNone\\nMaximum Deferred Sales Charge (Load) (as a percentage of the lower of net asset value at purchase or redemption)\\tNone\\t1.00%\\tNone\\n \\n\\nAnnual Fund Operating Expenses (expenses you pay each year as a percentage of the value of your investment)\\tClass A\\tClass C\\tClass I\\nManagement Fees\\t0.60%\\t0.60%\\t0.60%\\nDistribution and Service (12b-1) Fees\\t0.25%\\t1.00%\\tNone\\nOther Expenses\\t0.12%\\t0.12%\\t0.12%\\nTotal Annual Fund Operating Expenses\\t0.97%\\t1.72%\\t0.72%\\nExpense Reimbursement (1)\\t(0.07)%\\t(0.07)%\\t(0.07)%\\nTotal Annual Fund Operating Expenses After Expense Reimbursement\\t0.90%\\t1.65%\\t0.65%\\n(1)\\tThe investment adviser and administrator has agreed to reimburse the Fund\\u2019s expenses to the extent that Total Annual Fund Operating Expenses exceed 0.90% for Class A shares, 1.65% for Class C shares and 0.65% for Class I shares. This expense reimbursement will continue through May 31, 2019. Any amendment to or termination of this reimbursement would require approval of the Board of Trustees. The expense reimbursement relates to ordinary operating expenses only and does not include expenses such as: brokerage commissions, acquired fund fees and expenses of unaffiliated funds, interest expense, taxes or litigation expenses. Amounts reimbursed may be recouped by the investment adviser and administrator during the same fiscal year to the extent actual expenses are less than the contractual expense cap during such year.\\nExample. This Example is intended to help you compare the cost of investing in the Fund with the cost of investing in other mutual funds. The Example assumes that you invest $10,000 in the Fund for the time periods indicated and then redeem all of your shares at the end of those periods. The Example also assumes that your investment has a 5% return each year, that the operating expenses remain the same and that any expense reimbursement arrangement remains in place for the contractual period. Although your actual costs may be higher or lower, based on these assumptions your costs would be:\\n\\n \\tExpenses with Redemption\\tExpenses without Redemption\\n \\t1 Year\\t3 Years\\t5 Years\\t10 Years\\t1 Year\\t3 Years\\t5 Years\\t10 Years\\nClass A shares\\t$315\\t$520\\t$743\\t$1,382\\t$315\\t$520\\t$743\\t$1,382\\nClass C shares\\t$268\\t$535\\t$927\\t$2,024\\t$168\\t$535\\t$927\\t$2,024\\nClass I shares\\t$66\\t$223\\t$394\\t$888\\t$66\\t$223\\t$394\\t$888\\nPortfolio Turnover\\n\\nThe Fund pays transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d the portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when Fund shares are held in a taxable account. These costs, which are not reflected in Annual Fund Operating Expenses or in the Example, affect the Fund\\u2019s performance. During the most recent fiscal year, the Fund's portfolio turnover rate was 62% of the average value of its portfolio.\\n\\nEaton Vance TABS Municipal Bond Funds\\t7\\tProspectus dated June 1, 2018\\n \\n \\n\\nPrincipal Investment Strategies\\n\\nUnder normal market conditions, the Fund invests at least 80% of its net assets (plus any borrowings for investment purposes) in a diversified portfolio of municipal obligations the interest on which is exempt from regular federal income tax (the \\u201c80% Policy\\u201d). In seeking the Fund\\u2019s investment objective, the portfolio managers emphasize tax-exempt income. The Fund normally invests in municipal obligations rated in the three highest rating categories (those rated A or higher by S&P Global Ratings (\\u201cS&P\\u201d), Fitch Ratings (\\u201cFitch\\u201d) or Moody\\u2019s Investors Service, Inc. (\\u201cMoody\\u2019s\\u201d)) or, if unrated, determined by the investment adviser to be of comparable quality at the time of purchase. The Fund will not invest more than 50% of its net assets in municipal obligations rated A at the time of purchase by S&P, Fitch or Moody\\u2019s or, if unrated determined by the investment adviser to be of comparable quality. For purposes of rating restrictions, if securities are rated differently by two or more rating agencies, the highest rating is used. The Fund may continue to hold securities that are downgraded (including bonds downgraded to below investment grade credit quality (\\u201cjunk bonds\\u201d)) if the investment adviser believes it would be advantageous to do so. The Fund will not invest in a municipal obligation the interest on which the Fund\\u2019s investment adviser believes is subject to the federal alternative minimum tax.\\n\\nFor its investment in municipal obligations, the Fund invests primarily in general obligation or revenue bonds. The Fund currently targets an average portfolio duration of approximately 5 - 7 years and an average weighted portfolio maturity of approximately 5 - 13 years, but may invest in securities of any maturity or duration, and may in the future alter its maturity or duration target range. The Fund may use various techniques to shorten or lengthen its dollar weighted average portfolio duration, including the acquisition of municipal obligations at a premium or discount. The portfolio managers generally will seek to enhance after-tax total return by actively engaging in relative value trading within the portfolio to take advantage of price opportunities in the markets for municipal obligations. With respect to 20% of its net assets, the Fund may invest in municipal obligations that are not exempt from regular federal income tax, direct obligations of the U.S. Treasury and/or obligations of U.S. Government agencies, instrumentalities and government-sponsored enterprises. The Fund may also invest in cash and money market instruments.\\n\\nThe investment adviser\\u2019s process for selecting municipal obligations for purchase and sale generally includes consideration of the creditworthiness of the issuer or person obligated to repay the obligation. In evaluating creditworthiness, the investment adviser considers ratings assigned by rating agencies and generally performs additional credit and investment analysis.\\n\\nPrincipal Risks\\n\\nMarket Risk. The value of investments held by the Fund may increase or decrease in response to economic, political and financial events (whether real, expected or perceived) in the U.S. and global markets. The frequency and magnitude of such changes in value cannot be predicted. Certain securities and other investments held by the Fund may experience increased volatility, illiquidity, or other potentially adverse effects in reaction to changing market conditions. Actions taken by the U.S. Federal Reserve or foreign central banks to stimulate or stabilize economic growth, such as decreases or increases in short-term interest rates, could cause high volatility in markets. No active trading market may exist for certain investments, which may impair the ability of the Fund to sell or to realize the current valuation of such investments in the event of the need to liquidate such assets. Fixed-income markets may experience periods of relatively high volatility in an environment where U.S. treasury yields are rising.\\n\\nMunicipal Obligation Risk. The amount of public information available about municipal obligations is generally less than for corporate equities or bonds, meaning that the investment performance of municipal obligations may be more dependent on the analytical abilities of the investment adviser than stock or corporate bond investments. The secondary market for municipal obligations also tends to be less well-developed and less liquid than many other securities markets, which may limit the Fund\\u2019s ability to sell its municipal obligations at attractive prices. The differences between the price at which an obligation can be purchased and the price at which it can be sold may widen during periods of market distress. Less liquid obligations can become more difficult to value and be subject to erratic price movements. The increased presence of nontraditional participants (such as proprietary trading desks of investment banks and hedge funds) or the absence of traditional participants (such as individuals, insurance companies, banks and life insurance companies) in the municipal markets may lead to greater volatility in the markets because non-traditional participants may trade more frequently or in greater volume. \\n\\nInterest Rate Risk. In general, the value of income securities will fluctuate based on changes in interest rates. The value of these securities is likely to increase when interest rates fall and decline when interest rates rise. Generally, securities with longer durations are more sensitive to changes in interest rates than shorter duration securities, causing them to be more volatile. Conversely, fixed income securities with shorter durations will be less volatile but may provide lower returns than fixed income securities with longer durations. In a rising interest rate environment, the durations of income securities that have the ability to be prepaid or called by the issuer may be extended. In a declining interest rate environment, the proceeds from prepaid or maturing instruments may have to be reinvested at a lower interest rate.\\n\\nEaton Vance TABS Municipal Bond Funds\\t8\\tProspectus dated June 1, 2018\\n \\nCredit Risk. Investments in municipal obligations and other debt obligations (referred to below as \\u201cdebt instruments\\u201d) are subject to the risk of non-payment of scheduled principal and interest. Changes in economic conditions or other circumstances may reduce the capacity of the party obligated to make principal and interest payments on such instruments and may lead to defaults. Such non-payments and defaults may reduce the value of Fund shares and income distributions. The value of debt instruments also may decline because of concerns about the issuer\\u2019s ability to make principal and interest payments. In addition, the credit ratings of debt instruments may be lowered if the financial condition of the party obligated to make payments with respect to such instruments deteriorates. In order to enforce its rights in the event of a default, bankruptcy or similar situation, the Fund may be required to retain legal or similar counsel, which may increase the Fund\\u2019s operating expenses and adversely affect net asset value. Municipal obligations may be insured as to principal and interest payments. If the claims-paying ability or other rating of the insurer is downgraded by a rating agency, the value of such obligations may be negatively affected.\\n\\nRisks of Principal Only Investments. Principal only investments entitle the Fund to receive the stated value of such investment when held to maturity. The values of principal only investments are subject to greater fluctuation in response to changes in market interest rates than obligations that pay interest currently. The Fund will accrue income on these investments and distribute that income each year. The Fund may be required to sell other investments to obtain cash needed for such income distributions.\\n\\nU.S. Government Securities Risk. Although certain U.S. Government-sponsored agencies (such as the Federal Home Loan Mortgage Corporation and the Federal National Mortgage Association) may be chartered or sponsored by acts of Congress, their securities are neither issued nor guaranteed by the U.S. Treasury. U.S. Treasury securities generally have a lower return than other obligations because of their higher credit quality and market liquidity.\\n\\nTax Risk. Income from tax-exempt municipal obligations could be declared taxable because of changes in tax laws, adverse interpretations by the relevant taxing authority or the non-compliant conduct of the issuer of an obligation.\\n\\nMoney Market Instrument Risk. Money market instruments may be adversely affected by market and economic events, such as a sharp rise in prevailing short-term interest rates; adverse developments in the banking industry, which issues or guarantees many money market instruments; adverse economic, political or other developments affecting issuers of money market instruments; changes in the credit quality of issuers; and default by a counterparty.\\n\\nGeneral Fund Investing Risks. The Fund is not a complete investment program and there is no guarantee that the Fund will achieve its investment objective. It is possible to lose money by investing in the Fund. The Fund is designed to be a long-term investment vehicle and is not suited for short-term trading. Investors in the Fund should have a long-term investment perspective and be able to tolerate potentially sharp declines in value. Purchase and redemption activities by Fund shareholders may impact the management of the Fund and its ability to achieve its investment objective(s). In addition, the redemption by one or more large shareholders or groups of shareholders of their holdings in the Fund could have an adverse impact on the remaining shareholders in the Fund. An investment in the Fund is not a deposit in a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency.\\n\\nPerformance\\n\\nThe following bar chart and table provide some indication of the risks of investing in the Fund by showing changes in the Fund\\u2019s performance from year to year and how the Fund\\u2019s average annual returns over time compare with those of two broad-based securities market indices. The returns in the bar chart are for Class A shares and do not reflect a sales charge. If the sales charge was reflected, the returns would be lower. Past performance (both before and after taxes) is not necessarily an indication of how the Fund will perform in the future. The Fund\\u2019s performance reflects the effects of expense reductions. Absent these reductions, performance would have been lower. Updated Fund performance information can be obtained by visiting www.eatonvance.com.\\n\\n\\n\\nDuring the period from December 31, 2010 to December 31, 2017, the highest quarterly total return for Class A was 3.75% for the quarter ended June 30, 2011, and the lowest quarterly return was \\u20134.07% for the quarter ended December 31, 2016. The year-to-date total return through the end of the most recent calendar quarter (December 31, 2017 to March 31, 2018) was -1.71%.\\n\\nEaton Vance TABS Municipal Bond Funds\\t9\\tProspectus dated June 1, 2018\\n \\n \\n\\nAverage Annual Total Return as of December 31, 2017\\tOne Year\\tFive Years\\tLife of Fund\\nClass A Return Before Taxes\\t2.08%\\t2.12%\\t4.21%\\nClass A Return After Taxes on Distributions\\t2.05%\\t2.08%\\t4.09%\\nClass A Return After Taxes on Distributions and the Sale of Class A Shares\\t1.94%\\t1.97%\\t3.59%\\nClass C Return Before Taxes\\t2.67%\\t1.82%\\t3.74%\\nClass I Return Before Taxes\\t4.71%\\t2.82%\\t4.78%\\nBloomberg Barclays Municipal Managed Money Intermediate (1-17 Year) Bond Index (reflects no deduction for fees, expenses or taxes)\\t4.88%\\t2.55%\\t4.26%\\nBloomberg Barclays 7 Year Municipal Bond Index (reflects no deduction for fees, expenses or taxes)\\t4.49%\\t2.44%\\t4.30%\\nThese returns reflect the maximum sales charge for Class A (2.25%) and any applicable contingent deferred sales charge (\\u201cCDSC\\u201d) for Class C. Class A, Class C and Class I commenced operations on February 1, 2010. Effective February 17, 2015, the Fund changed its name, objective and investment strategy to invest at least 80% of its net assets in a diversified portfolio of municipal obligations, the interest on which is exempt from regular federal income tax. Investors cannot invest directly in an Index.\\n\\nAfter-tax returns are calculated using the highest historical individual federal income tax rates and does not reflect the impact of state and local taxes. Actual after-tax returns depend on a shareholder\\u2019s tax situation and the actual characterization of distributions, and may differ from those shown. After-tax returns are not relevant to shareholders who hold shares in tax-deferred accounts or to shares held by non-taxable entities. After-tax returns for other Classes of shares will vary from the after-tax returns presented for Class A shares. Return After Taxes on Distributions for a period may be the same as Return Before Taxes for that period because no taxable distributions were made during that period. Also, Return After Taxes on distributions and Sale of Fund Shares for a period may be greater than or equal to Return After Taxes on Distributions for the same period because of losses realized on the sale of Fund shares.\\n\\nManagement\\n\\nInvestment Adviser. Eaton Vance Management (\\u201cEaton Vance\\u201d).\\n\\nPortfolio Managers\\n\\nThe portfolio managers of the Fund are part of Eaton Vance\\u2019s Tax-Advantaged Bond Strategies (\\u201cTABS\\u201d) division.\\n\\nJames H. Evans, (lead portfolio manager), Vice President of Eaton Vance, has managed the Fund since it commenced operations in February 2010.\\n\\nBrian C. Barney, Vice President of Eaton Vance, has managed the Fund since June 2010.\\n\\nChristopher J. Harshman, Vice President of Eaton Vance, has managed the Fund since June 2010.\\n\\nFor important information about purchase and sale of shares, taxes and financial intermediary compensation, please turn to \\u201cImportant Information Regarding Fund Shares\\u201d on page 11 of this Prospectus.\\n\\nEaton Vance TABS Municipal Bond Funds\\t10\\tProspectus dated June 1, 2018\\n \\n \\n\\nImportant Information Regarding Fund Shares\\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase, redeem or exchange Fund shares on any business day, which is any day the New York Stock Exchange is open for business. You may purchase, redeem or exchange Fund shares either through your financial intermediary or (except for purchases of Class C shares by accounts with no specified financial intermediary) directly from a Fund either by writing to the Fund, P.O. Box 9653, Providence, RI 02940-9653, or by calling 1-800-262-1122. The minimum initial purchase or exchange into a Fund is $1,000 for each Class (with the exception of Class I) and $250,000 for Class I (waived in certain circumstances). There is no minimum for subsequent investments.\\n\\nTax Information\\n\\nEach Fund\\u2019s distributions are expected to primarily be exempt from regular federal income tax. However, the Fund may also distribute taxable income to the extent that it invests in taxable municipal obligations or other obligations which generate taxable income. Distributions of any net realized gains are expected to be taxable.\\n\\nPayments to Broker-Dealers and Other Financial Intermediaries\\n\\nIf you purchase a Fund\\u2019s shares through a broker-dealer or other financial intermediary (such as a bank) (collectively, \\u201cfinancial intermediaries\\u201d), the Fund, its principal underwriter and its affiliates may pay the financial intermediary for the sale of Fund shares and related services. These payments may create a conflict of interest by influencing the financial intermediary and your salesperson to recommend a Fund over another investment. Ask your salesperson or visit your financial intermediary\\u2019s website for more information.\",\n          \"Fund Summary\\n\\nFund/Class:\\nFidelity Freedom\\u00ae 2050 Fund/Fidelity Freedom\\u00ae 2050 Fund\\n\\nInvestment Objective\\n\\nThe fund seeks high total return until its target retirement date. Thereafter the fund's objective will be to seek high current income and, as a secondary objective, capital appreciation.\\n\\nFee Table\\n\\nThe following table describes the fees and expenses that may be incurred when you buy and hold shares of the fund.\\n\\nShareholder fees\\n\\n(fees paid directly from your investment) \\tNone \\nAnnual Operating Expenses\\n\\n(expenses that you pay each year as a % of the value of your investment)\\n\\nManagement fee(a) \\t \\t0.75% \\nDistribution and/or Service (12b-1) fees \\t \\tNone \\nOther expenses(a) \\t \\t0.00% \\nTotal annual operating expenses(a) \\t \\t0.75% \\n(a)   Adjusted to reflect current fees.\\n\\nThis example helps compare the cost of investing in the fund with the cost of investing in other funds.\\n\\nLet's say, hypothetically, that the annual return for shares of the fund is 5% and that your shareholder fees and the annual operating expenses for shares of the fund are exactly as described in the fee table. This example illustrates the effect of fees and expenses, but is not meant to suggest actual or expected fees and expenses or returns, all of which may vary. For every $10,000 you invested, here's how much you would pay in total expenses if you sell all of your shares at the end of each time period indicated:\\n\\n1 year \\t$76 \\n3 years \\t$238 \\n5 years \\t$415 \\n10 years \\t$926 \\nPortfolio Turnover\\n\\nThe fund will not incur transaction costs, such as commissions, when it buys and sells shares of underlying Fidelity\\u00ae funds (or \\\"turns over\\\" its portfolio), but it could incur transaction costs if it were to buy and sell other types of securities directly. If the fund were to buy and sell other types of securities directly, a higher portfolio turnover rate could indicate higher transaction costs and could result in higher taxes when fund shares are held in a taxable account. Such costs, if incurred, would not be reflected in annual operating expenses or in the example and would affect the fund's performance. During the most recent fiscal year, the fund's portfolio turnover rate was 16% of the average value of its portfolio.\\n\\nPrincipal Investment Strategies\\n\\nInvesting in a combination of Fidelity\\u00ae domestic equity funds, international equity funds (developed and emerging markets), bond funds, and short-term funds (underlying Fidelity\\u00ae funds).\\nAllocating assets according to a neutral asset allocation strategy shown in the glide path below that becomes increasingly conservative until it reaches an allocation similar to that of the Fidelity Freedom\\u00ae Income Fund, approximately 10 to 19 years after the year 2050 (approximately 17% in domestic equity funds, 7% in international equity funds, 46% in bond funds, and 30% in short-term funds).\\nBuying and selling futures contracts (both long and short positions) in an effort to manage cash flows efficiently, remain fully invested, or facilitate asset allocation.\\nFMR Co., Inc. (FMRC) may continue to seek high total return for several years beyond the fund's target retirement date in an effort to achieve the fund's overall investment objective.\\n\\nAs of March 31, 2018, the fund's neutral asset allocation to underlying Fidelity\\u00ae funds and futures was approximately:\\n   \\tDomestic Equity Funds* \\t63% \\n   \\tInternational Equity Funds* \\t27% \\n   \\tBond Funds* \\t10% \\n \\tShort-Term Funds* \\t0% \\n\\n* FMRC may change these percentages over time. As a result of the active asset allocation strategy (discussed below), actual allocations may differ from the neutral allocations above. The allocation percentages may not add to 100% due to rounding.\\n\\nFMRC may use an active asset allocation strategy to increase or decrease neutral asset class exposures reflected above by up to 10% for equity funds (includes domestic equity and international equity funds), bond funds and short-term funds to reflect FMRC's market outlook, which is primarily focused on the intermediate term. The asset allocations in the glide path and pie chart above are referred to as neutral because they do not reflect any decisions made by FMRC to overweight or underweight an asset class.\\nFMRC may also make active asset allocations within other asset classes (including commodities, high yield debt, floating rate debt, real estate debt, inflation-protected debt, and emerging markets debt) from 0% to 10% individually but no more than 25% in aggregate within those other asset classes. Such asset classes are not reflected in the neutral asset allocations reflected in the glide path and pie chart above.\\nDesigned for investors who anticipate retiring in or within a few years of 2050 (target retirement date) at or around age 65 and plan to gradually withdraw the value of their account in the fund over time.\\nPrincipal Investment Risks\\n\\nShareholders should consider that no target date fund is intended as a complete retirement program and there is no guarantee that any single fund will provide sufficient retirement income at or through your retirement. The fund's share price fluctuates, which means you could lose money by investing in the fund, including losses near, at or after the target retirement date.\\n\\nAsset Allocation Risk.  The fund is subject to risks resulting from the Adviser's asset allocation decisions. The selection of underlying funds and the allocation of the fund's assets among various asset classes could cause the fund to lose value or its results to lag relevant benchmarks or other funds with similar objectives. In addition, the fund's active asset allocation strategy may cause the fund to have a risk profile different than that portrayed above from time to time and may increase losses.\\nInvesting in Other Funds.  The fund bears all risks of investment strategies employed by the underlying funds, including the risk that the underlying funds will not meet their investment objectives.\\nStock Market Volatility.  Stock markets are volatile and can decline significantly in response to adverse issuer, political, regulatory, market, or economic developments. Different parts of the market, including different market sectors, and different types of securities can react differently to these developments.\\nInterest Rate Changes.  Interest rate increases can cause the price of a debt or money market security to decrease.\\nForeign Exposure.  Foreign markets, particularly emerging markets, can be more volatile than the U.S. market due to increased risks of adverse issuer, political, regulatory, market, or economic developments and can perform differently from the U.S. market. Emerging markets can be subject to greater social, economic, regulatory, and political uncertainties and can be extremely volatile. Foreign exchange rates also can be extremely volatile.\\nIndustry Exposure.  Market conditions, interest rates, and economic, regulatory, or financial developments could significantly affect a single industry or group of related industries.\\nIssuer-Specific Changes.  The value of an individual security or particular type of security can be more volatile than, and can perform differently from, the market as a whole. Lower-quality debt securities (those of less than investment-grade quality, also referred to as high yield debt securities or junk bonds) and certain types of other securities involve greater risk of default or price changes due to changes in the credit quality of the issuer. The value of lower-quality debt securities and certain types of other securities can be more volatile due to increased sensitivity to adverse issuer, political, regulatory, market, or economic developments.\\nLeverage Risk.  Leverage can increase market exposure, magnify investment risks, and cause losses to be realized more quickly.\\n\\\"Growth\\\" Investing.  \\\"Growth\\\" stocks can perform differently from the market as a whole and other types of stocks and can be more volatile than other types of stocks.\\n\\\"Value\\\" Investing.  \\\"Value\\\" stocks can perform differently from the market as a whole and other types of stocks and can continue to be undervalued by the market for long periods of time.\\nCommodity-Linked Investing.  The value of commodities and commodity-linked investments may be affected by the performance of the overall commodities markets as well as weather, political, tax, and other regulatory and market developments. Commodity-linked investments may be more volatile and less liquid than the underlying commodity, instruments, or measures.\\nAn investment in the fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency. You could lose money by investing in the fund.\\n\\nPerformance\\n\\nThe following information is intended to help you understand the risks of investing in the fund. The information illustrates the changes in the performance of the fund's shares from year to year and compares the performance of the fund's shares to the performance of a securities market index and a hypothetical composite of market indexes over various periods of time. The indexes have characteristics relevant to the fund's investment strategies. Index descriptions appear in the \\\"Additional Index Information\\\" section of the prospectus. Prior to June 1, 2017, the fund operated under a different pricing structure. The fund\\u2019s historical performance prior to June 1, 2017 does not reflect the fund\\u2019s current pricing structure. Past performance (before and after taxes) is not an indication of future performance.\\n\\nVisit www.fidelity.com for more recent performance information.\\n\\nYear-by-Year Returns\\n\\n\\n\\nDuring the periods shown in the chart: \\tReturns \\tQuarter ended \\nHighest Quarter Return \\t19.11% \\tJune 30, 2009 \\nLowest Quarter Return \\t(23.40)% \\tDecember 31, 2008 \\nYear-to-Date Return \\t(0.25)% \\tMarch 31, 2018 \\nAverage Annual Returns\\n\\nAfter-tax returns are calculated using the historical highest individual federal marginal income tax rates, but do not reflect the impact of state or local taxes. Actual after-tax returns may differ depending on your individual circumstances. The after-tax returns shown are not relevant if you hold your shares in a retirement account or in another tax-deferred arrangement, such as an employee benefit plan (profit sharing, 401(k), or 403(b) plan).\\n\\nFor the periods ended December 31, 2017 \\tPast 1 year \\tPast 5 years \\tPast 10 years \\nFidelity Freedom\\u00ae 2050 Fund \\nReturn Before Taxes \\t22.28% \\t11.30% \\t5.31% \\nReturn After Taxes on Distributions \\t21.05% \\t9.54% \\t4.09% \\nReturn After Taxes on Distributions and Sale of Fund Shares \\t13.44% \\t8.52% \\t3.85% \\nS&P 500\\u00ae Index\\n(reflects no deduction for fees, expenses, or taxes) \\t21.83% \\t15.79% \\t8.50% \\nFidelity Freedom 2050 Composite Index\\u2120\\n(reflects no deduction for fees or expenses) \\t20.95% \\t12.05% \\t6.60% \\nInvestment Adviser\\n\\nFMRC (the Adviser), an affiliate of Fidelity Management & Research Company (FMR), is the fund's manager.\\n\\nPortfolio Manager(s)\\n\\nAndrew Dierdorf (co-manager) has managed the fund since June 2011.\\n\\nBrett Sumsion (co-manager) has managed the fund since January 2014.\\n\\nPurchase and Sale of Shares\\n\\nYou may buy or sell shares through a Fidelity\\u00ae brokerage or mutual fund account, through a retirement account, or through an investment professional. You may buy or sell shares in various ways:\\n\\nInternet\\n\\nwww.fidelity.com\\n\\nPhone\\n\\nFidelity Automated Service Telephone (FAST\\u00ae) 1-800-544-5555\\n\\nTo reach a Fidelity representative 1-800-544-6666\\n\\nMail\\n\\nAdditional purchases:\\n\\nFidelity Investments\\nP.O. Box 770001\\nCincinnati, OH 45277-0003\\nRedemptions:\\n\\nFidelity Investments\\nP.O. Box 770001\\nCincinnati, OH 45277-0035\\nTDD- Service for the Deaf and Hearing Impaired\\n\\n1-800-544-0118\\n\\nThe price to buy one share is its net asset value per share (NAV). Shares will be bought at the NAV next calculated after an order is received in proper form.\\n\\nThe price to sell one share is its NAV. Shares will be sold at the NAV next calculated after an order is received in proper form.\\n\\nThe fund is open for business each day the New York Stock Exchange (NYSE) is open.\\n\\nInitial Purchase Minimum \\t$2,500 \\nFor Fidelity\\u00ae Simplified Employee Pension-IRA, Keogh, and Investment Only Retirement accounts \\t$500 \\nThrough regular investment plans in Fidelity\\u00ae Traditional IRAs, Roth IRAs, and Rollover IRAs (requires monthly purchases of $200 until fund balance is $2,500) \\t$200 \\nThe fund may waive or lower purchase minimums in other circumstances.\\n\\nTax Information\\n\\nDistributions you receive from the fund are subject to federal income tax and generally will be taxed as ordinary income or capital gains, and may also be subject to state or local taxes, unless you are investing through a tax-advantaged retirement account (in which case you may be taxed later, upon withdrawal of your investment from such account).\\n\\nPayments to Broker-Dealers and Other Financial Intermediaries\\n\\nThe fund, the Adviser, Fidelity Distributors Corporation (FDC), and/or their affiliates may pay intermediaries, which may include banks, broker-dealers, retirement plan sponsors, administrators, or service-providers (who may be affiliated with the Adviser or FDC), for the sale of fund shares and related services. These payments may create a conflict of interest by influencing your intermediary and your investment professional to recommend the fund over another investment. Ask your investment professional or visit your intermediary's web site for more information.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_distance\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 253,\n        \"samples\": [\n          \"2 \\tWith limited exceptions, for Class A shares, if your Fund account balance is below $650 at the start of business on the Friday prior to the last full week of September of each year, the account will be assessed an account fee of $20. 3 \\tThrough July 31, 2019, Ivy Investment Management Company (IICO), the Fund\\u2019s investment manager, Ivy Distributors, Inc. (IDI), the Fund\\u2019s distributor, and/or Waddell & Reed Services Company, doing business as WI Services Company (WISC), the Fund\\u2019s transfer agent, have contractually agreed to reimburse sufficient management fees, 12b-1 fees and/or shareholder servicing fees to cap the total annual ordinary fund operating expenses (which would exclude interest, taxes, brokerage commissions, acquired fund fees and expenses and extraordinary expenses, if any) as follows: Class E shares at 1.10%. 4\\t \\tProspectus\\t \\tDomestic Equity Funds\\nTable of Contents\\nExample\\n\\nThis example is intended to help you compare the cost of investing in the shares of the Fund with the cost of investing in other mutual funds. 4 \\tThrough July 31, 2020, IICO, IDI and/or WISC have contractually agreed to reimburse sufficient management fees, 12b-1 fees and/or shareholder servicing fees to cap the total annual ordinary fund operating expenses (which would exclude interest, taxes, brokerage commissions, acquired fund fees and expenses and extraordinary expenses, if any) as follows: Class A shares at 1.04%; Class B shares at 2.13%; Class E shares at 1.13%; and Class I shares and Class Y shares at 0.84%. 5 \\tThrough July 31, 2020, IDI and/or WISC have contractually agreed to reimburse sufficient 12b-1 and/or shareholder servicing fees to ensure that the total annual ordinary fund operating expenses of the Class Y shares do not exceed the total annual ordinary fund operating expenses of the Class A shares, as calculated at the end of each month.\",\n          \"A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when fund shares are held in a taxable account. An investment in the fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency. Ask your investment professional or visit your intermediary's web site for more information. Box 770001\\nCincinnati, OH 45277-0035\\nOvernight Express:\\nFidelity Investments\\n100 Crosby Parkway\\nCovington, KY 41015\\nTDD- Service for the Deaf and Hearing Impaired\\n\\n1-800-544-0118\\n\\nThe price to buy one share is its net asset value per share (NAV). Currently, the Board of Trustees of the fund has not authorized such payments for shares of the fund.\",\n          \"2\\n\\n \\n\\nAnnual Total Returns\\n\\nThe following bar chart and table are intended to help you understand the risks of investing in the Fund. 3\\n\\n \\n\\nAverage Annual Total Returns for Periods Ended December 31, 2017\\t \\t \\n \\t \\t \\tSince\\n \\t \\t \\tInception\\n \\t \\t \\t(Dec. 28,\\n \\t1 Year\\t5 Years\\t2009)\\nVanguard Short-Term Treasury Index Fund Admiral Shares\\t \\t \\t \\nReturn Before Taxes\\t0.40%\\t0.50%\\t0.81%\\nReturn After Taxes on Distributions\\t\\u20130.08\\t0.20\\t0.54\\nReturn After Taxes on Distributions and Sale of Fund Shares\\t0.22\\t0.25\\t0.51\\nBloomberg Barclays US Treasury 1-3 Year Bond Index\\t \\t \\t \\n(reflects no deduction for fees, expenses, or taxes)\\t0.42%\\t0.57%\\t0.89%\\n \\n\\nActual after-tax returns depend on your tax situation and may differ from those shown in the preceding table. 4\\n\\n \\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase or redeem shares online through our website (vanguard.com), by mail (The Vanguard Group, P.O. A higher portfolio turnover rate may indicate higher transaction costs and may result in more taxes when Fund shares are held in a taxable account. All of the Fund\\u2019s investments will be selected through the sampling process, and under normal circumstances, at least 80% of the Fund\\u2019s assets will be invested in bonds included in the Index.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df['sentences_distance'][149]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "dweQxPa3mqi1",
        "outputId": "c9307df9-f625-4a6c-b0f7-7d7789842fc2"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'19\\n\\nMainStay VP Epoch U.S. Small Cap Portfolio\\n\\nPrincipal Risks\\nYou can lose money by investing in the Portfolio. 20\\n\\nMainStay VP Epoch U.S. Small Cap Portfolio\\n\\nPerformance data for the classes varies based on differences in their fee and expense structures. 21\\n\\nMainStay VP Epoch U.S. Small Cap Portfolio\\n\\nCompensation to Broker/Dealers and Other Financial Intermediaries\\nThe Portfolio and/or its related companies may pay NYLIAC or other participating insurance companies, broker/dealers, or other financial intermediaries for the sale of Portfolio shares and related services. A higher portfolio turnover rate may indicate higher transaction costs. Also, issuers of convertible securities are often not as strong financially as those issuing securities with higher credit ratings, are more likely to encounter financial difficulties and typically are more vulnerable to changes in the economy, such as a recession or a sustained period of rising interest rates, which could affect their ability to make interest and principal payments.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentence_match(summary, knowledge, num_sent):\n",
        "    sentences = sent_tokenize(summary)\n",
        "    sentence_scores = []\n",
        "    for j, sentence in enumerate(sentences):\n",
        "        set_tokens = set(tokenizer(sentence))\n",
        "\n",
        "        # Find the number of common words between the knowledge base and the sentence\n",
        "        inter_knwoledge = set_tokens.intersection(knowledge)\n",
        "\n",
        "        sentence_scores.append(len(inter_knwoledge))\n",
        "\n",
        "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
        "    top_sentences = sentences[len(sentences)-num_sent-1:]\n",
        "    return ' '.join(top_sentences)\n",
        "\n",
        "X_train_df['sentences_match'] = X_train_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "X_valid_df['sentences_match'] = X_valid_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "X_test_df['sentences_match'] = X_test_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
        "\n",
        "# produce train_X and test_X\n",
        "train_X = X_train_df['sentences_match'].values\n",
        "train_X = [' '.join(tokenizer(txt)) for txt in train_X]\n",
        "\n",
        "valid_X = X_valid_df['sentences_match'].values\n",
        "valid_X = [' '.join(tokenizer(txt)) for txt in valid_X]\n",
        "\n",
        "test_X = X_test_df['sentences_match'].values\n",
        "test_X = [' '.join(tokenizer(txt)) for txt in test_X]\n",
        "\n",
        "# produce train_y and valid_y\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "encoded_train_y = encoder.fit_transform(y_train)\n",
        "label_train_y = to_categorical(encoded_train_y, num_classes=3)\n",
        "\n",
        "encoded_valid_y = encoder.transform(y_valid)\n",
        "label_valid_y = to_categorical(encoded_valid_y, num_classes=3)\n",
        "\n",
        "encoded_test_y = encoder.fit_transform(y_test)\n",
        "label_test_y = to_categorical(encoded_test_y, num_classes=3)\n",
        "\n",
        "num_words = 2500 # Size of the vocabulary used. we only consider the 2500 most common words. The other words are removed from the texts.\n",
        "maxlen = 150 # Number of word considered for each document. we cut or lengthen the texts to have texts of 150 words.\n",
        "word_dimension = 50 # dimension of our word vectors.\n",
        "\n",
        "keras_tokenizer = Tokenizer(num_words=num_words)\n",
        "\n",
        "keras_tokenizer.fit_on_texts(train_X)\n",
        "\n",
        "word_index = keras_tokenizer.word_index\n",
        "\n",
        "sequences_train = keras_tokenizer.texts_to_sequences(train_X)\n",
        "sequences_valid = keras_tokenizer.texts_to_sequences(valid_X)\n",
        "sequences_test = keras_tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "# truncate or lenthen each text so they have the same length.\n",
        "feature_train = pad_sequences(sequences_train, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "feature_valid = pad_sequences(sequences_valid, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "feature_test = pad_sequences(sequences_test, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
        "\n",
        "# create our embedding matrix\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, word_dimension))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "uspfbfShk4L_"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "WhMwZ72PmIva",
        "outputId": "5b429548-5860-43f0-d66d-172e4920103b"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               summary  \\\n",
              "149  MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...   \n",
              "436  INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...   \n",
              "394  Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...   \n",
              "440  INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...   \n",
              "330  Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...   \n",
              "..                                                 ...   \n",
              "152  MainStay VP MacKay International Equity Portfo...   \n",
              "158  MainStay VP MacKay Small Cap Core Portfolio\\n\\...   \n",
              "374  Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...   \n",
              "308  PIMCO Gurtin California Municipal Intermediate...   \n",
              "287  Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...   \n",
              "\n",
              "                                    sentences_distance  \\\n",
              "149  19\\n\\nMainStay VP Epoch U.S. Small Cap Portfol...   \n",
              "436  1 Year\\t3 Years\\t5 Years\\t10 Years\\n$93\\t$290\\...   \n",
              "394  2. 3. A higher portfolio turnover rate may ind...   \n",
              "440  1 Year\\t3 Years\\t5 Years\\t10 Years\\nFund Share...   \n",
              "330  \"Growth\" Investing. \"Growth\" stocks can perfor...   \n",
              "..                                                 ...   \n",
              "152  35\\n\\nMainStay VP MacKay International Equity ...   \n",
              "158  47\\n\\nMainStay VP MacKay Small Cap Core Portfo...   \n",
              "374  A figure of 1.00 would indicate perfect correl...   \n",
              "308  A higher portfolio turnover rate may indicate ...   \n",
              "287  2 \\tWith limited exceptions, for Class A share...   \n",
              "\n",
              "                                       sentences_match  \n",
              "149  The security selection process focuses on free...  \n",
              "436  There are no minimum initial or subsequent pur...  \n",
              "394  Total annual Fund operating expenses differ fr...  \n",
              "440  These payments may create a conflict of intere...  \n",
              "330  This example helps compare the cost of investi...  \n",
              "..                                                 ...  \n",
              "152  The table does not include any separate accoun...  \n",
              "158  The table does not include any separate accoun...  \n",
              "374  Tracking error may occur because of difference...  \n",
              "308  This Fee Waiver Agreement renews annually unle...  \n",
              "287  ∎\\t \\tInitial Public Offering (IPO) Risk. ∎\\t ...  \n",
              "\n",
              "[294 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-53f2fab6-e3bd-49d4-aaab-b3394393486f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>sentences_distance</th>\n",
              "      <th>sentences_match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>MainStay VP Epoch U.S. Small Cap Portfolio\\n\\n...</td>\n",
              "      <td>19\\n\\nMainStay VP Epoch U.S. Small Cap Portfol...</td>\n",
              "      <td>The security selection process focuses on free...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>INVESTMENT OBJECTIVE\\nThe USAA Managed Allocat...</td>\n",
              "      <td>1 Year\\t3 Years\\t5 Years\\t10 Years\\n$93\\t$290\\...</td>\n",
              "      <td>There are no minimum initial or subsequent pur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>Franklin Payout 2018 Fund\\n\\nInvestment Goal\\n...</td>\n",
              "      <td>2. 3. A higher portfolio turnover rate may ind...</td>\n",
              "      <td>Total annual Fund operating expenses differ fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>INVESTMENT OBJECTIVE\\nThe USAA Aggressive Grow...</td>\n",
              "      <td>1 Year\\t3 Years\\t5 Years\\t10 Years\\nFund Share...</td>\n",
              "      <td>These payments may create a conflict of intere...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>Fund Summary\\n\\nFund/Class:\\nFidelity Freedom®...</td>\n",
              "      <td>\"Growth\" Investing. \"Growth\" stocks can perfor...</td>\n",
              "      <td>This example helps compare the cost of investi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>MainStay VP MacKay International Equity Portfo...</td>\n",
              "      <td>35\\n\\nMainStay VP MacKay International Equity ...</td>\n",
              "      <td>The table does not include any separate accoun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>MainStay VP MacKay Small Cap Core Portfolio\\n\\...</td>\n",
              "      <td>47\\n\\nMainStay VP MacKay Small Cap Core Portfo...</td>\n",
              "      <td>The table does not include any separate accoun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>Franklin LibertyQ U.S. Mid Cap Equity ETF\\n\\nI...</td>\n",
              "      <td>A figure of 1.00 would indicate perfect correl...</td>\n",
              "      <td>Tracking error may occur because of difference...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>PIMCO Gurtin California Municipal Intermediate...</td>\n",
              "      <td>A higher portfolio turnover rate may indicate ...</td>\n",
              "      <td>This Fee Waiver Agreement renews annually unle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>Ivy Small Cap Growth Fund\\n\\nObjective\\n\\nTo s...</td>\n",
              "      <td>2 \\tWith limited exceptions, for Class A share...</td>\n",
              "      <td>∎\\t \\tInitial Public Offering (IPO) Risk. ∎\\t ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53f2fab6-e3bd-49d4-aaab-b3394393486f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-53f2fab6-e3bd-49d4-aaab-b3394393486f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-53f2fab6-e3bd-49d4-aaab-b3394393486f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c1fa9612-82f0-4191-82d9-854d7508e470\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c1fa9612-82f0-4191-82d9-854d7508e470')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c1fa9612-82f0-4191-82d9-854d7508e470 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df",
              "summary": "{\n  \"name\": \"X_train_df\",\n  \"rows\": 294,\n  \"fields\": [\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 292,\n        \"samples\": [\n          \"Vanguard Short-Term Treasury Index Fund\\n\\nInvestment Objective\\n\\nThe Fund seeks to track the performance of a market-weighted Treasury index with a short-term dollar-weighted average maturity.\\n\\nFees and Expenses\\n\\nThe following table describes the fees and expenses you may pay if you buy and hold Admiral Shares of the Fund.\\n\\nShareholder Fees\\t \\n(Fees paid directly from your investment)\\t \\n \\nSales Charge (Load) Imposed on Purchases\\tNone\\nPurchase Fee\\tNone\\nSales Charge (Load) Imposed on Reinvested Dividends\\tNone\\nRedemption Fee\\tNone\\nAccount Service Fee (for certain fund account balances below $10,000)\\t$20/year\\n \\nAnnual Fund Operating Expenses\\t \\n(Expenses that you pay each year as a percentage of the value of your investment)\\t \\n \\nManagement Fees\\t0.06%\\n12b-1 Distribution Fee\\tNone\\nOther Expenses\\t0.01%\\nTotal Annual Fund Operating Expenses\\t0.07%\\n \\n\\nExample\\n\\nThe following example is intended to help you compare the cost of investing in the Fund\\u2019s Admiral Shares with the cost of investing in other mutual funds. It illustrates the hypothetical expenses that you would incur over various periods if you were to invest $10,000 in the Fund\\u2019s shares. This example assumes that the shares provide a return of 5% each year and that total annual fund operating expenses remain as stated in the preceding table. You would incur these hypothetical expenses whether or not you redeem your investment at the end of the given period. Although your actual costs may be higher or lower, based on these assumptions your costs would be:\\n\\n1 Year\\t3 Years\\t5 Years\\t10 Years\\n$7\\t$23\\t$40\\t$90\\n \\n\\n1\\n\\n \\n\\nPortfolio Turnover\\n\\nThe Fund pays transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d its portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in more taxes when Fund shares are held in a taxable account. These costs, which are not reflected in annual fund operating expenses or in the previous expense example, reduce the Fund\\u2019s performance. During the most recent fiscal year, the Fund\\u2019s portfolio turnover rate was 67% of the average value of its portfolio.\\n\\nPrincipal Investment Strategies\\n\\nThe Fund employs an indexing investment approach designed to track the performance of the Bloomberg Barclays US Treasury 1\\u20133 Year Bond Index. This Index includes fixed income securities issued by the U.S. Treasury (not including inflation-protected securities), all with maturities between 1 and 3 years.\\n\\nThe Fund invests by sampling the Index, meaning that it holds a range of securities that, in the aggregate, approximates the full Index in terms of key risk factors and other characteristics. All of the Fund\\u2019s investments will be selected through the sampling process, and under normal circumstances, at least 80% of the Fund\\u2019s assets will be invested in bonds included in the Index. The Fund maintains a dollar-weighted average maturity consistent with that of the Index. As of August 31, 2018, the dollar-weighted average maturity of the Index was 2.0 years.\\n\\nPrincipal Risks\\n\\nThe Fund is designed for investors with a low tolerance for risk, but you could still lose money by investing in it. The Fund is subject to the following risks, which could affect the Fund\\u2019s performance:\\n\\n\\u2022 Interest rate risk, which is the chance that bond prices will decline because of rising interest rates. Interest rate risk should be low for the Fund because it invests primarily in short-term bonds, whose prices are less sensitive to interest rate changes than are the prices of longer-term bonds.\\n\\n\\u2022 Income risk, which is the chance that the Fund\\u2019s income will decline because of falling interest rates. Income risk is generally high for short-term bond funds, so investors should expect the Fund\\u2019s monthly income to fluctuate.\\n\\n\\u2022 Index sampling risk, which is the chance that the securities selected for the Fund, in the aggregate, will not provide investment performance matching that of the Fund\\u2018s target index. Index sampling risk for the Fund is expected to be low.\\n\\nAn investment in the Fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency.\\n\\n2\\n\\n \\n\\nAnnual Total Returns\\n\\nThe following bar chart and table are intended to help you understand the risks of investing in the Fund. The bar chart shows how the performance of the Fund\\u2019s Admiral Shares has varied from one calendar year to another over the periods shown. The table shows how the average annual total returns of the Admiral Shares compare with those of the Fund\\u2019s target index, which has investment characteristics similar to those of the Fund. The Fund\\u2019s Signal\\u00ae Shares were renamed Admiral Shares on October 16, 2013. Keep in mind that the Fund\\u2019s past performance (before and after taxes) does not indicate how the Fund will perform in the future. Updated performance information is available on our website at vanguard.com/performance or by calling Vanguard toll-free at 800-662-7447.\\n\\nAnnual Total Returns \\u2014 Vanguard Short-Term Treasury Index Fund Admiral Shares1\\n\\n\\n1 The year-to-date return as of the most recent calendar quarter, which ended on September 30, 2018, was 0.17%.\\n\\nDuring the periods shown in the bar chart, the highest return for a calendar quarter was 1.16% (quarter ended June 30, 2010), and the lowest return for a quarter was \\u20130.45% (quarter ended December 31, 2016).\\n\\n3\\n\\n \\n\\nAverage Annual Total Returns for Periods Ended December 31, 2017\\t \\t \\n \\t \\t \\tSince\\n \\t \\t \\tInception\\n \\t \\t \\t(Dec. 28,\\n \\t1 Year\\t5 Years\\t2009)\\nVanguard Short-Term Treasury Index Fund Admiral Shares\\t \\t \\t \\nReturn Before Taxes\\t0.40%\\t0.50%\\t0.81%\\nReturn After Taxes on Distributions\\t\\u20130.08\\t0.20\\t0.54\\nReturn After Taxes on Distributions and Sale of Fund Shares\\t0.22\\t0.25\\t0.51\\nBloomberg Barclays US Treasury 1-3 Year Bond Index\\t \\t \\t \\n(reflects no deduction for fees, expenses, or taxes)\\t0.42%\\t0.57%\\t0.89%\\n \\n\\nActual after-tax returns depend on your tax situation and may differ from those shown in the preceding table. When after-tax returns are calculated, it is assumed that the shareholder was in the highest individual federal marginal income tax bracket at the time of each distribution of income or capital gains or upon redemption. State and local income taxes are not reflected in the calculations. Please note that after-tax returns are not relevant for a shareholder who holds fund shares in a tax-deferred account, such as an individual retirement account or a 401(k) plan. Also, figures captioned Return After Taxes on Distributions and Sale of Fund Shares may be higher than other figures for the same period if a capital loss occurs upon redemption and results in an assumed tax deduction for the shareholder.\\n\\nInvestment Advisor\\nThe Vanguard Group, Inc. (Vanguard)\\n\\nPortfolio Manager\\n\\nJoshua C. Barrickman, CFA, Principal of Vanguard and head of Vanguard\\u2019s Fixed Income Indexing Americas. He has managed the Fund since 2013.\\n\\n4\\n\\n \\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase or redeem shares online through our website (vanguard.com), by mail (The Vanguard Group, P.O. Box 1110, Valley Forge, PA 19482-1110), or by telephone (800-662-2739). The minimum investment amount required to open and maintain a Fund account for Admiral Shares is $3,000. The minimum investment amount required to add to an existing Fund account is generally $1. Financial intermediaries, institutional, and Vanguard-advised clients should contact Vanguard for information on special eligibility rules that may apply to them regarding Admiral Shares. If you are investing through an intermediary, please contact that firm directly for more information regarding your eligibility. If you are investing through an employer-sponsored retirement or savings plan, your plan administrator or your benefits office can provide you with detailed information on how you can invest through your plan.\\n\\nTax Information\\n\\nThe Fund\\u2019s distributions may be taxable as ordinary income or capital gain. If you are investing through a tax-advantaged account, such as an IRA or an employer-sponsored retirement or savings plan, special tax rules apply.\\n\\nPayments to Financial Intermediaries\\n\\nThe Fund and its investment advisor do not pay financial intermediaries for sales of Fund shares.\",\n          \"Eaton Vance TABS Intermediate-Term Municipal Bond Fund\\n\\nInvestment Objective\\n\\nThe Fund\\u2019s investment objective is to seek after-tax total return.\\n\\nFees and Expenses of the Fund\\n\\nThis table describes the fees and expenses that you may pay if you buy and hold shares of the Fund. Investors may also pay commissions or other fees to their financial intermediary when they buy and hold shares of the Fund, which are not reflected below. You may qualify for a reduced sales charge on purchases of Class A shares if you invest, or agree to invest over a 13-month period, at least $100,000 in Eaton Vance funds. Certain financial intermediaries also may offer variations in Fund sales charges to their customers as described in Appendix A \\u2013 Financial Intermediary Sales Charge Variations in this Prospectus. More information about these and other discounts is available from your financial intermediary and in Sales Charges beginning on page 20 of this Prospectus and page 21 of the Fund\\u2019s Statement of Additional Information.\\n\\nShareholder Fees (fees paid directly from your investment)\\tClass A\\tClass C\\tClass I\\nMaximum Sales Charge (Load) Imposed on Purchases (as a percentage of offering price)\\t2.25%\\tNone\\tNone\\nMaximum Deferred Sales Charge (Load) (as a percentage of the lower of net asset value at purchase or redemption)\\tNone\\t1.00%\\tNone\\n \\n\\nAnnual Fund Operating Expenses (expenses you pay each year as a percentage of the value of your investment)\\tClass A\\tClass C\\tClass I\\nManagement Fees\\t0.60%\\t0.60%\\t0.60%\\nDistribution and Service (12b-1) Fees\\t0.25%\\t1.00%\\tNone\\nOther Expenses\\t0.12%\\t0.12%\\t0.12%\\nTotal Annual Fund Operating Expenses\\t0.97%\\t1.72%\\t0.72%\\nExpense Reimbursement (1)\\t(0.07)%\\t(0.07)%\\t(0.07)%\\nTotal Annual Fund Operating Expenses After Expense Reimbursement\\t0.90%\\t1.65%\\t0.65%\\n(1)\\tThe investment adviser and administrator has agreed to reimburse the Fund\\u2019s expenses to the extent that Total Annual Fund Operating Expenses exceed 0.90% for Class A shares, 1.65% for Class C shares and 0.65% for Class I shares. This expense reimbursement will continue through May 31, 2019. Any amendment to or termination of this reimbursement would require approval of the Board of Trustees. The expense reimbursement relates to ordinary operating expenses only and does not include expenses such as: brokerage commissions, acquired fund fees and expenses of unaffiliated funds, interest expense, taxes or litigation expenses. Amounts reimbursed may be recouped by the investment adviser and administrator during the same fiscal year to the extent actual expenses are less than the contractual expense cap during such year.\\nExample. This Example is intended to help you compare the cost of investing in the Fund with the cost of investing in other mutual funds. The Example assumes that you invest $10,000 in the Fund for the time periods indicated and then redeem all of your shares at the end of those periods. The Example also assumes that your investment has a 5% return each year, that the operating expenses remain the same and that any expense reimbursement arrangement remains in place for the contractual period. Although your actual costs may be higher or lower, based on these assumptions your costs would be:\\n\\n \\tExpenses with Redemption\\tExpenses without Redemption\\n \\t1 Year\\t3 Years\\t5 Years\\t10 Years\\t1 Year\\t3 Years\\t5 Years\\t10 Years\\nClass A shares\\t$315\\t$520\\t$743\\t$1,382\\t$315\\t$520\\t$743\\t$1,382\\nClass C shares\\t$268\\t$535\\t$927\\t$2,024\\t$168\\t$535\\t$927\\t$2,024\\nClass I shares\\t$66\\t$223\\t$394\\t$888\\t$66\\t$223\\t$394\\t$888\\nPortfolio Turnover\\n\\nThe Fund pays transaction costs, such as commissions, when it buys and sells securities (or \\u201cturns over\\u201d the portfolio). A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when Fund shares are held in a taxable account. These costs, which are not reflected in Annual Fund Operating Expenses or in the Example, affect the Fund\\u2019s performance. During the most recent fiscal year, the Fund's portfolio turnover rate was 62% of the average value of its portfolio.\\n\\nEaton Vance TABS Municipal Bond Funds\\t7\\tProspectus dated June 1, 2018\\n \\n \\n\\nPrincipal Investment Strategies\\n\\nUnder normal market conditions, the Fund invests at least 80% of its net assets (plus any borrowings for investment purposes) in a diversified portfolio of municipal obligations the interest on which is exempt from regular federal income tax (the \\u201c80% Policy\\u201d). In seeking the Fund\\u2019s investment objective, the portfolio managers emphasize tax-exempt income. The Fund normally invests in municipal obligations rated in the three highest rating categories (those rated A or higher by S&P Global Ratings (\\u201cS&P\\u201d), Fitch Ratings (\\u201cFitch\\u201d) or Moody\\u2019s Investors Service, Inc. (\\u201cMoody\\u2019s\\u201d)) or, if unrated, determined by the investment adviser to be of comparable quality at the time of purchase. The Fund will not invest more than 50% of its net assets in municipal obligations rated A at the time of purchase by S&P, Fitch or Moody\\u2019s or, if unrated determined by the investment adviser to be of comparable quality. For purposes of rating restrictions, if securities are rated differently by two or more rating agencies, the highest rating is used. The Fund may continue to hold securities that are downgraded (including bonds downgraded to below investment grade credit quality (\\u201cjunk bonds\\u201d)) if the investment adviser believes it would be advantageous to do so. The Fund will not invest in a municipal obligation the interest on which the Fund\\u2019s investment adviser believes is subject to the federal alternative minimum tax.\\n\\nFor its investment in municipal obligations, the Fund invests primarily in general obligation or revenue bonds. The Fund currently targets an average portfolio duration of approximately 5 - 7 years and an average weighted portfolio maturity of approximately 5 - 13 years, but may invest in securities of any maturity or duration, and may in the future alter its maturity or duration target range. The Fund may use various techniques to shorten or lengthen its dollar weighted average portfolio duration, including the acquisition of municipal obligations at a premium or discount. The portfolio managers generally will seek to enhance after-tax total return by actively engaging in relative value trading within the portfolio to take advantage of price opportunities in the markets for municipal obligations. With respect to 20% of its net assets, the Fund may invest in municipal obligations that are not exempt from regular federal income tax, direct obligations of the U.S. Treasury and/or obligations of U.S. Government agencies, instrumentalities and government-sponsored enterprises. The Fund may also invest in cash and money market instruments.\\n\\nThe investment adviser\\u2019s process for selecting municipal obligations for purchase and sale generally includes consideration of the creditworthiness of the issuer or person obligated to repay the obligation. In evaluating creditworthiness, the investment adviser considers ratings assigned by rating agencies and generally performs additional credit and investment analysis.\\n\\nPrincipal Risks\\n\\nMarket Risk. The value of investments held by the Fund may increase or decrease in response to economic, political and financial events (whether real, expected or perceived) in the U.S. and global markets. The frequency and magnitude of such changes in value cannot be predicted. Certain securities and other investments held by the Fund may experience increased volatility, illiquidity, or other potentially adverse effects in reaction to changing market conditions. Actions taken by the U.S. Federal Reserve or foreign central banks to stimulate or stabilize economic growth, such as decreases or increases in short-term interest rates, could cause high volatility in markets. No active trading market may exist for certain investments, which may impair the ability of the Fund to sell or to realize the current valuation of such investments in the event of the need to liquidate such assets. Fixed-income markets may experience periods of relatively high volatility in an environment where U.S. treasury yields are rising.\\n\\nMunicipal Obligation Risk. The amount of public information available about municipal obligations is generally less than for corporate equities or bonds, meaning that the investment performance of municipal obligations may be more dependent on the analytical abilities of the investment adviser than stock or corporate bond investments. The secondary market for municipal obligations also tends to be less well-developed and less liquid than many other securities markets, which may limit the Fund\\u2019s ability to sell its municipal obligations at attractive prices. The differences between the price at which an obligation can be purchased and the price at which it can be sold may widen during periods of market distress. Less liquid obligations can become more difficult to value and be subject to erratic price movements. The increased presence of nontraditional participants (such as proprietary trading desks of investment banks and hedge funds) or the absence of traditional participants (such as individuals, insurance companies, banks and life insurance companies) in the municipal markets may lead to greater volatility in the markets because non-traditional participants may trade more frequently or in greater volume. \\n\\nInterest Rate Risk. In general, the value of income securities will fluctuate based on changes in interest rates. The value of these securities is likely to increase when interest rates fall and decline when interest rates rise. Generally, securities with longer durations are more sensitive to changes in interest rates than shorter duration securities, causing them to be more volatile. Conversely, fixed income securities with shorter durations will be less volatile but may provide lower returns than fixed income securities with longer durations. In a rising interest rate environment, the durations of income securities that have the ability to be prepaid or called by the issuer may be extended. In a declining interest rate environment, the proceeds from prepaid or maturing instruments may have to be reinvested at a lower interest rate.\\n\\nEaton Vance TABS Municipal Bond Funds\\t8\\tProspectus dated June 1, 2018\\n \\nCredit Risk. Investments in municipal obligations and other debt obligations (referred to below as \\u201cdebt instruments\\u201d) are subject to the risk of non-payment of scheduled principal and interest. Changes in economic conditions or other circumstances may reduce the capacity of the party obligated to make principal and interest payments on such instruments and may lead to defaults. Such non-payments and defaults may reduce the value of Fund shares and income distributions. The value of debt instruments also may decline because of concerns about the issuer\\u2019s ability to make principal and interest payments. In addition, the credit ratings of debt instruments may be lowered if the financial condition of the party obligated to make payments with respect to such instruments deteriorates. In order to enforce its rights in the event of a default, bankruptcy or similar situation, the Fund may be required to retain legal or similar counsel, which may increase the Fund\\u2019s operating expenses and adversely affect net asset value. Municipal obligations may be insured as to principal and interest payments. If the claims-paying ability or other rating of the insurer is downgraded by a rating agency, the value of such obligations may be negatively affected.\\n\\nRisks of Principal Only Investments. Principal only investments entitle the Fund to receive the stated value of such investment when held to maturity. The values of principal only investments are subject to greater fluctuation in response to changes in market interest rates than obligations that pay interest currently. The Fund will accrue income on these investments and distribute that income each year. The Fund may be required to sell other investments to obtain cash needed for such income distributions.\\n\\nU.S. Government Securities Risk. Although certain U.S. Government-sponsored agencies (such as the Federal Home Loan Mortgage Corporation and the Federal National Mortgage Association) may be chartered or sponsored by acts of Congress, their securities are neither issued nor guaranteed by the U.S. Treasury. U.S. Treasury securities generally have a lower return than other obligations because of their higher credit quality and market liquidity.\\n\\nTax Risk. Income from tax-exempt municipal obligations could be declared taxable because of changes in tax laws, adverse interpretations by the relevant taxing authority or the non-compliant conduct of the issuer of an obligation.\\n\\nMoney Market Instrument Risk. Money market instruments may be adversely affected by market and economic events, such as a sharp rise in prevailing short-term interest rates; adverse developments in the banking industry, which issues or guarantees many money market instruments; adverse economic, political or other developments affecting issuers of money market instruments; changes in the credit quality of issuers; and default by a counterparty.\\n\\nGeneral Fund Investing Risks. The Fund is not a complete investment program and there is no guarantee that the Fund will achieve its investment objective. It is possible to lose money by investing in the Fund. The Fund is designed to be a long-term investment vehicle and is not suited for short-term trading. Investors in the Fund should have a long-term investment perspective and be able to tolerate potentially sharp declines in value. Purchase and redemption activities by Fund shareholders may impact the management of the Fund and its ability to achieve its investment objective(s). In addition, the redemption by one or more large shareholders or groups of shareholders of their holdings in the Fund could have an adverse impact on the remaining shareholders in the Fund. An investment in the Fund is not a deposit in a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency.\\n\\nPerformance\\n\\nThe following bar chart and table provide some indication of the risks of investing in the Fund by showing changes in the Fund\\u2019s performance from year to year and how the Fund\\u2019s average annual returns over time compare with those of two broad-based securities market indices. The returns in the bar chart are for Class A shares and do not reflect a sales charge. If the sales charge was reflected, the returns would be lower. Past performance (both before and after taxes) is not necessarily an indication of how the Fund will perform in the future. The Fund\\u2019s performance reflects the effects of expense reductions. Absent these reductions, performance would have been lower. Updated Fund performance information can be obtained by visiting www.eatonvance.com.\\n\\n\\n\\nDuring the period from December 31, 2010 to December 31, 2017, the highest quarterly total return for Class A was 3.75% for the quarter ended June 30, 2011, and the lowest quarterly return was \\u20134.07% for the quarter ended December 31, 2016. The year-to-date total return through the end of the most recent calendar quarter (December 31, 2017 to March 31, 2018) was -1.71%.\\n\\nEaton Vance TABS Municipal Bond Funds\\t9\\tProspectus dated June 1, 2018\\n \\n \\n\\nAverage Annual Total Return as of December 31, 2017\\tOne Year\\tFive Years\\tLife of Fund\\nClass A Return Before Taxes\\t2.08%\\t2.12%\\t4.21%\\nClass A Return After Taxes on Distributions\\t2.05%\\t2.08%\\t4.09%\\nClass A Return After Taxes on Distributions and the Sale of Class A Shares\\t1.94%\\t1.97%\\t3.59%\\nClass C Return Before Taxes\\t2.67%\\t1.82%\\t3.74%\\nClass I Return Before Taxes\\t4.71%\\t2.82%\\t4.78%\\nBloomberg Barclays Municipal Managed Money Intermediate (1-17 Year) Bond Index (reflects no deduction for fees, expenses or taxes)\\t4.88%\\t2.55%\\t4.26%\\nBloomberg Barclays 7 Year Municipal Bond Index (reflects no deduction for fees, expenses or taxes)\\t4.49%\\t2.44%\\t4.30%\\nThese returns reflect the maximum sales charge for Class A (2.25%) and any applicable contingent deferred sales charge (\\u201cCDSC\\u201d) for Class C. Class A, Class C and Class I commenced operations on February 1, 2010. Effective February 17, 2015, the Fund changed its name, objective and investment strategy to invest at least 80% of its net assets in a diversified portfolio of municipal obligations, the interest on which is exempt from regular federal income tax. Investors cannot invest directly in an Index.\\n\\nAfter-tax returns are calculated using the highest historical individual federal income tax rates and does not reflect the impact of state and local taxes. Actual after-tax returns depend on a shareholder\\u2019s tax situation and the actual characterization of distributions, and may differ from those shown. After-tax returns are not relevant to shareholders who hold shares in tax-deferred accounts or to shares held by non-taxable entities. After-tax returns for other Classes of shares will vary from the after-tax returns presented for Class A shares. Return After Taxes on Distributions for a period may be the same as Return Before Taxes for that period because no taxable distributions were made during that period. Also, Return After Taxes on distributions and Sale of Fund Shares for a period may be greater than or equal to Return After Taxes on Distributions for the same period because of losses realized on the sale of Fund shares.\\n\\nManagement\\n\\nInvestment Adviser. Eaton Vance Management (\\u201cEaton Vance\\u201d).\\n\\nPortfolio Managers\\n\\nThe portfolio managers of the Fund are part of Eaton Vance\\u2019s Tax-Advantaged Bond Strategies (\\u201cTABS\\u201d) division.\\n\\nJames H. Evans, (lead portfolio manager), Vice President of Eaton Vance, has managed the Fund since it commenced operations in February 2010.\\n\\nBrian C. Barney, Vice President of Eaton Vance, has managed the Fund since June 2010.\\n\\nChristopher J. Harshman, Vice President of Eaton Vance, has managed the Fund since June 2010.\\n\\nFor important information about purchase and sale of shares, taxes and financial intermediary compensation, please turn to \\u201cImportant Information Regarding Fund Shares\\u201d on page 11 of this Prospectus.\\n\\nEaton Vance TABS Municipal Bond Funds\\t10\\tProspectus dated June 1, 2018\\n \\n \\n\\nImportant Information Regarding Fund Shares\\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase, redeem or exchange Fund shares on any business day, which is any day the New York Stock Exchange is open for business. You may purchase, redeem or exchange Fund shares either through your financial intermediary or (except for purchases of Class C shares by accounts with no specified financial intermediary) directly from a Fund either by writing to the Fund, P.O. Box 9653, Providence, RI 02940-9653, or by calling 1-800-262-1122. The minimum initial purchase or exchange into a Fund is $1,000 for each Class (with the exception of Class I) and $250,000 for Class I (waived in certain circumstances). There is no minimum for subsequent investments.\\n\\nTax Information\\n\\nEach Fund\\u2019s distributions are expected to primarily be exempt from regular federal income tax. However, the Fund may also distribute taxable income to the extent that it invests in taxable municipal obligations or other obligations which generate taxable income. Distributions of any net realized gains are expected to be taxable.\\n\\nPayments to Broker-Dealers and Other Financial Intermediaries\\n\\nIf you purchase a Fund\\u2019s shares through a broker-dealer or other financial intermediary (such as a bank) (collectively, \\u201cfinancial intermediaries\\u201d), the Fund, its principal underwriter and its affiliates may pay the financial intermediary for the sale of Fund shares and related services. These payments may create a conflict of interest by influencing the financial intermediary and your salesperson to recommend a Fund over another investment. Ask your salesperson or visit your financial intermediary\\u2019s website for more information.\",\n          \"Fund Summary\\n\\nFund/Class:\\nFidelity Freedom\\u00ae 2050 Fund/Fidelity Freedom\\u00ae 2050 Fund\\n\\nInvestment Objective\\n\\nThe fund seeks high total return until its target retirement date. Thereafter the fund's objective will be to seek high current income and, as a secondary objective, capital appreciation.\\n\\nFee Table\\n\\nThe following table describes the fees and expenses that may be incurred when you buy and hold shares of the fund.\\n\\nShareholder fees\\n\\n(fees paid directly from your investment) \\tNone \\nAnnual Operating Expenses\\n\\n(expenses that you pay each year as a % of the value of your investment)\\n\\nManagement fee(a) \\t \\t0.75% \\nDistribution and/or Service (12b-1) fees \\t \\tNone \\nOther expenses(a) \\t \\t0.00% \\nTotal annual operating expenses(a) \\t \\t0.75% \\n(a)   Adjusted to reflect current fees.\\n\\nThis example helps compare the cost of investing in the fund with the cost of investing in other funds.\\n\\nLet's say, hypothetically, that the annual return for shares of the fund is 5% and that your shareholder fees and the annual operating expenses for shares of the fund are exactly as described in the fee table. This example illustrates the effect of fees and expenses, but is not meant to suggest actual or expected fees and expenses or returns, all of which may vary. For every $10,000 you invested, here's how much you would pay in total expenses if you sell all of your shares at the end of each time period indicated:\\n\\n1 year \\t$76 \\n3 years \\t$238 \\n5 years \\t$415 \\n10 years \\t$926 \\nPortfolio Turnover\\n\\nThe fund will not incur transaction costs, such as commissions, when it buys and sells shares of underlying Fidelity\\u00ae funds (or \\\"turns over\\\" its portfolio), but it could incur transaction costs if it were to buy and sell other types of securities directly. If the fund were to buy and sell other types of securities directly, a higher portfolio turnover rate could indicate higher transaction costs and could result in higher taxes when fund shares are held in a taxable account. Such costs, if incurred, would not be reflected in annual operating expenses or in the example and would affect the fund's performance. During the most recent fiscal year, the fund's portfolio turnover rate was 16% of the average value of its portfolio.\\n\\nPrincipal Investment Strategies\\n\\nInvesting in a combination of Fidelity\\u00ae domestic equity funds, international equity funds (developed and emerging markets), bond funds, and short-term funds (underlying Fidelity\\u00ae funds).\\nAllocating assets according to a neutral asset allocation strategy shown in the glide path below that becomes increasingly conservative until it reaches an allocation similar to that of the Fidelity Freedom\\u00ae Income Fund, approximately 10 to 19 years after the year 2050 (approximately 17% in domestic equity funds, 7% in international equity funds, 46% in bond funds, and 30% in short-term funds).\\nBuying and selling futures contracts (both long and short positions) in an effort to manage cash flows efficiently, remain fully invested, or facilitate asset allocation.\\nFMR Co., Inc. (FMRC) may continue to seek high total return for several years beyond the fund's target retirement date in an effort to achieve the fund's overall investment objective.\\n\\nAs of March 31, 2018, the fund's neutral asset allocation to underlying Fidelity\\u00ae funds and futures was approximately:\\n   \\tDomestic Equity Funds* \\t63% \\n   \\tInternational Equity Funds* \\t27% \\n   \\tBond Funds* \\t10% \\n \\tShort-Term Funds* \\t0% \\n\\n* FMRC may change these percentages over time. As a result of the active asset allocation strategy (discussed below), actual allocations may differ from the neutral allocations above. The allocation percentages may not add to 100% due to rounding.\\n\\nFMRC may use an active asset allocation strategy to increase or decrease neutral asset class exposures reflected above by up to 10% for equity funds (includes domestic equity and international equity funds), bond funds and short-term funds to reflect FMRC's market outlook, which is primarily focused on the intermediate term. The asset allocations in the glide path and pie chart above are referred to as neutral because they do not reflect any decisions made by FMRC to overweight or underweight an asset class.\\nFMRC may also make active asset allocations within other asset classes (including commodities, high yield debt, floating rate debt, real estate debt, inflation-protected debt, and emerging markets debt) from 0% to 10% individually but no more than 25% in aggregate within those other asset classes. Such asset classes are not reflected in the neutral asset allocations reflected in the glide path and pie chart above.\\nDesigned for investors who anticipate retiring in or within a few years of 2050 (target retirement date) at or around age 65 and plan to gradually withdraw the value of their account in the fund over time.\\nPrincipal Investment Risks\\n\\nShareholders should consider that no target date fund is intended as a complete retirement program and there is no guarantee that any single fund will provide sufficient retirement income at or through your retirement. The fund's share price fluctuates, which means you could lose money by investing in the fund, including losses near, at or after the target retirement date.\\n\\nAsset Allocation Risk.  The fund is subject to risks resulting from the Adviser's asset allocation decisions. The selection of underlying funds and the allocation of the fund's assets among various asset classes could cause the fund to lose value or its results to lag relevant benchmarks or other funds with similar objectives. In addition, the fund's active asset allocation strategy may cause the fund to have a risk profile different than that portrayed above from time to time and may increase losses.\\nInvesting in Other Funds.  The fund bears all risks of investment strategies employed by the underlying funds, including the risk that the underlying funds will not meet their investment objectives.\\nStock Market Volatility.  Stock markets are volatile and can decline significantly in response to adverse issuer, political, regulatory, market, or economic developments. Different parts of the market, including different market sectors, and different types of securities can react differently to these developments.\\nInterest Rate Changes.  Interest rate increases can cause the price of a debt or money market security to decrease.\\nForeign Exposure.  Foreign markets, particularly emerging markets, can be more volatile than the U.S. market due to increased risks of adverse issuer, political, regulatory, market, or economic developments and can perform differently from the U.S. market. Emerging markets can be subject to greater social, economic, regulatory, and political uncertainties and can be extremely volatile. Foreign exchange rates also can be extremely volatile.\\nIndustry Exposure.  Market conditions, interest rates, and economic, regulatory, or financial developments could significantly affect a single industry or group of related industries.\\nIssuer-Specific Changes.  The value of an individual security or particular type of security can be more volatile than, and can perform differently from, the market as a whole. Lower-quality debt securities (those of less than investment-grade quality, also referred to as high yield debt securities or junk bonds) and certain types of other securities involve greater risk of default or price changes due to changes in the credit quality of the issuer. The value of lower-quality debt securities and certain types of other securities can be more volatile due to increased sensitivity to adverse issuer, political, regulatory, market, or economic developments.\\nLeverage Risk.  Leverage can increase market exposure, magnify investment risks, and cause losses to be realized more quickly.\\n\\\"Growth\\\" Investing.  \\\"Growth\\\" stocks can perform differently from the market as a whole and other types of stocks and can be more volatile than other types of stocks.\\n\\\"Value\\\" Investing.  \\\"Value\\\" stocks can perform differently from the market as a whole and other types of stocks and can continue to be undervalued by the market for long periods of time.\\nCommodity-Linked Investing.  The value of commodities and commodity-linked investments may be affected by the performance of the overall commodities markets as well as weather, political, tax, and other regulatory and market developments. Commodity-linked investments may be more volatile and less liquid than the underlying commodity, instruments, or measures.\\nAn investment in the fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency. You could lose money by investing in the fund.\\n\\nPerformance\\n\\nThe following information is intended to help you understand the risks of investing in the fund. The information illustrates the changes in the performance of the fund's shares from year to year and compares the performance of the fund's shares to the performance of a securities market index and a hypothetical composite of market indexes over various periods of time. The indexes have characteristics relevant to the fund's investment strategies. Index descriptions appear in the \\\"Additional Index Information\\\" section of the prospectus. Prior to June 1, 2017, the fund operated under a different pricing structure. The fund\\u2019s historical performance prior to June 1, 2017 does not reflect the fund\\u2019s current pricing structure. Past performance (before and after taxes) is not an indication of future performance.\\n\\nVisit www.fidelity.com for more recent performance information.\\n\\nYear-by-Year Returns\\n\\n\\n\\nDuring the periods shown in the chart: \\tReturns \\tQuarter ended \\nHighest Quarter Return \\t19.11% \\tJune 30, 2009 \\nLowest Quarter Return \\t(23.40)% \\tDecember 31, 2008 \\nYear-to-Date Return \\t(0.25)% \\tMarch 31, 2018 \\nAverage Annual Returns\\n\\nAfter-tax returns are calculated using the historical highest individual federal marginal income tax rates, but do not reflect the impact of state or local taxes. Actual after-tax returns may differ depending on your individual circumstances. The after-tax returns shown are not relevant if you hold your shares in a retirement account or in another tax-deferred arrangement, such as an employee benefit plan (profit sharing, 401(k), or 403(b) plan).\\n\\nFor the periods ended December 31, 2017 \\tPast 1 year \\tPast 5 years \\tPast 10 years \\nFidelity Freedom\\u00ae 2050 Fund \\nReturn Before Taxes \\t22.28% \\t11.30% \\t5.31% \\nReturn After Taxes on Distributions \\t21.05% \\t9.54% \\t4.09% \\nReturn After Taxes on Distributions and Sale of Fund Shares \\t13.44% \\t8.52% \\t3.85% \\nS&P 500\\u00ae Index\\n(reflects no deduction for fees, expenses, or taxes) \\t21.83% \\t15.79% \\t8.50% \\nFidelity Freedom 2050 Composite Index\\u2120\\n(reflects no deduction for fees or expenses) \\t20.95% \\t12.05% \\t6.60% \\nInvestment Adviser\\n\\nFMRC (the Adviser), an affiliate of Fidelity Management & Research Company (FMR), is the fund's manager.\\n\\nPortfolio Manager(s)\\n\\nAndrew Dierdorf (co-manager) has managed the fund since June 2011.\\n\\nBrett Sumsion (co-manager) has managed the fund since January 2014.\\n\\nPurchase and Sale of Shares\\n\\nYou may buy or sell shares through a Fidelity\\u00ae brokerage or mutual fund account, through a retirement account, or through an investment professional. You may buy or sell shares in various ways:\\n\\nInternet\\n\\nwww.fidelity.com\\n\\nPhone\\n\\nFidelity Automated Service Telephone (FAST\\u00ae) 1-800-544-5555\\n\\nTo reach a Fidelity representative 1-800-544-6666\\n\\nMail\\n\\nAdditional purchases:\\n\\nFidelity Investments\\nP.O. Box 770001\\nCincinnati, OH 45277-0003\\nRedemptions:\\n\\nFidelity Investments\\nP.O. Box 770001\\nCincinnati, OH 45277-0035\\nTDD- Service for the Deaf and Hearing Impaired\\n\\n1-800-544-0118\\n\\nThe price to buy one share is its net asset value per share (NAV). Shares will be bought at the NAV next calculated after an order is received in proper form.\\n\\nThe price to sell one share is its NAV. Shares will be sold at the NAV next calculated after an order is received in proper form.\\n\\nThe fund is open for business each day the New York Stock Exchange (NYSE) is open.\\n\\nInitial Purchase Minimum \\t$2,500 \\nFor Fidelity\\u00ae Simplified Employee Pension-IRA, Keogh, and Investment Only Retirement accounts \\t$500 \\nThrough regular investment plans in Fidelity\\u00ae Traditional IRAs, Roth IRAs, and Rollover IRAs (requires monthly purchases of $200 until fund balance is $2,500) \\t$200 \\nThe fund may waive or lower purchase minimums in other circumstances.\\n\\nTax Information\\n\\nDistributions you receive from the fund are subject to federal income tax and generally will be taxed as ordinary income or capital gains, and may also be subject to state or local taxes, unless you are investing through a tax-advantaged retirement account (in which case you may be taxed later, upon withdrawal of your investment from such account).\\n\\nPayments to Broker-Dealers and Other Financial Intermediaries\\n\\nThe fund, the Adviser, Fidelity Distributors Corporation (FDC), and/or their affiliates may pay intermediaries, which may include banks, broker-dealers, retirement plan sponsors, administrators, or service-providers (who may be affiliated with the Adviser or FDC), for the sale of fund shares and related services. These payments may create a conflict of interest by influencing your intermediary and your investment professional to recommend the fund over another investment. Ask your investment professional or visit your intermediary's web site for more information.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_distance\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 253,\n        \"samples\": [\n          \"2 \\tWith limited exceptions, for Class A shares, if your Fund account balance is below $650 at the start of business on the Friday prior to the last full week of September of each year, the account will be assessed an account fee of $20. 3 \\tThrough July 31, 2019, Ivy Investment Management Company (IICO), the Fund\\u2019s investment manager, Ivy Distributors, Inc. (IDI), the Fund\\u2019s distributor, and/or Waddell & Reed Services Company, doing business as WI Services Company (WISC), the Fund\\u2019s transfer agent, have contractually agreed to reimburse sufficient management fees, 12b-1 fees and/or shareholder servicing fees to cap the total annual ordinary fund operating expenses (which would exclude interest, taxes, brokerage commissions, acquired fund fees and expenses and extraordinary expenses, if any) as follows: Class E shares at 1.10%. 4\\t \\tProspectus\\t \\tDomestic Equity Funds\\nTable of Contents\\nExample\\n\\nThis example is intended to help you compare the cost of investing in the shares of the Fund with the cost of investing in other mutual funds. 4 \\tThrough July 31, 2020, IICO, IDI and/or WISC have contractually agreed to reimburse sufficient management fees, 12b-1 fees and/or shareholder servicing fees to cap the total annual ordinary fund operating expenses (which would exclude interest, taxes, brokerage commissions, acquired fund fees and expenses and extraordinary expenses, if any) as follows: Class A shares at 1.04%; Class B shares at 2.13%; Class E shares at 1.13%; and Class I shares and Class Y shares at 0.84%. 5 \\tThrough July 31, 2020, IDI and/or WISC have contractually agreed to reimburse sufficient 12b-1 and/or shareholder servicing fees to ensure that the total annual ordinary fund operating expenses of the Class Y shares do not exceed the total annual ordinary fund operating expenses of the Class A shares, as calculated at the end of each month.\",\n          \"A higher portfolio turnover rate may indicate higher transaction costs and may result in higher taxes when fund shares are held in a taxable account. An investment in the fund is not a deposit of a bank and is not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency. Ask your investment professional or visit your intermediary's web site for more information. Box 770001\\nCincinnati, OH 45277-0035\\nOvernight Express:\\nFidelity Investments\\n100 Crosby Parkway\\nCovington, KY 41015\\nTDD- Service for the Deaf and Hearing Impaired\\n\\n1-800-544-0118\\n\\nThe price to buy one share is its net asset value per share (NAV). Currently, the Board of Trustees of the fund has not authorized such payments for shares of the fund.\",\n          \"2\\n\\n \\n\\nAnnual Total Returns\\n\\nThe following bar chart and table are intended to help you understand the risks of investing in the Fund. 3\\n\\n \\n\\nAverage Annual Total Returns for Periods Ended December 31, 2017\\t \\t \\n \\t \\t \\tSince\\n \\t \\t \\tInception\\n \\t \\t \\t(Dec. 28,\\n \\t1 Year\\t5 Years\\t2009)\\nVanguard Short-Term Treasury Index Fund Admiral Shares\\t \\t \\t \\nReturn Before Taxes\\t0.40%\\t0.50%\\t0.81%\\nReturn After Taxes on Distributions\\t\\u20130.08\\t0.20\\t0.54\\nReturn After Taxes on Distributions and Sale of Fund Shares\\t0.22\\t0.25\\t0.51\\nBloomberg Barclays US Treasury 1-3 Year Bond Index\\t \\t \\t \\n(reflects no deduction for fees, expenses, or taxes)\\t0.42%\\t0.57%\\t0.89%\\n \\n\\nActual after-tax returns depend on your tax situation and may differ from those shown in the preceding table. 4\\n\\n \\n\\nPurchase and Sale of Fund Shares\\n\\nYou may purchase or redeem shares online through our website (vanguard.com), by mail (The Vanguard Group, P.O. A higher portfolio turnover rate may indicate higher transaction costs and may result in more taxes when Fund shares are held in a taxable account. All of the Fund\\u2019s investments will be selected through the sampling process, and under normal circumstances, at least 80% of the Fund\\u2019s assets will be invested in bonds included in the Index.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_match\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 255,\n        \"samples\": [\n          \"Tracking error may occur because of differences between the securities held in the Fund\\u2019s portfolio and those included in the Emerging Markets Underlying Index, pricing differences (including differences between a security\\u2019s price at the local market close and the Fund\\u2019s valuation of a security at the time of calculation of the Fund\\u2019s NAV), transaction costs, the Fund\\u2019s holding of cash, differences in timing of the accrual of dividends or interest, tax gains or losses, changes to the Emerging Markets Underlying Index or the need to meet various new or existing regulatory requirements. Under the representative sampling technique, the investment manager will select securities that collectively have an investment profile similar to that of the Emerging Markets Underlying Index, including securities that resemble those included in the Emerging Markets Underlying Index in terms of risk factors, performance attributes and other characteristics, such as market capitalization and industry weightings. When there are more sellers than buyers, prices tend to fall. While MSCI provides descriptions of what the Emerging Markets Underlying Index is designed to achieve, MSCI does not guarantee the quality, accuracy or completeness of data in respect of its indices, and does not guarantee that the Emerging Markets Underlying Index will be in line with the described index methodology. You can obtain updated performance information at libertyshares.com or by calling (800) DIAL BEN/342-5236. You may also incur usual and customary brokerage commissions when buying or selling shares of the Fund, which are not reflected in the Example that follows.\",\n          \"This example does not include any fees paid at the fee-based account or plan level. This example helps compare the cost of investing in the fund with the cost of investing in other funds. This example illustrates the effect of fees and expenses, but is not meant to suggest actual or expected fees and expenses or returns, all of which may vary. Using fundamental analysis of factors such as each issuer's financial condition and industry position, as well as market and economic conditions, to select investments. You could lose money by investing in the fund. You may buy or sell shares in various ways:\\n\\nInternet\\n\\nPlan Accounts:\\n\\nwww.401k.com\\t\\nAll Other Accounts:\\n\\nwww.fidelity.com\\nPhone\\n\\nPlan Accounts:\\n\\nFor Individual Accounts (investing through a retirement plan sponsor or other institution), refer to your plan materials or contact that institution directly.\",\n          \"\\u220e\\t \\tLow-Rated Securities Risk. \\u220e\\t \\tManagement Risk. \\u220e\\t \\tMarket Risk. \\u220e\\t \\tMortgage-Backed and Asset-Backed Securities Risk. \\u220e\\t \\tU.S. Government Securities Risk. \\u220e\\t \\tValue Stock Risk.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def create_CNN_model():\n",
        "    CNN = Sequential()\n",
        "    CNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=True))\n",
        "\n",
        "    CNN.add(Convolution1D(256, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Convolution1D(128, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Convolution1D(64, 3, activation = 'relu'))\n",
        "    CNN.add(BatchNormalization())\n",
        "    CNN.add(MaxPooling1D(pool_size = 3))\n",
        "\n",
        "    CNN.add(Flatten())\n",
        "    CNN.add(Dense(units = 512 , activation = 'relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(Dropout(0.3))\n",
        "    CNN.add(Dense(units = 256 , activation = 'relu', kernel_regularizer=l2(0.01)))\n",
        "    CNN.add(Dropout(0.3))\n",
        "    CNN.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "    opt = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "    CNN.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    return CNN\n",
        "\n",
        "CNN_model = create_CNN_model()\n",
        "\n",
        "# 创建ModelCheckpoint回调函数\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5',\n",
        "                             monitor='val_accuracy',\n",
        "                             save_best_only=True,\n",
        "                             mode='max',\n",
        "                             verbose=1)\n",
        "\n",
        "class_weights = {0: 1.0, 1: 0.5, 2: 3.0}\n",
        "CNN_history = CNN_model.fit(feature_train, label_train_y,\n",
        "                            epochs=700, batch_size=128,\n",
        "                            validation_data=(feature_valid, label_valid_y),\n",
        "                            class_weight=class_weights,\n",
        "                            callbacks=[checkpoint])  # 将回调函数传递给fit函数"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gRNEmwO77Oy",
        "outputId": "9cb042fd-b477-404f-b9ee-68e66ecb9cf5"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/700\n",
            "1/3 [=========>....................] - ETA: 47s - loss: 8.1034 - accuracy: 0.2969\n",
            "Epoch 1: val_accuracy improved from -inf to 0.48649, saving model to best_model.h5\n",
            "3/3 [==============================] - 24s 210ms/step - loss: 8.0705 - accuracy: 0.2755 - val_loss: 7.9020 - val_accuracy: 0.4865\n",
            "Epoch 2/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 8.1411 - accuracy: 0.2656\n",
            "Epoch 2: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 8.0777 - accuracy: 0.2585 - val_loss: 7.8531 - val_accuracy: 0.3108\n",
            "Epoch 3/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.9477 - accuracy: 0.3594\n",
            "Epoch 3: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 7.9210 - accuracy: 0.3231 - val_loss: 7.8073 - val_accuracy: 0.3108\n",
            "Epoch 4/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.8780 - accuracy: 0.2812\n",
            "Epoch 4: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 7.8606 - accuracy: 0.2857 - val_loss: 7.7566 - val_accuracy: 0.3108\n",
            "Epoch 5/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.8727 - accuracy: 0.2812\n",
            "Epoch 5: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 7.8546 - accuracy: 0.2619 - val_loss: 7.6903 - val_accuracy: 0.3108\n",
            "Epoch 6/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.7576 - accuracy: 0.2891\n",
            "Epoch 6: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 7.7395 - accuracy: 0.2687 - val_loss: 7.6199 - val_accuracy: 0.3108\n",
            "Epoch 7/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.6624 - accuracy: 0.2422\n",
            "Epoch 7: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 7.6702 - accuracy: 0.2585 - val_loss: 7.5475 - val_accuracy: 0.3108\n",
            "Epoch 8/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.5246 - accuracy: 0.2734\n",
            "Epoch 8: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 7.6152 - accuracy: 0.2551 - val_loss: 7.4805 - val_accuracy: 0.3108\n",
            "Epoch 9/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.5029 - accuracy: 0.1953\n",
            "Epoch 9: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 7.5342 - accuracy: 0.2551 - val_loss: 7.4090 - val_accuracy: 0.3108\n",
            "Epoch 10/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.4794 - accuracy: 0.2344\n",
            "Epoch 10: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 7.4660 - accuracy: 0.2585 - val_loss: 7.3341 - val_accuracy: 0.3108\n",
            "Epoch 11/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.3493 - accuracy: 0.2031\n",
            "Epoch 11: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 7.3656 - accuracy: 0.2551 - val_loss: 7.2591 - val_accuracy: 0.3108\n",
            "Epoch 12/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.2871 - accuracy: 0.2656\n",
            "Epoch 12: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 7.3142 - accuracy: 0.2551 - val_loss: 7.1811 - val_accuracy: 0.3108\n",
            "Epoch 13/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.2244 - accuracy: 0.2266\n",
            "Epoch 13: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 7.2264 - accuracy: 0.2483 - val_loss: 7.1102 - val_accuracy: 0.3108\n",
            "Epoch 14/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.0937 - accuracy: 0.2734\n",
            "Epoch 14: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 7.1107 - accuracy: 0.2551 - val_loss: 7.0386 - val_accuracy: 0.3108\n",
            "Epoch 15/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.0855 - accuracy: 0.2578\n",
            "Epoch 15: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 7.0618 - accuracy: 0.2517 - val_loss: 6.9639 - val_accuracy: 0.3108\n",
            "Epoch 16/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 7.0156 - accuracy: 0.2812\n",
            "Epoch 16: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 7.0020 - accuracy: 0.2517 - val_loss: 6.8867 - val_accuracy: 0.3108\n",
            "Epoch 17/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.9511 - accuracy: 0.2344\n",
            "Epoch 17: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.9220 - accuracy: 0.2483 - val_loss: 6.8127 - val_accuracy: 0.3108\n",
            "Epoch 18/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.9084 - accuracy: 0.2891\n",
            "Epoch 18: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.8585 - accuracy: 0.2517 - val_loss: 6.7421 - val_accuracy: 0.3108\n",
            "Epoch 19/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.7421 - accuracy: 0.2188\n",
            "Epoch 19: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 6.7767 - accuracy: 0.2517 - val_loss: 6.6743 - val_accuracy: 0.3108\n",
            "Epoch 20/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.7086 - accuracy: 0.2031\n",
            "Epoch 20: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.7055 - accuracy: 0.2517 - val_loss: 6.6069 - val_accuracy: 0.3108\n",
            "Epoch 21/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.6694 - accuracy: 0.2578\n",
            "Epoch 21: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 6.6588 - accuracy: 0.2517 - val_loss: 6.5362 - val_accuracy: 0.3108\n",
            "Epoch 22/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.6358 - accuracy: 0.2422\n",
            "Epoch 22: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 6.5866 - accuracy: 0.2517 - val_loss: 6.4672 - val_accuracy: 0.3108\n",
            "Epoch 23/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.5074 - accuracy: 0.2969\n",
            "Epoch 23: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 6.5100 - accuracy: 0.2517 - val_loss: 6.3993 - val_accuracy: 0.3108\n",
            "Epoch 24/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.4970 - accuracy: 0.2109\n",
            "Epoch 24: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 6.4583 - accuracy: 0.2551 - val_loss: 6.3330 - val_accuracy: 0.3108\n",
            "Epoch 25/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.4483 - accuracy: 0.2578\n",
            "Epoch 25: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.3798 - accuracy: 0.2517 - val_loss: 6.2696 - val_accuracy: 0.3108\n",
            "Epoch 26/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.3365 - accuracy: 0.2891\n",
            "Epoch 26: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.3238 - accuracy: 0.2517 - val_loss: 6.2081 - val_accuracy: 0.3108\n",
            "Epoch 27/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.2964 - accuracy: 0.2109\n",
            "Epoch 27: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 6.2668 - accuracy: 0.2517 - val_loss: 6.1465 - val_accuracy: 0.3108\n",
            "Epoch 28/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.2707 - accuracy: 0.2734\n",
            "Epoch 28: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 6.1910 - accuracy: 0.2517 - val_loss: 6.0840 - val_accuracy: 0.3108\n",
            "Epoch 29/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.1358 - accuracy: 0.2812\n",
            "Epoch 29: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 6.1396 - accuracy: 0.2517 - val_loss: 6.0232 - val_accuracy: 0.3108\n",
            "Epoch 30/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.0595 - accuracy: 0.2188\n",
            "Epoch 30: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 6.0742 - accuracy: 0.2517 - val_loss: 5.9656 - val_accuracy: 0.3108\n",
            "Epoch 31/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.0151 - accuracy: 0.2500\n",
            "Epoch 31: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 5.9908 - accuracy: 0.2517 - val_loss: 5.9071 - val_accuracy: 0.3108\n",
            "Epoch 32/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 6.0385 - accuracy: 0.2578\n",
            "Epoch 32: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.9592 - accuracy: 0.2517 - val_loss: 5.8481 - val_accuracy: 0.3108\n",
            "Epoch 33/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.8632 - accuracy: 0.2812\n",
            "Epoch 33: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.8782 - accuracy: 0.2517 - val_loss: 5.7914 - val_accuracy: 0.3108\n",
            "Epoch 34/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.7871 - accuracy: 0.2422\n",
            "Epoch 34: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.8320 - accuracy: 0.2517 - val_loss: 5.7372 - val_accuracy: 0.3108\n",
            "Epoch 35/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.8200 - accuracy: 0.2656\n",
            "Epoch 35: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 5.7718 - accuracy: 0.2517 - val_loss: 5.6806 - val_accuracy: 0.3108\n",
            "Epoch 36/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.7417 - accuracy: 0.2344\n",
            "Epoch 36: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 5.7244 - accuracy: 0.2517 - val_loss: 5.6244 - val_accuracy: 0.3108\n",
            "Epoch 37/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.6324 - accuracy: 0.2656\n",
            "Epoch 37: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.6759 - accuracy: 0.2517 - val_loss: 5.5701 - val_accuracy: 0.3108\n",
            "Epoch 38/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.6506 - accuracy: 0.2578\n",
            "Epoch 38: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.5885 - accuracy: 0.2517 - val_loss: 5.5185 - val_accuracy: 0.3108\n",
            "Epoch 39/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.5733 - accuracy: 0.2578\n",
            "Epoch 39: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 5.5509 - accuracy: 0.2517 - val_loss: 5.4673 - val_accuracy: 0.3108\n",
            "Epoch 40/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.5303 - accuracy: 0.2812\n",
            "Epoch 40: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.5072 - accuracy: 0.2517 - val_loss: 5.4145 - val_accuracy: 0.3108\n",
            "Epoch 41/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.4628 - accuracy: 0.2500\n",
            "Epoch 41: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.4341 - accuracy: 0.2517 - val_loss: 5.3617 - val_accuracy: 0.3108\n",
            "Epoch 42/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.4174 - accuracy: 0.2344\n",
            "Epoch 42: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 5.3964 - accuracy: 0.2517 - val_loss: 5.3086 - val_accuracy: 0.3108\n",
            "Epoch 43/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.3688 - accuracy: 0.2344\n",
            "Epoch 43: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 5.3387 - accuracy: 0.2517 - val_loss: 5.2557 - val_accuracy: 0.3108\n",
            "Epoch 44/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.3224 - accuracy: 0.2578\n",
            "Epoch 44: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 5.2898 - accuracy: 0.2517 - val_loss: 5.2027 - val_accuracy: 0.3108\n",
            "Epoch 45/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.2606 - accuracy: 0.2891\n",
            "Epoch 45: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.2468 - accuracy: 0.2517 - val_loss: 5.1521 - val_accuracy: 0.3108\n",
            "Epoch 46/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.2145 - accuracy: 0.2891\n",
            "Epoch 46: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 5.1853 - accuracy: 0.2517 - val_loss: 5.1026 - val_accuracy: 0.3108\n",
            "Epoch 47/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.1443 - accuracy: 0.2500\n",
            "Epoch 47: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 5.1308 - accuracy: 0.2517 - val_loss: 5.0547 - val_accuracy: 0.3108\n",
            "Epoch 48/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.0575 - accuracy: 0.2266\n",
            "Epoch 48: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 5.0886 - accuracy: 0.2517 - val_loss: 5.0061 - val_accuracy: 0.3108\n",
            "Epoch 49/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 5.0106 - accuracy: 0.1797\n",
            "Epoch 49: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 5.0430 - accuracy: 0.2517 - val_loss: 4.9592 - val_accuracy: 0.3108\n",
            "Epoch 50/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.9799 - accuracy: 0.2031\n",
            "Epoch 50: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.9932 - accuracy: 0.2517 - val_loss: 4.9138 - val_accuracy: 0.3108\n",
            "Epoch 51/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.9778 - accuracy: 0.2344\n",
            "Epoch 51: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.9519 - accuracy: 0.2517 - val_loss: 4.8678 - val_accuracy: 0.3108\n",
            "Epoch 52/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.9225 - accuracy: 0.2578\n",
            "Epoch 52: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.9280 - accuracy: 0.2517 - val_loss: 4.8196 - val_accuracy: 0.3108\n",
            "Epoch 53/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.8786 - accuracy: 0.2500\n",
            "Epoch 53: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.8626 - accuracy: 0.2517 - val_loss: 4.7725 - val_accuracy: 0.3108\n",
            "Epoch 54/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.8303 - accuracy: 0.2891\n",
            "Epoch 54: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.8174 - accuracy: 0.2517 - val_loss: 4.7271 - val_accuracy: 0.3108\n",
            "Epoch 55/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.8376 - accuracy: 0.2812\n",
            "Epoch 55: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.7684 - accuracy: 0.2517 - val_loss: 4.6824 - val_accuracy: 0.3108\n",
            "Epoch 56/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.6872 - accuracy: 0.2656\n",
            "Epoch 56: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.7143 - accuracy: 0.2517 - val_loss: 4.6374 - val_accuracy: 0.3108\n",
            "Epoch 57/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.6866 - accuracy: 0.2422\n",
            "Epoch 57: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.6667 - accuracy: 0.2517 - val_loss: 4.5953 - val_accuracy: 0.3108\n",
            "Epoch 58/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.6199 - accuracy: 0.2344\n",
            "Epoch 58: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.6319 - accuracy: 0.2517 - val_loss: 4.5548 - val_accuracy: 0.3108\n",
            "Epoch 59/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.5863 - accuracy: 0.1953\n",
            "Epoch 59: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.5888 - accuracy: 0.2517 - val_loss: 4.5150 - val_accuracy: 0.3108\n",
            "Epoch 60/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.6569 - accuracy: 0.2500\n",
            "Epoch 60: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 4.5534 - accuracy: 0.2517 - val_loss: 4.4736 - val_accuracy: 0.3108\n",
            "Epoch 61/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.5068 - accuracy: 0.2422\n",
            "Epoch 61: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.5015 - accuracy: 0.2517 - val_loss: 4.4298 - val_accuracy: 0.3108\n",
            "Epoch 62/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.4759 - accuracy: 0.2344\n",
            "Epoch 62: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.4747 - accuracy: 0.2517 - val_loss: 4.3874 - val_accuracy: 0.3108\n",
            "Epoch 63/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.4814 - accuracy: 0.2734\n",
            "Epoch 63: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.4360 - accuracy: 0.2517 - val_loss: 4.3460 - val_accuracy: 0.3108\n",
            "Epoch 64/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.4661 - accuracy: 0.2969\n",
            "Epoch 64: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.3931 - accuracy: 0.2517 - val_loss: 4.3048 - val_accuracy: 0.3108\n",
            "Epoch 65/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.3418 - accuracy: 0.2266\n",
            "Epoch 65: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 4.3470 - accuracy: 0.2517 - val_loss: 4.2654 - val_accuracy: 0.3108\n",
            "Epoch 66/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.3283 - accuracy: 0.2188\n",
            "Epoch 66: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 4.3063 - accuracy: 0.2517 - val_loss: 4.2288 - val_accuracy: 0.3108\n",
            "Epoch 67/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.2683 - accuracy: 0.2500\n",
            "Epoch 67: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 4.2646 - accuracy: 0.2517 - val_loss: 4.1928 - val_accuracy: 0.3108\n",
            "Epoch 68/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.2422 - accuracy: 0.2344\n",
            "Epoch 68: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.2359 - accuracy: 0.2517 - val_loss: 4.1551 - val_accuracy: 0.3108\n",
            "Epoch 69/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.2458 - accuracy: 0.2734\n",
            "Epoch 69: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.1913 - accuracy: 0.2517 - val_loss: 4.1186 - val_accuracy: 0.3108\n",
            "Epoch 70/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.1691 - accuracy: 0.2656\n",
            "Epoch 70: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.1657 - accuracy: 0.2517 - val_loss: 4.0824 - val_accuracy: 0.3108\n",
            "Epoch 71/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.0982 - accuracy: 0.2500\n",
            "Epoch 71: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 4.1174 - accuracy: 0.2517 - val_loss: 4.0446 - val_accuracy: 0.3108\n",
            "Epoch 72/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.0753 - accuracy: 0.2188\n",
            "Epoch 72: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 4.0908 - accuracy: 0.2517 - val_loss: 4.0072 - val_accuracy: 0.3108\n",
            "Epoch 73/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.0656 - accuracy: 0.2891\n",
            "Epoch 73: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 4.0471 - accuracy: 0.2517 - val_loss: 3.9718 - val_accuracy: 0.3108\n",
            "Epoch 74/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 4.0344 - accuracy: 0.2344\n",
            "Epoch 74: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 4.0123 - accuracy: 0.2517 - val_loss: 3.9371 - val_accuracy: 0.3108\n",
            "Epoch 75/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.9442 - accuracy: 0.2656\n",
            "Epoch 75: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 3.9802 - accuracy: 0.2517 - val_loss: 3.9036 - val_accuracy: 0.3108\n",
            "Epoch 76/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.9606 - accuracy: 0.2188\n",
            "Epoch 76: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 3.9361 - accuracy: 0.2517 - val_loss: 3.8706 - val_accuracy: 0.3108\n",
            "Epoch 77/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.9663 - accuracy: 0.2812\n",
            "Epoch 77: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 3.9184 - accuracy: 0.2517 - val_loss: 3.8395 - val_accuracy: 0.3108\n",
            "Epoch 78/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.8858 - accuracy: 0.2734\n",
            "Epoch 78: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.8644 - accuracy: 0.2517 - val_loss: 3.8100 - val_accuracy: 0.3108\n",
            "Epoch 79/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.8751 - accuracy: 0.2344\n",
            "Epoch 79: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 3.8313 - accuracy: 0.2517 - val_loss: 3.7804 - val_accuracy: 0.3108\n",
            "Epoch 80/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.7632 - accuracy: 0.2188\n",
            "Epoch 80: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 3.7965 - accuracy: 0.2517 - val_loss: 3.7488 - val_accuracy: 0.3108\n",
            "Epoch 81/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.7704 - accuracy: 0.2500\n",
            "Epoch 81: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.7610 - accuracy: 0.2517 - val_loss: 3.7147 - val_accuracy: 0.3108\n",
            "Epoch 82/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.7503 - accuracy: 0.2656\n",
            "Epoch 82: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.7377 - accuracy: 0.2517 - val_loss: 3.6796 - val_accuracy: 0.3108\n",
            "Epoch 83/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.7600 - accuracy: 0.2891\n",
            "Epoch 83: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.7142 - accuracy: 0.2517 - val_loss: 3.6479 - val_accuracy: 0.3108\n",
            "Epoch 84/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.6958 - accuracy: 0.2500\n",
            "Epoch 84: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.6719 - accuracy: 0.2517 - val_loss: 3.6192 - val_accuracy: 0.3108\n",
            "Epoch 85/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.6833 - accuracy: 0.3047\n",
            "Epoch 85: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.6493 - accuracy: 0.2517 - val_loss: 3.5899 - val_accuracy: 0.3108\n",
            "Epoch 86/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.6196 - accuracy: 0.3359\n",
            "Epoch 86: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.6146 - accuracy: 0.2517 - val_loss: 3.5571 - val_accuracy: 0.3108\n",
            "Epoch 87/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.5726 - accuracy: 0.2344\n",
            "Epoch 87: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.5929 - accuracy: 0.2517 - val_loss: 3.5248 - val_accuracy: 0.3108\n",
            "Epoch 88/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.5835 - accuracy: 0.2344\n",
            "Epoch 88: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.5619 - accuracy: 0.2517 - val_loss: 3.4957 - val_accuracy: 0.3108\n",
            "Epoch 89/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4827 - accuracy: 0.2500\n",
            "Epoch 89: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 3.5268 - accuracy: 0.2517 - val_loss: 3.4709 - val_accuracy: 0.3108\n",
            "Epoch 90/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.5020 - accuracy: 0.2422\n",
            "Epoch 90: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 3.4997 - accuracy: 0.2517 - val_loss: 3.4476 - val_accuracy: 0.3108\n",
            "Epoch 91/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4431 - accuracy: 0.2578\n",
            "Epoch 91: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.4570 - accuracy: 0.2517 - val_loss: 3.4229 - val_accuracy: 0.3108\n",
            "Epoch 92/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4616 - accuracy: 0.2734\n",
            "Epoch 92: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.4404 - accuracy: 0.2517 - val_loss: 3.3980 - val_accuracy: 0.3108\n",
            "Epoch 93/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.4255 - accuracy: 0.2656\n",
            "Epoch 93: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.4124 - accuracy: 0.2517 - val_loss: 3.3760 - val_accuracy: 0.3108\n",
            "Epoch 94/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.3963 - accuracy: 0.2500\n",
            "Epoch 94: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.3810 - accuracy: 0.2517 - val_loss: 3.3513 - val_accuracy: 0.3108\n",
            "Epoch 95/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.3562 - accuracy: 0.2266\n",
            "Epoch 95: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 3.3576 - accuracy: 0.2517 - val_loss: 3.3248 - val_accuracy: 0.3108\n",
            "Epoch 96/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.3577 - accuracy: 0.2422\n",
            "Epoch 96: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.3398 - accuracy: 0.2517 - val_loss: 3.2988 - val_accuracy: 0.3108\n",
            "Epoch 97/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.2956 - accuracy: 0.1953\n",
            "Epoch 97: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.2992 - accuracy: 0.2517 - val_loss: 3.2728 - val_accuracy: 0.3108\n",
            "Epoch 98/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.2514 - accuracy: 0.2422\n",
            "Epoch 98: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.2675 - accuracy: 0.2517 - val_loss: 3.2489 - val_accuracy: 0.3108\n",
            "Epoch 99/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1972 - accuracy: 0.2500\n",
            "Epoch 99: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.2488 - accuracy: 0.2517 - val_loss: 3.2243 - val_accuracy: 0.3108\n",
            "Epoch 100/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.2067 - accuracy: 0.2812\n",
            "Epoch 100: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.2193 - accuracy: 0.2517 - val_loss: 3.1982 - val_accuracy: 0.3108\n",
            "Epoch 101/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.2307 - accuracy: 0.2891\n",
            "Epoch 101: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 3.2012 - accuracy: 0.2517 - val_loss: 3.1736 - val_accuracy: 0.3108\n",
            "Epoch 102/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1789 - accuracy: 0.2656\n",
            "Epoch 102: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.1741 - accuracy: 0.2517 - val_loss: 3.1482 - val_accuracy: 0.3108\n",
            "Epoch 103/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1444 - accuracy: 0.2422\n",
            "Epoch 103: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 3.1393 - accuracy: 0.2517 - val_loss: 3.1239 - val_accuracy: 0.3108\n",
            "Epoch 104/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1163 - accuracy: 0.2031\n",
            "Epoch 104: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.1211 - accuracy: 0.2517 - val_loss: 3.1016 - val_accuracy: 0.3108\n",
            "Epoch 105/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.1357 - accuracy: 0.2812\n",
            "Epoch 105: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.0987 - accuracy: 0.2517 - val_loss: 3.0816 - val_accuracy: 0.3108\n",
            "Epoch 106/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.0351 - accuracy: 0.2266\n",
            "Epoch 106: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 3.0695 - accuracy: 0.2517 - val_loss: 3.0577 - val_accuracy: 0.3108\n",
            "Epoch 107/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.0906 - accuracy: 0.2812\n",
            "Epoch 107: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.0498 - accuracy: 0.2517 - val_loss: 3.0327 - val_accuracy: 0.3108\n",
            "Epoch 108/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 3.0537 - accuracy: 0.2266\n",
            "Epoch 108: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.0286 - accuracy: 0.2517 - val_loss: 3.0079 - val_accuracy: 0.3108\n",
            "Epoch 109/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9902 - accuracy: 0.2188\n",
            "Epoch 109: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.0044 - accuracy: 0.2517 - val_loss: 2.9821 - val_accuracy: 0.3108\n",
            "Epoch 110/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9929 - accuracy: 0.2578\n",
            "Epoch 110: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.9773 - accuracy: 0.2517 - val_loss: 2.9596 - val_accuracy: 0.3108\n",
            "Epoch 111/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9893 - accuracy: 0.2656\n",
            "Epoch 111: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.9564 - accuracy: 0.2517 - val_loss: 2.9402 - val_accuracy: 0.3108\n",
            "Epoch 112/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9379 - accuracy: 0.2109\n",
            "Epoch 112: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.9348 - accuracy: 0.2517 - val_loss: 2.9258 - val_accuracy: 0.3108\n",
            "Epoch 113/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9951 - accuracy: 0.3125\n",
            "Epoch 113: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.9130 - accuracy: 0.2517 - val_loss: 2.9107 - val_accuracy: 0.3108\n",
            "Epoch 114/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9591 - accuracy: 0.3047\n",
            "Epoch 114: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.8878 - accuracy: 0.2517 - val_loss: 2.8871 - val_accuracy: 0.3108\n",
            "Epoch 115/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.9142 - accuracy: 0.2500\n",
            "Epoch 115: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 2.8710 - accuracy: 0.2517 - val_loss: 2.8598 - val_accuracy: 0.3108\n",
            "Epoch 116/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.8016 - accuracy: 0.2188\n",
            "Epoch 116: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.8448 - accuracy: 0.2517 - val_loss: 2.8340 - val_accuracy: 0.3108\n",
            "Epoch 117/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.8414 - accuracy: 0.2344\n",
            "Epoch 117: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.8310 - accuracy: 0.2517 - val_loss: 2.8119 - val_accuracy: 0.3108\n",
            "Epoch 118/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7479 - accuracy: 0.2500\n",
            "Epoch 118: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.8071 - accuracy: 0.2517 - val_loss: 2.7923 - val_accuracy: 0.3108\n",
            "Epoch 119/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7307 - accuracy: 0.2656\n",
            "Epoch 119: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.7907 - accuracy: 0.2517 - val_loss: 2.7745 - val_accuracy: 0.3108\n",
            "Epoch 120/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7133 - accuracy: 0.2188\n",
            "Epoch 120: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.7613 - accuracy: 0.2517 - val_loss: 2.7578 - val_accuracy: 0.3108\n",
            "Epoch 121/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7770 - accuracy: 0.2578\n",
            "Epoch 121: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.7450 - accuracy: 0.2517 - val_loss: 2.7477 - val_accuracy: 0.3108\n",
            "Epoch 122/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7391 - accuracy: 0.2734\n",
            "Epoch 122: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.7290 - accuracy: 0.2517 - val_loss: 2.7385 - val_accuracy: 0.3108\n",
            "Epoch 123/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.7102 - accuracy: 0.2266\n",
            "Epoch 123: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.7148 - accuracy: 0.2517 - val_loss: 2.7222 - val_accuracy: 0.3108\n",
            "Epoch 124/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6592 - accuracy: 0.2422\n",
            "Epoch 124: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.6832 - accuracy: 0.2517 - val_loss: 2.7015 - val_accuracy: 0.3108\n",
            "Epoch 125/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6215 - accuracy: 0.2500\n",
            "Epoch 125: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.6553 - accuracy: 0.2517 - val_loss: 2.6796 - val_accuracy: 0.3108\n",
            "Epoch 126/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6557 - accuracy: 0.2734\n",
            "Epoch 126: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.6451 - accuracy: 0.2517 - val_loss: 2.6575 - val_accuracy: 0.3108\n",
            "Epoch 127/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6248 - accuracy: 0.2344\n",
            "Epoch 127: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.6311 - accuracy: 0.2517 - val_loss: 2.6359 - val_accuracy: 0.3108\n",
            "Epoch 128/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6242 - accuracy: 0.2812\n",
            "Epoch 128: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.6129 - accuracy: 0.2517 - val_loss: 2.6183 - val_accuracy: 0.3108\n",
            "Epoch 129/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6507 - accuracy: 0.2656\n",
            "Epoch 129: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.5917 - accuracy: 0.2517 - val_loss: 2.6011 - val_accuracy: 0.3108\n",
            "Epoch 130/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.6217 - accuracy: 0.2734\n",
            "Epoch 130: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 2.5736 - accuracy: 0.2517 - val_loss: 2.5812 - val_accuracy: 0.3108\n",
            "Epoch 131/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.5164 - accuracy: 0.2500\n",
            "Epoch 131: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 2.5561 - accuracy: 0.2517 - val_loss: 2.5584 - val_accuracy: 0.3108\n",
            "Epoch 132/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.5990 - accuracy: 0.3047\n",
            "Epoch 132: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 2.5521 - accuracy: 0.2517 - val_loss: 2.5376 - val_accuracy: 0.3108\n",
            "Epoch 133/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.5134 - accuracy: 0.2031\n",
            "Epoch 133: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.5239 - accuracy: 0.2517 - val_loss: 2.5175 - val_accuracy: 0.3108\n",
            "Epoch 134/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4866 - accuracy: 0.2266\n",
            "Epoch 134: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.5048 - accuracy: 0.2517 - val_loss: 2.5050 - val_accuracy: 0.3108\n",
            "Epoch 135/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.5337 - accuracy: 0.2656\n",
            "Epoch 135: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.4774 - accuracy: 0.2517 - val_loss: 2.4965 - val_accuracy: 0.3108\n",
            "Epoch 136/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.5030 - accuracy: 0.2500\n",
            "Epoch 136: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.4814 - accuracy: 0.2517 - val_loss: 2.4773 - val_accuracy: 0.3108\n",
            "Epoch 137/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4464 - accuracy: 0.2812\n",
            "Epoch 137: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.4638 - accuracy: 0.2517 - val_loss: 2.4550 - val_accuracy: 0.3108\n",
            "Epoch 138/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4023 - accuracy: 0.2422\n",
            "Epoch 138: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.4382 - accuracy: 0.2517 - val_loss: 2.4382 - val_accuracy: 0.3108\n",
            "Epoch 139/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3892 - accuracy: 0.2266\n",
            "Epoch 139: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.4291 - accuracy: 0.2517 - val_loss: 2.4227 - val_accuracy: 0.3108\n",
            "Epoch 140/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4460 - accuracy: 0.2500\n",
            "Epoch 140: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.4139 - accuracy: 0.2517 - val_loss: 2.4109 - val_accuracy: 0.3108\n",
            "Epoch 141/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3581 - accuracy: 0.2188\n",
            "Epoch 141: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3879 - accuracy: 0.2517 - val_loss: 2.4020 - val_accuracy: 0.3108\n",
            "Epoch 142/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.4078 - accuracy: 0.2891\n",
            "Epoch 142: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3862 - accuracy: 0.2517 - val_loss: 2.3888 - val_accuracy: 0.3108\n",
            "Epoch 143/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3660 - accuracy: 0.2969\n",
            "Epoch 143: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3563 - accuracy: 0.2517 - val_loss: 2.3737 - val_accuracy: 0.3108\n",
            "Epoch 144/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3609 - accuracy: 0.2500\n",
            "Epoch 144: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3574 - accuracy: 0.2517 - val_loss: 2.3536 - val_accuracy: 0.3108\n",
            "Epoch 145/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3095 - accuracy: 0.2188\n",
            "Epoch 145: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3363 - accuracy: 0.2517 - val_loss: 2.3356 - val_accuracy: 0.3108\n",
            "Epoch 146/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.3845 - accuracy: 0.2734\n",
            "Epoch 146: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3207 - accuracy: 0.2517 - val_loss: 2.3191 - val_accuracy: 0.3108\n",
            "Epoch 147/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2837 - accuracy: 0.2734\n",
            "Epoch 147: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 2.3057 - accuracy: 0.2517 - val_loss: 2.3073 - val_accuracy: 0.3108\n",
            "Epoch 148/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2334 - accuracy: 0.2188\n",
            "Epoch 148: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.2764 - accuracy: 0.2517 - val_loss: 2.3000 - val_accuracy: 0.3108\n",
            "Epoch 149/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1967 - accuracy: 0.1797\n",
            "Epoch 149: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.2670 - accuracy: 0.2517 - val_loss: 2.2924 - val_accuracy: 0.3108\n",
            "Epoch 150/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2538 - accuracy: 0.2891\n",
            "Epoch 150: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.2622 - accuracy: 0.2517 - val_loss: 2.2810 - val_accuracy: 0.3108\n",
            "Epoch 151/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2247 - accuracy: 0.2344\n",
            "Epoch 151: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.2349 - accuracy: 0.2517 - val_loss: 2.2681 - val_accuracy: 0.3108\n",
            "Epoch 152/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2155 - accuracy: 0.2656\n",
            "Epoch 152: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.2332 - accuracy: 0.2517 - val_loss: 2.2591 - val_accuracy: 0.3108\n",
            "Epoch 153/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1924 - accuracy: 0.2422\n",
            "Epoch 153: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.2155 - accuracy: 0.2517 - val_loss: 2.2473 - val_accuracy: 0.3108\n",
            "Epoch 154/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2282 - accuracy: 0.2969\n",
            "Epoch 154: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.2004 - accuracy: 0.2517 - val_loss: 2.2366 - val_accuracy: 0.3108\n",
            "Epoch 155/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1770 - accuracy: 0.2266\n",
            "Epoch 155: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.2025 - accuracy: 0.2517 - val_loss: 2.2198 - val_accuracy: 0.3108\n",
            "Epoch 156/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.2100 - accuracy: 0.2812\n",
            "Epoch 156: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.1878 - accuracy: 0.2517 - val_loss: 2.2021 - val_accuracy: 0.3108\n",
            "Epoch 157/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1727 - accuracy: 0.2578\n",
            "Epoch 157: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.1746 - accuracy: 0.2517 - val_loss: 2.1923 - val_accuracy: 0.3108\n",
            "Epoch 158/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1274 - accuracy: 0.2188\n",
            "Epoch 158: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.1562 - accuracy: 0.2517 - val_loss: 2.1843 - val_accuracy: 0.3108\n",
            "Epoch 159/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1503 - accuracy: 0.2891\n",
            "Epoch 159: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.1400 - accuracy: 0.2517 - val_loss: 2.1750 - val_accuracy: 0.3108\n",
            "Epoch 160/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0967 - accuracy: 0.2891\n",
            "Epoch 160: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.1344 - accuracy: 0.2517 - val_loss: 2.1622 - val_accuracy: 0.3108\n",
            "Epoch 161/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1148 - accuracy: 0.2891\n",
            "Epoch 161: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.1130 - accuracy: 0.2517 - val_loss: 2.1528 - val_accuracy: 0.3108\n",
            "Epoch 162/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1204 - accuracy: 0.2266\n",
            "Epoch 162: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.1015 - accuracy: 0.2517 - val_loss: 2.1488 - val_accuracy: 0.3108\n",
            "Epoch 163/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1149 - accuracy: 0.2812\n",
            "Epoch 163: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.0962 - accuracy: 0.2517 - val_loss: 2.1455 - val_accuracy: 0.3108\n",
            "Epoch 164/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0737 - accuracy: 0.2500\n",
            "Epoch 164: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 2.0767 - accuracy: 0.2517 - val_loss: 2.1372 - val_accuracy: 0.3108\n",
            "Epoch 165/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0773 - accuracy: 0.2500\n",
            "Epoch 165: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0638 - accuracy: 0.2517 - val_loss: 2.1293 - val_accuracy: 0.3108\n",
            "Epoch 166/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0052 - accuracy: 0.2344\n",
            "Epoch 166: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0552 - accuracy: 0.2517 - val_loss: 2.1254 - val_accuracy: 0.3108\n",
            "Epoch 167/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0503 - accuracy: 0.2734\n",
            "Epoch 167: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0529 - accuracy: 0.2517 - val_loss: 2.1178 - val_accuracy: 0.3108\n",
            "Epoch 168/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0417 - accuracy: 0.2422\n",
            "Epoch 168: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0304 - accuracy: 0.2517 - val_loss: 2.1008 - val_accuracy: 0.3108\n",
            "Epoch 169/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.1064 - accuracy: 0.2422\n",
            "Epoch 169: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0213 - accuracy: 0.2517 - val_loss: 2.0914 - val_accuracy: 0.3108\n",
            "Epoch 170/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0257 - accuracy: 0.2109\n",
            "Epoch 170: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.0203 - accuracy: 0.2517 - val_loss: 2.0868 - val_accuracy: 0.3108\n",
            "Epoch 171/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 2.0023 - accuracy: 0.2422\n",
            "Epoch 171: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.9988 - accuracy: 0.2517 - val_loss: 2.0811 - val_accuracy: 0.3108\n",
            "Epoch 172/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9834 - accuracy: 0.2109\n",
            "Epoch 172: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.9968 - accuracy: 0.2517 - val_loss: 2.0610 - val_accuracy: 0.3108\n",
            "Epoch 173/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9315 - accuracy: 0.2422\n",
            "Epoch 173: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9798 - accuracy: 0.2517 - val_loss: 2.0401 - val_accuracy: 0.3108\n",
            "Epoch 174/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9953 - accuracy: 0.2266\n",
            "Epoch 174: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9744 - accuracy: 0.2517 - val_loss: 2.0231 - val_accuracy: 0.3108\n",
            "Epoch 175/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9335 - accuracy: 0.2422\n",
            "Epoch 175: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9636 - accuracy: 0.2517 - val_loss: 2.0054 - val_accuracy: 0.3108\n",
            "Epoch 176/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9469 - accuracy: 0.2500\n",
            "Epoch 176: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9568 - accuracy: 0.2517 - val_loss: 1.9947 - val_accuracy: 0.3108\n",
            "Epoch 177/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9803 - accuracy: 0.2656\n",
            "Epoch 177: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.9437 - accuracy: 0.2517 - val_loss: 1.9888 - val_accuracy: 0.3108\n",
            "Epoch 178/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9955 - accuracy: 0.2969\n",
            "Epoch 178: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.9321 - accuracy: 0.2517 - val_loss: 1.9881 - val_accuracy: 0.3108\n",
            "Epoch 179/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9380 - accuracy: 0.2969\n",
            "Epoch 179: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9301 - accuracy: 0.2517 - val_loss: 1.9895 - val_accuracy: 0.3108\n",
            "Epoch 180/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9137 - accuracy: 0.2266\n",
            "Epoch 180: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.9184 - accuracy: 0.2517 - val_loss: 1.9908 - val_accuracy: 0.3108\n",
            "Epoch 181/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8499 - accuracy: 0.1953\n",
            "Epoch 181: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8999 - accuracy: 0.2517 - val_loss: 1.9958 - val_accuracy: 0.3108\n",
            "Epoch 182/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8934 - accuracy: 0.2734\n",
            "Epoch 182: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.8851 - accuracy: 0.2517 - val_loss: 2.0027 - val_accuracy: 0.3108\n",
            "Epoch 183/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9087 - accuracy: 0.2266\n",
            "Epoch 183: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8842 - accuracy: 0.2517 - val_loss: 1.9988 - val_accuracy: 0.3108\n",
            "Epoch 184/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.9135 - accuracy: 0.2266\n",
            "Epoch 184: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.8721 - accuracy: 0.2517 - val_loss: 1.9855 - val_accuracy: 0.3108\n",
            "Epoch 185/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8930 - accuracy: 0.2891\n",
            "Epoch 185: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.8595 - accuracy: 0.2517 - val_loss: 1.9640 - val_accuracy: 0.3108\n",
            "Epoch 186/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8713 - accuracy: 0.2891\n",
            "Epoch 186: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.8562 - accuracy: 0.2517 - val_loss: 1.9418 - val_accuracy: 0.3108\n",
            "Epoch 187/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8636 - accuracy: 0.2656\n",
            "Epoch 187: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8441 - accuracy: 0.2517 - val_loss: 1.9242 - val_accuracy: 0.3108\n",
            "Epoch 188/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8348 - accuracy: 0.2344\n",
            "Epoch 188: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8400 - accuracy: 0.2517 - val_loss: 1.9099 - val_accuracy: 0.3108\n",
            "Epoch 189/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7902 - accuracy: 0.2109\n",
            "Epoch 189: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.8278 - accuracy: 0.2517 - val_loss: 1.8986 - val_accuracy: 0.3108\n",
            "Epoch 190/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8306 - accuracy: 0.2812\n",
            "Epoch 190: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.8179 - accuracy: 0.2517 - val_loss: 1.8952 - val_accuracy: 0.3108\n",
            "Epoch 191/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8331 - accuracy: 0.2344\n",
            "Epoch 191: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.8103 - accuracy: 0.2517 - val_loss: 1.8955 - val_accuracy: 0.3108\n",
            "Epoch 192/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8422 - accuracy: 0.2344\n",
            "Epoch 192: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.7987 - accuracy: 0.2517 - val_loss: 1.8975 - val_accuracy: 0.3108\n",
            "Epoch 193/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7550 - accuracy: 0.2578\n",
            "Epoch 193: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.7885 - accuracy: 0.2517 - val_loss: 1.9006 - val_accuracy: 0.3108\n",
            "Epoch 194/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7787 - accuracy: 0.2188\n",
            "Epoch 194: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.7872 - accuracy: 0.2517 - val_loss: 1.8989 - val_accuracy: 0.3108\n",
            "Epoch 195/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8002 - accuracy: 0.2422\n",
            "Epoch 195: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.7771 - accuracy: 0.2517 - val_loss: 1.8928 - val_accuracy: 0.3108\n",
            "Epoch 196/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7722 - accuracy: 0.2344\n",
            "Epoch 196: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.7826 - accuracy: 0.2517 - val_loss: 1.8782 - val_accuracy: 0.3108\n",
            "Epoch 197/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8130 - accuracy: 0.2031\n",
            "Epoch 197: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.7608 - accuracy: 0.2517 - val_loss: 1.8635 - val_accuracy: 0.3108\n",
            "Epoch 198/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7518 - accuracy: 0.2656\n",
            "Epoch 198: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.7646 - accuracy: 0.2517 - val_loss: 1.8573 - val_accuracy: 0.3108\n",
            "Epoch 199/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.8118 - accuracy: 0.2578\n",
            "Epoch 199: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.7425 - accuracy: 0.2517 - val_loss: 1.8547 - val_accuracy: 0.3108\n",
            "Epoch 200/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7545 - accuracy: 0.2422\n",
            "Epoch 200: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.7401 - accuracy: 0.2517 - val_loss: 1.8451 - val_accuracy: 0.3108\n",
            "Epoch 201/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7159 - accuracy: 0.2344\n",
            "Epoch 201: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.7327 - accuracy: 0.2517 - val_loss: 1.8310 - val_accuracy: 0.3108\n",
            "Epoch 202/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7288 - accuracy: 0.2266\n",
            "Epoch 202: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.7210 - accuracy: 0.2517 - val_loss: 1.8233 - val_accuracy: 0.3108\n",
            "Epoch 203/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6864 - accuracy: 0.2656\n",
            "Epoch 203: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.7223 - accuracy: 0.2517 - val_loss: 1.8257 - val_accuracy: 0.3108\n",
            "Epoch 204/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7433 - accuracy: 0.2891\n",
            "Epoch 204: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.7110 - accuracy: 0.2517 - val_loss: 1.8242 - val_accuracy: 0.3108\n",
            "Epoch 205/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7267 - accuracy: 0.2500\n",
            "Epoch 205: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.7120 - accuracy: 0.2517 - val_loss: 1.8157 - val_accuracy: 0.3108\n",
            "Epoch 206/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.7344 - accuracy: 0.2734\n",
            "Epoch 206: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.7031 - accuracy: 0.2517 - val_loss: 1.8070 - val_accuracy: 0.3108\n",
            "Epoch 207/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6451 - accuracy: 0.2031\n",
            "Epoch 207: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.6978 - accuracy: 0.2517 - val_loss: 1.7921 - val_accuracy: 0.3108\n",
            "Epoch 208/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6858 - accuracy: 0.2422\n",
            "Epoch 208: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6759 - accuracy: 0.2517 - val_loss: 1.7877 - val_accuracy: 0.3108\n",
            "Epoch 209/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6671 - accuracy: 0.2344\n",
            "Epoch 209: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6675 - accuracy: 0.2517 - val_loss: 1.7868 - val_accuracy: 0.3108\n",
            "Epoch 210/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6640 - accuracy: 0.2266\n",
            "Epoch 210: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.6684 - accuracy: 0.2517 - val_loss: 1.7861 - val_accuracy: 0.3108\n",
            "Epoch 211/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6995 - accuracy: 0.2578\n",
            "Epoch 211: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.6580 - accuracy: 0.2517 - val_loss: 1.7875 - val_accuracy: 0.3108\n",
            "Epoch 212/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6437 - accuracy: 0.2734\n",
            "Epoch 212: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.6581 - accuracy: 0.2517 - val_loss: 1.7785 - val_accuracy: 0.3108\n",
            "Epoch 213/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6349 - accuracy: 0.2734\n",
            "Epoch 213: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6411 - accuracy: 0.2517 - val_loss: 1.7614 - val_accuracy: 0.3108\n",
            "Epoch 214/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6276 - accuracy: 0.2266\n",
            "Epoch 214: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6432 - accuracy: 0.2517 - val_loss: 1.7485 - val_accuracy: 0.3108\n",
            "Epoch 215/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6683 - accuracy: 0.2578\n",
            "Epoch 215: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.6379 - accuracy: 0.2517 - val_loss: 1.7385 - val_accuracy: 0.3108\n",
            "Epoch 216/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6058 - accuracy: 0.2344\n",
            "Epoch 216: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6340 - accuracy: 0.2517 - val_loss: 1.7284 - val_accuracy: 0.3108\n",
            "Epoch 217/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6363 - accuracy: 0.2812\n",
            "Epoch 217: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6258 - accuracy: 0.2517 - val_loss: 1.7204 - val_accuracy: 0.3108\n",
            "Epoch 218/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6038 - accuracy: 0.2422\n",
            "Epoch 218: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6093 - accuracy: 0.2517 - val_loss: 1.7138 - val_accuracy: 0.3108\n",
            "Epoch 219/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6058 - accuracy: 0.2578\n",
            "Epoch 219: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.6119 - accuracy: 0.2517 - val_loss: 1.7070 - val_accuracy: 0.3108\n",
            "Epoch 220/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6540 - accuracy: 0.2812\n",
            "Epoch 220: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.6123 - accuracy: 0.2517 - val_loss: 1.6987 - val_accuracy: 0.3108\n",
            "Epoch 221/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6099 - accuracy: 0.2422\n",
            "Epoch 221: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.5948 - accuracy: 0.2517 - val_loss: 1.6917 - val_accuracy: 0.3108\n",
            "Epoch 222/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5995 - accuracy: 0.2266\n",
            "Epoch 222: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.5941 - accuracy: 0.2517 - val_loss: 1.6863 - val_accuracy: 0.3108\n",
            "Epoch 223/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6218 - accuracy: 0.2656\n",
            "Epoch 223: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.5848 - accuracy: 0.2517 - val_loss: 1.6848 - val_accuracy: 0.3108\n",
            "Epoch 224/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.6330 - accuracy: 0.3125\n",
            "Epoch 224: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.5831 - accuracy: 0.2517 - val_loss: 1.6890 - val_accuracy: 0.3108\n",
            "Epoch 225/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5886 - accuracy: 0.2891\n",
            "Epoch 225: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.5746 - accuracy: 0.2517 - val_loss: 1.6937 - val_accuracy: 0.3108\n",
            "Epoch 226/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5658 - accuracy: 0.1797\n",
            "Epoch 226: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.5816 - accuracy: 0.2517 - val_loss: 1.6922 - val_accuracy: 0.3108\n",
            "Epoch 227/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5533 - accuracy: 0.3125\n",
            "Epoch 227: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.5748 - accuracy: 0.2517 - val_loss: 1.6848 - val_accuracy: 0.3108\n",
            "Epoch 228/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5751 - accuracy: 0.2891\n",
            "Epoch 228: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.5617 - accuracy: 0.2517 - val_loss: 1.6656 - val_accuracy: 0.3108\n",
            "Epoch 229/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5560 - accuracy: 0.2969\n",
            "Epoch 229: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.5574 - accuracy: 0.2517 - val_loss: 1.6457 - val_accuracy: 0.3108\n",
            "Epoch 230/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5957 - accuracy: 0.2734\n",
            "Epoch 230: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.5538 - accuracy: 0.2517 - val_loss: 1.6347 - val_accuracy: 0.3108\n",
            "Epoch 231/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5393 - accuracy: 0.2109\n",
            "Epoch 231: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.5474 - accuracy: 0.2517 - val_loss: 1.6319 - val_accuracy: 0.3108\n",
            "Epoch 232/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5611 - accuracy: 0.2656\n",
            "Epoch 232: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.5393 - accuracy: 0.2517 - val_loss: 1.6308 - val_accuracy: 0.3108\n",
            "Epoch 233/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5325 - accuracy: 0.2578\n",
            "Epoch 233: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5308 - accuracy: 0.2517 - val_loss: 1.6287 - val_accuracy: 0.3108\n",
            "Epoch 234/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5163 - accuracy: 0.2500\n",
            "Epoch 234: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5343 - accuracy: 0.2517 - val_loss: 1.6263 - val_accuracy: 0.3108\n",
            "Epoch 235/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5433 - accuracy: 0.2812\n",
            "Epoch 235: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.5187 - accuracy: 0.2517 - val_loss: 1.6250 - val_accuracy: 0.3108\n",
            "Epoch 236/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5249 - accuracy: 0.2188\n",
            "Epoch 236: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5114 - accuracy: 0.2517 - val_loss: 1.6239 - val_accuracy: 0.3108\n",
            "Epoch 237/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4801 - accuracy: 0.2422\n",
            "Epoch 237: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5093 - accuracy: 0.2517 - val_loss: 1.6226 - val_accuracy: 0.3108\n",
            "Epoch 238/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4806 - accuracy: 0.2422\n",
            "Epoch 238: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5069 - accuracy: 0.2517 - val_loss: 1.6195 - val_accuracy: 0.3108\n",
            "Epoch 239/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5206 - accuracy: 0.2734\n",
            "Epoch 239: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.5020 - accuracy: 0.2517 - val_loss: 1.6135 - val_accuracy: 0.3108\n",
            "Epoch 240/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.5481 - accuracy: 0.2266\n",
            "Epoch 240: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4999 - accuracy: 0.2517 - val_loss: 1.6049 - val_accuracy: 0.3108\n",
            "Epoch 241/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4915 - accuracy: 0.2500\n",
            "Epoch 241: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4945 - accuracy: 0.2517 - val_loss: 1.5957 - val_accuracy: 0.3108\n",
            "Epoch 242/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4644 - accuracy: 0.2578\n",
            "Epoch 242: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4934 - accuracy: 0.2517 - val_loss: 1.5872 - val_accuracy: 0.3108\n",
            "Epoch 243/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4378 - accuracy: 0.1719\n",
            "Epoch 243: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4888 - accuracy: 0.2517 - val_loss: 1.5828 - val_accuracy: 0.3108\n",
            "Epoch 244/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4710 - accuracy: 0.2969\n",
            "Epoch 244: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4891 - accuracy: 0.2517 - val_loss: 1.5824 - val_accuracy: 0.3108\n",
            "Epoch 245/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4143 - accuracy: 0.2891\n",
            "Epoch 245: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4672 - accuracy: 0.2517 - val_loss: 1.5856 - val_accuracy: 0.3108\n",
            "Epoch 246/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4654 - accuracy: 0.2812\n",
            "Epoch 246: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.4702 - accuracy: 0.2517 - val_loss: 1.5891 - val_accuracy: 0.3108\n",
            "Epoch 247/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4016 - accuracy: 0.2031\n",
            "Epoch 247: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4674 - accuracy: 0.2517 - val_loss: 1.5938 - val_accuracy: 0.3108\n",
            "Epoch 248/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4769 - accuracy: 0.2266\n",
            "Epoch 248: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4675 - accuracy: 0.2517 - val_loss: 1.5981 - val_accuracy: 0.3108\n",
            "Epoch 249/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4307 - accuracy: 0.2266\n",
            "Epoch 249: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4616 - accuracy: 0.2517 - val_loss: 1.5960 - val_accuracy: 0.3108\n",
            "Epoch 250/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4601 - accuracy: 0.2891\n",
            "Epoch 250: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4548 - accuracy: 0.2517 - val_loss: 1.5904 - val_accuracy: 0.3108\n",
            "Epoch 251/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4851 - accuracy: 0.2578\n",
            "Epoch 251: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.4545 - accuracy: 0.2517 - val_loss: 1.5814 - val_accuracy: 0.3108\n",
            "Epoch 252/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4128 - accuracy: 0.2109\n",
            "Epoch 252: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4459 - accuracy: 0.2517 - val_loss: 1.5732 - val_accuracy: 0.3108\n",
            "Epoch 253/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4434 - accuracy: 0.2109\n",
            "Epoch 253: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.4401 - accuracy: 0.2517 - val_loss: 1.5698 - val_accuracy: 0.3108\n",
            "Epoch 254/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4010 - accuracy: 0.2422\n",
            "Epoch 254: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.4416 - accuracy: 0.2517 - val_loss: 1.5661 - val_accuracy: 0.3108\n",
            "Epoch 255/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4673 - accuracy: 0.2344\n",
            "Epoch 255: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4306 - accuracy: 0.2517 - val_loss: 1.5599 - val_accuracy: 0.3108\n",
            "Epoch 256/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4301 - accuracy: 0.2656\n",
            "Epoch 256: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4282 - accuracy: 0.2517 - val_loss: 1.5494 - val_accuracy: 0.3108\n",
            "Epoch 257/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4097 - accuracy: 0.2266\n",
            "Epoch 257: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4278 - accuracy: 0.2517 - val_loss: 1.5350 - val_accuracy: 0.3108\n",
            "Epoch 258/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4057 - accuracy: 0.2422\n",
            "Epoch 258: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4151 - accuracy: 0.2517 - val_loss: 1.5292 - val_accuracy: 0.3108\n",
            "Epoch 259/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4276 - accuracy: 0.3047\n",
            "Epoch 259: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4190 - accuracy: 0.2517 - val_loss: 1.5294 - val_accuracy: 0.3108\n",
            "Epoch 260/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3972 - accuracy: 0.2109\n",
            "Epoch 260: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4125 - accuracy: 0.2517 - val_loss: 1.5308 - val_accuracy: 0.3108\n",
            "Epoch 261/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4291 - accuracy: 0.2734\n",
            "Epoch 261: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4186 - accuracy: 0.2517 - val_loss: 1.5345 - val_accuracy: 0.3108\n",
            "Epoch 262/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4312 - accuracy: 0.2266\n",
            "Epoch 262: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4080 - accuracy: 0.2517 - val_loss: 1.5327 - val_accuracy: 0.3108\n",
            "Epoch 263/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4685 - accuracy: 0.2344\n",
            "Epoch 263: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.4074 - accuracy: 0.2517 - val_loss: 1.5289 - val_accuracy: 0.3108\n",
            "Epoch 264/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4086 - accuracy: 0.2734\n",
            "Epoch 264: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.4039 - accuracy: 0.2517 - val_loss: 1.5253 - val_accuracy: 0.3108\n",
            "Epoch 265/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4032 - accuracy: 0.2812\n",
            "Epoch 265: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3977 - accuracy: 0.2517 - val_loss: 1.5151 - val_accuracy: 0.3108\n",
            "Epoch 266/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4265 - accuracy: 0.2500\n",
            "Epoch 266: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3954 - accuracy: 0.2517 - val_loss: 1.4994 - val_accuracy: 0.3108\n",
            "Epoch 267/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3882 - accuracy: 0.2500\n",
            "Epoch 267: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3931 - accuracy: 0.2517 - val_loss: 1.4874 - val_accuracy: 0.3108\n",
            "Epoch 268/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3723 - accuracy: 0.2578\n",
            "Epoch 268: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3888 - accuracy: 0.2517 - val_loss: 1.4865 - val_accuracy: 0.3108\n",
            "Epoch 269/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4167 - accuracy: 0.2344\n",
            "Epoch 269: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3818 - accuracy: 0.2517 - val_loss: 1.4889 - val_accuracy: 0.3108\n",
            "Epoch 270/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4014 - accuracy: 0.2969\n",
            "Epoch 270: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3795 - accuracy: 0.2517 - val_loss: 1.4946 - val_accuracy: 0.3108\n",
            "Epoch 271/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.4031 - accuracy: 0.2500\n",
            "Epoch 271: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3773 - accuracy: 0.2517 - val_loss: 1.4999 - val_accuracy: 0.3108\n",
            "Epoch 272/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3696 - accuracy: 0.2891\n",
            "Epoch 272: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3792 - accuracy: 0.2517 - val_loss: 1.5026 - val_accuracy: 0.3108\n",
            "Epoch 273/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3404 - accuracy: 0.2578\n",
            "Epoch 273: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3775 - accuracy: 0.2517 - val_loss: 1.4966 - val_accuracy: 0.3108\n",
            "Epoch 274/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3461 - accuracy: 0.2031\n",
            "Epoch 274: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3679 - accuracy: 0.2517 - val_loss: 1.4879 - val_accuracy: 0.3108\n",
            "Epoch 275/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3521 - accuracy: 0.2500\n",
            "Epoch 275: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3668 - accuracy: 0.2517 - val_loss: 1.4817 - val_accuracy: 0.3108\n",
            "Epoch 276/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3146 - accuracy: 0.2188\n",
            "Epoch 276: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3647 - accuracy: 0.2517 - val_loss: 1.4719 - val_accuracy: 0.3108\n",
            "Epoch 277/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3594 - accuracy: 0.2344\n",
            "Epoch 277: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3622 - accuracy: 0.2517 - val_loss: 1.4636 - val_accuracy: 0.3108\n",
            "Epoch 278/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3259 - accuracy: 0.2500\n",
            "Epoch 278: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3549 - accuracy: 0.2517 - val_loss: 1.4623 - val_accuracy: 0.3108\n",
            "Epoch 279/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3991 - accuracy: 0.2891\n",
            "Epoch 279: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3561 - accuracy: 0.2517 - val_loss: 1.4630 - val_accuracy: 0.3108\n",
            "Epoch 280/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3562 - accuracy: 0.2734\n",
            "Epoch 280: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3539 - accuracy: 0.2517 - val_loss: 1.4598 - val_accuracy: 0.3108\n",
            "Epoch 281/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3673 - accuracy: 0.2031\n",
            "Epoch 281: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3496 - accuracy: 0.2517 - val_loss: 1.4555 - val_accuracy: 0.3108\n",
            "Epoch 282/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3447 - accuracy: 0.2734\n",
            "Epoch 282: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.3453 - accuracy: 0.2517 - val_loss: 1.4570 - val_accuracy: 0.3108\n",
            "Epoch 283/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3980 - accuracy: 0.2734\n",
            "Epoch 283: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3452 - accuracy: 0.2517 - val_loss: 1.4598 - val_accuracy: 0.3108\n",
            "Epoch 284/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2674 - accuracy: 0.2500\n",
            "Epoch 284: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3438 - accuracy: 0.2517 - val_loss: 1.4600 - val_accuracy: 0.3108\n",
            "Epoch 285/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3847 - accuracy: 0.2422\n",
            "Epoch 285: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3397 - accuracy: 0.2517 - val_loss: 1.4569 - val_accuracy: 0.3108\n",
            "Epoch 286/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3078 - accuracy: 0.2344\n",
            "Epoch 286: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3368 - accuracy: 0.2517 - val_loss: 1.4578 - val_accuracy: 0.3108\n",
            "Epoch 287/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3038 - accuracy: 0.2500\n",
            "Epoch 287: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3333 - accuracy: 0.2517 - val_loss: 1.4587 - val_accuracy: 0.3108\n",
            "Epoch 288/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3321 - accuracy: 0.2578\n",
            "Epoch 288: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3313 - accuracy: 0.2517 - val_loss: 1.4544 - val_accuracy: 0.3108\n",
            "Epoch 289/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3465 - accuracy: 0.2812\n",
            "Epoch 289: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.3274 - accuracy: 0.2517 - val_loss: 1.4536 - val_accuracy: 0.3108\n",
            "Epoch 290/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3390 - accuracy: 0.2656\n",
            "Epoch 290: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.3248 - accuracy: 0.2517 - val_loss: 1.4528 - val_accuracy: 0.3108\n",
            "Epoch 291/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3456 - accuracy: 0.2656\n",
            "Epoch 291: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.3232 - accuracy: 0.2517 - val_loss: 1.4475 - val_accuracy: 0.3108\n",
            "Epoch 292/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3384 - accuracy: 0.2500\n",
            "Epoch 292: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3195 - accuracy: 0.2517 - val_loss: 1.4416 - val_accuracy: 0.3108\n",
            "Epoch 293/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3232 - accuracy: 0.2109\n",
            "Epoch 293: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3156 - accuracy: 0.2517 - val_loss: 1.4346 - val_accuracy: 0.3108\n",
            "Epoch 294/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3132 - accuracy: 0.3047\n",
            "Epoch 294: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.3151 - accuracy: 0.2517 - val_loss: 1.4276 - val_accuracy: 0.3108\n",
            "Epoch 295/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3212 - accuracy: 0.2344\n",
            "Epoch 295: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3102 - accuracy: 0.2517 - val_loss: 1.4225 - val_accuracy: 0.3108\n",
            "Epoch 296/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3257 - accuracy: 0.2188\n",
            "Epoch 296: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3113 - accuracy: 0.2517 - val_loss: 1.4232 - val_accuracy: 0.3108\n",
            "Epoch 297/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3200 - accuracy: 0.2734\n",
            "Epoch 297: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3060 - accuracy: 0.2517 - val_loss: 1.4300 - val_accuracy: 0.3108\n",
            "Epoch 298/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3278 - accuracy: 0.2656\n",
            "Epoch 298: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.3049 - accuracy: 0.2517 - val_loss: 1.4355 - val_accuracy: 0.3108\n",
            "Epoch 299/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3175 - accuracy: 0.2734\n",
            "Epoch 299: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3019 - accuracy: 0.2517 - val_loss: 1.4356 - val_accuracy: 0.3108\n",
            "Epoch 300/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3294 - accuracy: 0.2891\n",
            "Epoch 300: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.3011 - accuracy: 0.2517 - val_loss: 1.4297 - val_accuracy: 0.3108\n",
            "Epoch 301/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3432 - accuracy: 0.2812\n",
            "Epoch 301: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2979 - accuracy: 0.2517 - val_loss: 1.4238 - val_accuracy: 0.3108\n",
            "Epoch 302/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2388 - accuracy: 0.2500\n",
            "Epoch 302: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2942 - accuracy: 0.2517 - val_loss: 1.4213 - val_accuracy: 0.3108\n",
            "Epoch 303/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3058 - accuracy: 0.2812\n",
            "Epoch 303: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2935 - accuracy: 0.2517 - val_loss: 1.4202 - val_accuracy: 0.3108\n",
            "Epoch 304/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3516 - accuracy: 0.2656\n",
            "Epoch 304: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2892 - accuracy: 0.2517 - val_loss: 1.4194 - val_accuracy: 0.3108\n",
            "Epoch 305/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2827 - accuracy: 0.2812\n",
            "Epoch 305: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2876 - accuracy: 0.2517 - val_loss: 1.4183 - val_accuracy: 0.3108\n",
            "Epoch 306/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3219 - accuracy: 0.1953\n",
            "Epoch 306: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.2898 - accuracy: 0.2517 - val_loss: 1.4121 - val_accuracy: 0.3108\n",
            "Epoch 307/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3355 - accuracy: 0.2969\n",
            "Epoch 307: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2885 - accuracy: 0.2517 - val_loss: 1.4072 - val_accuracy: 0.3108\n",
            "Epoch 308/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2371 - accuracy: 0.2031\n",
            "Epoch 308: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.2829 - accuracy: 0.2517 - val_loss: 1.4053 - val_accuracy: 0.3108\n",
            "Epoch 309/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2386 - accuracy: 0.2500\n",
            "Epoch 309: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2825 - accuracy: 0.2517 - val_loss: 1.4053 - val_accuracy: 0.3108\n",
            "Epoch 310/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2625 - accuracy: 0.2344\n",
            "Epoch 310: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2787 - accuracy: 0.2517 - val_loss: 1.4069 - val_accuracy: 0.3108\n",
            "Epoch 311/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3014 - accuracy: 0.2734\n",
            "Epoch 311: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2794 - accuracy: 0.2517 - val_loss: 1.4078 - val_accuracy: 0.3108\n",
            "Epoch 312/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2557 - accuracy: 0.2969\n",
            "Epoch 312: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2731 - accuracy: 0.2517 - val_loss: 1.4047 - val_accuracy: 0.3108\n",
            "Epoch 313/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2241 - accuracy: 0.2578\n",
            "Epoch 313: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2744 - accuracy: 0.2517 - val_loss: 1.3972 - val_accuracy: 0.3108\n",
            "Epoch 314/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3257 - accuracy: 0.2656\n",
            "Epoch 314: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2680 - accuracy: 0.2517 - val_loss: 1.3923 - val_accuracy: 0.3108\n",
            "Epoch 315/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2281 - accuracy: 0.2656\n",
            "Epoch 315: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2686 - accuracy: 0.2517 - val_loss: 1.3901 - val_accuracy: 0.3108\n",
            "Epoch 316/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2797 - accuracy: 0.2656\n",
            "Epoch 316: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2700 - accuracy: 0.2517 - val_loss: 1.3871 - val_accuracy: 0.3108\n",
            "Epoch 317/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2711 - accuracy: 0.2031\n",
            "Epoch 317: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2651 - accuracy: 0.2517 - val_loss: 1.3852 - val_accuracy: 0.3108\n",
            "Epoch 318/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.3002 - accuracy: 0.2578\n",
            "Epoch 318: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2643 - accuracy: 0.2517 - val_loss: 1.3866 - val_accuracy: 0.3108\n",
            "Epoch 319/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2281 - accuracy: 0.2891\n",
            "Epoch 319: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2646 - accuracy: 0.2517 - val_loss: 1.3850 - val_accuracy: 0.3108\n",
            "Epoch 320/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2276 - accuracy: 0.2266\n",
            "Epoch 320: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2585 - accuracy: 0.2517 - val_loss: 1.3802 - val_accuracy: 0.3108\n",
            "Epoch 321/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2420 - accuracy: 0.3047\n",
            "Epoch 321: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2560 - accuracy: 0.2517 - val_loss: 1.3759 - val_accuracy: 0.3108\n",
            "Epoch 322/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2402 - accuracy: 0.2656\n",
            "Epoch 322: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2544 - accuracy: 0.2517 - val_loss: 1.3739 - val_accuracy: 0.3108\n",
            "Epoch 323/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2410 - accuracy: 0.2344\n",
            "Epoch 323: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2569 - accuracy: 0.2517 - val_loss: 1.3733 - val_accuracy: 0.3108\n",
            "Epoch 324/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2314 - accuracy: 0.2578\n",
            "Epoch 324: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2527 - accuracy: 0.2517 - val_loss: 1.3713 - val_accuracy: 0.3108\n",
            "Epoch 325/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2128 - accuracy: 0.2109\n",
            "Epoch 325: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2517 - accuracy: 0.2517 - val_loss: 1.3662 - val_accuracy: 0.3108\n",
            "Epoch 326/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2134 - accuracy: 0.2344\n",
            "Epoch 326: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2499 - accuracy: 0.2517 - val_loss: 1.3628 - val_accuracy: 0.3108\n",
            "Epoch 327/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2837 - accuracy: 0.3047\n",
            "Epoch 327: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2469 - accuracy: 0.2517 - val_loss: 1.3608 - val_accuracy: 0.3108\n",
            "Epoch 328/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1838 - accuracy: 0.2188\n",
            "Epoch 328: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2509 - accuracy: 0.2517 - val_loss: 1.3579 - val_accuracy: 0.3108\n",
            "Epoch 329/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2668 - accuracy: 0.2734\n",
            "Epoch 329: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2475 - accuracy: 0.2517 - val_loss: 1.3564 - val_accuracy: 0.3108\n",
            "Epoch 330/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1950 - accuracy: 0.2422\n",
            "Epoch 330: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2424 - accuracy: 0.2517 - val_loss: 1.3543 - val_accuracy: 0.3108\n",
            "Epoch 331/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2410 - accuracy: 0.2344\n",
            "Epoch 331: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2437 - accuracy: 0.2517 - val_loss: 1.3506 - val_accuracy: 0.3108\n",
            "Epoch 332/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2304 - accuracy: 0.2422\n",
            "Epoch 332: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2424 - accuracy: 0.2517 - val_loss: 1.3485 - val_accuracy: 0.3108\n",
            "Epoch 333/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2565 - accuracy: 0.2344\n",
            "Epoch 333: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2393 - accuracy: 0.2517 - val_loss: 1.3484 - val_accuracy: 0.3108\n",
            "Epoch 334/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2418 - accuracy: 0.1953\n",
            "Epoch 334: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2396 - accuracy: 0.2517 - val_loss: 1.3501 - val_accuracy: 0.3108\n",
            "Epoch 335/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1636 - accuracy: 0.2031\n",
            "Epoch 335: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2338 - accuracy: 0.2517 - val_loss: 1.3527 - val_accuracy: 0.3108\n",
            "Epoch 336/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1930 - accuracy: 0.2812\n",
            "Epoch 336: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2309 - accuracy: 0.2517 - val_loss: 1.3551 - val_accuracy: 0.3108\n",
            "Epoch 337/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2025 - accuracy: 0.2578\n",
            "Epoch 337: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2337 - accuracy: 0.2517 - val_loss: 1.3536 - val_accuracy: 0.3108\n",
            "Epoch 338/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2238 - accuracy: 0.2188\n",
            "Epoch 338: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2288 - accuracy: 0.2517 - val_loss: 1.3510 - val_accuracy: 0.3108\n",
            "Epoch 339/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2850 - accuracy: 0.2734\n",
            "Epoch 339: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2306 - accuracy: 0.2517 - val_loss: 1.3523 - val_accuracy: 0.3108\n",
            "Epoch 340/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1993 - accuracy: 0.2188\n",
            "Epoch 340: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2290 - accuracy: 0.2517 - val_loss: 1.3574 - val_accuracy: 0.3108\n",
            "Epoch 341/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2360 - accuracy: 0.2891\n",
            "Epoch 341: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2258 - accuracy: 0.2517 - val_loss: 1.3658 - val_accuracy: 0.3108\n",
            "Epoch 342/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2134 - accuracy: 0.2344\n",
            "Epoch 342: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2216 - accuracy: 0.2517 - val_loss: 1.3734 - val_accuracy: 0.3108\n",
            "Epoch 343/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1923 - accuracy: 0.2812\n",
            "Epoch 343: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2275 - accuracy: 0.2517 - val_loss: 1.3758 - val_accuracy: 0.3108\n",
            "Epoch 344/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2271 - accuracy: 0.2891\n",
            "Epoch 344: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2215 - accuracy: 0.2517 - val_loss: 1.3739 - val_accuracy: 0.3108\n",
            "Epoch 345/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1127 - accuracy: 0.1562\n",
            "Epoch 345: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2232 - accuracy: 0.2517 - val_loss: 1.3678 - val_accuracy: 0.3108\n",
            "Epoch 346/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2076 - accuracy: 0.2578\n",
            "Epoch 346: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2204 - accuracy: 0.2517 - val_loss: 1.3628 - val_accuracy: 0.3108\n",
            "Epoch 347/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2087 - accuracy: 0.2500\n",
            "Epoch 347: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2238 - accuracy: 0.2517 - val_loss: 1.3563 - val_accuracy: 0.3108\n",
            "Epoch 348/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1771 - accuracy: 0.2344\n",
            "Epoch 348: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2203 - accuracy: 0.2517 - val_loss: 1.3515 - val_accuracy: 0.3108\n",
            "Epoch 349/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2296 - accuracy: 0.2500\n",
            "Epoch 349: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2167 - accuracy: 0.2517 - val_loss: 1.3489 - val_accuracy: 0.3108\n",
            "Epoch 350/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2495 - accuracy: 0.2812\n",
            "Epoch 350: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2157 - accuracy: 0.2517 - val_loss: 1.3484 - val_accuracy: 0.3108\n",
            "Epoch 351/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2303 - accuracy: 0.2422\n",
            "Epoch 351: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2138 - accuracy: 0.2517 - val_loss: 1.3504 - val_accuracy: 0.3108\n",
            "Epoch 352/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2608 - accuracy: 0.2891\n",
            "Epoch 352: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2139 - accuracy: 0.2517 - val_loss: 1.3539 - val_accuracy: 0.3108\n",
            "Epoch 353/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1984 - accuracy: 0.2656\n",
            "Epoch 353: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2118 - accuracy: 0.2517 - val_loss: 1.3566 - val_accuracy: 0.3108\n",
            "Epoch 354/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2193 - accuracy: 0.2266\n",
            "Epoch 354: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2123 - accuracy: 0.2517 - val_loss: 1.3573 - val_accuracy: 0.3108\n",
            "Epoch 355/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1766 - accuracy: 0.2578\n",
            "Epoch 355: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2101 - accuracy: 0.2517 - val_loss: 1.3565 - val_accuracy: 0.3108\n",
            "Epoch 356/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1443 - accuracy: 0.2188\n",
            "Epoch 356: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2107 - accuracy: 0.2517 - val_loss: 1.3512 - val_accuracy: 0.3108\n",
            "Epoch 357/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2424 - accuracy: 0.2422\n",
            "Epoch 357: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2078 - accuracy: 0.2517 - val_loss: 1.3466 - val_accuracy: 0.3108\n",
            "Epoch 358/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2918 - accuracy: 0.2656\n",
            "Epoch 358: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2068 - accuracy: 0.2517 - val_loss: 1.3425 - val_accuracy: 0.3108\n",
            "Epoch 359/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2291 - accuracy: 0.2734\n",
            "Epoch 359: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2088 - accuracy: 0.2517 - val_loss: 1.3381 - val_accuracy: 0.3108\n",
            "Epoch 360/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2368 - accuracy: 0.3047\n",
            "Epoch 360: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2022 - accuracy: 0.2517 - val_loss: 1.3358 - val_accuracy: 0.3108\n",
            "Epoch 361/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2309 - accuracy: 0.2969\n",
            "Epoch 361: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2021 - accuracy: 0.2517 - val_loss: 1.3326 - val_accuracy: 0.3108\n",
            "Epoch 362/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2820 - accuracy: 0.2500\n",
            "Epoch 362: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2026 - accuracy: 0.2517 - val_loss: 1.3283 - val_accuracy: 0.3108\n",
            "Epoch 363/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2156 - accuracy: 0.2812\n",
            "Epoch 363: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2016 - accuracy: 0.2517 - val_loss: 1.3253 - val_accuracy: 0.3108\n",
            "Epoch 364/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2323 - accuracy: 0.2344\n",
            "Epoch 364: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.2002 - accuracy: 0.2517 - val_loss: 1.3229 - val_accuracy: 0.3108\n",
            "Epoch 365/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2058 - accuracy: 0.2500\n",
            "Epoch 365: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2002 - accuracy: 0.2517 - val_loss: 1.3203 - val_accuracy: 0.3108\n",
            "Epoch 366/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1983 - accuracy: 0.2500\n",
            "Epoch 366: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1977 - accuracy: 0.2517 - val_loss: 1.3170 - val_accuracy: 0.3108\n",
            "Epoch 367/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2001 - accuracy: 0.2656\n",
            "Epoch 367: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1977 - accuracy: 0.2517 - val_loss: 1.3134 - val_accuracy: 0.3108\n",
            "Epoch 368/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1476 - accuracy: 0.1875\n",
            "Epoch 368: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1969 - accuracy: 0.2517 - val_loss: 1.3137 - val_accuracy: 0.3108\n",
            "Epoch 369/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2201 - accuracy: 0.2734\n",
            "Epoch 369: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1982 - accuracy: 0.2517 - val_loss: 1.3161 - val_accuracy: 0.3108\n",
            "Epoch 370/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1504 - accuracy: 0.2891\n",
            "Epoch 370: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1927 - accuracy: 0.2517 - val_loss: 1.3176 - val_accuracy: 0.3108\n",
            "Epoch 371/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2004 - accuracy: 0.2188\n",
            "Epoch 371: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1956 - accuracy: 0.2517 - val_loss: 1.3184 - val_accuracy: 0.3108\n",
            "Epoch 372/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2125 - accuracy: 0.2812\n",
            "Epoch 372: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1916 - accuracy: 0.2517 - val_loss: 1.3194 - val_accuracy: 0.3108\n",
            "Epoch 373/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2104 - accuracy: 0.2344\n",
            "Epoch 373: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1917 - accuracy: 0.2517 - val_loss: 1.3165 - val_accuracy: 0.3108\n",
            "Epoch 374/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1979 - accuracy: 0.2344\n",
            "Epoch 374: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1893 - accuracy: 0.2517 - val_loss: 1.3149 - val_accuracy: 0.3108\n",
            "Epoch 375/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1743 - accuracy: 0.2266\n",
            "Epoch 375: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1899 - accuracy: 0.2517 - val_loss: 1.3159 - val_accuracy: 0.3108\n",
            "Epoch 376/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1757 - accuracy: 0.2422\n",
            "Epoch 376: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1878 - accuracy: 0.2517 - val_loss: 1.3164 - val_accuracy: 0.3108\n",
            "Epoch 377/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1652 - accuracy: 0.2344\n",
            "Epoch 377: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1887 - accuracy: 0.2517 - val_loss: 1.3136 - val_accuracy: 0.3108\n",
            "Epoch 378/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2333 - accuracy: 0.2578\n",
            "Epoch 378: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1881 - accuracy: 0.2517 - val_loss: 1.3099 - val_accuracy: 0.3108\n",
            "Epoch 379/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2518 - accuracy: 0.2734\n",
            "Epoch 379: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1874 - accuracy: 0.2517 - val_loss: 1.3074 - val_accuracy: 0.3108\n",
            "Epoch 380/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1617 - accuracy: 0.2188\n",
            "Epoch 380: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1860 - accuracy: 0.2517 - val_loss: 1.3058 - val_accuracy: 0.3108\n",
            "Epoch 381/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1572 - accuracy: 0.2500\n",
            "Epoch 381: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1859 - accuracy: 0.2517 - val_loss: 1.3045 - val_accuracy: 0.3108\n",
            "Epoch 382/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1569 - accuracy: 0.2422\n",
            "Epoch 382: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1857 - accuracy: 0.2517 - val_loss: 1.3038 - val_accuracy: 0.3108\n",
            "Epoch 383/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1583 - accuracy: 0.2188\n",
            "Epoch 383: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1829 - accuracy: 0.2517 - val_loss: 1.3035 - val_accuracy: 0.3108\n",
            "Epoch 384/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1922 - accuracy: 0.2344\n",
            "Epoch 384: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1835 - accuracy: 0.2517 - val_loss: 1.3053 - val_accuracy: 0.3108\n",
            "Epoch 385/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1889 - accuracy: 0.2812\n",
            "Epoch 385: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1830 - accuracy: 0.2517 - val_loss: 1.3082 - val_accuracy: 0.3108\n",
            "Epoch 386/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1259 - accuracy: 0.2578\n",
            "Epoch 386: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1833 - accuracy: 0.2517 - val_loss: 1.3098 - val_accuracy: 0.3108\n",
            "Epoch 387/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2116 - accuracy: 0.2500\n",
            "Epoch 387: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1804 - accuracy: 0.2517 - val_loss: 1.3103 - val_accuracy: 0.3108\n",
            "Epoch 388/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1498 - accuracy: 0.2422\n",
            "Epoch 388: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1797 - accuracy: 0.2517 - val_loss: 1.3096 - val_accuracy: 0.3108\n",
            "Epoch 389/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1891 - accuracy: 0.2344\n",
            "Epoch 389: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.1770 - accuracy: 0.2517 - val_loss: 1.3112 - val_accuracy: 0.3108\n",
            "Epoch 390/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1910 - accuracy: 0.2266\n",
            "Epoch 390: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1777 - accuracy: 0.2517 - val_loss: 1.3158 - val_accuracy: 0.3108\n",
            "Epoch 391/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2010 - accuracy: 0.2656\n",
            "Epoch 391: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1780 - accuracy: 0.2517 - val_loss: 1.3218 - val_accuracy: 0.3108\n",
            "Epoch 392/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1193 - accuracy: 0.2109\n",
            "Epoch 392: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1785 - accuracy: 0.2517 - val_loss: 1.3243 - val_accuracy: 0.3108\n",
            "Epoch 393/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1496 - accuracy: 0.2188\n",
            "Epoch 393: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1788 - accuracy: 0.2517 - val_loss: 1.3244 - val_accuracy: 0.3108\n",
            "Epoch 394/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1801 - accuracy: 0.2344\n",
            "Epoch 394: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1704 - accuracy: 0.2517 - val_loss: 1.3248 - val_accuracy: 0.3108\n",
            "Epoch 395/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2210 - accuracy: 0.2578\n",
            "Epoch 395: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.1749 - accuracy: 0.2517 - val_loss: 1.3222 - val_accuracy: 0.3108\n",
            "Epoch 396/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1947 - accuracy: 0.2031\n",
            "Epoch 396: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1758 - accuracy: 0.2517 - val_loss: 1.3171 - val_accuracy: 0.3108\n",
            "Epoch 397/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1617 - accuracy: 0.2422\n",
            "Epoch 397: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.1730 - accuracy: 0.2517 - val_loss: 1.3134 - val_accuracy: 0.3108\n",
            "Epoch 398/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1808 - accuracy: 0.2188\n",
            "Epoch 398: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 1.1712 - accuracy: 0.2517 - val_loss: 1.3101 - val_accuracy: 0.3108\n",
            "Epoch 399/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1352 - accuracy: 0.2734\n",
            "Epoch 399: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1730 - accuracy: 0.2517 - val_loss: 1.3080 - val_accuracy: 0.3108\n",
            "Epoch 400/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1107 - accuracy: 0.2578\n",
            "Epoch 400: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1700 - accuracy: 0.2517 - val_loss: 1.3075 - val_accuracy: 0.3108\n",
            "Epoch 401/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1790 - accuracy: 0.2266\n",
            "Epoch 401: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1709 - accuracy: 0.2517 - val_loss: 1.3095 - val_accuracy: 0.3108\n",
            "Epoch 402/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1526 - accuracy: 0.2812\n",
            "Epoch 402: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1704 - accuracy: 0.2517 - val_loss: 1.3122 - val_accuracy: 0.3108\n",
            "Epoch 403/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1723 - accuracy: 0.2656\n",
            "Epoch 403: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1672 - accuracy: 0.2517 - val_loss: 1.3109 - val_accuracy: 0.3108\n",
            "Epoch 404/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1635 - accuracy: 0.2578\n",
            "Epoch 404: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1662 - accuracy: 0.2517 - val_loss: 1.3068 - val_accuracy: 0.3108\n",
            "Epoch 405/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1803 - accuracy: 0.2578\n",
            "Epoch 405: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1680 - accuracy: 0.2517 - val_loss: 1.3014 - val_accuracy: 0.3108\n",
            "Epoch 406/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1759 - accuracy: 0.2734\n",
            "Epoch 406: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1678 - accuracy: 0.2517 - val_loss: 1.2948 - val_accuracy: 0.3108\n",
            "Epoch 407/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2016 - accuracy: 0.2578\n",
            "Epoch 407: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.1656 - accuracy: 0.2517 - val_loss: 1.2892 - val_accuracy: 0.3108\n",
            "Epoch 408/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2281 - accuracy: 0.2969\n",
            "Epoch 408: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1637 - accuracy: 0.2517 - val_loss: 1.2869 - val_accuracy: 0.3108\n",
            "Epoch 409/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1581 - accuracy: 0.2734\n",
            "Epoch 409: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1661 - accuracy: 0.2517 - val_loss: 1.2856 - val_accuracy: 0.3108\n",
            "Epoch 410/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1958 - accuracy: 0.2656\n",
            "Epoch 410: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1650 - accuracy: 0.2517 - val_loss: 1.2842 - val_accuracy: 0.3108\n",
            "Epoch 411/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1361 - accuracy: 0.2344\n",
            "Epoch 411: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1647 - accuracy: 0.2517 - val_loss: 1.2829 - val_accuracy: 0.3108\n",
            "Epoch 412/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2027 - accuracy: 0.2734\n",
            "Epoch 412: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1647 - accuracy: 0.2517 - val_loss: 1.2821 - val_accuracy: 0.3108\n",
            "Epoch 413/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1143 - accuracy: 0.2188\n",
            "Epoch 413: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1622 - accuracy: 0.2517 - val_loss: 1.2834 - val_accuracy: 0.3108\n",
            "Epoch 414/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2017 - accuracy: 0.2500\n",
            "Epoch 414: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1631 - accuracy: 0.2517 - val_loss: 1.2859 - val_accuracy: 0.3108\n",
            "Epoch 415/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1553 - accuracy: 0.2266\n",
            "Epoch 415: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1615 - accuracy: 0.2517 - val_loss: 1.2880 - val_accuracy: 0.3108\n",
            "Epoch 416/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1685 - accuracy: 0.2188\n",
            "Epoch 416: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1613 - accuracy: 0.2517 - val_loss: 1.2913 - val_accuracy: 0.3108\n",
            "Epoch 417/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1509 - accuracy: 0.2188\n",
            "Epoch 417: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1615 - accuracy: 0.2517 - val_loss: 1.2954 - val_accuracy: 0.3108\n",
            "Epoch 418/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1354 - accuracy: 0.2656\n",
            "Epoch 418: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1603 - accuracy: 0.2517 - val_loss: 1.2979 - val_accuracy: 0.3108\n",
            "Epoch 419/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1133 - accuracy: 0.2500\n",
            "Epoch 419: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1606 - accuracy: 0.2517 - val_loss: 1.2980 - val_accuracy: 0.3108\n",
            "Epoch 420/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1475 - accuracy: 0.2422\n",
            "Epoch 420: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1588 - accuracy: 0.2517 - val_loss: 1.2989 - val_accuracy: 0.3108\n",
            "Epoch 421/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1539 - accuracy: 0.2500\n",
            "Epoch 421: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1592 - accuracy: 0.2517 - val_loss: 1.2999 - val_accuracy: 0.3108\n",
            "Epoch 422/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1952 - accuracy: 0.2734\n",
            "Epoch 422: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1575 - accuracy: 0.2517 - val_loss: 1.2999 - val_accuracy: 0.3108\n",
            "Epoch 423/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2173 - accuracy: 0.2656\n",
            "Epoch 423: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1584 - accuracy: 0.2517 - val_loss: 1.2991 - val_accuracy: 0.3108\n",
            "Epoch 424/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1810 - accuracy: 0.2578\n",
            "Epoch 424: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1556 - accuracy: 0.2517 - val_loss: 1.2983 - val_accuracy: 0.3108\n",
            "Epoch 425/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1963 - accuracy: 0.2344\n",
            "Epoch 425: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1555 - accuracy: 0.2517 - val_loss: 1.2969 - val_accuracy: 0.3108\n",
            "Epoch 426/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1384 - accuracy: 0.2500\n",
            "Epoch 426: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1548 - accuracy: 0.2517 - val_loss: 1.2947 - val_accuracy: 0.3108\n",
            "Epoch 427/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1797 - accuracy: 0.2812\n",
            "Epoch 427: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1569 - accuracy: 0.2517 - val_loss: 1.2907 - val_accuracy: 0.3108\n",
            "Epoch 428/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1693 - accuracy: 0.2656\n",
            "Epoch 428: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1548 - accuracy: 0.2517 - val_loss: 1.2855 - val_accuracy: 0.3108\n",
            "Epoch 429/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1671 - accuracy: 0.2812\n",
            "Epoch 429: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1537 - accuracy: 0.2517 - val_loss: 1.2810 - val_accuracy: 0.3108\n",
            "Epoch 430/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1225 - accuracy: 0.2422\n",
            "Epoch 430: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1538 - accuracy: 0.2517 - val_loss: 1.2788 - val_accuracy: 0.3108\n",
            "Epoch 431/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2071 - accuracy: 0.3203\n",
            "Epoch 431: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1543 - accuracy: 0.2517 - val_loss: 1.2780 - val_accuracy: 0.3108\n",
            "Epoch 432/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1250 - accuracy: 0.2344\n",
            "Epoch 432: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1518 - accuracy: 0.2517 - val_loss: 1.2760 - val_accuracy: 0.3108\n",
            "Epoch 433/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1462 - accuracy: 0.2656\n",
            "Epoch 433: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1522 - accuracy: 0.2517 - val_loss: 1.2745 - val_accuracy: 0.3108\n",
            "Epoch 434/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1152 - accuracy: 0.2344\n",
            "Epoch 434: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1520 - accuracy: 0.2517 - val_loss: 1.2739 - val_accuracy: 0.3108\n",
            "Epoch 435/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2117 - accuracy: 0.2656\n",
            "Epoch 435: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1527 - accuracy: 0.2517 - val_loss: 1.2750 - val_accuracy: 0.3108\n",
            "Epoch 436/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1368 - accuracy: 0.2969\n",
            "Epoch 436: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1512 - accuracy: 0.2517 - val_loss: 1.2768 - val_accuracy: 0.3108\n",
            "Epoch 437/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1496 - accuracy: 0.2578\n",
            "Epoch 437: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1521 - accuracy: 0.2517 - val_loss: 1.2760 - val_accuracy: 0.3108\n",
            "Epoch 438/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1094 - accuracy: 0.2578\n",
            "Epoch 438: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1493 - accuracy: 0.2517 - val_loss: 1.2749 - val_accuracy: 0.3108\n",
            "Epoch 439/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1900 - accuracy: 0.2500\n",
            "Epoch 439: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1496 - accuracy: 0.2517 - val_loss: 1.2735 - val_accuracy: 0.3108\n",
            "Epoch 440/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1576 - accuracy: 0.2188\n",
            "Epoch 440: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1501 - accuracy: 0.2517 - val_loss: 1.2730 - val_accuracy: 0.3108\n",
            "Epoch 441/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1403 - accuracy: 0.2578\n",
            "Epoch 441: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1503 - accuracy: 0.2517 - val_loss: 1.2730 - val_accuracy: 0.3108\n",
            "Epoch 442/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1485 - accuracy: 0.3359\n",
            "Epoch 442: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1503 - accuracy: 0.2517 - val_loss: 1.2728 - val_accuracy: 0.3108\n",
            "Epoch 443/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1469 - accuracy: 0.2578\n",
            "Epoch 443: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1481 - accuracy: 0.2517 - val_loss: 1.2717 - val_accuracy: 0.3108\n",
            "Epoch 444/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1859 - accuracy: 0.2734\n",
            "Epoch 444: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1495 - accuracy: 0.2517 - val_loss: 1.2712 - val_accuracy: 0.3108\n",
            "Epoch 445/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1753 - accuracy: 0.2422\n",
            "Epoch 445: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1503 - accuracy: 0.2517 - val_loss: 1.2701 - val_accuracy: 0.3108\n",
            "Epoch 446/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1097 - accuracy: 0.2109\n",
            "Epoch 446: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1461 - accuracy: 0.2517 - val_loss: 1.2700 - val_accuracy: 0.3108\n",
            "Epoch 447/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1159 - accuracy: 0.2812\n",
            "Epoch 447: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1467 - accuracy: 0.2517 - val_loss: 1.2703 - val_accuracy: 0.3108\n",
            "Epoch 448/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1931 - accuracy: 0.2734\n",
            "Epoch 448: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1462 - accuracy: 0.2517 - val_loss: 1.2696 - val_accuracy: 0.3108\n",
            "Epoch 449/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1415 - accuracy: 0.2656\n",
            "Epoch 449: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1462 - accuracy: 0.2517 - val_loss: 1.2712 - val_accuracy: 0.3108\n",
            "Epoch 450/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1881 - accuracy: 0.3281\n",
            "Epoch 450: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1466 - accuracy: 0.2517 - val_loss: 1.2742 - val_accuracy: 0.3108\n",
            "Epoch 451/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1486 - accuracy: 0.2266\n",
            "Epoch 451: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1447 - accuracy: 0.2517 - val_loss: 1.2749 - val_accuracy: 0.3108\n",
            "Epoch 452/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0956 - accuracy: 0.2422\n",
            "Epoch 452: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1449 - accuracy: 0.2517 - val_loss: 1.2752 - val_accuracy: 0.3108\n",
            "Epoch 453/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1022 - accuracy: 0.2266\n",
            "Epoch 453: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1446 - accuracy: 0.2517 - val_loss: 1.2761 - val_accuracy: 0.3108\n",
            "Epoch 454/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0981 - accuracy: 0.2188\n",
            "Epoch 454: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1460 - accuracy: 0.2517 - val_loss: 1.2766 - val_accuracy: 0.3108\n",
            "Epoch 455/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1534 - accuracy: 0.2344\n",
            "Epoch 455: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1431 - accuracy: 0.2517 - val_loss: 1.2759 - val_accuracy: 0.3108\n",
            "Epoch 456/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1214 - accuracy: 0.2422\n",
            "Epoch 456: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1446 - accuracy: 0.2517 - val_loss: 1.2741 - val_accuracy: 0.3108\n",
            "Epoch 457/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1261 - accuracy: 0.2500\n",
            "Epoch 457: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1432 - accuracy: 0.2517 - val_loss: 1.2712 - val_accuracy: 0.3108\n",
            "Epoch 458/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1719 - accuracy: 0.2578\n",
            "Epoch 458: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1447 - accuracy: 0.2517 - val_loss: 1.2670 - val_accuracy: 0.3108\n",
            "Epoch 459/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1236 - accuracy: 0.2578\n",
            "Epoch 459: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1439 - accuracy: 0.2517 - val_loss: 1.2632 - val_accuracy: 0.3108\n",
            "Epoch 460/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0971 - accuracy: 0.2500\n",
            "Epoch 460: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1421 - accuracy: 0.2517 - val_loss: 1.2596 - val_accuracy: 0.3108\n",
            "Epoch 461/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2103 - accuracy: 0.2656\n",
            "Epoch 461: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1410 - accuracy: 0.2517 - val_loss: 1.2581 - val_accuracy: 0.3108\n",
            "Epoch 462/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1560 - accuracy: 0.2656\n",
            "Epoch 462: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1429 - accuracy: 0.2517 - val_loss: 1.2577 - val_accuracy: 0.3108\n",
            "Epoch 463/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1621 - accuracy: 0.2734\n",
            "Epoch 463: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1415 - accuracy: 0.2517 - val_loss: 1.2565 - val_accuracy: 0.3108\n",
            "Epoch 464/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1202 - accuracy: 0.3047\n",
            "Epoch 464: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1418 - accuracy: 0.2517 - val_loss: 1.2560 - val_accuracy: 0.3108\n",
            "Epoch 465/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1144 - accuracy: 0.2266\n",
            "Epoch 465: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1412 - accuracy: 0.2517 - val_loss: 1.2557 - val_accuracy: 0.3108\n",
            "Epoch 466/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1337 - accuracy: 0.2266\n",
            "Epoch 466: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1402 - accuracy: 0.2517 - val_loss: 1.2573 - val_accuracy: 0.3108\n",
            "Epoch 467/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1816 - accuracy: 0.2734\n",
            "Epoch 467: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1413 - accuracy: 0.2517 - val_loss: 1.2601 - val_accuracy: 0.3108\n",
            "Epoch 468/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1128 - accuracy: 0.2734\n",
            "Epoch 468: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1397 - accuracy: 0.2517 - val_loss: 1.2625 - val_accuracy: 0.3108\n",
            "Epoch 469/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0993 - accuracy: 0.2578\n",
            "Epoch 469: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1403 - accuracy: 0.2517 - val_loss: 1.2631 - val_accuracy: 0.3108\n",
            "Epoch 470/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1480 - accuracy: 0.2188\n",
            "Epoch 470: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1379 - accuracy: 0.2517 - val_loss: 1.2638 - val_accuracy: 0.3108\n",
            "Epoch 471/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1626 - accuracy: 0.2656\n",
            "Epoch 471: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1391 - accuracy: 0.2517 - val_loss: 1.2631 - val_accuracy: 0.3108\n",
            "Epoch 472/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1752 - accuracy: 0.2969\n",
            "Epoch 472: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1394 - accuracy: 0.2517 - val_loss: 1.2619 - val_accuracy: 0.3108\n",
            "Epoch 473/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1723 - accuracy: 0.2812\n",
            "Epoch 473: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1384 - accuracy: 0.2517 - val_loss: 1.2628 - val_accuracy: 0.3108\n",
            "Epoch 474/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1230 - accuracy: 0.2656\n",
            "Epoch 474: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1390 - accuracy: 0.2517 - val_loss: 1.2649 - val_accuracy: 0.3108\n",
            "Epoch 475/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1490 - accuracy: 0.2500\n",
            "Epoch 475: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1375 - accuracy: 0.2517 - val_loss: 1.2670 - val_accuracy: 0.3108\n",
            "Epoch 476/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1369 - accuracy: 0.2734\n",
            "Epoch 476: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1365 - accuracy: 0.2517 - val_loss: 1.2691 - val_accuracy: 0.3108\n",
            "Epoch 477/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1309 - accuracy: 0.2109\n",
            "Epoch 477: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1380 - accuracy: 0.2517 - val_loss: 1.2701 - val_accuracy: 0.3108\n",
            "Epoch 478/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1484 - accuracy: 0.2891\n",
            "Epoch 478: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1376 - accuracy: 0.2517 - val_loss: 1.2707 - val_accuracy: 0.3108\n",
            "Epoch 479/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1102 - accuracy: 0.2500\n",
            "Epoch 479: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1370 - accuracy: 0.2517 - val_loss: 1.2715 - val_accuracy: 0.3108\n",
            "Epoch 480/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1280 - accuracy: 0.2344\n",
            "Epoch 480: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1365 - accuracy: 0.2517 - val_loss: 1.2721 - val_accuracy: 0.3108\n",
            "Epoch 481/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1553 - accuracy: 0.2734\n",
            "Epoch 481: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1363 - accuracy: 0.2517 - val_loss: 1.2723 - val_accuracy: 0.3108\n",
            "Epoch 482/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1257 - accuracy: 0.2031\n",
            "Epoch 482: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1362 - accuracy: 0.2517 - val_loss: 1.2736 - val_accuracy: 0.3108\n",
            "Epoch 483/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1328 - accuracy: 0.2969\n",
            "Epoch 483: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1360 - accuracy: 0.2517 - val_loss: 1.2754 - val_accuracy: 0.3108\n",
            "Epoch 484/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1191 - accuracy: 0.2344\n",
            "Epoch 484: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1366 - accuracy: 0.2517 - val_loss: 1.2763 - val_accuracy: 0.3108\n",
            "Epoch 485/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1146 - accuracy: 0.1875\n",
            "Epoch 485: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1360 - accuracy: 0.2517 - val_loss: 1.2772 - val_accuracy: 0.3108\n",
            "Epoch 486/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1950 - accuracy: 0.2266\n",
            "Epoch 486: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1356 - accuracy: 0.2517 - val_loss: 1.2779 - val_accuracy: 0.3108\n",
            "Epoch 487/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0978 - accuracy: 0.2188\n",
            "Epoch 487: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1362 - accuracy: 0.2517 - val_loss: 1.2790 - val_accuracy: 0.3108\n",
            "Epoch 488/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1283 - accuracy: 0.2656\n",
            "Epoch 488: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1346 - accuracy: 0.2517 - val_loss: 1.2792 - val_accuracy: 0.3108\n",
            "Epoch 489/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1448 - accuracy: 0.2344\n",
            "Epoch 489: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1347 - accuracy: 0.2517 - val_loss: 1.2772 - val_accuracy: 0.3108\n",
            "Epoch 490/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2154 - accuracy: 0.2422\n",
            "Epoch 490: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1331 - accuracy: 0.2517 - val_loss: 1.2760 - val_accuracy: 0.3108\n",
            "Epoch 491/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1403 - accuracy: 0.2266\n",
            "Epoch 491: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1340 - accuracy: 0.2517 - val_loss: 1.2747 - val_accuracy: 0.3108\n",
            "Epoch 492/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0947 - accuracy: 0.2578\n",
            "Epoch 492: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1339 - accuracy: 0.2517 - val_loss: 1.2728 - val_accuracy: 0.3108\n",
            "Epoch 493/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1204 - accuracy: 0.2266\n",
            "Epoch 493: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1342 - accuracy: 0.2517 - val_loss: 1.2709 - val_accuracy: 0.3108\n",
            "Epoch 494/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1118 - accuracy: 0.2266\n",
            "Epoch 494: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1344 - accuracy: 0.2517 - val_loss: 1.2698 - val_accuracy: 0.3108\n",
            "Epoch 495/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1697 - accuracy: 0.2734\n",
            "Epoch 495: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1335 - accuracy: 0.2517 - val_loss: 1.2692 - val_accuracy: 0.3108\n",
            "Epoch 496/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1241 - accuracy: 0.2344\n",
            "Epoch 496: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1342 - accuracy: 0.2517 - val_loss: 1.2687 - val_accuracy: 0.3108\n",
            "Epoch 497/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1444 - accuracy: 0.2891\n",
            "Epoch 497: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1341 - accuracy: 0.2517 - val_loss: 1.2688 - val_accuracy: 0.3108\n",
            "Epoch 498/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1656 - accuracy: 0.2266\n",
            "Epoch 498: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1329 - accuracy: 0.2517 - val_loss: 1.2694 - val_accuracy: 0.3108\n",
            "Epoch 499/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0892 - accuracy: 0.2500\n",
            "Epoch 499: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1337 - accuracy: 0.2517 - val_loss: 1.2705 - val_accuracy: 0.3108\n",
            "Epoch 500/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1388 - accuracy: 0.2266\n",
            "Epoch 500: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1341 - accuracy: 0.2517 - val_loss: 1.2692 - val_accuracy: 0.3108\n",
            "Epoch 501/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1457 - accuracy: 0.2266\n",
            "Epoch 501: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1317 - accuracy: 0.2517 - val_loss: 1.2665 - val_accuracy: 0.3108\n",
            "Epoch 502/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1320 - accuracy: 0.2344\n",
            "Epoch 502: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1321 - accuracy: 0.2517 - val_loss: 1.2643 - val_accuracy: 0.3108\n",
            "Epoch 503/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1553 - accuracy: 0.2422\n",
            "Epoch 503: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1305 - accuracy: 0.2517 - val_loss: 1.2635 - val_accuracy: 0.3108\n",
            "Epoch 504/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1546 - accuracy: 0.2266\n",
            "Epoch 504: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1318 - accuracy: 0.2517 - val_loss: 1.2635 - val_accuracy: 0.3108\n",
            "Epoch 505/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0425 - accuracy: 0.1875\n",
            "Epoch 505: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1310 - accuracy: 0.2517 - val_loss: 1.2644 - val_accuracy: 0.3108\n",
            "Epoch 506/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1410 - accuracy: 0.2891\n",
            "Epoch 506: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1316 - accuracy: 0.2517 - val_loss: 1.2665 - val_accuracy: 0.3108\n",
            "Epoch 507/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1025 - accuracy: 0.2891\n",
            "Epoch 507: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1318 - accuracy: 0.2517 - val_loss: 1.2671 - val_accuracy: 0.3108\n",
            "Epoch 508/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0858 - accuracy: 0.2500\n",
            "Epoch 508: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1311 - accuracy: 0.2517 - val_loss: 1.2650 - val_accuracy: 0.3108\n",
            "Epoch 509/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1313 - accuracy: 0.2500\n",
            "Epoch 509: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1309 - accuracy: 0.2517 - val_loss: 1.2612 - val_accuracy: 0.3108\n",
            "Epoch 510/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0906 - accuracy: 0.2578\n",
            "Epoch 510: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1287 - accuracy: 0.2517 - val_loss: 1.2569 - val_accuracy: 0.3108\n",
            "Epoch 511/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1456 - accuracy: 0.2422\n",
            "Epoch 511: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1300 - accuracy: 0.2517 - val_loss: 1.2538 - val_accuracy: 0.3108\n",
            "Epoch 512/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1689 - accuracy: 0.2500\n",
            "Epoch 512: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1302 - accuracy: 0.2517 - val_loss: 1.2511 - val_accuracy: 0.3108\n",
            "Epoch 513/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1505 - accuracy: 0.2578\n",
            "Epoch 513: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1315 - accuracy: 0.2517 - val_loss: 1.2480 - val_accuracy: 0.3108\n",
            "Epoch 514/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0887 - accuracy: 0.2656\n",
            "Epoch 514: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1303 - accuracy: 0.2517 - val_loss: 1.2449 - val_accuracy: 0.3108\n",
            "Epoch 515/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1476 - accuracy: 0.2812\n",
            "Epoch 515: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1302 - accuracy: 0.2517 - val_loss: 1.2427 - val_accuracy: 0.3108\n",
            "Epoch 516/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1758 - accuracy: 0.2734\n",
            "Epoch 516: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1294 - accuracy: 0.2517 - val_loss: 1.2431 - val_accuracy: 0.3108\n",
            "Epoch 517/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1131 - accuracy: 0.2266\n",
            "Epoch 517: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1304 - accuracy: 0.2517 - val_loss: 1.2461 - val_accuracy: 0.3108\n",
            "Epoch 518/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1116 - accuracy: 0.2344\n",
            "Epoch 518: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1305 - accuracy: 0.2517 - val_loss: 1.2491 - val_accuracy: 0.3108\n",
            "Epoch 519/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1834 - accuracy: 0.2812\n",
            "Epoch 519: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1299 - accuracy: 0.2517 - val_loss: 1.2498 - val_accuracy: 0.3108\n",
            "Epoch 520/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1014 - accuracy: 0.2344\n",
            "Epoch 520: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1289 - accuracy: 0.2517 - val_loss: 1.2494 - val_accuracy: 0.3108\n",
            "Epoch 521/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1556 - accuracy: 0.2812\n",
            "Epoch 521: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1289 - accuracy: 0.2517 - val_loss: 1.2494 - val_accuracy: 0.3108\n",
            "Epoch 522/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0839 - accuracy: 0.2266\n",
            "Epoch 522: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1280 - accuracy: 0.2517 - val_loss: 1.2485 - val_accuracy: 0.3108\n",
            "Epoch 523/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1710 - accuracy: 0.2422\n",
            "Epoch 523: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1288 - accuracy: 0.2517 - val_loss: 1.2482 - val_accuracy: 0.3108\n",
            "Epoch 524/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1281 - accuracy: 0.2109\n",
            "Epoch 524: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1283 - accuracy: 0.2517 - val_loss: 1.2483 - val_accuracy: 0.3108\n",
            "Epoch 525/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1386 - accuracy: 0.2500\n",
            "Epoch 525: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1280 - accuracy: 0.2517 - val_loss: 1.2486 - val_accuracy: 0.3108\n",
            "Epoch 526/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1571 - accuracy: 0.2969\n",
            "Epoch 526: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1287 - accuracy: 0.2517 - val_loss: 1.2484 - val_accuracy: 0.3108\n",
            "Epoch 527/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1545 - accuracy: 0.2344\n",
            "Epoch 527: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1278 - accuracy: 0.2517 - val_loss: 1.2479 - val_accuracy: 0.3108\n",
            "Epoch 528/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0891 - accuracy: 0.2578\n",
            "Epoch 528: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1274 - accuracy: 0.2517 - val_loss: 1.2484 - val_accuracy: 0.3108\n",
            "Epoch 529/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0926 - accuracy: 0.2109\n",
            "Epoch 529: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1277 - accuracy: 0.2517 - val_loss: 1.2499 - val_accuracy: 0.3108\n",
            "Epoch 530/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1607 - accuracy: 0.2656\n",
            "Epoch 530: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1276 - accuracy: 0.2517 - val_loss: 1.2527 - val_accuracy: 0.3108\n",
            "Epoch 531/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1532 - accuracy: 0.2578\n",
            "Epoch 531: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1268 - accuracy: 0.2517 - val_loss: 1.2553 - val_accuracy: 0.3108\n",
            "Epoch 532/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1161 - accuracy: 0.2188\n",
            "Epoch 532: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1271 - accuracy: 0.2517 - val_loss: 1.2580 - val_accuracy: 0.3108\n",
            "Epoch 533/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1455 - accuracy: 0.2734\n",
            "Epoch 533: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1275 - accuracy: 0.2517 - val_loss: 1.2610 - val_accuracy: 0.3108\n",
            "Epoch 534/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1261 - accuracy: 0.2734\n",
            "Epoch 534: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1261 - accuracy: 0.2517 - val_loss: 1.2624 - val_accuracy: 0.3108\n",
            "Epoch 535/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0928 - accuracy: 0.2422\n",
            "Epoch 535: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1261 - accuracy: 0.2517 - val_loss: 1.2625 - val_accuracy: 0.3108\n",
            "Epoch 536/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1409 - accuracy: 0.2266\n",
            "Epoch 536: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1259 - accuracy: 0.2517 - val_loss: 1.2621 - val_accuracy: 0.3108\n",
            "Epoch 537/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1030 - accuracy: 0.2656\n",
            "Epoch 537: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1272 - accuracy: 0.2517 - val_loss: 1.2608 - val_accuracy: 0.3108\n",
            "Epoch 538/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1251 - accuracy: 0.2734\n",
            "Epoch 538: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1263 - accuracy: 0.2517 - val_loss: 1.2586 - val_accuracy: 0.3108\n",
            "Epoch 539/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1881 - accuracy: 0.2656\n",
            "Epoch 539: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1264 - accuracy: 0.2517 - val_loss: 1.2563 - val_accuracy: 0.3108\n",
            "Epoch 540/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1549 - accuracy: 0.2344\n",
            "Epoch 540: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1258 - accuracy: 0.2517 - val_loss: 1.2543 - val_accuracy: 0.3108\n",
            "Epoch 541/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0670 - accuracy: 0.2578\n",
            "Epoch 541: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1261 - accuracy: 0.2517 - val_loss: 1.2533 - val_accuracy: 0.3108\n",
            "Epoch 542/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0837 - accuracy: 0.2656\n",
            "Epoch 542: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1261 - accuracy: 0.2517 - val_loss: 1.2528 - val_accuracy: 0.3108\n",
            "Epoch 543/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2173 - accuracy: 0.2422\n",
            "Epoch 543: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1274 - accuracy: 0.2517 - val_loss: 1.2528 - val_accuracy: 0.3108\n",
            "Epoch 544/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1060 - accuracy: 0.2188\n",
            "Epoch 544: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1269 - accuracy: 0.2517 - val_loss: 1.2533 - val_accuracy: 0.3108\n",
            "Epoch 545/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0725 - accuracy: 0.2109\n",
            "Epoch 545: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1259 - accuracy: 0.2517 - val_loss: 1.2536 - val_accuracy: 0.3108\n",
            "Epoch 546/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1238 - accuracy: 0.2969\n",
            "Epoch 546: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1253 - accuracy: 0.2517 - val_loss: 1.2525 - val_accuracy: 0.3108\n",
            "Epoch 547/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1858 - accuracy: 0.2891\n",
            "Epoch 547: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1258 - accuracy: 0.2517 - val_loss: 1.2507 - val_accuracy: 0.3108\n",
            "Epoch 548/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0798 - accuracy: 0.2109\n",
            "Epoch 548: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1250 - accuracy: 0.2517 - val_loss: 1.2493 - val_accuracy: 0.3108\n",
            "Epoch 549/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1914 - accuracy: 0.2344\n",
            "Epoch 549: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1270 - accuracy: 0.2517 - val_loss: 1.2485 - val_accuracy: 0.3108\n",
            "Epoch 550/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1124 - accuracy: 0.2422\n",
            "Epoch 550: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1256 - accuracy: 0.2517 - val_loss: 1.2482 - val_accuracy: 0.3108\n",
            "Epoch 551/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1240 - accuracy: 0.2578\n",
            "Epoch 551: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1260 - accuracy: 0.2517 - val_loss: 1.2483 - val_accuracy: 0.3108\n",
            "Epoch 552/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1357 - accuracy: 0.2734\n",
            "Epoch 552: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1253 - accuracy: 0.2517 - val_loss: 1.2492 - val_accuracy: 0.3108\n",
            "Epoch 553/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0968 - accuracy: 0.2734\n",
            "Epoch 553: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1244 - accuracy: 0.2517 - val_loss: 1.2493 - val_accuracy: 0.3108\n",
            "Epoch 554/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1424 - accuracy: 0.2734\n",
            "Epoch 554: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1243 - accuracy: 0.2517 - val_loss: 1.2490 - val_accuracy: 0.3108\n",
            "Epoch 555/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1914 - accuracy: 0.2500\n",
            "Epoch 555: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1243 - accuracy: 0.2517 - val_loss: 1.2488 - val_accuracy: 0.3108\n",
            "Epoch 556/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1754 - accuracy: 0.2656\n",
            "Epoch 556: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1243 - accuracy: 0.2517 - val_loss: 1.2478 - val_accuracy: 0.3108\n",
            "Epoch 557/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1151 - accuracy: 0.2109\n",
            "Epoch 557: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1235 - accuracy: 0.2517 - val_loss: 1.2461 - val_accuracy: 0.3108\n",
            "Epoch 558/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1528 - accuracy: 0.2344\n",
            "Epoch 558: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1246 - accuracy: 0.2517 - val_loss: 1.2457 - val_accuracy: 0.3108\n",
            "Epoch 559/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1261 - accuracy: 0.2500\n",
            "Epoch 559: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1244 - accuracy: 0.2517 - val_loss: 1.2464 - val_accuracy: 0.3108\n",
            "Epoch 560/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1362 - accuracy: 0.2891\n",
            "Epoch 560: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1242 - accuracy: 0.2517 - val_loss: 1.2480 - val_accuracy: 0.3108\n",
            "Epoch 561/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1591 - accuracy: 0.2422\n",
            "Epoch 561: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1247 - accuracy: 0.2517 - val_loss: 1.2503 - val_accuracy: 0.3108\n",
            "Epoch 562/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1267 - accuracy: 0.2109\n",
            "Epoch 562: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1234 - accuracy: 0.2517 - val_loss: 1.2529 - val_accuracy: 0.3108\n",
            "Epoch 563/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0984 - accuracy: 0.2344\n",
            "Epoch 563: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1248 - accuracy: 0.2517 - val_loss: 1.2557 - val_accuracy: 0.3108\n",
            "Epoch 564/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1663 - accuracy: 0.2656\n",
            "Epoch 564: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1235 - accuracy: 0.2517 - val_loss: 1.2579 - val_accuracy: 0.3108\n",
            "Epoch 565/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1240 - accuracy: 0.2734\n",
            "Epoch 565: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1240 - accuracy: 0.2517 - val_loss: 1.2590 - val_accuracy: 0.3108\n",
            "Epoch 566/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1315 - accuracy: 0.2734\n",
            "Epoch 566: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1236 - accuracy: 0.2517 - val_loss: 1.2588 - val_accuracy: 0.3108\n",
            "Epoch 567/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1301 - accuracy: 0.2812\n",
            "Epoch 567: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 1.1236 - accuracy: 0.2517 - val_loss: 1.2568 - val_accuracy: 0.3108\n",
            "Epoch 568/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1494 - accuracy: 0.2031\n",
            "Epoch 568: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 1.1239 - accuracy: 0.2517 - val_loss: 1.2535 - val_accuracy: 0.3108\n",
            "Epoch 569/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1070 - accuracy: 0.2656\n",
            "Epoch 569: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1232 - accuracy: 0.2517 - val_loss: 1.2529 - val_accuracy: 0.3108\n",
            "Epoch 570/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1549 - accuracy: 0.2266\n",
            "Epoch 570: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1228 - accuracy: 0.2517 - val_loss: 1.2540 - val_accuracy: 0.3108\n",
            "Epoch 571/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1407 - accuracy: 0.2734\n",
            "Epoch 571: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1223 - accuracy: 0.2517 - val_loss: 1.2551 - val_accuracy: 0.3108\n",
            "Epoch 572/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1060 - accuracy: 0.2500\n",
            "Epoch 572: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1228 - accuracy: 0.2517 - val_loss: 1.2554 - val_accuracy: 0.3108\n",
            "Epoch 573/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1448 - accuracy: 0.2109\n",
            "Epoch 573: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1226 - accuracy: 0.2517 - val_loss: 1.2559 - val_accuracy: 0.3108\n",
            "Epoch 574/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1051 - accuracy: 0.2500\n",
            "Epoch 574: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1228 - accuracy: 0.2517 - val_loss: 1.2581 - val_accuracy: 0.3108\n",
            "Epoch 575/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1533 - accuracy: 0.2656\n",
            "Epoch 575: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1220 - accuracy: 0.2517 - val_loss: 1.2609 - val_accuracy: 0.3108\n",
            "Epoch 576/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1068 - accuracy: 0.2500\n",
            "Epoch 576: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1235 - accuracy: 0.2517 - val_loss: 1.2627 - val_accuracy: 0.3108\n",
            "Epoch 577/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1115 - accuracy: 0.2578\n",
            "Epoch 577: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1233 - accuracy: 0.2517 - val_loss: 1.2631 - val_accuracy: 0.3108\n",
            "Epoch 578/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1289 - accuracy: 0.2422\n",
            "Epoch 578: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1224 - accuracy: 0.2517 - val_loss: 1.2627 - val_accuracy: 0.3108\n",
            "Epoch 579/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0895 - accuracy: 0.2969\n",
            "Epoch 579: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1235 - accuracy: 0.2517 - val_loss: 1.2618 - val_accuracy: 0.3108\n",
            "Epoch 580/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1133 - accuracy: 0.2734\n",
            "Epoch 580: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.1224 - accuracy: 0.2517 - val_loss: 1.2592 - val_accuracy: 0.3108\n",
            "Epoch 581/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1950 - accuracy: 0.3125\n",
            "Epoch 581: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1227 - accuracy: 0.2517 - val_loss: 1.2559 - val_accuracy: 0.3108\n",
            "Epoch 582/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0581 - accuracy: 0.2891\n",
            "Epoch 582: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1230 - accuracy: 0.2517 - val_loss: 1.2518 - val_accuracy: 0.3108\n",
            "Epoch 583/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1267 - accuracy: 0.2656\n",
            "Epoch 583: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1227 - accuracy: 0.2517 - val_loss: 1.2479 - val_accuracy: 0.3108\n",
            "Epoch 584/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0618 - accuracy: 0.2031\n",
            "Epoch 584: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1230 - accuracy: 0.2517 - val_loss: 1.2442 - val_accuracy: 0.3108\n",
            "Epoch 585/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1114 - accuracy: 0.2188\n",
            "Epoch 585: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1225 - accuracy: 0.2517 - val_loss: 1.2430 - val_accuracy: 0.3108\n",
            "Epoch 586/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0658 - accuracy: 0.2109\n",
            "Epoch 586: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1213 - accuracy: 0.2517 - val_loss: 1.2442 - val_accuracy: 0.3108\n",
            "Epoch 587/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1233 - accuracy: 0.2734\n",
            "Epoch 587: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1220 - accuracy: 0.2517 - val_loss: 1.2465 - val_accuracy: 0.3108\n",
            "Epoch 588/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1391 - accuracy: 0.2812\n",
            "Epoch 588: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1226 - accuracy: 0.2517 - val_loss: 1.2502 - val_accuracy: 0.3108\n",
            "Epoch 589/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0655 - accuracy: 0.2188\n",
            "Epoch 589: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1216 - accuracy: 0.2517 - val_loss: 1.2530 - val_accuracy: 0.3108\n",
            "Epoch 590/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1293 - accuracy: 0.2031\n",
            "Epoch 590: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1229 - accuracy: 0.2517 - val_loss: 1.2551 - val_accuracy: 0.3108\n",
            "Epoch 591/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1197 - accuracy: 0.2344\n",
            "Epoch 591: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1204 - accuracy: 0.2517 - val_loss: 1.2584 - val_accuracy: 0.3108\n",
            "Epoch 592/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1091 - accuracy: 0.2578\n",
            "Epoch 592: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1216 - accuracy: 0.2517 - val_loss: 1.2600 - val_accuracy: 0.3108\n",
            "Epoch 593/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1054 - accuracy: 0.2500\n",
            "Epoch 593: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1220 - accuracy: 0.2517 - val_loss: 1.2590 - val_accuracy: 0.3108\n",
            "Epoch 594/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1242 - accuracy: 0.2344\n",
            "Epoch 594: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1227 - accuracy: 0.2517 - val_loss: 1.2570 - val_accuracy: 0.3108\n",
            "Epoch 595/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1018 - accuracy: 0.2422\n",
            "Epoch 595: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1227 - accuracy: 0.2517 - val_loss: 1.2550 - val_accuracy: 0.3108\n",
            "Epoch 596/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1223 - accuracy: 0.2344\n",
            "Epoch 596: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1218 - accuracy: 0.2517 - val_loss: 1.2533 - val_accuracy: 0.3108\n",
            "Epoch 597/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0794 - accuracy: 0.2656\n",
            "Epoch 597: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1219 - accuracy: 0.2517 - val_loss: 1.2509 - val_accuracy: 0.3108\n",
            "Epoch 598/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1451 - accuracy: 0.2422\n",
            "Epoch 598: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1223 - accuracy: 0.2517 - val_loss: 1.2473 - val_accuracy: 0.3108\n",
            "Epoch 599/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0481 - accuracy: 0.1953\n",
            "Epoch 599: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1208 - accuracy: 0.2517 - val_loss: 1.2437 - val_accuracy: 0.3108\n",
            "Epoch 600/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1778 - accuracy: 0.2500\n",
            "Epoch 600: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1214 - accuracy: 0.2517 - val_loss: 1.2414 - val_accuracy: 0.3108\n",
            "Epoch 601/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1732 - accuracy: 0.2812\n",
            "Epoch 601: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1205 - accuracy: 0.2517 - val_loss: 1.2393 - val_accuracy: 0.3108\n",
            "Epoch 602/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1524 - accuracy: 0.2656\n",
            "Epoch 602: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1207 - accuracy: 0.2517 - val_loss: 1.2381 - val_accuracy: 0.3108\n",
            "Epoch 603/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1954 - accuracy: 0.2500\n",
            "Epoch 603: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1208 - accuracy: 0.2517 - val_loss: 1.2383 - val_accuracy: 0.3108\n",
            "Epoch 604/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1279 - accuracy: 0.2109\n",
            "Epoch 604: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1202 - accuracy: 0.2517 - val_loss: 1.2385 - val_accuracy: 0.3108\n",
            "Epoch 605/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1646 - accuracy: 0.2812\n",
            "Epoch 605: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1214 - accuracy: 0.2517 - val_loss: 1.2386 - val_accuracy: 0.3108\n",
            "Epoch 606/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1322 - accuracy: 0.3125\n",
            "Epoch 606: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1212 - accuracy: 0.2517 - val_loss: 1.2393 - val_accuracy: 0.3108\n",
            "Epoch 607/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1203 - accuracy: 0.2578\n",
            "Epoch 607: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1207 - accuracy: 0.2517 - val_loss: 1.2394 - val_accuracy: 0.3108\n",
            "Epoch 608/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0780 - accuracy: 0.2188\n",
            "Epoch 608: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1193 - accuracy: 0.2517 - val_loss: 1.2402 - val_accuracy: 0.3108\n",
            "Epoch 609/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1314 - accuracy: 0.2734\n",
            "Epoch 609: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1205 - accuracy: 0.2517 - val_loss: 1.2415 - val_accuracy: 0.3108\n",
            "Epoch 610/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0575 - accuracy: 0.2109\n",
            "Epoch 610: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1207 - accuracy: 0.2517 - val_loss: 1.2426 - val_accuracy: 0.3108\n",
            "Epoch 611/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2032 - accuracy: 0.3438\n",
            "Epoch 611: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1219 - accuracy: 0.2517 - val_loss: 1.2438 - val_accuracy: 0.3108\n",
            "Epoch 612/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1228 - accuracy: 0.2500\n",
            "Epoch 612: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1204 - accuracy: 0.2517 - val_loss: 1.2426 - val_accuracy: 0.3108\n",
            "Epoch 613/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1304 - accuracy: 0.2109\n",
            "Epoch 613: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1195 - accuracy: 0.2517 - val_loss: 1.2413 - val_accuracy: 0.3108\n",
            "Epoch 614/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1502 - accuracy: 0.2734\n",
            "Epoch 614: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1211 - accuracy: 0.2517 - val_loss: 1.2416 - val_accuracy: 0.3108\n",
            "Epoch 615/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1375 - accuracy: 0.2578\n",
            "Epoch 615: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1201 - accuracy: 0.2517 - val_loss: 1.2430 - val_accuracy: 0.3108\n",
            "Epoch 616/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1306 - accuracy: 0.2734\n",
            "Epoch 616: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1202 - accuracy: 0.2517 - val_loss: 1.2446 - val_accuracy: 0.3108\n",
            "Epoch 617/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1208 - accuracy: 0.2734\n",
            "Epoch 617: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1204 - accuracy: 0.2517 - val_loss: 1.2463 - val_accuracy: 0.3108\n",
            "Epoch 618/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1606 - accuracy: 0.2500\n",
            "Epoch 618: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1204 - accuracy: 0.2517 - val_loss: 1.2477 - val_accuracy: 0.3108\n",
            "Epoch 619/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0770 - accuracy: 0.2266\n",
            "Epoch 619: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1197 - accuracy: 0.2517 - val_loss: 1.2477 - val_accuracy: 0.3108\n",
            "Epoch 620/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1724 - accuracy: 0.2812\n",
            "Epoch 620: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1204 - accuracy: 0.2517 - val_loss: 1.2478 - val_accuracy: 0.3108\n",
            "Epoch 621/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1267 - accuracy: 0.3359\n",
            "Epoch 621: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1203 - accuracy: 0.2517 - val_loss: 1.2487 - val_accuracy: 0.3108\n",
            "Epoch 622/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1413 - accuracy: 0.3047\n",
            "Epoch 622: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1196 - accuracy: 0.2517 - val_loss: 1.2502 - val_accuracy: 0.3108\n",
            "Epoch 623/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1221 - accuracy: 0.2109\n",
            "Epoch 623: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1203 - accuracy: 0.2517 - val_loss: 1.2505 - val_accuracy: 0.3108\n",
            "Epoch 624/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1238 - accuracy: 0.2500\n",
            "Epoch 624: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1202 - accuracy: 0.2517 - val_loss: 1.2514 - val_accuracy: 0.3108\n",
            "Epoch 625/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1515 - accuracy: 0.2656\n",
            "Epoch 625: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1193 - accuracy: 0.2517 - val_loss: 1.2529 - val_accuracy: 0.3108\n",
            "Epoch 626/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0999 - accuracy: 0.2734\n",
            "Epoch 626: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1190 - accuracy: 0.2517 - val_loss: 1.2529 - val_accuracy: 0.3108\n",
            "Epoch 627/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1246 - accuracy: 0.2266\n",
            "Epoch 627: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1195 - accuracy: 0.2517 - val_loss: 1.2514 - val_accuracy: 0.3108\n",
            "Epoch 628/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1782 - accuracy: 0.2734\n",
            "Epoch 628: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1206 - accuracy: 0.2517 - val_loss: 1.2499 - val_accuracy: 0.3108\n",
            "Epoch 629/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1034 - accuracy: 0.2500\n",
            "Epoch 629: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1206 - accuracy: 0.2517 - val_loss: 1.2477 - val_accuracy: 0.3108\n",
            "Epoch 630/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1590 - accuracy: 0.2344\n",
            "Epoch 630: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1200 - accuracy: 0.2517 - val_loss: 1.2469 - val_accuracy: 0.3108\n",
            "Epoch 631/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1501 - accuracy: 0.2344\n",
            "Epoch 631: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1205 - accuracy: 0.2517 - val_loss: 1.2478 - val_accuracy: 0.3108\n",
            "Epoch 632/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1119 - accuracy: 0.2344\n",
            "Epoch 632: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1199 - accuracy: 0.2517 - val_loss: 1.2486 - val_accuracy: 0.3108\n",
            "Epoch 633/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1209 - accuracy: 0.2500\n",
            "Epoch 633: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1187 - accuracy: 0.2517 - val_loss: 1.2491 - val_accuracy: 0.3108\n",
            "Epoch 634/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1279 - accuracy: 0.2734\n",
            "Epoch 634: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1183 - accuracy: 0.2517 - val_loss: 1.2494 - val_accuracy: 0.3108\n",
            "Epoch 635/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0628 - accuracy: 0.2188\n",
            "Epoch 635: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1198 - accuracy: 0.2517 - val_loss: 1.2500 - val_accuracy: 0.3108\n",
            "Epoch 636/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0975 - accuracy: 0.2734\n",
            "Epoch 636: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1186 - accuracy: 0.2517 - val_loss: 1.2508 - val_accuracy: 0.3108\n",
            "Epoch 637/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1480 - accuracy: 0.2734\n",
            "Epoch 637: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1195 - accuracy: 0.2517 - val_loss: 1.2498 - val_accuracy: 0.3108\n",
            "Epoch 638/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1151 - accuracy: 0.2656\n",
            "Epoch 638: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1197 - accuracy: 0.2517 - val_loss: 1.2480 - val_accuracy: 0.3108\n",
            "Epoch 639/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0820 - accuracy: 0.2734\n",
            "Epoch 639: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1195 - accuracy: 0.2517 - val_loss: 1.2467 - val_accuracy: 0.3108\n",
            "Epoch 640/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1660 - accuracy: 0.2578\n",
            "Epoch 640: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1202 - accuracy: 0.2517 - val_loss: 1.2451 - val_accuracy: 0.3108\n",
            "Epoch 641/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0742 - accuracy: 0.2266\n",
            "Epoch 641: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1191 - accuracy: 0.2517 - val_loss: 1.2442 - val_accuracy: 0.3108\n",
            "Epoch 642/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1267 - accuracy: 0.2578\n",
            "Epoch 642: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1177 - accuracy: 0.2517 - val_loss: 1.2451 - val_accuracy: 0.3108\n",
            "Epoch 643/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1640 - accuracy: 0.2734\n",
            "Epoch 643: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1177 - accuracy: 0.2517 - val_loss: 1.2474 - val_accuracy: 0.3108\n",
            "Epoch 644/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1281 - accuracy: 0.2734\n",
            "Epoch 644: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1193 - accuracy: 0.2517 - val_loss: 1.2509 - val_accuracy: 0.3108\n",
            "Epoch 645/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0784 - accuracy: 0.2422\n",
            "Epoch 645: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1190 - accuracy: 0.2517 - val_loss: 1.2521 - val_accuracy: 0.3108\n",
            "Epoch 646/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1633 - accuracy: 0.2969\n",
            "Epoch 646: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1201 - accuracy: 0.2517 - val_loss: 1.2522 - val_accuracy: 0.3108\n",
            "Epoch 647/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1388 - accuracy: 0.2578\n",
            "Epoch 647: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1199 - accuracy: 0.2517 - val_loss: 1.2513 - val_accuracy: 0.3108\n",
            "Epoch 648/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0904 - accuracy: 0.2578\n",
            "Epoch 648: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1193 - accuracy: 0.2517 - val_loss: 1.2490 - val_accuracy: 0.3108\n",
            "Epoch 649/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1316 - accuracy: 0.2500\n",
            "Epoch 649: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.1201 - accuracy: 0.2517 - val_loss: 1.2469 - val_accuracy: 0.3108\n",
            "Epoch 650/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1134 - accuracy: 0.2812\n",
            "Epoch 650: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1184 - accuracy: 0.2517 - val_loss: 1.2458 - val_accuracy: 0.3108\n",
            "Epoch 651/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1018 - accuracy: 0.2500\n",
            "Epoch 651: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1190 - accuracy: 0.2517 - val_loss: 1.2462 - val_accuracy: 0.3108\n",
            "Epoch 652/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1596 - accuracy: 0.3047\n",
            "Epoch 652: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1184 - accuracy: 0.2517 - val_loss: 1.2473 - val_accuracy: 0.3108\n",
            "Epoch 653/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1201 - accuracy: 0.2500\n",
            "Epoch 653: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1190 - accuracy: 0.2517 - val_loss: 1.2470 - val_accuracy: 0.3108\n",
            "Epoch 654/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0992 - accuracy: 0.2734\n",
            "Epoch 654: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1181 - accuracy: 0.2517 - val_loss: 1.2467 - val_accuracy: 0.3108\n",
            "Epoch 655/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0796 - accuracy: 0.1797\n",
            "Epoch 655: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1189 - accuracy: 0.2517 - val_loss: 1.2461 - val_accuracy: 0.3108\n",
            "Epoch 656/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1096 - accuracy: 0.2734\n",
            "Epoch 656: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1194 - accuracy: 0.2517 - val_loss: 1.2472 - val_accuracy: 0.3108\n",
            "Epoch 657/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1169 - accuracy: 0.2031\n",
            "Epoch 657: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1202 - accuracy: 0.2517 - val_loss: 1.2488 - val_accuracy: 0.3108\n",
            "Epoch 658/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1633 - accuracy: 0.2031\n",
            "Epoch 658: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1181 - accuracy: 0.2517 - val_loss: 1.2508 - val_accuracy: 0.3108\n",
            "Epoch 659/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1314 - accuracy: 0.2500\n",
            "Epoch 659: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1199 - accuracy: 0.2517 - val_loss: 1.2523 - val_accuracy: 0.3108\n",
            "Epoch 660/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1147 - accuracy: 0.2812\n",
            "Epoch 660: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1199 - accuracy: 0.2517 - val_loss: 1.2519 - val_accuracy: 0.3108\n",
            "Epoch 661/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0952 - accuracy: 0.2109\n",
            "Epoch 661: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1188 - accuracy: 0.2517 - val_loss: 1.2513 - val_accuracy: 0.3108\n",
            "Epoch 662/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1845 - accuracy: 0.2734\n",
            "Epoch 662: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1187 - accuracy: 0.2517 - val_loss: 1.2515 - val_accuracy: 0.3108\n",
            "Epoch 663/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1114 - accuracy: 0.2344\n",
            "Epoch 663: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1182 - accuracy: 0.2517 - val_loss: 1.2504 - val_accuracy: 0.3108\n",
            "Epoch 664/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1368 - accuracy: 0.2578\n",
            "Epoch 664: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1193 - accuracy: 0.2517 - val_loss: 1.2494 - val_accuracy: 0.3108\n",
            "Epoch 665/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1298 - accuracy: 0.2344\n",
            "Epoch 665: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1185 - accuracy: 0.2517 - val_loss: 1.2486 - val_accuracy: 0.3108\n",
            "Epoch 666/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1223 - accuracy: 0.2656\n",
            "Epoch 666: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1187 - accuracy: 0.2517 - val_loss: 1.2474 - val_accuracy: 0.3108\n",
            "Epoch 667/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0578 - accuracy: 0.2578\n",
            "Epoch 667: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1172 - accuracy: 0.2517 - val_loss: 1.2456 - val_accuracy: 0.3108\n",
            "Epoch 668/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1402 - accuracy: 0.2500\n",
            "Epoch 668: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1189 - accuracy: 0.2517 - val_loss: 1.2452 - val_accuracy: 0.3108\n",
            "Epoch 669/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1323 - accuracy: 0.2109\n",
            "Epoch 669: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1191 - accuracy: 0.2517 - val_loss: 1.2454 - val_accuracy: 0.3108\n",
            "Epoch 670/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1458 - accuracy: 0.2578\n",
            "Epoch 670: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1183 - accuracy: 0.2517 - val_loss: 1.2465 - val_accuracy: 0.3108\n",
            "Epoch 671/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1530 - accuracy: 0.2266\n",
            "Epoch 671: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1187 - accuracy: 0.2517 - val_loss: 1.2475 - val_accuracy: 0.3108\n",
            "Epoch 672/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0853 - accuracy: 0.2656\n",
            "Epoch 672: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1189 - accuracy: 0.2517 - val_loss: 1.2483 - val_accuracy: 0.3108\n",
            "Epoch 673/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1753 - accuracy: 0.2578\n",
            "Epoch 673: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1192 - accuracy: 0.2517 - val_loss: 1.2492 - val_accuracy: 0.3108\n",
            "Epoch 674/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0770 - accuracy: 0.2578\n",
            "Epoch 674: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1177 - accuracy: 0.2517 - val_loss: 1.2500 - val_accuracy: 0.3108\n",
            "Epoch 675/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1191 - accuracy: 0.2891\n",
            "Epoch 675: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1191 - accuracy: 0.2517 - val_loss: 1.2494 - val_accuracy: 0.3108\n",
            "Epoch 676/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1000 - accuracy: 0.2500\n",
            "Epoch 676: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1178 - accuracy: 0.2517 - val_loss: 1.2468 - val_accuracy: 0.3108\n",
            "Epoch 677/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1224 - accuracy: 0.2266\n",
            "Epoch 677: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1184 - accuracy: 0.2517 - val_loss: 1.2455 - val_accuracy: 0.3108\n",
            "Epoch 678/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0681 - accuracy: 0.1797\n",
            "Epoch 678: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1168 - accuracy: 0.2517 - val_loss: 1.2459 - val_accuracy: 0.3108\n",
            "Epoch 679/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0877 - accuracy: 0.2812\n",
            "Epoch 679: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1179 - accuracy: 0.2517 - val_loss: 1.2465 - val_accuracy: 0.3108\n",
            "Epoch 680/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1179 - accuracy: 0.2188\n",
            "Epoch 680: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1197 - accuracy: 0.2517 - val_loss: 1.2458 - val_accuracy: 0.3108\n",
            "Epoch 681/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0443 - accuracy: 0.2734\n",
            "Epoch 681: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1186 - accuracy: 0.2517 - val_loss: 1.2449 - val_accuracy: 0.3108\n",
            "Epoch 682/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1012 - accuracy: 0.2734\n",
            "Epoch 682: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1178 - accuracy: 0.2517 - val_loss: 1.2449 - val_accuracy: 0.3108\n",
            "Epoch 683/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1677 - accuracy: 0.3047\n",
            "Epoch 683: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1162 - accuracy: 0.2517 - val_loss: 1.2467 - val_accuracy: 0.3108\n",
            "Epoch 684/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1269 - accuracy: 0.2344\n",
            "Epoch 684: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1179 - accuracy: 0.2517 - val_loss: 1.2480 - val_accuracy: 0.3108\n",
            "Epoch 685/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0898 - accuracy: 0.2344\n",
            "Epoch 685: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1172 - accuracy: 0.2517 - val_loss: 1.2490 - val_accuracy: 0.3108\n",
            "Epoch 686/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0853 - accuracy: 0.2109\n",
            "Epoch 686: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1185 - accuracy: 0.2517 - val_loss: 1.2507 - val_accuracy: 0.3108\n",
            "Epoch 687/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1238 - accuracy: 0.2812\n",
            "Epoch 687: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1182 - accuracy: 0.2517 - val_loss: 1.2535 - val_accuracy: 0.3108\n",
            "Epoch 688/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1066 - accuracy: 0.3125\n",
            "Epoch 688: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1175 - accuracy: 0.2517 - val_loss: 1.2565 - val_accuracy: 0.3108\n",
            "Epoch 689/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0873 - accuracy: 0.2578\n",
            "Epoch 689: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1176 - accuracy: 0.2517 - val_loss: 1.2574 - val_accuracy: 0.3108\n",
            "Epoch 690/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0651 - accuracy: 0.2109\n",
            "Epoch 690: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.1182 - accuracy: 0.2517 - val_loss: 1.2566 - val_accuracy: 0.3108\n",
            "Epoch 691/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1336 - accuracy: 0.2109\n",
            "Epoch 691: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1187 - accuracy: 0.2517 - val_loss: 1.2564 - val_accuracy: 0.3108\n",
            "Epoch 692/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1633 - accuracy: 0.2422\n",
            "Epoch 692: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1195 - accuracy: 0.2517 - val_loss: 1.2566 - val_accuracy: 0.3108\n",
            "Epoch 693/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0995 - accuracy: 0.2031\n",
            "Epoch 693: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1186 - accuracy: 0.2517 - val_loss: 1.2560 - val_accuracy: 0.3108\n",
            "Epoch 694/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1314 - accuracy: 0.2266\n",
            "Epoch 694: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1174 - accuracy: 0.2517 - val_loss: 1.2565 - val_accuracy: 0.3108\n",
            "Epoch 695/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1458 - accuracy: 0.2031\n",
            "Epoch 695: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1175 - accuracy: 0.2517 - val_loss: 1.2584 - val_accuracy: 0.3108\n",
            "Epoch 696/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1229 - accuracy: 0.2812\n",
            "Epoch 696: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1175 - accuracy: 0.2517 - val_loss: 1.2602 - val_accuracy: 0.3108\n",
            "Epoch 697/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1059 - accuracy: 0.2734\n",
            "Epoch 697: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1168 - accuracy: 0.2517 - val_loss: 1.2615 - val_accuracy: 0.3108\n",
            "Epoch 698/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.2288 - accuracy: 0.3281\n",
            "Epoch 698: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1196 - accuracy: 0.2517 - val_loss: 1.2630 - val_accuracy: 0.3108\n",
            "Epoch 699/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.0476 - accuracy: 0.2578\n",
            "Epoch 699: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1182 - accuracy: 0.2517 - val_loss: 1.2630 - val_accuracy: 0.3108\n",
            "Epoch 700/700\n",
            "1/3 [=========>....................] - ETA: 0s - loss: 1.1091 - accuracy: 0.2500\n",
            "Epoch 700: val_accuracy did not improve from 0.48649\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1191 - accuracy: 0.2517 - val_loss: 1.2624 - val_accuracy: 0.3108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_CNN = CNN_model.predict(feature_valid)\n",
        "\n",
        "# convert the validation vector\n",
        "valid_y_CNN = y_valid_CNN.copy()\n",
        "for i in range(len(y_valid_CNN)):\n",
        "    j = np.where(y_valid_CNN[i] == np.amax(y_valid_CNN[i]))\n",
        "    valid_y_CNN[i] = [0, 0, 0]\n",
        "    valid_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_valid_y,valid_y_CNN))\n",
        "print(classification_report(label_valid_y,valid_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN))"
      ],
      "metadata": {
        "id": "y89umNf39Ufr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_CNN = CNN_model.predict(feature_test)\n",
        "# convert the test vector\n",
        "test_y_CNN = y_test_CNN.copy()\n",
        "for i in range(len(y_test_CNN)):\n",
        "    j = np.where(y_test_CNN[i] == np.amax(y_test_CNN[i]))\n",
        "    test_y_CNN[i] = [0, 0, 0]\n",
        "    test_y_CNN[i][j] = 1\n",
        "\n",
        "print(accuracy_score(label_test_y,test_y_CNN))\n",
        "print(classification_report(label_test_y,test_y_CNN))\n",
        "print(\"auc score: \",roc_auc_score(label_test_y,test_y_CNN))"
      ],
      "metadata": {
        "id": "i6gRsP5r9ggG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#预训练（直接预测）"
      ],
      "metadata": {
        "id": "EdytGrk9xsok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('cpu')  # 使用CPU\n",
        "\n",
        "# 加载预训练的DistilBERT模型和tokenizer\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=len(set(y_train)))\n",
        "model.to(device)\n",
        "\n",
        "# 准备数据集\n",
        "def prepare_data(texts, labels, tokenizer, max_length=512):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_length, padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt')\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "    labels = torch.tensor(labels)\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "encoded_train_y = encoder.fit_transform(y_train)\n",
        "label_train_y = to_categorical(encoded_train_y, num_classes=3)\n",
        "\n",
        "encoded_valid_y = encoder.transform(y_valid)\n",
        "label_valid_y = to_categorical(encoded_valid_y, num_classes=3)\n",
        "\n",
        "encoded_test_y = encoder.fit_transform(y_test)\n",
        "label_test_y = to_categorical(encoded_test_y, num_classes=3)\n",
        "\n",
        "\n",
        "train_input_ids, train_attention_masks, train_labels = prepare_data(X_train, encoded_train_y, tokenizer)\n",
        "val_input_ids, val_attention_masks, val_labels = prepare_data(X_valid, encoded_valid_y, tokenizer)\n",
        "test_input_ids, test_attention_masks, test_labels = prepare_data(X_test, encoded_test_y, tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fde2g3hxxNu",
        "outputId": "7cd2e5c4-4d3c-48eb-a5d2-4b0f79aa235b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建DataLoader\n",
        "train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "validation_data = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)\n",
        "\n",
        "test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "djeMTd87z_l-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from transformers import Adafactor\n",
        "# 训练模型\n",
        "optimizer = Adafactor(model.parameters(), lr=0.01, eps=(1e-30, 1e-3), clip_threshold=1.0, beta1=None, weight_decay=0.0, relative_step=False, scale_parameter=False, warmup_init=False)\n",
        "epochs = 20\n",
        "batch_size = 8\n",
        "accumulation_steps = 4  # 每4个步骤更新一次模型参数\n",
        "\n",
        "for _ in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {_+1} Training\")\n",
        "    for i, batch in enumerate(progress_bar):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "        loss = loss / accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(train_loss/(i+1))})\n",
        "\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    for batch in tqdm(validation_dataloader, desc=f\"Epoch {_+1} Validation\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            val_preds.extend(preds.cpu().tolist())\n",
        "            val_labels.extend(b_labels.cpu().tolist())\n",
        "\n",
        "    print(f\"Epoch {_+1}, Validation Accuracy: {accuracy_score(val_labels, val_preds)}, Validation F1 (micro): {f1_score(val_labels, val_preds, average='micro')}\")\n",
        "\n",
        "# 在测试集上预测\n",
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        test_preds.extend(preds.cpu().tolist())\n",
        "        test_labels.extend(b_labels.cpu().tolist())\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy_score(test_labels, test_preds)}, Test F1 (micro): {f1_score(test_labels, test_preds, average='micro')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpV-GCNS7MgZ",
        "outputId": "4ffea114-d875-4823-b8d3-4303dc4bb918"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training: 100%|██████████| 10/10 [00:13<00:00,  1.32s/it, training_loss=0.979]\n",
            "Epoch 1 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Accuracy: 0.5, Validation F1 (micro): 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Training: 100%|██████████| 10/10 [00:13<00:00,  1.36s/it, training_loss=0.986]\n",
            "Epoch 2 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Training: 100%|██████████| 10/10 [00:13<00:00,  1.38s/it, training_loss=1.515]\n",
            "Epoch 3 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Training: 100%|██████████| 10/10 [00:13<00:00,  1.34s/it, training_loss=1.201]\n",
            "Epoch 4 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Validation Accuracy: 0.3108108108108108, Validation F1 (micro): 0.3108108108108108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Training: 100%|██████████| 10/10 [00:13<00:00,  1.32s/it, training_loss=1.643]\n",
            "Epoch 5 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Training: 100%|██████████| 10/10 [00:13<00:00,  1.31s/it, training_loss=1.227]\n",
            "Epoch 6 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Training: 100%|██████████| 10/10 [00:13<00:00,  1.32s/it, training_loss=0.959]\n",
            "Epoch 7 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=0.981]\n",
            "Epoch 8 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=1.037]\n",
            "Epoch 9 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=0.988]\n",
            "Epoch 10 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=0.992]\n",
            "Epoch 11 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Training: 100%|██████████| 10/10 [00:13<00:00,  1.32s/it, training_loss=0.995]\n",
            "Epoch 12 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=0.980]\n",
            "Epoch 13 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=0.976]\n",
            "Epoch 14 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=1.007]\n",
            "Epoch 15 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=0.960]\n",
            "Epoch 16 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=0.987]\n",
            "Epoch 17 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=0.948]\n",
            "Epoch 18 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=0.982]\n",
            "Epoch 19 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Training: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it, training_loss=0.990]\n",
            "Epoch 20 Validation: 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Validation Accuracy: 0.4864864864864865, Validation F1 (micro): 0.4864864864864865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.46236559139784944, Test F1 (micro): 0.46236559139784944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Uns0nPB9UAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}